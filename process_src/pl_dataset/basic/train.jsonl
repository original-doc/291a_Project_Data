{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "adjust", "original_string": "def adjust(self, unfreeze: str) -> str:\r\n        \"\"\"Remove version restrictions unless they are strict.\r\n\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# anything\").adjust(\"none\")\r\n        'arrow<=1.2.2,>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# strict\").adjust(\"none\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# my name\").adjust(\"all\")\r\n        'arrow>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# strict\").adjust(\"all\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow\").adjust(\"all\")\r\n        'arrow'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# cool\").adjust(\"major\")\r\n        'arrow<2.0,>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# strict\").adjust(\"major\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0\").adjust(\"major\")\r\n        'arrow>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow\").adjust(\"major\")\r\n        'arrow'\r\n\r\n        \"\"\"\r\n        out = str(self)\r\n        if self.strict:\r\n            return f\"{out}  {self.strict_string}\"\r\n        specs = [(spec.operator, spec.version) for spec in self.specifier]\r\n        if unfreeze == \"major\":\r\n            for operator, version in specs:\r\n                if operator in (\"<\", \"<=\"):\r\n                    major = Version(version).major\r\n                    # replace upper bound with major version increased by one\r\n                    return out.replace(f\"{operator}{version}\", f\"<{major + 1}.0\")\r\n        elif unfreeze == \"all\":\r\n            for operator, version in specs:\r\n                if operator in (\"<\", \"<=\"):\r\n                    # drop upper bound\r\n                    return out.replace(f\"{operator}{version},\", \"\")\r\n        elif unfreeze != \"none\":\r\n            raise ValueError(f\"Unexpected unfreeze: {unfreeze!r} value.\")\r\n        return out", "language": "python", "code": "def adjust(self, unfreeze: str) -> str:\r\n        \"\"\"Remove version restrictions unless they are strict.\r\n\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# anything\").adjust(\"none\")\r\n        'arrow<=1.2.2,>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# strict\").adjust(\"none\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# my name\").adjust(\"all\")\r\n        'arrow>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# strict\").adjust(\"all\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow\").adjust(\"all\")\r\n        'arrow'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# cool\").adjust(\"major\")\r\n        'arrow<2.0,>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# strict\").adjust(\"major\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0\").adjust(\"major\")\r\n        'arrow>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow\").adjust(\"major\")\r\n        'arrow'\r\n\r\n        \"\"\"\r\n        out = str(self)\r\n        if self.strict:\r\n            return f\"{out}  {self.strict_string}\"\r\n        specs = [(spec.operator, spec.version) for spec in self.specifier]\r\n        if unfreeze == \"major\":\r\n            for operator, version in specs:\r\n                if operator in (\"<\", \"<=\"):\r\n                    major = Version(version).major\r\n                    # replace upper bound with major version increased by one\r\n                    return out.replace(f\"{operator}{version}\", f\"<{major + 1}.0\")\r\n        elif unfreeze == \"all\":\r\n            for operator, version in specs:\r\n                if operator in (\"<\", \"<=\"):\r\n                    # drop upper bound\r\n                    return out.replace(f\"{operator}{version},\", \"\")\r\n        elif unfreeze != \"none\":\r\n            raise ValueError(f\"Unexpected unfreeze: {unfreeze!r} value.\")\r\n        return out", "code_tokens": ["def", "adjust", "(", "self", ",", "unfreeze", ":", "str", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Remove", "version", "restrictions", "unless", "they", "are", "strict", ".", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", "\"", ",", "comment", "=", "\"", "'", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", "'", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", "\"", ",", "comment", "=", "\"", "'", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", "\"", ",", "comment", "=", "\"", "'", "arrow", ">", "=", "1", ".", "2", ".", "0", "'", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", ">", "=", "1", ".", "2", ".", "0", ",", "<", "=", "1", ".", "2", ".", "2", "\"", ",", "comment", "=", "\"", "'", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", "\"", ")", ".", "adjust", "(", "\"", "all", "\"", ")", "'", "arrow", "'", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", ">", "=", "1", ".", "2", ".", "0", ",", "<", "=", "1", ".", "2", ".", "2", "\"", ",", "comment", "=", "\"", "'", "arrow", "<", "2", ".", "0", ",", ">", "=", "1", ".", "2", ".", "0", "'", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", ">", "=", "1", ".", "2", ".", "0", ",", "<", "=", "1", ".", "2", ".", "2", "\"", ",", "comment", "=", "\"", "'", "arrow", "<", "=", "1", ".", "2", ".", "2", ",", ">", "=", "1", ".", "2", ".", "0", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", ">", "=", "1", ".", "2", ".", "0", "\"", ")", ".", "adjust", "(", "\"", "major", "\"", ")", "'", "arrow", ">", "=", "1", ".", "2", ".", "0", "'", ">", ">", ">", "_RequirementWithComment", "(", "\"", "arrow", "\"", ")", ".", "adjust", "(", "\"", "major", "\"", ")", "'", "arrow", "'", "\"", "\"", "\"", "out", "=", "str", "(", "self", ")", "if", "self", ".", "strict", ":", "return", "f", "\"", "{", "out", "}", "{", "self", ".", "strict_string", "}", "\"", "specs", "=", "[", "(", "spec", ".", "operator", ",", "spec", ".", "version", ")", "for", "spec", "in", "self", ".", "specifier", "]", "if", "unfreeze", "=", "=", "\"", "major", "\"", ":", "for", "operator", ",", "version", "in", "specs", ":", "if", "operator", "in", "(", "\"", "<", "\"", ",", "\"", "<", "=", "\"", ")", ":", "major", "=", "Version", "(", "version", ")", ".", "major", "return", "out", ".", "replace", "(", "f", "\"", "{", "operator", "}", "{", "version", "}", "\"", ",", "f", "\"", "<", "{", "major", "+", "1", "}", ".", "0", "\"", ")", "elif", "unfreeze", "=", "=", "\"", "all", "\"", ":", "for", "operator", ",", "version", "in", "specs", ":", "if", "operator", "in", "(", "\"", "<", "\"", ",", "\"", "<", "=", "\"", ")", ":", "return", "out", ".", "replace", "(", "f", "\"", "{", "operator", "}", "{", "version", "}", ",", "\"", ",", "\"", "\"", ")", "elif", "unfreeze", "!", "=", "\"", "none", "\"", ":", "raise", "ValueError", "(", "f", "\"", "Unexpected", "unfreeze", ":", "{", "unfreeze", "!", "r", "}", "value", ".", "\"", ")", "return", "out"], "docstring": "Remove version restrictions unless they are strict.\r\n\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# anything\").adjust(\"none\")\r\n        'arrow<=1.2.2,>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# strict\").adjust(\"none\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow<=1.2.2,>=1.2.0\", comment=\"# my name\").adjust(\"all\")\r\n        'arrow>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# strict\").adjust(\"all\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow\").adjust(\"all\")\r\n        'arrow'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# cool\").adjust(\"major\")\r\n        'arrow<2.0,>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0, <=1.2.2\", comment=\"# strict\").adjust(\"major\")\r\n        'arrow<=1.2.2,>=1.2.0  # strict'\r\n        >>> _RequirementWithComment(\"arrow>=1.2.0\").adjust(\"major\")\r\n        'arrow>=1.2.0'\r\n        >>> _RequirementWithComment(\"arrow\").adjust(\"major\")\r\n        'arrow'", "docstring_tokens": ["remove", "version", "restrictions", "unless", "they", "are", "strict", "_requirementwithcomment", "arrow", "1", "2", "2", "1", "2", "0", "comment", "anything", "adjust", "none", "arrow", "1", "2", "2", "1", "2", "0", "_requirementwithcomment", "arrow", "1", "2", "2", "1", "2", "0", "comment", "strict", "adjust", "none", "arrow", "1", "2", "2", "1", "2", "0", "strict", "_requirementwithcomment", "arrow", "1", "2", "2", "1", "2", "0", "comment", "my", "name", "adjust", "all", "arrow", "1", "2", "0", "_requirementwithcomment", "arrow", "1", "2", "0", "1", "2", "2", "comment", "strict", "adjust", "all", "arrow", "1", "2", "2", "1", "2", "0", "strict", "_requirementwithcomment", "arrow", "adjust", "all", "arrow", "_requirementwithcomment", "arrow", "1", "2", "0", "1", "2", "2", "comment", "cool", "adjust", "major", "arrow", "2", "0", "1", "2", "0", "_requirementwithcomment", "arrow", "1", "2", "0", "1", "2", "2", "comment", "strict", "adjust", "major", "arrow", "1", "2", "2", "1", "2", "0", "strict", "_requirementwithcomment", "arrow", "1", "2", "0", "adjust", "major", "arrow", "1", "2", "0", "_requirementwithcomment", "arrow", "adjust", "major", "arrow"], "docstring_summary": "Remove version restrictions unless they are strict.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "_RequirementWithComment", "start_line": 57, "end_line": 97, "hash": "699a50588e299a64f91fc02b316eee69", "complexity": 10, "parameters": ["unfreeze"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "_parse_requirements", "original_string": "def _parse_requirements(lines: Iterable[str]) -> Iterator[_RequirementWithComment]:\r\n    \"\"\"Adapted from `pkg_resources.parse_requirements` to include comments.\r\n\r\n    >>> txt = ['# ignored', '', 'this # is an', '--piparg', 'example', 'foo # strict', 'thing', '-r different/file.txt']\r\n    >>> [r.adjust('none') for r in _parse_requirements(txt)]\r\n    ['this', 'example', 'foo  # strict', 'thing']\r\n\r\n    \"\"\"\r\n    pip_argument = None\r\n    for line in lines:\r\n        line = line.strip()\r\n        if not line or line.startswith(\"#\"):\r\n            continue\r\n        # Drop comments -- a hash without a space may be in a URL.\r\n        if \" #\" in line:\r\n            comment_pos = line.find(\" #\")\r\n            line, comment = line[:comment_pos], line[comment_pos:]\r\n        else:\r\n            comment = \"\"\r\n        # If there's a pip argument, save it\r\n        if line.startswith(\"--\"):\r\n            pip_argument = line\r\n            continue\r\n        if line.startswith(\"-r \"):\r\n            # linked requirement files are unsupported\r\n            continue\r\n        yield _RequirementWithComment(line, comment=comment, pip_argument=pip_argument)\r\n        pip_argument = None", "language": "python", "code": "def _parse_requirements(lines: Iterable[str]) -> Iterator[_RequirementWithComment]:\r\n    \"\"\"Adapted from `pkg_resources.parse_requirements` to include comments.\r\n\r\n    >>> txt = ['# ignored', '', 'this # is an', '--piparg', 'example', 'foo # strict', 'thing', '-r different/file.txt']\r\n    >>> [r.adjust('none') for r in _parse_requirements(txt)]\r\n    ['this', 'example', 'foo  # strict', 'thing']\r\n\r\n    \"\"\"\r\n    pip_argument = None\r\n    for line in lines:\r\n        line = line.strip()\r\n        if not line or line.startswith(\"#\"):\r\n            continue\r\n        # Drop comments -- a hash without a space may be in a URL.\r\n        if \" #\" in line:\r\n            comment_pos = line.find(\" #\")\r\n            line, comment = line[:comment_pos], line[comment_pos:]\r\n        else:\r\n            comment = \"\"\r\n        # If there's a pip argument, save it\r\n        if line.startswith(\"--\"):\r\n            pip_argument = line\r\n            continue\r\n        if line.startswith(\"-r \"):\r\n            # linked requirement files are unsupported\r\n            continue\r\n        yield _RequirementWithComment(line, comment=comment, pip_argument=pip_argument)\r\n        pip_argument = None", "code_tokens": ["def", "_parse_requirements", "(", "lines", ":", "Iterable", "[", "str", "]", ")", "-", ">", "Iterator", "[", "_RequirementWithComment", "]", ":", "\"", "\"", "\"", "Adapted", "from", "`", "pkg_resources", ".", "parse_requirements", "`", "to", "include", "comments", ".", ">", ">", ">", "txt", "=", "[", "'", ">", ">", ">", "[", "r", ".", "adjust", "(", "'", "none", "'", ")", "for", "r", "in", "_parse_requirements", "(", "txt", ")", "]", "[", "'", "this", "'", ",", "'", "example", "'", ",", "'", "foo", "\"", "\"", "\"", "pip_argument", "=", "None", "for", "line", "in", "lines", ":", "line", "=", "line", ".", "strip", "(", ")", "if", "not", "line", "or", "line", ".", "startswith", "(", "\"", "continue", "if", "\"", "comment_pos", "=", "line", ".", "find", "(", "\"", "line", ",", "comment", "=", "line", "[", ":", "comment_pos", "]", ",", "line", "[", "comment_pos", ":", "]", "else", ":", "comment", "=", "\"", "\"", "if", "line", ".", "startswith", "(", "\"", "-", "-", "\"", ")", ":", "pip_argument", "=", "line", "continue", "if", "line", ".", "startswith", "(", "\"", "-", "r", "\"", ")", ":", "continue", "yield", "_RequirementWithComment", "(", "line", ",", "comment", "=", "comment", ",", "pip_argument", "=", "pip_argument", ")", "pip_argument", "=", "None"], "docstring": "Adapted from `pkg_resources.parse_requirements` to include comments.\r\n\r\n    >>> txt = ['# ignored', '', 'this # is an', '--piparg', 'example', 'foo # strict', 'thing', '-r different/file.txt']\r\n    >>> [r.adjust('none') for r in _parse_requirements(txt)]\r\n    ['this', 'example', 'foo  # strict', 'thing']", "docstring_tokens": ["adapted", "from", "pkg_resources", "parse_requirements", "to", "include", "comments", "txt", "ignored", "this", "is", "an", "piparg", "example", "foo", "strict", "thing", "r", "different", "file", "txt", "r", "adjust", "none", "for", "r", "in", "_parse_requirements", "txt", "this", "example", "foo", "strict", "thing"], "docstring_summary": "Adapted from `pkg_resources.parse_requirements` to include comments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 100, "end_line": 127, "hash": "20dafd3ef11f197c667748180dff5668", "complexity": 7, "parameters": ["lines"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "load_requirements", "original_string": "def load_requirements(path_dir: str, file_name: str = \"base.txt\", unfreeze: str = \"all\") -> list[str]:\r\n    \"\"\"Loading requirements from a file.\r\n\r\n    >>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\r\n    >>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    ['sphinx<...]\r\n\r\n    \"\"\"\r\n    assert unfreeze in {\"none\", \"major\", \"all\"}\r\n    path = Path(path_dir) / file_name\r\n    if not path.exists():\r\n        logging.warning(f\"Folder {path_dir} does not have any base requirements.\")\r\n        return []\r\n    assert path.exists(), (path_dir, file_name, path)\r\n    text = path.read_text().splitlines()\r\n    return [req.adjust(unfreeze) for req in _parse_requirements(text)]", "language": "python", "code": "def load_requirements(path_dir: str, file_name: str = \"base.txt\", unfreeze: str = \"all\") -> list[str]:\r\n    \"\"\"Loading requirements from a file.\r\n\r\n    >>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\r\n    >>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    ['sphinx<...]\r\n\r\n    \"\"\"\r\n    assert unfreeze in {\"none\", \"major\", \"all\"}\r\n    path = Path(path_dir) / file_name\r\n    if not path.exists():\r\n        logging.warning(f\"Folder {path_dir} does not have any base requirements.\")\r\n        return []\r\n    assert path.exists(), (path_dir, file_name, path)\r\n    text = path.read_text().splitlines()\r\n    return [req.adjust(unfreeze) for req in _parse_requirements(text)]", "code_tokens": ["def", "load_requirements", "(", "path_dir", ":", "str", ",", "file_name", ":", "str", "=", "\"", "base", ".", "txt", "\"", ",", "unfreeze", ":", "str", "=", "\"", "all", "\"", ")", "-", ">", "list", "[", "str", "]", ":", "\"", "\"", "\"", "Loading", "requirements", "from", "a", "file", ".", ">", ">", ">", "path_req", "=", "os", ".", "path", ".", "join", "(", "_PROJECT_ROOT", ",", "\"", "requirements", "\"", ")", ">", ">", ">", "load_requirements", "(", "path_req", ",", "\"", "docs", ".", "txt", "\"", ",", "unfreeze", "=", "\"", "major", "\"", ")", "[", "'", "sphinx", "<", ".", ".", ".", "]", "\"", "\"", "\"", "assert", "unfreeze", "in", "{", "\"", "none", "\"", ",", "\"", "major", "\"", ",", "\"", "all", "\"", "}", "path", "=", "Path", "(", "path_dir", ")", "/", "file_name", "if", "not", "path", ".", "exists", "(", ")", ":", "logging", ".", "warning", "(", "f", "\"", "Folder", "{", "path_dir", "}", "does", "not", "have", "any", "base", "requirements", ".", "\"", ")", "return", "[", "]", "assert", "path", ".", "exists", "(", ")", ",", "(", "path_dir", ",", "file_name", ",", "path", ")", "text", "=", "path", ".", "read_text", "(", ")", ".", "splitlines", "(", ")", "return", "[", "req", ".", "adjust", "(", "unfreeze", ")", "for", "req", "in", "_parse_requirements", "(", "text", ")", "]"], "docstring": "Loading requirements from a file.\r\n\r\n    >>> path_req = os.path.join(_PROJECT_ROOT, \"requirements\")\r\n    >>> load_requirements(path_req, \"docs.txt\", unfreeze=\"major\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    ['sphinx<...]", "docstring_tokens": ["loading", "requirements", "from", "a", "file", "path_req", "os", "path", "join", "_project_root", "requirements", "load_requirements", "path_req", "docs", "txt", "unfreeze", "major", "doctest", "ellipsis", "normalize_whitespace", "sphinx"], "docstring_summary": "Loading requirements from a file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 130, "end_line": 145, "hash": "51ab7f172b3a3f29cf4466860a7ca346", "complexity": 3, "parameters": ["path_dir", "file_name", "unfreeze"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "load_readme_description", "original_string": "def load_readme_description(path_dir: str, homepage: str, version: str) -> str:\r\n    \"\"\"Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'\r\n\r\n    \"\"\"\r\n    path_readme = os.path.join(path_dir, \"README.md\")\r\n    with open(path_readme, encoding=\"utf-8\") as fopen:\r\n        text = fopen.read()\r\n\r\n    # drop images from readme\r\n    text = text.replace(\r\n        \"![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\", \"\"\r\n    )\r\n\r\n    # https://github.com/Lightning-AI/lightning/raw/master/docs/source/_static/images/lightning_module/pt_to_pl.png\r\n    github_source_url = os.path.join(homepage, \"raw\", version)\r\n    # replace relative repository path to absolute link to the release\r\n    #  do not replace all \"docs\" as in the readme we reger some other sources with particular path to docs\r\n    text = text.replace(\r\n        \"docs/source-pytorch/_static/\", f\"{os.path.join(github_source_url, 'docs/source-app/_static/')}\"\r\n    )\r\n\r\n    # readthedocs badge\r\n    text = text.replace(\"badge/?version=stable\", f\"badge/?version={version}\")\r\n    text = text.replace(\"pytorch-lightning.readthedocs.io/en/stable/\", f\"pytorch-lightning.readthedocs.io/en/{version}\")\r\n    # codecov badge\r\n    text = text.replace(\"/branch/master/graph/badge.svg\", f\"/release/{version}/graph/badge.svg\")\r\n    # github actions badge\r\n    text = text.replace(\"badge.svg?branch=master&event=push\", f\"badge.svg?tag={version}\")\r\n    # azure pipelines badge\r\n    text = text.replace(\"?branchName=master\", f\"?branchName=refs%2Ftags%2F{version}\")\r\n\r\n    skip_begin = r\"<!-- following section will be skipped from PyPI description -->\"\r\n    skip_end = r\"<!-- end skipping PyPI description -->\"\r\n    # todo: wrap content as commented description\r\n    return re.sub(rf\"{skip_begin}.+?{skip_end}\", \"<!--  -->\", text, flags=re.IGNORECASE + re.DOTALL)\r\n\r\n    # # https://github.com/Borda/pytorch-lightning/releases/download/1.1.0a6/codecov_badge.png\r\n    # github_release_url = os.path.join(homepage, \"releases\", \"download\", version)\r\n    # # download badge and replace url with local file\r\n    # text = _parse_for_badge(text, github_release_url)\r", "language": "python", "code": "def load_readme_description(path_dir: str, homepage: str, version: str) -> str:\r\n    \"\"\"Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'\r\n\r\n    \"\"\"\r\n    path_readme = os.path.join(path_dir, \"README.md\")\r\n    with open(path_readme, encoding=\"utf-8\") as fopen:\r\n        text = fopen.read()\r\n\r\n    # drop images from readme\r\n    text = text.replace(\r\n        \"![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\", \"\"\r\n    )\r\n\r\n    # https://github.com/Lightning-AI/lightning/raw/master/docs/source/_static/images/lightning_module/pt_to_pl.png\r\n    github_source_url = os.path.join(homepage, \"raw\", version)\r\n    # replace relative repository path to absolute link to the release\r\n    #  do not replace all \"docs\" as in the readme we reger some other sources with particular path to docs\r\n    text = text.replace(\r\n        \"docs/source-pytorch/_static/\", f\"{os.path.join(github_source_url, 'docs/source-app/_static/')}\"\r\n    )\r\n\r\n    # readthedocs badge\r\n    text = text.replace(\"badge/?version=stable\", f\"badge/?version={version}\")\r\n    text = text.replace(\"pytorch-lightning.readthedocs.io/en/stable/\", f\"pytorch-lightning.readthedocs.io/en/{version}\")\r\n    # codecov badge\r\n    text = text.replace(\"/branch/master/graph/badge.svg\", f\"/release/{version}/graph/badge.svg\")\r\n    # github actions badge\r\n    text = text.replace(\"badge.svg?branch=master&event=push\", f\"badge.svg?tag={version}\")\r\n    # azure pipelines badge\r\n    text = text.replace(\"?branchName=master\", f\"?branchName=refs%2Ftags%2F{version}\")\r\n\r\n    skip_begin = r\"<!-- following section will be skipped from PyPI description -->\"\r\n    skip_end = r\"<!-- end skipping PyPI description -->\"\r\n    # todo: wrap content as commented description\r\n    return re.sub(rf\"{skip_begin}.+?{skip_end}\", \"<!--  -->\", text, flags=re.IGNORECASE + re.DOTALL)\r\n\r\n    # # https://github.com/Borda/pytorch-lightning/releases/download/1.1.0a6/codecov_badge.png\r\n    # github_release_url = os.path.join(homepage, \"releases\", \"download\", version)\r\n    # # download badge and replace url with local file\r\n    # text = _parse_for_badge(text, github_release_url)\r", "code_tokens": ["def", "load_readme_description", "(", "path_dir", ":", "str", ",", "homepage", ":", "str", ",", "version", ":", "str", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Load", "readme", "as", "decribtion", ".", ">", ">", ">", "load_readme_description", "(", "_PROJECT_ROOT", ",", "\"", "\"", ",", "\"", "\"", ")", "'", ".", ".", ".", "PyTorch", "Lightning", "is", "just", "organized", "PyTorch", ".", ".", ".", "'", "\"", "\"", "\"", "path_readme", "=", "os", ".", "path", ".", "join", "(", "path_dir", ",", "\"", "README", ".", "md", "\"", ")", "with", "open", "(", "path_readme", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "text", "=", "fopen", ".", "read", "(", ")", "text", "=", "text", ".", "replace", "(", "\"", "!", "[", "PT", "to", "PL", "]", "(", "docs", "/", "source", "-", "pytorch", "/", "_static", "/", "images", "/", "general", "/", "pl_quick_start_full_compressed", ".", "gif", ")", "\"", ",", "\"", "\"", ")", "github_source_url", "=", "os", ".", "path", ".", "join", "(", "homepage", ",", "\"", "raw", "\"", ",", "version", ")", "text", "=", "text", ".", "replace", "(", "\"", "docs", "/", "source", "-", "pytorch", "/", "_static", "/", "\"", ",", "f", "\"", "{", "os", ".", "path", ".", "join", "(", "github_source_url", ",", "'", "docs", "/", "source", "-", "app", "/", "_static", "/", "'", ")", "}", "\"", ")", "text", "=", "text", ".", "replace", "(", "\"", "badge", "/", "?", "version", "=", "stable", "\"", ",", "f", "\"", "badge", "/", "?", "version", "=", "{", "version", "}", "\"", ")", "text", "=", "text", ".", "replace", "(", "\"", "pytorch", "-", "lightning", ".", "readthedocs", ".", "io", "/", "en", "/", "stable", "/", "\"", ",", "f", "\"", "pytorch", "-", "lightning", ".", "readthedocs", ".", "io", "/", "en", "/", "{", "version", "}", "\"", ")", "text", "=", "text", ".", "replace", "(", "\"", "/", "branch", "/", "master", "/", "graph", "/", "badge", ".", "svg", "\"", ",", "f", "\"", "/", "release", "/", "{", "version", "}", "/", "graph", "/", "badge", ".", "svg", "\"", ")", "text", "=", "text", ".", "replace", "(", "\"", "badge", ".", "svg", "?", "branch", "=", "master", "&", "event", "=", "push", "\"", ",", "f", "\"", "badge", ".", "svg", "?", "tag", "=", "{", "version", "}", "\"", ")", "text", "=", "text", ".", "replace", "(", "\"", "?", "branchName", "=", "master", "\"", ",", "f", "\"", "?", "branchName", "=", "refs", "%", "2Ftags", "%", "2F", "{", "version", "}", "\"", ")", "skip_begin", "=", "r", "\"", "<", "!", "-", "-", "following", "section", "will", "be", "skipped", "from", "PyPI", "description", "-", "-", ">", "\"", "skip_end", "=", "r", "\"", "<", "!", "-", "-", "end", "skipping", "PyPI", "description", "-", "-", ">", "\"", "return", "re", ".", "sub", "(", "rf", "\"", "{", "skip_begin", "}", ".", "+", "?", "{", "skip_end", "}", "\"", ",", "\"", "<", "!", "-", "-", "-", "-", ">", "\"", ",", "text", ",", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "DOTALL", ")"], "docstring": "Load readme as decribtion.\r\n\r\n    >>> load_readme_description(_PROJECT_ROOT, \"\", \"\")  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    '...PyTorch Lightning is just organized PyTorch...'", "docstring_tokens": ["load", "readme", "as", "decribtion", "load_readme_description", "_project_root", "doctest", "ellipsis", "normalize_whitespace", "pytorch", "lightning", "is", "just", "organized", "pytorch"], "docstring_summary": "Load readme as decribtion.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 148, "end_line": 190, "hash": "0e616e657902720df027768a224df289", "complexity": 2, "parameters": ["path_dir", "homepage", "version"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "distribute_version", "original_string": "def distribute_version(src_folder: str, ver_file: str = \"version.info\") -> None:\r\n    \"\"\"Copy the global version to all packages.\"\"\"\r\n    ls_ver = glob.glob(os.path.join(src_folder, \"*\", \"__version__.py\"))\r\n    ver_template = os.path.join(src_folder, ver_file)\r\n    for fpath in ls_ver:\r\n        fpath = os.path.join(os.path.dirname(fpath), ver_file)\r\n        print(\"Distributing the version to\", fpath)\r\n        if os.path.isfile(fpath):\r\n            os.remove(fpath)\r\n        shutil.copy2(ver_template, fpath)", "language": "python", "code": "def distribute_version(src_folder: str, ver_file: str = \"version.info\") -> None:\r\n    \"\"\"Copy the global version to all packages.\"\"\"\r\n    ls_ver = glob.glob(os.path.join(src_folder, \"*\", \"__version__.py\"))\r\n    ver_template = os.path.join(src_folder, ver_file)\r\n    for fpath in ls_ver:\r\n        fpath = os.path.join(os.path.dirname(fpath), ver_file)\r\n        print(\"Distributing the version to\", fpath)\r\n        if os.path.isfile(fpath):\r\n            os.remove(fpath)\r\n        shutil.copy2(ver_template, fpath)", "code_tokens": ["def", "distribute_version", "(", "src_folder", ":", "str", ",", "ver_file", ":", "str", "=", "\"", "version", ".", "info", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Copy", "the", "global", "version", "to", "all", "packages", ".", "\"", "\"", "\"", "ls_ver", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "src_folder", ",", "\"", "*", "\"", ",", "\"", "__version__", ".", "py", "\"", ")", ")", "ver_template", "=", "os", ".", "path", ".", "join", "(", "src_folder", ",", "ver_file", ")", "for", "fpath", "in", "ls_ver", ":", "fpath", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "fpath", ")", ",", "ver_file", ")", "print", "(", "\"", "Distributing", "the", "version", "to", "\"", ",", "fpath", ")", "if", "os", ".", "path", ".", "isfile", "(", "fpath", ")", ":", "os", ".", "remove", "(", "fpath", ")", "shutil", ".", "copy2", "(", "ver_template", ",", "fpath", ")"], "docstring": "Copy the global version to all packages.", "docstring_tokens": ["copy", "the", "global", "version", "to", "all", "packages"], "docstring_summary": "Copy the global version to all packages.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 193, "end_line": 202, "hash": "05c1e4adf32aa458f6d8812fca6a65cc", "complexity": 3, "parameters": ["src_folder", "ver_file"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "_load_aggregate_requirements", "original_string": "def _load_aggregate_requirements(req_dir: str = \"requirements\", freeze_requirements: bool = False) -> None:\r\n    \"\"\"Load all base requirements from all particular packages and prune duplicates.\r\n\r\n    >>> _load_aggregate_requirements(os.path.join(_PROJECT_ROOT, \"requirements\"))\r\n\r\n    \"\"\"\r\n    requires = [\r\n        load_requirements(d, unfreeze=\"none\" if freeze_requirements else \"major\")\r\n        for d in glob.glob(os.path.join(req_dir, \"*\"))\r\n        # skip empty folder (git artifacts), and resolving Will's special issue\r\n        if os.path.isdir(d) and len(glob.glob(os.path.join(d, \"*\"))) > 0 and not os.path.basename(d).startswith(\"_\")\r\n    ]\r\n    if not requires:\r\n        return\r\n    # TODO: add some smarter version aggregation per each package\r\n    requires = sorted(set(chain(*requires)))\r\n    with open(os.path.join(req_dir, \"base.txt\"), \"w\") as fp:\r\n        fp.writelines([ln + os.linesep for ln in requires] + [os.linesep])", "language": "python", "code": "def _load_aggregate_requirements(req_dir: str = \"requirements\", freeze_requirements: bool = False) -> None:\r\n    \"\"\"Load all base requirements from all particular packages and prune duplicates.\r\n\r\n    >>> _load_aggregate_requirements(os.path.join(_PROJECT_ROOT, \"requirements\"))\r\n\r\n    \"\"\"\r\n    requires = [\r\n        load_requirements(d, unfreeze=\"none\" if freeze_requirements else \"major\")\r\n        for d in glob.glob(os.path.join(req_dir, \"*\"))\r\n        # skip empty folder (git artifacts), and resolving Will's special issue\r\n        if os.path.isdir(d) and len(glob.glob(os.path.join(d, \"*\"))) > 0 and not os.path.basename(d).startswith(\"_\")\r\n    ]\r\n    if not requires:\r\n        return\r\n    # TODO: add some smarter version aggregation per each package\r\n    requires = sorted(set(chain(*requires)))\r\n    with open(os.path.join(req_dir, \"base.txt\"), \"w\") as fp:\r\n        fp.writelines([ln + os.linesep for ln in requires] + [os.linesep])", "code_tokens": ["def", "_load_aggregate_requirements", "(", "req_dir", ":", "str", "=", "\"", "requirements", "\"", ",", "freeze_requirements", ":", "bool", "=", "False", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Load", "all", "base", "requirements", "from", "all", "particular", "packages", "and", "prune", "duplicates", ".", ">", ">", ">", "_load_aggregate_requirements", "(", "os", ".", "path", ".", "join", "(", "_PROJECT_ROOT", ",", "\"", "requirements", "\"", ")", ")", "\"", "\"", "\"", "requires", "=", "[", "load_requirements", "(", "d", ",", "unfreeze", "=", "\"", "none", "\"", "if", "freeze_requirements", "else", "\"", "major", "\"", ")", "for", "d", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "req_dir", ",", "\"", "*", "\"", ")", ")", "if", "os", ".", "path", ".", "isdir", "(", "d", ")", "and", "len", "(", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "d", ",", "\"", "*", "\"", ")", ")", ")", ">", "0", "and", "not", "os", ".", "path", ".", "basename", "(", "d", ")", ".", "startswith", "(", "\"", "_", "\"", ")", "]", "if", "not", "requires", ":", "return", "requires", "=", "sorted", "(", "set", "(", "chain", "(", "*", "requires", ")", ")", ")", "with", "open", "(", "os", ".", "path", ".", "join", "(", "req_dir", ",", "\"", "base", ".", "txt", "\"", ")", ",", "\"", "w", "\"", ")", "as", "fp", ":", "fp", ".", "writelines", "(", "[", "ln", "+", "os", ".", "linesep", "for", "ln", "in", "requires", "]", "+", "[", "os", ".", "linesep", "]", ")"], "docstring": "Load all base requirements from all particular packages and prune duplicates.\r\n\r\n    >>> _load_aggregate_requirements(os.path.join(_PROJECT_ROOT, \"requirements\"))", "docstring_tokens": ["load", "all", "base", "requirements", "from", "all", "particular", "packages", "and", "prune", "duplicates", "_load_aggregate_requirements", "os", "path", "join", "_project_root", "requirements"], "docstring_summary": "Load all base requirements from all particular packages and prune duplicates.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 205, "end_line": 222, "hash": "3928e84e8999b9f0ba00d2794d0e27e1", "complexity": 9, "parameters": ["req_dir", "freeze_requirements"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "_replace_imports", "original_string": "def _replace_imports(lines: list[str], mapping: list[tuple[str, str]], lightning_by: str = \"\") -> list[str]:\r\n    \"\"\"Replace imports of standalone package to lightning.\r\n\r\n    >>> lns = [\r\n    ...     '\"lightning_app\"',\r\n    ...     \"lightning_app\",\r\n    ...     \"lightning_app/\",\r\n    ...     \"delete_cloud_lightning_apps\",\r\n    ...     \"from lightning_app import\",\r\n    ...     \"lightning_apps = []\",\r\n    ...     \"lightning_app and pytorch_lightning are ours\",\r\n    ...     \"def _lightning_app():\",\r\n    ...     \":class:`~lightning_app.core.flow.LightningFlow`\",\r\n    ...     \"http://pytorch_lightning.ai\",\r\n    ...     \"from lightning import __version__\",\r\n    ...     \"@lightning.ai\"\r\n    ... ]\r\n    >>> mapping = [(\"lightning_app\", \"lightning.app\"), (\"pytorch_lightning\", \"lightning.pytorch\")]\r\n    >>> _replace_imports(lns, mapping, lightning_by=\"lightning_fabric\")  # doctest: +NORMALIZE_WHITESPACE\r\n    ['\"lightning.app\"', \\\r\n     'lightning.app', \\\r\n     'lightning_app/', \\\r\n     'delete_cloud_lightning_apps', \\\r\n     'from lightning.app import', \\\r\n     'lightning_apps = []', \\\r\n     'lightning.app and lightning.pytorch are ours', \\\r\n     'def _lightning_app():', \\\r\n     ':class:`~lightning.app.core.flow.LightningFlow`', \\\r\n     'http://pytorch_lightning.ai', \\\r\n     'from lightning_fabric import __version__', \\\r\n     '@lightning.ai']\r\n\r\n    \"\"\"\r\n    out = lines[:]\r\n    for source_import, target_import in mapping:\r\n        for i, ln in enumerate(out):\r\n            out[i] = re.sub(\r\n                rf\"([^_/@]|^){source_import}([^_\\w/]|$)\",\r\n                rf\"\\1{target_import}\\2\",\r\n                ln,\r\n            )\r\n            if lightning_by:  # in addition, replace base package\r\n                out[i] = out[i].replace(\"from lightning import \", f\"from {lightning_by} import \")\r\n                out[i] = out[i].replace(\"import lightning \", f\"import {lightning_by} \")\r\n    return out", "language": "python", "code": "def _replace_imports(lines: list[str], mapping: list[tuple[str, str]], lightning_by: str = \"\") -> list[str]:\r\n    \"\"\"Replace imports of standalone package to lightning.\r\n\r\n    >>> lns = [\r\n    ...     '\"lightning_app\"',\r\n    ...     \"lightning_app\",\r\n    ...     \"lightning_app/\",\r\n    ...     \"delete_cloud_lightning_apps\",\r\n    ...     \"from lightning_app import\",\r\n    ...     \"lightning_apps = []\",\r\n    ...     \"lightning_app and pytorch_lightning are ours\",\r\n    ...     \"def _lightning_app():\",\r\n    ...     \":class:`~lightning_app.core.flow.LightningFlow`\",\r\n    ...     \"http://pytorch_lightning.ai\",\r\n    ...     \"from lightning import __version__\",\r\n    ...     \"@lightning.ai\"\r\n    ... ]\r\n    >>> mapping = [(\"lightning_app\", \"lightning.app\"), (\"pytorch_lightning\", \"lightning.pytorch\")]\r\n    >>> _replace_imports(lns, mapping, lightning_by=\"lightning_fabric\")  # doctest: +NORMALIZE_WHITESPACE\r\n    ['\"lightning.app\"', \\\r\n     'lightning.app', \\\r\n     'lightning_app/', \\\r\n     'delete_cloud_lightning_apps', \\\r\n     'from lightning.app import', \\\r\n     'lightning_apps = []', \\\r\n     'lightning.app and lightning.pytorch are ours', \\\r\n     'def _lightning_app():', \\\r\n     ':class:`~lightning.app.core.flow.LightningFlow`', \\\r\n     'http://pytorch_lightning.ai', \\\r\n     'from lightning_fabric import __version__', \\\r\n     '@lightning.ai']\r\n\r\n    \"\"\"\r\n    out = lines[:]\r\n    for source_import, target_import in mapping:\r\n        for i, ln in enumerate(out):\r\n            out[i] = re.sub(\r\n                rf\"([^_/@]|^){source_import}([^_\\w/]|$)\",\r\n                rf\"\\1{target_import}\\2\",\r\n                ln,\r\n            )\r\n            if lightning_by:  # in addition, replace base package\r\n                out[i] = out[i].replace(\"from lightning import \", f\"from {lightning_by} import \")\r\n                out[i] = out[i].replace(\"import lightning \", f\"import {lightning_by} \")\r\n    return out", "code_tokens": ["def", "_replace_imports", "(", "lines", ":", "list", "[", "str", "]", ",", "mapping", ":", "list", "[", "tuple", "[", "str", ",", "str", "]", "]", ",", "lightning_by", ":", "str", "=", "\"", "\"", ")", "-", ">", "list", "[", "str", "]", ":", "\"", "\"", "\"", "Replace", "imports", "of", "standalone", "package", "to", "lightning", ".", ">", ">", ">", "lns", "=", "[", ".", ".", ".", "'", "\"", "lightning_app", "\"", "'", ",", ".", ".", ".", "\"", "lightning_app", "\"", ",", ".", ".", ".", "\"", "lightning_app", "/", "\"", ",", ".", ".", ".", "\"", "delete_cloud_lightning_apps", "\"", ",", ".", ".", ".", "\"", "from", "lightning_app", "import", "\"", ",", ".", ".", ".", "\"", "lightning_apps", "=", "[", "]", "\"", ",", ".", ".", ".", "\"", "lightning_app", "and", "pytorch_lightning", "are", "ours", "\"", ",", ".", ".", ".", "\"", "def", "_lightning_app", "(", ")", ":", "\"", ",", ".", ".", ".", "\"", ":", "class", ":", "`", "~", "lightning_app", ".", "core", ".", "flow", ".", "LightningFlow", "`", "\"", ",", ".", ".", ".", "\"", "http", ":", "/", "/", "pytorch_lightning", ".", "ai", "\"", ",", ".", ".", ".", "\"", "from", "lightning", "import", "__version__", "\"", ",", ".", ".", ".", "\"", "@", "lightning", ".", "ai", "\"", ".", ".", ".", "]", ">", ">", ">", "mapping", "=", "[", "(", "\"", "lightning_app", "\"", ",", "\"", "lightning", ".", "app", "\"", ")", ",", "(", "\"", "pytorch_lightning", "\"", ",", "\"", "lightning", ".", "pytorch", "\"", ")", "]", ">", ">", ">", "_replace_imports", "(", "lns", ",", "mapping", ",", "lightning_by", "=", "\"", "lightning_fabric", "\"", ")", "[", "'", "\"", "lightning", ".", "app", "\"", "'", ",", "\\", "'", "lightning", ".", "app", "'", ",", "\\", "'", "lightning_app", "/", "'", ",", "\\", "'", "delete_cloud_lightning_apps", "'", ",", "\\", "'", "from", "lightning", ".", "app", "import", "'", ",", "\\", "'", "lightning_apps", "=", "[", "]", "'", ",", "\\", "'", "lightning", ".", "app", "and", "lightning", ".", "pytorch", "are", "ours", "'", ",", "\\", "'", "def", "_lightning_app", "(", ")", ":", "'", ",", "\\", "'", ":", "class", ":", "`", "~", "lightning", ".", "app", ".", "core", ".", "flow", ".", "LightningFlow", "`", "'", ",", "\\", "'", "http", ":", "/", "/", "pytorch_lightning", ".", "ai", "'", ",", "\\", "'", "from", "lightning_fabric", "import", "__version__", "'", ",", "\\", "'", "@", "lightning", ".", "ai", "'", "]", "\"", "\"", "\"", "out", "=", "lines", "[", ":", "]", "for", "source_import", ",", "target_import", "in", "mapping", ":", "for", "i", ",", "ln", "in", "enumerate", "(", "out", ")", ":", "out", "[", "i", "]", "=", "re", ".", "sub", "(", "rf", "\"", "(", "[", "^", "_", "/", "@", "]", "|", "^", ")", "{", "source_import", "}", "(", "[", "^", "_", "\\", "w", "/", "]", "|", "$", ")", "\"", ",", "rf", "\"", "\\", "1", "{", "target_import", "}", "\\", "2", "\"", ",", "ln", ",", ")", "if", "lightning_by", ":", "out", "[", "i", "]", "=", "out", "[", "i", "]", ".", "replace", "(", "\"", "from", "lightning", "import", "\"", ",", "f", "\"", "from", "{", "lightning_by", "}", "import", "\"", ")", "out", "[", "i", "]", "=", "out", "[", "i", "]", ".", "replace", "(", "\"", "import", "lightning", "\"", ",", "f", "\"", "import", "{", "lightning_by", "}", "\"", ")", "return", "out"], "docstring": "Replace imports of standalone package to lightning.\r\n\r\n    >>> lns = [\r\n    ...     '\"lightning_app\"',\r\n    ...     \"lightning_app\",\r\n    ...     \"lightning_app/\",\r\n    ...     \"delete_cloud_lightning_apps\",\r\n    ...     \"from lightning_app import\",\r\n    ...     \"lightning_apps = []\",\r\n    ...     \"lightning_app and pytorch_lightning are ours\",\r\n    ...     \"def _lightning_app():\",\r\n    ...     \":class:`~lightning_app.core.flow.LightningFlow`\",\r\n    ...     \"http://pytorch_lightning.ai\",\r\n    ...     \"from lightning import __version__\",\r\n    ...     \"@lightning.ai\"\r\n    ... ]\r\n    >>> mapping = [(\"lightning_app\", \"lightning.app\"), (\"pytorch_lightning\", \"lightning.pytorch\")]\r\n    >>> _replace_imports(lns, mapping, lightning_by=\"lightning_fabric\")  # doctest: +NORMALIZE_WHITESPACE\r\n    ['\"lightning.app\"', \\\r\n     'lightning.app', \\\r\n     'lightning_app/', \\\r\n     'delete_cloud_lightning_apps', \\\r\n     'from lightning.app import', \\\r\n     'lightning_apps = []', \\\r\n     'lightning.app and lightning.pytorch are ours', \\\r\n     'def _lightning_app():', \\\r\n     ':class:`~lightning.app.core.flow.LightningFlow`', \\\r\n     'http://pytorch_lightning.ai', \\\r\n     'from lightning_fabric import __version__', \\\r\n     '@lightning.ai']", "docstring_tokens": ["replace", "imports", "of", "standalone", "package", "to", "lightning", "lns", "lightning_app", "lightning_app", "lightning_app", "delete_cloud_lightning_apps", "from", "lightning_app", "import", "lightning_apps", "lightning_app", "and", "pytorch_lightning", "are", "ours", "def", "_lightning_app", "class", "lightning_app", "core", "flow", "lightningflow", "http", "pytorch_lightning", "ai", "from", "lightning", "import", "__version__", "lightning", "ai", "mapping", "lightning_app", "lightning", "app", "pytorch_lightning", "lightning", "pytorch", "_replace_imports", "lns", "mapping", "lightning_by", "lightning_fabric", "doctest", "normalize_whitespace", "lightning", "app", "lightning", "app", "lightning_app", "delete_cloud_lightning_apps", "from", "lightning", "app", "import", "lightning_apps", "lightning", "app", "and", "lightning", "pytorch", "are", "ours", "def", "_lightning_app", "class", "lightning", "app", "core", "flow", "lightningflow", "http", "pytorch_lightning", "ai", "from", "lightning_fabric", "import", "__version__", "lightning", "ai"], "docstring_summary": "Replace imports of standalone package to lightning.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 235, "end_line": 279, "hash": "570e98693d738c4b47c1d9b3e8a039cc", "complexity": 4, "parameters": ["lines", "mapping", "str]]", "lightning_by"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "copy_replace_imports", "original_string": "def copy_replace_imports(\r\n    source_dir: str,\r\n    source_imports: Sequence[str],\r\n    target_imports: Sequence[str],\r\n    target_dir: Optional[str] = None,\r\n    lightning_by: str = \"\",\r\n) -> None:\r\n    \"\"\"Copy package content with import adjustments.\"\"\"\r\n    print(f\"Replacing imports: {locals()}\")\r\n    assert len(source_imports) == len(target_imports), (\r\n        \"source and target imports must have the same length, \"\r\n        f\"source: {len(source_imports)}, target: {len(target_imports)}\"\r\n    )\r\n    if target_dir is None:\r\n        target_dir = source_dir\r\n\r\n    ls = _retrieve_files(source_dir)\r\n    for fp in ls:\r\n        fp_new = fp.replace(source_dir, target_dir)\r\n        _, ext = os.path.splitext(fp)\r\n        if ext in (\".png\", \".jpg\", \".ico\"):\r\n            os.makedirs(dirname(fp_new), exist_ok=True)\r\n            if not isfile(fp_new):\r\n                shutil.copy(fp, fp_new)\r\n            continue\r\n        if ext in (\".pyc\",):\r\n            continue\r\n        # Try to parse everything else\r\n        with open(fp, encoding=\"utf-8\") as fopen:\r\n            try:\r\n                lines = fopen.readlines()\r\n            except UnicodeDecodeError:\r\n                # a binary file, skip\r\n                print(f\"Skipped replacing imports for {fp}\")\r\n                continue\r\n        lines = _replace_imports(lines, list(zip(source_imports, target_imports)), lightning_by=lightning_by)\r\n        os.makedirs(os.path.dirname(fp_new), exist_ok=True)\r\n        with open(fp_new, \"w\", encoding=\"utf-8\") as fopen:\r\n            fopen.writelines(lines)", "language": "python", "code": "def copy_replace_imports(\r\n    source_dir: str,\r\n    source_imports: Sequence[str],\r\n    target_imports: Sequence[str],\r\n    target_dir: Optional[str] = None,\r\n    lightning_by: str = \"\",\r\n) -> None:\r\n    \"\"\"Copy package content with import adjustments.\"\"\"\r\n    print(f\"Replacing imports: {locals()}\")\r\n    assert len(source_imports) == len(target_imports), (\r\n        \"source and target imports must have the same length, \"\r\n        f\"source: {len(source_imports)}, target: {len(target_imports)}\"\r\n    )\r\n    if target_dir is None:\r\n        target_dir = source_dir\r\n\r\n    ls = _retrieve_files(source_dir)\r\n    for fp in ls:\r\n        fp_new = fp.replace(source_dir, target_dir)\r\n        _, ext = os.path.splitext(fp)\r\n        if ext in (\".png\", \".jpg\", \".ico\"):\r\n            os.makedirs(dirname(fp_new), exist_ok=True)\r\n            if not isfile(fp_new):\r\n                shutil.copy(fp, fp_new)\r\n            continue\r\n        if ext in (\".pyc\",):\r\n            continue\r\n        # Try to parse everything else\r\n        with open(fp, encoding=\"utf-8\") as fopen:\r\n            try:\r\n                lines = fopen.readlines()\r\n            except UnicodeDecodeError:\r\n                # a binary file, skip\r\n                print(f\"Skipped replacing imports for {fp}\")\r\n                continue\r\n        lines = _replace_imports(lines, list(zip(source_imports, target_imports)), lightning_by=lightning_by)\r\n        os.makedirs(os.path.dirname(fp_new), exist_ok=True)\r\n        with open(fp_new, \"w\", encoding=\"utf-8\") as fopen:\r\n            fopen.writelines(lines)", "code_tokens": ["def", "copy_replace_imports", "(", "source_dir", ":", "str", ",", "source_imports", ":", "Sequence", "[", "str", "]", ",", "target_imports", ":", "Sequence", "[", "str", "]", ",", "target_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "lightning_by", ":", "str", "=", "\"", "\"", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Copy", "package", "content", "with", "import", "adjustments", ".", "\"", "\"", "\"", "print", "(", "f", "\"", "Replacing", "imports", ":", "{", "locals", "(", ")", "}", "\"", ")", "assert", "len", "(", "source_imports", ")", "=", "=", "len", "(", "target_imports", ")", ",", "(", "\"", "source", "and", "target", "imports", "must", "have", "the", "same", "length", ",", "\"", "f", "\"", "source", ":", "{", "len", "(", "source_imports", ")", "}", ",", "target", ":", "{", "len", "(", "target_imports", ")", "}", "\"", ")", "if", "target_dir", "is", "None", ":", "target_dir", "=", "source_dir", "ls", "=", "_retrieve_files", "(", "source_dir", ")", "for", "fp", "in", "ls", ":", "fp_new", "=", "fp", ".", "replace", "(", "source_dir", ",", "target_dir", ")", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "fp", ")", "if", "ext", "in", "(", "\"", ".", "png", "\"", ",", "\"", ".", "jpg", "\"", ",", "\"", ".", "ico", "\"", ")", ":", "os", ".", "makedirs", "(", "dirname", "(", "fp_new", ")", ",", "exist_ok", "=", "True", ")", "if", "not", "isfile", "(", "fp_new", ")", ":", "shutil", ".", "copy", "(", "fp", ",", "fp_new", ")", "continue", "if", "ext", "in", "(", "\"", ".", "pyc", "\"", ",", ")", ":", "continue", "with", "open", "(", "fp", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "try", ":", "lines", "=", "fopen", ".", "readlines", "(", ")", "except", "UnicodeDecodeError", ":", "print", "(", "f", "\"", "Skipped", "replacing", "imports", "for", "{", "fp", "}", "\"", ")", "continue", "lines", "=", "_replace_imports", "(", "lines", ",", "list", "(", "zip", "(", "source_imports", ",", "target_imports", ")", ")", ",", "lightning_by", "=", "lightning_by", ")", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "fp_new", ")", ",", "exist_ok", "=", "True", ")", "with", "open", "(", "fp_new", ",", "\"", "w", "\"", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "fopen", ".", "writelines", "(", "lines", ")"], "docstring": "Copy package content with import adjustments.", "docstring_tokens": ["copy", "package", "content", "with", "import", "adjustments"], "docstring_summary": "Copy package content with import adjustments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 282, "end_line": 320, "hash": "33480a902e466b89840920704ec09bde", "complexity": 9, "parameters": ["source_dir", "source_imports", "target_imports", "target_dir", "lightning_by"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "create_mirror_package", "original_string": "def create_mirror_package(source_dir: str, package_mapping: dict[str, str]) -> None:\r\n    \"\"\"Create a mirror package with adjusted imports.\"\"\"\r\n    # replace imports and copy the code\r\n    mapping = package_mapping.copy()\r\n    mapping.pop(\"lightning\", None)  # pop this key to avoid replacing `lightning` to `lightning.lightning`\r\n\r\n    mapping = {f\"lightning.{sp}\": sl for sp, sl in mapping.items()}\r\n    for pkg_from, pkg_to in mapping.items():\r\n        source_imports, target_imports = zip(*mapping.items())\r\n        copy_replace_imports(\r\n            source_dir=os.path.join(source_dir, pkg_from.replace(\".\", os.sep)),\r\n            # pytorch_lightning uses lightning_fabric, so we need to replace all imports for all directories\r\n            source_imports=source_imports,\r\n            target_imports=target_imports,\r\n            target_dir=os.path.join(source_dir, pkg_to.replace(\".\", os.sep)),\r\n            lightning_by=pkg_from,\r\n        )", "language": "python", "code": "def create_mirror_package(source_dir: str, package_mapping: dict[str, str]) -> None:\r\n    \"\"\"Create a mirror package with adjusted imports.\"\"\"\r\n    # replace imports and copy the code\r\n    mapping = package_mapping.copy()\r\n    mapping.pop(\"lightning\", None)  # pop this key to avoid replacing `lightning` to `lightning.lightning`\r\n\r\n    mapping = {f\"lightning.{sp}\": sl for sp, sl in mapping.items()}\r\n    for pkg_from, pkg_to in mapping.items():\r\n        source_imports, target_imports = zip(*mapping.items())\r\n        copy_replace_imports(\r\n            source_dir=os.path.join(source_dir, pkg_from.replace(\".\", os.sep)),\r\n            # pytorch_lightning uses lightning_fabric, so we need to replace all imports for all directories\r\n            source_imports=source_imports,\r\n            target_imports=target_imports,\r\n            target_dir=os.path.join(source_dir, pkg_to.replace(\".\", os.sep)),\r\n            lightning_by=pkg_from,\r\n        )", "code_tokens": ["def", "create_mirror_package", "(", "source_dir", ":", "str", ",", "package_mapping", ":", "dict", "[", "str", ",", "str", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Create", "a", "mirror", "package", "with", "adjusted", "imports", ".", "\"", "\"", "\"", "mapping", "=", "package_mapping", ".", "copy", "(", ")", "mapping", ".", "pop", "(", "\"", "lightning", "\"", ",", "None", ")", "mapping", "=", "{", "f", "\"", "lightning", ".", "{", "sp", "}", "\"", ":", "sl", "for", "sp", ",", "sl", "in", "mapping", ".", "items", "(", ")", "}", "for", "pkg_from", ",", "pkg_to", "in", "mapping", ".", "items", "(", ")", ":", "source_imports", ",", "target_imports", "=", "zip", "(", "*", "mapping", ".", "items", "(", ")", ")", "copy_replace_imports", "(", "source_dir", "=", "os", ".", "path", ".", "join", "(", "source_dir", ",", "pkg_from", ".", "replace", "(", "\"", ".", "\"", ",", "os", ".", "sep", ")", ")", ",", "source_imports", "=", "source_imports", ",", "target_imports", "=", "target_imports", ",", "target_dir", "=", "os", ".", "path", ".", "join", "(", "source_dir", ",", "pkg_to", ".", "replace", "(", "\"", ".", "\"", ",", "os", ".", "sep", ")", ")", ",", "lightning_by", "=", "pkg_from", ",", ")"], "docstring": "Create a mirror package with adjusted imports.", "docstring_tokens": ["create", "a", "mirror", "package", "with", "adjusted", "imports"], "docstring_summary": "Create a mirror package with adjusted imports.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "function", "start_line": 323, "end_line": 339, "hash": "a85fcd94971ced662a7eafa0d33307ba", "complexity": 3, "parameters": ["source_dir", "package_mapping", "str]"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "copy_replace_imports", "original_string": "def copy_replace_imports(\r\n        source_dir: str,\r\n        source_import: str,\r\n        target_import: str,\r\n        target_dir: Optional[str] = None,\r\n        lightning_by: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Copy package content with import adjustments.\"\"\"\r\n        source_imports = source_import.strip().split(\",\")\r\n        target_imports = target_import.strip().split(\",\")\r\n        copy_replace_imports(\r\n            source_dir, source_imports, target_imports, target_dir=target_dir, lightning_by=lightning_by\r\n        )", "language": "python", "code": "def copy_replace_imports(\r\n        source_dir: str,\r\n        source_import: str,\r\n        target_import: str,\r\n        target_dir: Optional[str] = None,\r\n        lightning_by: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Copy package content with import adjustments.\"\"\"\r\n        source_imports = source_import.strip().split(\",\")\r\n        target_imports = target_import.strip().split(\",\")\r\n        copy_replace_imports(\r\n            source_dir, source_imports, target_imports, target_dir=target_dir, lightning_by=lightning_by\r\n        )", "code_tokens": ["def", "copy_replace_imports", "(", "source_dir", ":", "str", ",", "source_import", ":", "str", ",", "target_import", ":", "str", ",", "target_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "lightning_by", ":", "str", "=", "\"", "\"", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Copy", "package", "content", "with", "import", "adjustments", ".", "\"", "\"", "\"", "source_imports", "=", "source_import", ".", "strip", "(", ")", ".", "split", "(", "\"", ",", "\"", ")", "target_imports", "=", "target_import", ".", "strip", "(", ")", ".", "split", "(", "\"", ",", "\"", ")", "copy_replace_imports", "(", "source_dir", ",", "source_imports", ",", "target_imports", ",", "target_dir", "=", "target_dir", ",", "lightning_by", "=", "lightning_by", ")"], "docstring": "Copy package content with import adjustments.", "docstring_tokens": ["copy", "package", "content", "with", "import", "adjustments"], "docstring_summary": "Copy package content with import adjustments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "AssistantCLI", "start_line": 344, "end_line": 356, "hash": "73d8805267a347946204537f8b6e931e", "complexity": 1, "parameters": ["source_dir", "source_import", "target_import", "target_dir", "lightning_by"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "pull_docs_files", "original_string": "def pull_docs_files(\r\n        gh_user_repo: str,\r\n        target_dir: str = \"docs/source-pytorch/XXX\",\r\n        checkout: str = \"refs/tags/1.0.0\",\r\n        source_dir: str = \"docs/source\",\r\n        single_page: Optional[str] = None,\r\n        as_orphan: bool = False,\r\n    ) -> None:\r\n        \"\"\"Pull docs pages from external source and append to local docs.\r\n\r\n        Args:\r\n            gh_user_repo: standard GitHub user/repo string\r\n            target_dir: relative location inside the docs folder\r\n            checkout: specific tag or branch to checkout\r\n            source_dir: relative location inside the remote / external repo\r\n            single_page: copy only single page from the remote repo and name it as the repo name\r\n            as_orphan: append orphan statement to the page\r\n\r\n        \"\"\"\r\n        import zipfile\r\n\r\n        zip_url = f\"https://github.com/{gh_user_repo}/archive/{checkout}.zip\"\r\n\r\n        with tempfile.TemporaryDirectory() as tmp:\r\n            zip_file = os.path.join(tmp, \"repo.zip\")\r\n            try:\r\n                urllib.request.urlretrieve(zip_url, zip_file)\r\n            except urllib.error.HTTPError:\r\n                raise RuntimeError(f\"Requesting file '{zip_url}' does not exist or it is just unavailable.\")\r\n\r\n            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\r\n                zip_ref.extractall(tmp)\r\n\r\n            zip_dirs = [d for d in glob.glob(os.path.join(tmp, \"*\")) if os.path.isdir(d)]\r\n            # check that the extracted archive has only repo folder\r\n            assert len(zip_dirs) == 1\r\n            repo_dir = zip_dirs[0]\r\n\r\n            if single_page:  # special case for copying single page\r\n                single_page = os.path.join(repo_dir, source_dir, single_page)\r\n                assert os.path.isfile(single_page), f\"File '{single_page}' does not exist.\"\r\n                name = re.sub(r\"lightning[-_]?\", \"\", gh_user_repo.split(\"/\")[-1])\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, f\"{name}.rst\")\r\n                AssistantCLI._copy_rst(single_page, new_rst, as_orphan=as_orphan)\r\n                return\r\n            # continue with copying all pages\r\n            ls_pages = glob.glob(os.path.join(repo_dir, source_dir, \"*.rst\"))\r\n            ls_pages += glob.glob(os.path.join(repo_dir, source_dir, \"**\", \"*.rst\"))\r\n            for rst in ls_pages:\r\n                rel_rst = rst.replace(os.path.join(repo_dir, source_dir) + os.path.sep, \"\")\r\n                rel_dir = os.path.dirname(rel_rst)\r\n                os.makedirs(os.path.join(_PROJECT_ROOT, target_dir, rel_dir), exist_ok=True)\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, rel_rst)\r\n                if os.path.isfile(new_rst):\r\n                    logging.warning(f\"Page {new_rst} already exists in the local tree so it will be skipped.\")\r\n                    continue\r\n                AssistantCLI._copy_rst(rst, new_rst, as_orphan=as_orphan)", "language": "python", "code": "def pull_docs_files(\r\n        gh_user_repo: str,\r\n        target_dir: str = \"docs/source-pytorch/XXX\",\r\n        checkout: str = \"refs/tags/1.0.0\",\r\n        source_dir: str = \"docs/source\",\r\n        single_page: Optional[str] = None,\r\n        as_orphan: bool = False,\r\n    ) -> None:\r\n        \"\"\"Pull docs pages from external source and append to local docs.\r\n\r\n        Args:\r\n            gh_user_repo: standard GitHub user/repo string\r\n            target_dir: relative location inside the docs folder\r\n            checkout: specific tag or branch to checkout\r\n            source_dir: relative location inside the remote / external repo\r\n            single_page: copy only single page from the remote repo and name it as the repo name\r\n            as_orphan: append orphan statement to the page\r\n\r\n        \"\"\"\r\n        import zipfile\r\n\r\n        zip_url = f\"https://github.com/{gh_user_repo}/archive/{checkout}.zip\"\r\n\r\n        with tempfile.TemporaryDirectory() as tmp:\r\n            zip_file = os.path.join(tmp, \"repo.zip\")\r\n            try:\r\n                urllib.request.urlretrieve(zip_url, zip_file)\r\n            except urllib.error.HTTPError:\r\n                raise RuntimeError(f\"Requesting file '{zip_url}' does not exist or it is just unavailable.\")\r\n\r\n            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\r\n                zip_ref.extractall(tmp)\r\n\r\n            zip_dirs = [d for d in glob.glob(os.path.join(tmp, \"*\")) if os.path.isdir(d)]\r\n            # check that the extracted archive has only repo folder\r\n            assert len(zip_dirs) == 1\r\n            repo_dir = zip_dirs[0]\r\n\r\n            if single_page:  # special case for copying single page\r\n                single_page = os.path.join(repo_dir, source_dir, single_page)\r\n                assert os.path.isfile(single_page), f\"File '{single_page}' does not exist.\"\r\n                name = re.sub(r\"lightning[-_]?\", \"\", gh_user_repo.split(\"/\")[-1])\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, f\"{name}.rst\")\r\n                AssistantCLI._copy_rst(single_page, new_rst, as_orphan=as_orphan)\r\n                return\r\n            # continue with copying all pages\r\n            ls_pages = glob.glob(os.path.join(repo_dir, source_dir, \"*.rst\"))\r\n            ls_pages += glob.glob(os.path.join(repo_dir, source_dir, \"**\", \"*.rst\"))\r\n            for rst in ls_pages:\r\n                rel_rst = rst.replace(os.path.join(repo_dir, source_dir) + os.path.sep, \"\")\r\n                rel_dir = os.path.dirname(rel_rst)\r\n                os.makedirs(os.path.join(_PROJECT_ROOT, target_dir, rel_dir), exist_ok=True)\r\n                new_rst = os.path.join(_PROJECT_ROOT, target_dir, rel_rst)\r\n                if os.path.isfile(new_rst):\r\n                    logging.warning(f\"Page {new_rst} already exists in the local tree so it will be skipped.\")\r\n                    continue\r\n                AssistantCLI._copy_rst(rst, new_rst, as_orphan=as_orphan)", "code_tokens": ["def", "pull_docs_files", "(", "gh_user_repo", ":", "str", ",", "target_dir", ":", "str", "=", "\"", "docs", "/", "source", "-", "pytorch", "/", "XXX", "\"", ",", "checkout", ":", "str", "=", "\"", "refs", "/", "tags", "/", "1", ".", "0", ".", "0", "\"", ",", "source_dir", ":", "str", "=", "\"", "docs", "/", "source", "\"", ",", "single_page", ":", "Optional", "[", "str", "]", "=", "None", ",", "as_orphan", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Pull", "docs", "pages", "from", "external", "source", "and", "append", "to", "local", "docs", ".", "Args", ":", "gh_user_repo", ":", "standard", "GitHub", "user", "/", "repo", "string", "target_dir", ":", "relative", "location", "inside", "the", "docs", "folder", "checkout", ":", "specific", "tag", "or", "branch", "to", "checkout", "source_dir", ":", "relative", "location", "inside", "the", "remote", "/", "external", "repo", "single_page", ":", "copy", "only", "single", "page", "from", "the", "remote", "repo", "and", "name", "it", "as", "the", "repo", "name", "as_orphan", ":", "append", "orphan", "statement", "to", "the", "page", "\"", "\"", "\"", "import", "zipfile", "zip_url", "=", "f", "\"", "https", ":", "/", "/", "github", ".", "com", "/", "{", "gh_user_repo", "}", "/", "archive", "/", "{", "checkout", "}", ".", "zip", "\"", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tmp", ":", "zip_file", "=", "os", ".", "path", ".", "join", "(", "tmp", ",", "\"", "repo", ".", "zip", "\"", ")", "try", ":", "urllib", ".", "request", ".", "urlretrieve", "(", "zip_url", ",", "zip_file", ")", "except", "urllib", ".", "error", ".", "HTTPError", ":", "raise", "RuntimeError", "(", "f", "\"", "Requesting", "file", "'", "{", "zip_url", "}", "'", "does", "not", "exist", "or", "it", "is", "just", "unavailable", ".", "\"", ")", "with", "zipfile", ".", "ZipFile", "(", "zip_file", ",", "\"", "r", "\"", ")", "as", "zip_ref", ":", "zip_ref", ".", "extractall", "(", "tmp", ")", "zip_dirs", "=", "[", "d", "for", "d", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "tmp", ",", "\"", "*", "\"", ")", ")", "if", "os", ".", "path", ".", "isdir", "(", "d", ")", "]", "assert", "len", "(", "zip_dirs", ")", "=", "=", "1", "repo_dir", "=", "zip_dirs", "[", "0", "]", "if", "single_page", ":", "single_page", "=", "os", ".", "path", ".", "join", "(", "repo_dir", ",", "source_dir", ",", "single_page", ")", "assert", "os", ".", "path", ".", "isfile", "(", "single_page", ")", ",", "f", "\"", "File", "'", "{", "single_page", "}", "'", "does", "not", "exist", ".", "\"", "name", "=", "re", ".", "sub", "(", "r", "\"", "lightning", "[", "-", "_", "]", "?", "\"", ",", "\"", "\"", ",", "gh_user_repo", ".", "split", "(", "\"", "/", "\"", ")", "[", "-", "1", "]", ")", "new_rst", "=", "os", ".", "path", ".", "join", "(", "_PROJECT_ROOT", ",", "target_dir", ",", "f", "\"", "{", "name", "}", ".", "rst", "\"", ")", "AssistantCLI", ".", "_copy_rst", "(", "single_page", ",", "new_rst", ",", "as_orphan", "=", "as_orphan", ")", "return", "ls_pages", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "repo_dir", ",", "source_dir", ",", "\"", "*", ".", "rst", "\"", ")", ")", "ls_pages", "+", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "repo_dir", ",", "source_dir", ",", "\"", "*", "*", "\"", ",", "\"", "*", ".", "rst", "\"", ")", ")", "for", "rst", "in", "ls_pages", ":", "rel_rst", "=", "rst", ".", "replace", "(", "os", ".", "path", ".", "join", "(", "repo_dir", ",", "source_dir", ")", "+", "os", ".", "path", ".", "sep", ",", "\"", "\"", ")", "rel_dir", "=", "os", ".", "path", ".", "dirname", "(", "rel_rst", ")", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "_PROJECT_ROOT", ",", "target_dir", ",", "rel_dir", ")", ",", "exist_ok", "=", "True", ")", "new_rst", "=", "os", ".", "path", ".", "join", "(", "_PROJECT_ROOT", ",", "target_dir", ",", "rel_rst", ")", "if", "os", ".", "path", ".", "isfile", "(", "new_rst", ")", ":", "logging", ".", "warning", "(", "f", "\"", "Page", "{", "new_rst", "}", "already", "exists", "in", "the", "local", "tree", "so", "it", "will", "be", "skipped", ".", "\"", ")", "continue", "AssistantCLI", ".", "_copy_rst", "(", "rst", ",", "new_rst", ",", "as_orphan", "=", "as_orphan", ")"], "docstring": "Pull docs pages from external source and append to local docs.\r\n\r\n        Args:\r\n            gh_user_repo: standard GitHub user/repo string\r\n            target_dir: relative location inside the docs folder\r\n            checkout: specific tag or branch to checkout\r\n            source_dir: relative location inside the remote / external repo\r\n            single_page: copy only single page from the remote repo and name it as the repo name\r\n            as_orphan: append orphan statement to the page", "docstring_tokens": ["pull", "docs", "pages", "from", "external", "source", "and", "append", "to", "local", "docs", "args", "gh_user_repo", "standard", "github", "user", "repo", "string", "target_dir", "relative", "location", "inside", "the", "docs", "folder", "checkout", "specific", "tag", "or", "branch", "to", "checkout", "source_dir", "relative", "location", "inside", "the", "remote", "external", "repo", "single_page", "copy", "only", "single", "page", "from", "the", "remote", "repo", "and", "name", "it", "as", "the", "repo", "name", "as_orphan", "append", "orphan", "statement", "to", "the", "page"], "docstring_summary": "Pull docs pages from external source and append to local docs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "AssistantCLI", "start_line": 359, "end_line": 415, "hash": "7cfddef78c4d6be65753dcdcf884b2b6", "complexity": 9, "parameters": ["gh_user_repo", "target_dir", "checkout", "source_dir", "single_page", "as_orphan"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "_copy_rst", "original_string": "def _copy_rst(rst_in, rst_out, as_orphan: bool = False):\r\n        \"\"\"Copy RST page with optional inserting orphan statement.\"\"\"\r\n        with open(rst_in, encoding=\"utf-8\") as fopen:\r\n            page = fopen.read()\r\n        if as_orphan and \":orphan:\" not in page:\r\n            page = \":orphan:\\n\\n\" + page\r\n        with open(rst_out, \"w\", encoding=\"utf-8\") as fopen:\r\n            fopen.write(page)", "language": "python", "code": "def _copy_rst(rst_in, rst_out, as_orphan: bool = False):\r\n        \"\"\"Copy RST page with optional inserting orphan statement.\"\"\"\r\n        with open(rst_in, encoding=\"utf-8\") as fopen:\r\n            page = fopen.read()\r\n        if as_orphan and \":orphan:\" not in page:\r\n            page = \":orphan:\\n\\n\" + page\r\n        with open(rst_out, \"w\", encoding=\"utf-8\") as fopen:\r\n            fopen.write(page)", "code_tokens": ["def", "_copy_rst", "(", "rst_in", ",", "rst_out", ",", "as_orphan", ":", "bool", "=", "False", ")", ":", "\"", "\"", "\"", "Copy", "RST", "page", "with", "optional", "inserting", "orphan", "statement", ".", "\"", "\"", "\"", "with", "open", "(", "rst_in", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "page", "=", "fopen", ".", "read", "(", ")", "if", "as_orphan", "and", "\"", ":", "orphan", ":", "\"", "not", "in", "page", ":", "page", "=", "\"", ":", "orphan", ":", "\\", "n", "\\", "n", "\"", "+", "page", "with", "open", "(", "rst_out", ",", "\"", "w", "\"", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "fopen", ".", "write", "(", "page", ")"], "docstring": "Copy RST page with optional inserting orphan statement.", "docstring_tokens": ["copy", "rst", "page", "with", "optional", "inserting", "orphan", "statement"], "docstring_summary": "Copy RST page with optional inserting orphan statement.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "AssistantCLI", "start_line": 418, "end_line": 425, "hash": "29f0e7cfd8643e24d385398a7c4778eb", "complexity": 5, "parameters": ["rst_in", "rst_out", "as_orphan"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "convert_version2nightly", "original_string": "def convert_version2nightly(ver_file: str = \"src/version.info\") -> None:\r\n        \"\"\"Load the actual version and convert it to the nightly version.\"\"\"\r\n        from datetime import datetime\r\n\r\n        with open(ver_file) as fopen:\r\n            version = fopen.read().strip()\r\n        # parse X.Y.Z version and prune any suffix\r\n        vers = re.match(r\"(\\d+)\\.(\\d+)\\.(\\d+).*\", version)\r\n        # create timestamp  YYYYMMDD\r\n        timestamp = datetime.now().strftime(\"%Y%m%d\")\r\n        version = f\"{'.'.join(vers.groups())}.dev{timestamp}\"\r\n        with open(ver_file, \"w\") as fopen:\r\n            fopen.write(version + os.linesep)", "language": "python", "code": "def convert_version2nightly(ver_file: str = \"src/version.info\") -> None:\r\n        \"\"\"Load the actual version and convert it to the nightly version.\"\"\"\r\n        from datetime import datetime\r\n\r\n        with open(ver_file) as fopen:\r\n            version = fopen.read().strip()\r\n        # parse X.Y.Z version and prune any suffix\r\n        vers = re.match(r\"(\\d+)\\.(\\d+)\\.(\\d+).*\", version)\r\n        # create timestamp  YYYYMMDD\r\n        timestamp = datetime.now().strftime(\"%Y%m%d\")\r\n        version = f\"{'.'.join(vers.groups())}.dev{timestamp}\"\r\n        with open(ver_file, \"w\") as fopen:\r\n            fopen.write(version + os.linesep)", "code_tokens": ["def", "convert_version2nightly", "(", "ver_file", ":", "str", "=", "\"", "src", "/", "version", ".", "info", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Load", "the", "actual", "version", "and", "convert", "it", "to", "the", "nightly", "version", ".", "\"", "\"", "\"", "from", "datetime", "import", "datetime", "with", "open", "(", "ver_file", ")", "as", "fopen", ":", "version", "=", "fopen", ".", "read", "(", ")", ".", "strip", "(", ")", "vers", "=", "re", ".", "match", "(", "r", "\"", "(", "\\", "d", "+", ")", "\\", ".", "(", "\\", "d", "+", ")", "\\", ".", "(", "\\", "d", "+", ")", ".", "*", "\"", ",", "version", ")", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"", "%", "Y", "%", "m", "%", "d", "\"", ")", "version", "=", "f", "\"", "{", "'", ".", "'", ".", "join", "(", "vers", ".", "groups", "(", ")", ")", "}", ".", "dev", "{", "timestamp", "}", "\"", "with", "open", "(", "ver_file", ",", "\"", "w", "\"", ")", "as", "fopen", ":", "fopen", ".", "write", "(", "version", "+", "os", ".", "linesep", ")"], "docstring": "Load the actual version and convert it to the nightly version.", "docstring_tokens": ["load", "the", "actual", "version", "and", "convert", "it", "to", "the", "nightly", "version"], "docstring_summary": "Load the actual version and convert it to the nightly version.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "AssistantCLI", "start_line": 428, "end_line": 440, "hash": "cb0a8b38e85b5f64bc2dd498a2bf6069", "complexity": 3, "parameters": ["ver_file"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "generate_docker_tags", "original_string": "def generate_docker_tags(\r\n        release_version: str,\r\n        python_version: str,\r\n        torch_version: str,\r\n        cuda_version: str,\r\n        docker_project: str = \"pytorchlightning/pytorch_lightning\",\r\n        add_latest: bool = False,\r\n    ) -> None:\r\n        \"\"\"Generate docker tags for the given versions.\"\"\"\r\n        tags = [f\"latest-py{python_version}-torch{torch_version}-cuda{cuda_version}\"]\r\n        if release_version:\r\n            tags += [f\"{release_version}-py{python_version}-torch{torch_version}-cuda{cuda_version}\"]\r\n        if add_latest:\r\n            tags += [\"latest\"]\r\n\r\n        tags = [f\"{docker_project}:{tag}\" for tag in tags]\r\n        print(\",\".join(tags))", "language": "python", "code": "def generate_docker_tags(\r\n        release_version: str,\r\n        python_version: str,\r\n        torch_version: str,\r\n        cuda_version: str,\r\n        docker_project: str = \"pytorchlightning/pytorch_lightning\",\r\n        add_latest: bool = False,\r\n    ) -> None:\r\n        \"\"\"Generate docker tags for the given versions.\"\"\"\r\n        tags = [f\"latest-py{python_version}-torch{torch_version}-cuda{cuda_version}\"]\r\n        if release_version:\r\n            tags += [f\"{release_version}-py{python_version}-torch{torch_version}-cuda{cuda_version}\"]\r\n        if add_latest:\r\n            tags += [\"latest\"]\r\n\r\n        tags = [f\"{docker_project}:{tag}\" for tag in tags]\r\n        print(\",\".join(tags))", "code_tokens": ["def", "generate_docker_tags", "(", "release_version", ":", "str", ",", "python_version", ":", "str", ",", "torch_version", ":", "str", ",", "cuda_version", ":", "str", ",", "docker_project", ":", "str", "=", "\"", "pytorchlightning", "/", "pytorch_lightning", "\"", ",", "add_latest", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Generate", "docker", "tags", "for", "the", "given", "versions", ".", "\"", "\"", "\"", "tags", "=", "[", "f", "\"", "latest", "-", "py", "{", "python_version", "}", "-", "torch", "{", "torch_version", "}", "-", "cuda", "{", "cuda_version", "}", "\"", "]", "if", "release_version", ":", "tags", "+", "=", "[", "f", "\"", "{", "release_version", "}", "-", "py", "{", "python_version", "}", "-", "torch", "{", "torch_version", "}", "-", "cuda", "{", "cuda_version", "}", "\"", "]", "if", "add_latest", ":", "tags", "+", "=", "[", "\"", "latest", "\"", "]", "tags", "=", "[", "f", "\"", "{", "docker_project", "}", ":", "{", "tag", "}", "\"", "for", "tag", "in", "tags", "]", "print", "(", "\"", ",", "\"", ".", "join", "(", "tags", ")", ")"], "docstring": "Generate docker tags for the given versions.", "docstring_tokens": ["generate", "docker", "tags", "for", "the", "given", "versions"], "docstring_summary": "Generate docker tags for the given versions.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "AssistantCLI", "start_line": 443, "end_line": 459, "hash": "250b7814105e46e7168ef2a360ca7df8", "complexity": 4, "parameters": ["release_version", "python_version", "torch_version", "cuda_version", "docker_project", "add_latest"]}
{"repo": "pytorch-lightning", "path": ".actions\\assistant.py", "func_name": "prune_pytest_as_errors", "original_string": "def prune_pytest_as_errors(\r\n        pyproject_toml: str = \"pyproject.toml\", errors: tuple = (\"FutureWarning\", \"DeprecationWarning\")\r\n    ) -> None:\r\n        \"\"\"Prune pytest warnings as errors from the pyproject.toml file.\"\"\"\r\n        import tomlkit\r\n\r\n        with open(pyproject_toml, encoding=\"utf-8\") as fopen:\r\n            content = fopen.read()\r\n        pyproject = tomlkit.parse(content)\r\n        filterwarnings = pyproject.get(\"tool\", {}).get(\"pytest\", {}).get(\"ini_options\", {}).get(\"filterwarnings\", [])\r\n        if not filterwarnings:\r\n            return\r\n        filterwarnings = [wrn for wrn in filterwarnings if not any(f\"error::{err}\" in wrn for err in errors)]\r\n        pyproject[\"tool\"][\"pytest\"][\"ini_options\"][\"filterwarnings\"] = filterwarnings\r\n\r\n        with open(pyproject_toml, \"w\", encoding=\"utf-8\") as fopen:\r\n            fopen.write(tomlkit.dumps(pyproject))", "language": "python", "code": "def prune_pytest_as_errors(\r\n        pyproject_toml: str = \"pyproject.toml\", errors: tuple = (\"FutureWarning\", \"DeprecationWarning\")\r\n    ) -> None:\r\n        \"\"\"Prune pytest warnings as errors from the pyproject.toml file.\"\"\"\r\n        import tomlkit\r\n\r\n        with open(pyproject_toml, encoding=\"utf-8\") as fopen:\r\n            content = fopen.read()\r\n        pyproject = tomlkit.parse(content)\r\n        filterwarnings = pyproject.get(\"tool\", {}).get(\"pytest\", {}).get(\"ini_options\", {}).get(\"filterwarnings\", [])\r\n        if not filterwarnings:\r\n            return\r\n        filterwarnings = [wrn for wrn in filterwarnings if not any(f\"error::{err}\" in wrn for err in errors)]\r\n        pyproject[\"tool\"][\"pytest\"][\"ini_options\"][\"filterwarnings\"] = filterwarnings\r\n\r\n        with open(pyproject_toml, \"w\", encoding=\"utf-8\") as fopen:\r\n            fopen.write(tomlkit.dumps(pyproject))", "code_tokens": ["def", "prune_pytest_as_errors", "(", "pyproject_toml", ":", "str", "=", "\"", "pyproject", ".", "toml", "\"", ",", "errors", ":", "tuple", "=", "(", "\"", "FutureWarning", "\"", ",", "\"", "DeprecationWarning", "\"", ")", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Prune", "pytest", "warnings", "as", "errors", "from", "the", "pyproject", ".", "toml", "file", ".", "\"", "\"", "\"", "import", "tomlkit", "with", "open", "(", "pyproject_toml", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "content", "=", "fopen", ".", "read", "(", ")", "pyproject", "=", "tomlkit", ".", "parse", "(", "content", ")", "filterwarnings", "=", "pyproject", ".", "get", "(", "\"", "tool", "\"", ",", "{", "}", ")", ".", "get", "(", "\"", "pytest", "\"", ",", "{", "}", ")", ".", "get", "(", "\"", "ini_options", "\"", ",", "{", "}", ")", ".", "get", "(", "\"", "filterwarnings", "\"", ",", "[", "]", ")", "if", "not", "filterwarnings", ":", "return", "filterwarnings", "=", "[", "wrn", "for", "wrn", "in", "filterwarnings", "if", "not", "any", "(", "f", "\"", "error", ":", ":", "{", "err", "}", "\"", "in", "wrn", "for", "err", "in", "errors", ")", "]", "pyproject", "[", "\"", "tool", "\"", "]", "[", "\"", "pytest", "\"", "]", "[", "\"", "ini_options", "\"", "]", "[", "\"", "filterwarnings", "\"", "]", "=", "filterwarnings", "with", "open", "(", "pyproject_toml", ",", "\"", "w", "\"", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fopen", ":", "fopen", ".", "write", "(", "tomlkit", ".", "dumps", "(", "pyproject", ")", ")"], "docstring": "Prune pytest warnings as errors from the pyproject.toml file.", "docstring_tokens": ["prune", "pytest", "warnings", "as", "errors", "from", "the", "pyproject", "toml", "file"], "docstring_summary": "Prune pytest warnings as errors from the pyproject.toml file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/.actions\\assistant.py", "partition": "train", "function_type": "class_method", "class_name": "AssistantCLI", "start_line": 462, "end_line": 478, "hash": "262d21448c5b08a619cecf7d18e92e71", "complexity": 7, "parameters": ["pyproject_toml", "errors", "\"DeprecationWarning\")"]}
{"repo": "pytorch-lightning", "path": "docs\\source-pytorch\\conf.py", "func_name": "package_list_from_file", "original_string": "def package_list_from_file(file):\r\n    \"\"\"List up package name (not containing version and extras) from a package list file.\"\"\"\r\n    mocked_packages = []\r\n    with open(file) as fp:\r\n        for ln in fp.readlines():\r\n            # Example: `tqdm>=4.41.0` => `tqdm`\r\n            # `[` is for package with extras\r\n            found = [ln.index(ch) for ch in list(\",=<>#[\") if ch in ln]\r\n            pkg = ln[: min(found)] if found else ln\r\n            if pkg.rstrip():\r\n                mocked_packages.append(pkg.rstrip())\r\n    return mocked_packages", "language": "python", "code": "def package_list_from_file(file):\r\n    \"\"\"List up package name (not containing version and extras) from a package list file.\"\"\"\r\n    mocked_packages = []\r\n    with open(file) as fp:\r\n        for ln in fp.readlines():\r\n            # Example: `tqdm>=4.41.0` => `tqdm`\r\n            # `[` is for package with extras\r\n            found = [ln.index(ch) for ch in list(\",=<>#[\") if ch in ln]\r\n            pkg = ln[: min(found)] if found else ln\r\n            if pkg.rstrip():\r\n                mocked_packages.append(pkg.rstrip())\r\n    return mocked_packages", "code_tokens": ["def", "package_list_from_file", "(", "file", ")", ":", "\"", "\"", "\"", "List", "up", "package", "name", "(", "not", "containing", "version", "and", "extras", ")", "from", "a", "package", "list", "file", ".", "\"", "\"", "\"", "mocked_packages", "=", "[", "]", "with", "open", "(", "file", ")", "as", "fp", ":", "for", "ln", "in", "fp", ".", "readlines", "(", ")", ":", "found", "=", "[", "ln", ".", "index", "(", "ch", ")", "for", "ch", "in", "list", "(", "\"", ",", "=", "<", ">", "pkg", "=", "ln", "[", ":", "min", "(", "found", ")", "]", "if", "found", "else", "ln", "if", "pkg", ".", "rstrip", "(", ")", ":", "mocked_packages", ".", "append", "(", "pkg", ".", "rstrip", "(", ")", ")", "return", "mocked_packages"], "docstring": "List up package name (not containing version and extras) from a package list file.", "docstring_tokens": ["list", "up", "package", "name", "not", "containing", "version", "and", "extras", "from", "a", "package", "list", "file"], "docstring_summary": "List up package name (not containing version and extras) from a package list file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/docs\\source-pytorch\\conf.py", "partition": "train", "function_type": "function", "start_line": 545, "end_line": 556, "hash": "dc734e60cda9c82d8febdd77a234a635", "complexity": 7, "parameters": ["file"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency", "language": "python", "code": "def __init__(\r\n        self,\r\n        accelerator: Union[str, Accelerator] = \"auto\",\r\n        strategy: Union[str, Strategy] = \"auto\",\r\n        devices: Union[list[int], str, int] = \"auto\",\r\n        precision: Union[str, int] = \"32-true\",\r\n        plugins: Optional[Union[str, Any]] = None,\r\n        callbacks: Optional[Union[list[Any], Any]] = None,\r\n        loggers: Optional[Union[Logger, list[Logger]]] = None,\r\n        max_epochs: Optional[int] = 1000,\r\n        max_steps: Optional[int] = None,\r\n        grad_accum_steps: int = 1,\r\n        limit_train_batches: Union[int, float] = float(\"inf\"),\r\n        limit_val_batches: Union[int, float] = float(\"inf\"),\r\n        validation_frequency: int = 1,\r\n        use_distributed_sampler: bool = True,\r\n        checkpoint_dir: str = \"./checkpoints\",\r\n        checkpoint_frequency: int = 1,\r\n    ) -> None:\r\n        \"\"\"Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!\r\n\r\n        \"\"\"\r\n\r\n        self.fabric = L.Fabric(\r\n            accelerator=accelerator,\r\n            strategy=strategy,\r\n            devices=devices,\r\n            precision=precision,\r\n            plugins=plugins,\r\n            callbacks=callbacks,\r\n            loggers=loggers,\r\n        )\r\n        self.global_step = 0\r\n        self.grad_accum_steps: int = grad_accum_steps\r\n        self.current_epoch = 0\r\n\r\n        self.max_epochs = max_epochs\r\n        self.max_steps = max_steps\r\n        self.should_stop = False\r\n\r\n        # ensures limit_X_batches is either int or inf\r\n        if not isinstance(limit_train_batches, int):\r\n            assert limit_train_batches == float(\"inf\")\r\n\r\n        if not isinstance(limit_val_batches, int):\r\n            assert limit_val_batches == float(\"inf\")\r\n\r\n        self.limit_train_batches = limit_train_batches\r\n        self.limit_val_batches = limit_val_batches\r\n        self.validation_frequency = validation_frequency\r\n        self.use_distributed_sampler = use_distributed_sampler\r\n        self._current_train_return: Union[torch.Tensor, Mapping[str, Any]] = {}\r\n        self._current_val_return: Optional[Union[torch.Tensor, Mapping[str, Any]]] = {}\r\n\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.checkpoint_frequency = checkpoint_frequency", "code_tokens": ["def", "__init__", "(", "self", ",", "accelerator", ":", "Union", "[", "str", ",", "Accelerator", "]", "=", "\"", "auto", "\"", ",", "strategy", ":", "Union", "[", "str", ",", "Strategy", "]", "=", "\"", "auto", "\"", ",", "devices", ":", "Union", "[", "list", "[", "int", "]", ",", "str", ",", "int", "]", "=", "\"", "auto", "\"", ",", "precision", ":", "Union", "[", "str", ",", "int", "]", "=", "\"", "32", "-", "true", "\"", ",", "plugins", ":", "Optional", "[", "Union", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "callbacks", ":", "Optional", "[", "Union", "[", "list", "[", "Any", "]", ",", "Any", "]", "]", "=", "None", ",", "loggers", ":", "Optional", "[", "Union", "[", "Logger", ",", "list", "[", "Logger", "]", "]", "]", "=", "None", ",", "max_epochs", ":", "Optional", "[", "int", "]", "=", "1000", ",", "max_steps", ":", "Optional", "[", "int", "]", "=", "None", ",", "grad_accum_steps", ":", "int", "=", "1", ",", "limit_train_batches", ":", "Union", "[", "int", ",", "float", "]", "=", "float", "(", "\"", "inf", "\"", ")", ",", "limit_val_batches", ":", "Union", "[", "int", ",", "float", "]", "=", "float", "(", "\"", "inf", "\"", ")", ",", "validation_frequency", ":", "int", "=", "1", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "checkpoint_dir", ":", "str", "=", "\"", ".", "/", "checkpoints", "\"", ",", "checkpoint_frequency", ":", "int", "=", "1", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Exemplary", "Trainer", "with", "Fabric", ".", "This", "is", "a", "very", "simple", "trainer", "focused", "on", "readability", "but", "with", "reduced", "featureset", ".", "As", "a", "trainer", "with", "more", "included", "features", ",", "we", "recommend", "using", "the", ":", "class", ":", "`", "lightning", ".", "pytorch", ".", "Trainer", "`", ".", "Args", ":", "accelerator", ":", "The", "hardware", "to", "run", "on", ".", "Possible", "choices", "are", ":", "`", "`", "\"", "cpu", "\"", "`", "`", ",", "`", "`", "\"", "cuda", "\"", "`", "`", ",", "`", "`", "\"", "mps", "\"", "`", "`", ",", "`", "`", "\"", "gpu", "\"", "`", "`", ",", "`", "`", "\"", "tpu", "\"", "`", "`", ",", "`", "`", "\"", "auto", "\"", "`", "`", ".", "strategy", ":", "Strategy", "for", "how", "to", "run", "across", "multiple", "devices", ".", "Possible", "choices", "are", ":", "`", "`", "\"", "dp", "\"", "`", "`", ",", "`", "`", "\"", "ddp", "\"", "`", "`", ",", "`", "`", "\"", "ddp_spawn", "\"", "`", "`", ",", "`", "`", "\"", "deepspeed", "\"", "`", "`", ",", "`", "`", "\"", "fsdp", "\"", "`", "`", ".", "devices", ":", "Number", "of", "devices", "to", "train", "on", "(", "`", "`", "int", "`", "`", ")", ",", "which", "GPUs", "to", "train", "on", "(", "`", "`", "list", "`", "`", "or", "`", "`", "str", "`", "`", ")", ",", "or", "`", "`", "\"", "auto", "\"", "`", "`", ".", "The", "value", "applies", "per", "node", ".", "precision", ":", "Double", "precision", "(", "`", "`", "\"", "64", "\"", "`", "`", ")", ",", "full", "precision", "(", "`", "`", "\"", "32", "\"", "`", "`", ")", ",", "half", "precision", "AMP", "(", "`", "`", "\"", "16", "-", "mixed", "\"", "`", "`", ")", ",", "or", "bfloat16", "precision", "AMP", "(", "`", "`", "\"", "bf16", "-", "mixed", "\"", "`", "`", ")", ".", "plugins", ":", "One", "or", "several", "custom", "plugins", "callbacks", ":", "A", "single", "callback", "or", "a", "list", "of", "callbacks", ".", "The", "following", "hooks", "are", "supported", ":", "-", "on_train_epoch_start", "-", "on", "train_epoch_end", "-", "on_train_batch_start", "-", "on_train_batch_end", "-", "on_before_backward", "-", "on_after_backward", "-", "on_before_zero_grad", "-", "on_before_optimizer_step", "-", "on_validation_model_eval", "-", "on_validation_model_train", "-", "on_validation_epoch_start", "-", "on_validation_epoch_end", "-", "on_validation_batch_start", "-", "on_validation_batch_end", "loggers", ":", "A", "single", "logger", "or", "a", "list", "of", "loggers", ".", "See", ":", "meth", ":", "`", "~", "lightning", ".", "fabric", ".", "fabric", ".", "Fabric", ".", "log", "`", "for", "more", "information", ".", "max_epochs", ":", "The", "maximum", "number", "of", "epochs", "to", "train", "max_steps", ":", "The", "maximum", "number", "of", "(", "optimizer", ")", "steps", "to", "train", "grad_accum_steps", ":", "How", "many", "batches", "to", "process", "before", "each", "optimizer", "step", "limit_train_batches", ":", "Limits", "the", "number", "of", "train", "batches", "per", "epoch", "If", "greater", "than", "number", "of", "batches", "in", "the", "dataloader", ",", "this", "has", "no", "effect", ".", "limit_val_batches", ":", "Limits", "the", "number", "of", "validation", "batches", "per", "epoch", ".", "If", "greater", "than", "number", "of", "batches", "in", "the", "dataloader", ",", "this", "has", "no", "effect", ".", "validation_frequency", ":", "How", "many", "epochs", "to", "run", "before", "each", "validation", "epoch", ".", "use_distributed_sampler", ":", "Wraps", "the", "sampler", "of", "each", "dataloader", "with", "a", "respective", "distributed", "-", "aware", "sampler", "in", "case", "of", "distributed", "training", ".", "checkpoint_dir", ":", "Directory", "to", "store", "checkpoints", "to", ".", "checkpoint_frequency", ":", "How", "many", "epochs", "to", "run", "before", "each", "checkpoint", "is", "written", ".", "Warning", ":", "callbacks", "written", "for", "the", "lightning", "trainer", "(", "especially", "making", "assumptions", "on", "the", "trainer", ")", ",", "won", "'", "t", "work", "!", "\"", "\"", "\"", "self", ".", "fabric", "=", "L", ".", "Fabric", "(", "accelerator", "=", "accelerator", ",", "strategy", "=", "strategy", ",", "devices", "=", "devices", ",", "precision", "=", "precision", ",", "plugins", "=", "plugins", ",", "callbacks", "=", "callbacks", ",", "loggers", "=", "loggers", ",", ")", "self", ".", "global_step", "=", "0", "self", ".", "grad_accum_steps", ":", "int", "=", "grad_accum_steps", "self", ".", "current_epoch", "=", "0", "self", ".", "max_epochs", "=", "max_epochs", "self", ".", "max_steps", "=", "max_steps", "self", ".", "should_stop", "=", "False", "if", "not", "isinstance", "(", "limit_train_batches", ",", "int", ")", ":", "assert", "limit_train_batches", "=", "=", "float", "(", "\"", "inf", "\"", ")", "if", "not", "isinstance", "(", "limit_val_batches", ",", "int", ")", ":", "assert", "limit_val_batches", "=", "=", "float", "(", "\"", "inf", "\"", ")", "self", ".", "limit_train_batches", "=", "limit_train_batches", "self", ".", "limit_val_batches", "=", "limit_val_batches", "self", ".", "validation_frequency", "=", "validation_frequency", "self", ".", "use_distributed_sampler", "=", "use_distributed_sampler", "self", ".", "_current_train_return", ":", "Union", "[", "torch", ".", "Tensor", ",", "Mapping", "[", "str", ",", "Any", "]", "]", "=", "{", "}", "self", ".", "_current_val_return", ":", "Optional", "[", "Union", "[", "torch", ".", "Tensor", ",", "Mapping", "[", "str", ",", "Any", "]", "]", "]", "=", "{", "}", "self", ".", "checkpoint_dir", "=", "checkpoint_dir", "self", ".", "checkpoint_frequency", "=", "checkpoint_frequency"], "docstring": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced\r\n        featureset. As a trainer with more included features, we recommend using the\r\n        :class:`lightning.pytorch.Trainer`.\r\n\r\n        Args:\r\n            accelerator: The hardware to run on. Possible choices are:\r\n                ``\"cpu\"``, ``\"cuda\"``, ``\"mps\"``, ``\"gpu\"``, ``\"tpu\"``, ``\"auto\"``.\r\n            strategy: Strategy for how to run across multiple devices. Possible choices are:\r\n                ``\"dp\"``, ``\"ddp\"``, ``\"ddp_spawn\"``, ``\"deepspeed\"``, ``\"fsdp\"``.\r\n            devices: Number of devices to train on (``int``),\r\n                which GPUs to train on (``list`` or ``str``), or ``\"auto\"``.\r\n                The value applies per node.\r\n            precision: Double precision (``\"64\"``), full precision (``\"32\"``), half precision AMP (``\"16-mixed\"``),\r\n                or bfloat16 precision AMP (``\"bf16-mixed\"``).\r\n            plugins: One or several custom plugins\r\n            callbacks: A single callback or a list of callbacks. The following hooks are supported:\r\n                - on_train_epoch_start\r\n                - on train_epoch_end\r\n                - on_train_batch_start\r\n                - on_train_batch_end\r\n                - on_before_backward\r\n                - on_after_backward\r\n                - on_before_zero_grad\r\n                - on_before_optimizer_step\r\n                - on_validation_model_eval\r\n                - on_validation_model_train\r\n                - on_validation_epoch_start\r\n                - on_validation_epoch_end\r\n                - on_validation_batch_start\r\n                - on_validation_batch_end\r\n\r\n            loggers: A single logger or a list of loggers. See :meth:`~lightning.fabric.fabric.Fabric.log` for more\r\n                information.\r\n\r\n            max_epochs: The maximum number of epochs to train\r\n            max_steps: The maximum number of (optimizer) steps to train\r\n            grad_accum_steps: How many batches to process before each optimizer step\r\n            limit_train_batches: Limits the number of train batches per epoch\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            limit_val_batches: Limits the number of validation batches per epoch.\r\n                If greater than number of batches in the dataloader, this has no effect.\r\n            validation_frequency: How many epochs to run before each validation epoch.\r\n            use_distributed_sampler: Wraps the sampler of each dataloader with a respective distributed-aware sampler\r\n                in case of distributed training.\r\n            checkpoint_dir: Directory to store checkpoints to.\r\n            checkpoint_frequency: How many epochs to run before each checkpoint is written.\r\n\r\n        Warning:\r\n            callbacks written for the lightning trainer (especially making assumptions on the trainer), won't work!", "docstring_tokens": ["exemplary", "trainer", "with", "fabric", "this", "is", "a", "very", "simple", "trainer", "focused", "on", "readability", "but", "with", "reduced", "featureset", "as", "a", "trainer", "with", "more", "included", "features", "we", "recommend", "using", "the", "class", "lightning", "pytorch", "trainer", "args", "accelerator", "the", "hardware", "to", "run", "on", "possible", "choices", "are", "cpu", "cuda", "mps", "gpu", "tpu", "auto", "strategy", "strategy", "for", "how", "to", "run", "across", "multiple", "devices", "possible", "choices", "are", "dp", "ddp", "ddp_spawn", "deepspeed", "fsdp", "devices", "number", "of", "devices", "to", "train", "on", "int", "which", "gpus", "to", "train", "on", "list", "or", "str", "or", "auto", "the", "value", "applies", "per", "node", "precision", "double", "precision", "64", "full", "precision", "32", "half", "precision", "amp", "16", "mixed", "or", "bfloat16", "precision", "amp", "bf16", "mixed", "plugins", "one", "or", "several", "custom", "plugins", "callbacks", "a", "single", "callback", "or", "a", "list", "of", "callbacks", "the", "following", "hooks", "are", "supported", "on_train_epoch_start", "on", "train_epoch_end", "on_train_batch_start", "on_train_batch_end", "on_before_backward", "on_after_backward", "on_before_zero_grad", "on_before_optimizer_step", "on_validation_model_eval", "on_validation_model_train", "on_validation_epoch_start", "on_validation_epoch_end", "on_validation_batch_start", "on_validation_batch_end", "loggers", "a", "single", "logger", "or", "a", "list", "of", "loggers", "see", "meth", "lightning", "fabric", "fabric", "fabric", "log", "for", "more", "information", "max_epochs", "the", "maximum", "number", "of", "epochs", "to", "train", "max_steps", "the", "maximum", "number", "of", "optimizer", "steps", "to", "train", "grad_accum_steps", "how", "many", "batches", "to", "process", "before", "each", "optimizer", "step", "limit_train_batches", "limits", "the", "number", "of", "train", "batches", "per", "epoch", "if", "greater", "than", "number", "of", "batches", "in", "the", "dataloader", "this", "has", "no", "effect", "limit_val_batches", "limits", "the", "number", "of", "validation", "batches", "per", "epoch", "if", "greater", "than", "number", "of", "batches", "in", "the", "dataloader", "this", "has", "no", "effect", "validation_frequency", "how", "many", "epochs", "to", "run", "before", "each", "validation", "epoch", "use_distributed_sampler", "wraps", "the", "sampler", "of", "each", "dataloader", "with", "a", "respective", "distributed", "aware", "sampler", "in", "case", "of", "distributed", "training", "checkpoint_dir", "directory", "to", "store", "checkpoints", "to", "checkpoint_frequency", "how", "many", "epochs", "to", "run", "before", "each", "checkpoint", "is", "written", "warning", "callbacks", "written", "for", "the", "lightning", "trainer", "especially", "making", "assumptions", "on", "the", "trainer", "won", "t", "work"], "docstring_summary": "Exemplary Trainer with Fabric. This is a very simple trainer focused on readability but with reduced", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 18, "end_line": 121, "hash": "0bbe0adf3ec60b2f294e15b93da70f56", "complexity": 3, "parameters": ["accelerator", "Accelerator]", "strategy", "Strategy]", "devices", "str", "int]", "precision", "int]", "plugins", "Any]]", "callbacks", "Any]]", "loggers", "list[Logger]]]", "max_epochs", "max_steps", "grad_accum_steps", "limit_train_batches", "float]", "limit_val_batches", "float]", "validation_frequency", "use_distributed_sampler", "checkpoint_dir", "checkpoint_frequency"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "fit", "original_string": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False", "language": "python", "code": "def fit(\r\n        self,\r\n        model: L.LightningModule,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        val_loader: torch.utils.data.DataLoader,\r\n        ckpt_path: Optional[str] = None,\r\n    ):\r\n        \"\"\"The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.\r\n\r\n        \"\"\"\r\n        self.fabric.launch()\r\n\r\n        # setup dataloaders\r\n        train_loader = self.fabric.setup_dataloaders(train_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n        if val_loader is not None:\r\n            val_loader = self.fabric.setup_dataloaders(val_loader, use_distributed_sampler=self.use_distributed_sampler)\r\n\r\n        # setup model and optimizer\r\n        if isinstance(self.fabric.strategy, L.fabric.strategies.fsdp.FSDPStrategy):\r\n            # currently, there is no way to support fsdp with model.configure_optimizers in fabric\r\n            # as it would require fabric to hold a reference to the model, which we don't want to.\r\n            raise NotImplementedError(\"BYOT currently does not support FSDP\")\r\n\r\n        optimizer, scheduler_cfg = self._parse_optimizers_schedulers(model.configure_optimizers())\r\n        assert optimizer is not None\r\n        model, optimizer = self.fabric.setup(model, optimizer)\r\n\r\n        # assemble state (current epoch and global step will be added in save)\r\n        state = {\"model\": model, \"optim\": optimizer, \"scheduler\": scheduler_cfg}\r\n\r\n        # load last checkpoint if available\r\n        if ckpt_path is not None and os.path.isdir(ckpt_path):\r\n            latest_checkpoint_path = self.get_latest_checkpoint(self.checkpoint_dir)\r\n            if latest_checkpoint_path is not None:\r\n                self.load(state, latest_checkpoint_path)\r\n\r\n                # check if we even need to train here\r\n                if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                    self.should_stop = True\r\n\r\n        while not self.should_stop:\r\n            self.train_loop(\r\n                model, optimizer, train_loader, limit_batches=self.limit_train_batches, scheduler_cfg=scheduler_cfg\r\n            )\r\n\r\n            if self.should_validate:\r\n                self.val_loop(model, val_loader, limit_batches=self.limit_val_batches)\r\n\r\n            self.step_scheduler(model, scheduler_cfg, level=\"epoch\", current_value=self.current_epoch)\r\n\r\n            self.current_epoch += 1\r\n\r\n            # stopping condition on epoch level\r\n            if self.max_epochs is not None and self.current_epoch >= self.max_epochs:\r\n                self.should_stop = True\r\n\r\n            self.save(state)\r\n\r\n        # reset for next fit call\r\n        self.should_stop = False", "code_tokens": ["def", "fit", "(", "self", ",", "model", ":", "L", ".", "LightningModule", ",", "train_loader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "val_loader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "ckpt_path", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", ":", "\"", "\"", "\"", "The", "main", "entrypoint", "of", "the", "trainer", ",", "triggering", "the", "actual", "training", ".", "Args", ":", "model", ":", "the", "LightningModule", "to", "train", ".", "Can", "have", "the", "same", "hooks", "as", ":", "attr", ":", "`", "callbacks", "`", "(", "see", ":", "meth", ":", "`", "MyCustomTrainer", ".", "__init__", "`", ")", ".", "train_loader", ":", "the", "training", "dataloader", ".", "Has", "to", "be", "an", "iterable", "returning", "batches", ".", "val_loader", ":", "the", "validation", "dataloader", ".", "Has", "to", "be", "an", "iterable", "returning", "batches", ".", "If", "not", "specified", ",", "no", "validation", "will", "run", ".", "ckpt_path", ":", "Path", "to", "previous", "checkpoints", "to", "resume", "training", "from", ".", "If", "specified", ",", "will", "always", "look", "for", "the", "latest", "checkpoint", "within", "the", "given", "directory", ".", "\"", "\"", "\"", "self", ".", "fabric", ".", "launch", "(", ")", "train_loader", "=", "self", ".", "fabric", ".", "setup_dataloaders", "(", "train_loader", ",", "use_distributed_sampler", "=", "self", ".", "use_distributed_sampler", ")", "if", "val_loader", "is", "not", "None", ":", "val_loader", "=", "self", ".", "fabric", ".", "setup_dataloaders", "(", "val_loader", ",", "use_distributed_sampler", "=", "self", ".", "use_distributed_sampler", ")", "if", "isinstance", "(", "self", ".", "fabric", ".", "strategy", ",", "L", ".", "fabric", ".", "strategies", ".", "fsdp", ".", "FSDPStrategy", ")", ":", "raise", "NotImplementedError", "(", "\"", "BYOT", "currently", "does", "not", "support", "FSDP", "\"", ")", "optimizer", ",", "scheduler_cfg", "=", "self", ".", "_parse_optimizers_schedulers", "(", "model", ".", "configure_optimizers", "(", ")", ")", "assert", "optimizer", "is", "not", "None", "model", ",", "optimizer", "=", "self", ".", "fabric", ".", "setup", "(", "model", ",", "optimizer", ")", "state", "=", "{", "\"", "model", "\"", ":", "model", ",", "\"", "optim", "\"", ":", "optimizer", ",", "\"", "scheduler", "\"", ":", "scheduler_cfg", "}", "if", "ckpt_path", "is", "not", "None", "and", "os", ".", "path", ".", "isdir", "(", "ckpt_path", ")", ":", "latest_checkpoint_path", "=", "self", ".", "get_latest_checkpoint", "(", "self", ".", "checkpoint_dir", ")", "if", "latest_checkpoint_path", "is", "not", "None", ":", "self", ".", "load", "(", "state", ",", "latest_checkpoint_path", ")", "if", "self", ".", "max_epochs", "is", "not", "None", "and", "self", ".", "current_epoch", ">", "=", "self", ".", "max_epochs", ":", "self", ".", "should_stop", "=", "True", "while", "not", "self", ".", "should_stop", ":", "self", ".", "train_loop", "(", "model", ",", "optimizer", ",", "train_loader", ",", "limit_batches", "=", "self", ".", "limit_train_batches", ",", "scheduler_cfg", "=", "scheduler_cfg", ")", "if", "self", ".", "should_validate", ":", "self", ".", "val_loop", "(", "model", ",", "val_loader", ",", "limit_batches", "=", "self", ".", "limit_val_batches", ")", "self", ".", "step_scheduler", "(", "model", ",", "scheduler_cfg", ",", "level", "=", "\"", "epoch", "\"", ",", "current_value", "=", "self", ".", "current_epoch", ")", "self", ".", "current_epoch", "+", "=", "1", "if", "self", ".", "max_epochs", "is", "not", "None", "and", "self", ".", "current_epoch", ">", "=", "self", ".", "max_epochs", ":", "self", ".", "should_stop", "=", "True", "self", ".", "save", "(", "state", ")", "self", ".", "should_stop", "=", "False"], "docstring": "The main entrypoint of the trainer, triggering the actual training.\r\n\r\n        Args:\r\n            model: the LightningModule to train.\r\n                Can have the same hooks as :attr:`callbacks` (see :meth:`MyCustomTrainer.__init__`).\r\n            train_loader: the training dataloader. Has to be an iterable returning batches.\r\n            val_loader: the validation dataloader. Has to be an iterable returning batches.\r\n                If not specified, no validation will run.\r\n            ckpt_path: Path to previous checkpoints to resume training from.\r\n                If specified, will always look for the latest checkpoint within the given directory.", "docstring_tokens": ["the", "main", "entrypoint", "of", "the", "trainer", "triggering", "the", "actual", "training", "args", "model", "the", "lightningmodule", "to", "train", "can", "have", "the", "same", "hooks", "as", "attr", "callbacks", "see", "meth", "mycustomtrainer", "__init__", "train_loader", "the", "training", "dataloader", "has", "to", "be", "an", "iterable", "returning", "batches", "val_loader", "the", "validation", "dataloader", "has", "to", "be", "an", "iterable", "returning", "batches", "if", "not", "specified", "no", "validation", "will", "run", "ckpt_path", "path", "to", "previous", "checkpoints", "to", "resume", "training", "from", "if", "specified", "will", "always", "look", "for", "the", "latest", "checkpoint", "within", "the", "given", "directory"], "docstring_summary": "The main entrypoint of the trainer, triggering the actual training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 123, "end_line": 191, "hash": "f92144ac7f0974541f275cce4fcc2e5f", "complexity": 12, "parameters": ["model", "train_loader", "val_loader", "ckpt_path"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "train_loop", "original_string": "def train_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        optimizer: torch.optim.Optimizer,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]] = None,\r\n    ):\r\n        \"\"\"The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.\r\n\r\n        \"\"\"\r\n        self.fabric.call(\"on_train_epoch_start\")\r\n        iterable = self.progbar_wrapper(\r\n            train_loader, total=min(len(train_loader), limit_batches), desc=f\"Epoch {self.current_epoch}\"\r\n        )\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_train_batch_start\", batch, batch_idx)\r\n\r\n            # check if optimizer should step in gradient accumulation\r\n            should_optim_step = self.global_step % self.grad_accum_steps == 0\r\n            if should_optim_step:\r\n                # currently only supports a single optimizer\r\n                self.fabric.call(\"on_before_optimizer_step\", optimizer)\r\n\r\n                # optimizer step runs train step internally through closure\r\n                optimizer.step(partial(self.training_step, model=model, batch=batch, batch_idx=batch_idx))\r\n                self.fabric.call(\"on_before_zero_grad\", optimizer)\r\n\r\n                optimizer.zero_grad()\r\n\r\n            else:\r\n                # gradient accumulation -> no optimizer step\r\n                self.training_step(model=model, batch=batch, batch_idx=batch_idx)\r\n\r\n            self.fabric.call(\"on_train_batch_end\", self._current_train_return, batch, batch_idx)\r\n\r\n            # this guard ensures, we only step the scheduler once per global step\r\n            if should_optim_step:\r\n                self.step_scheduler(model, scheduler_cfg, level=\"step\", current_value=self.global_step)\r\n\r\n            # add output values to progress bar\r\n            self._format_iterable(iterable, self._current_train_return, \"train\")\r\n\r\n            # only increase global step if optimizer stepped\r\n            self.global_step += int(should_optim_step)\r\n\r\n            # stopping criterion on step level\r\n            if self.max_steps is not None and self.global_step >= self.max_steps:\r\n                self.should_stop = True\r\n                break\r\n\r\n        self.fabric.call(\"on_train_epoch_end\")", "language": "python", "code": "def train_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        optimizer: torch.optim.Optimizer,\r\n        train_loader: torch.utils.data.DataLoader,\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]] = None,\r\n    ):\r\n        \"\"\"The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.\r\n\r\n        \"\"\"\r\n        self.fabric.call(\"on_train_epoch_start\")\r\n        iterable = self.progbar_wrapper(\r\n            train_loader, total=min(len(train_loader), limit_batches), desc=f\"Epoch {self.current_epoch}\"\r\n        )\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_train_batch_start\", batch, batch_idx)\r\n\r\n            # check if optimizer should step in gradient accumulation\r\n            should_optim_step = self.global_step % self.grad_accum_steps == 0\r\n            if should_optim_step:\r\n                # currently only supports a single optimizer\r\n                self.fabric.call(\"on_before_optimizer_step\", optimizer)\r\n\r\n                # optimizer step runs train step internally through closure\r\n                optimizer.step(partial(self.training_step, model=model, batch=batch, batch_idx=batch_idx))\r\n                self.fabric.call(\"on_before_zero_grad\", optimizer)\r\n\r\n                optimizer.zero_grad()\r\n\r\n            else:\r\n                # gradient accumulation -> no optimizer step\r\n                self.training_step(model=model, batch=batch, batch_idx=batch_idx)\r\n\r\n            self.fabric.call(\"on_train_batch_end\", self._current_train_return, batch, batch_idx)\r\n\r\n            # this guard ensures, we only step the scheduler once per global step\r\n            if should_optim_step:\r\n                self.step_scheduler(model, scheduler_cfg, level=\"step\", current_value=self.global_step)\r\n\r\n            # add output values to progress bar\r\n            self._format_iterable(iterable, self._current_train_return, \"train\")\r\n\r\n            # only increase global step if optimizer stepped\r\n            self.global_step += int(should_optim_step)\r\n\r\n            # stopping criterion on step level\r\n            if self.max_steps is not None and self.global_step >= self.max_steps:\r\n                self.should_stop = True\r\n                break\r\n\r\n        self.fabric.call(\"on_train_epoch_end\")", "code_tokens": ["def", "train_loop", "(", "self", ",", "model", ":", "L", ".", "LightningModule", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "train_loader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "limit_batches", ":", "Union", "[", "int", ",", "float", "]", "=", "float", "(", "\"", "inf", "\"", ")", ",", "scheduler_cfg", ":", "Optional", "[", "Mapping", "[", "str", ",", "Union", "[", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "LRScheduler", ",", "bool", ",", "str", ",", "int", "]", "]", "]", "=", "None", ",", ")", ":", "\"", "\"", "\"", "The", "training", "loop", "running", "a", "single", "training", "epoch", ".", "Args", ":", "model", ":", "the", "LightningModule", "to", "train", "optimizer", ":", "the", "optimizer", ",", "optimizing", "the", "LightningModule", ".", "train_loader", ":", "The", "dataloader", "yielding", "the", "training", "batches", ".", "limit_batches", ":", "Limits", "the", "batches", "during", "this", "training", "epoch", ".", "If", "greater", "than", "the", "number", "of", "batches", "in", "the", "`", "`", "train_loader", "`", "`", ",", "this", "has", "no", "effect", ".", "scheduler_cfg", ":", "The", "learning", "rate", "scheduler", "configuration", ".", "Have", "a", "look", "at", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "configure_optimizers", "`", "for", "supported", "values", ".", "\"", "\"", "\"", "self", ".", "fabric", ".", "call", "(", "\"", "on_train_epoch_start", "\"", ")", "iterable", "=", "self", ".", "progbar_wrapper", "(", "train_loader", ",", "total", "=", "min", "(", "len", "(", "train_loader", ")", ",", "limit_batches", ")", ",", "desc", "=", "f", "\"", "Epoch", "{", "self", ".", "current_epoch", "}", "\"", ")", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "iterable", ")", ":", "if", "self", ".", "should_stop", "or", "batch_idx", ">", "=", "limit_batches", ":", "break", "self", ".", "fabric", ".", "call", "(", "\"", "on_train_batch_start", "\"", ",", "batch", ",", "batch_idx", ")", "should_optim_step", "=", "self", ".", "global_step", "%", "self", ".", "grad_accum_steps", "=", "=", "0", "if", "should_optim_step", ":", "self", ".", "fabric", ".", "call", "(", "\"", "on_before_optimizer_step", "\"", ",", "optimizer", ")", "optimizer", ".", "step", "(", "partial", "(", "self", ".", "training_step", ",", "model", "=", "model", ",", "batch", "=", "batch", ",", "batch_idx", "=", "batch_idx", ")", ")", "self", ".", "fabric", ".", "call", "(", "\"", "on_before_zero_grad", "\"", ",", "optimizer", ")", "optimizer", ".", "zero_grad", "(", ")", "else", ":", "self", ".", "training_step", "(", "model", "=", "model", ",", "batch", "=", "batch", ",", "batch_idx", "=", "batch_idx", ")", "self", ".", "fabric", ".", "call", "(", "\"", "on_train_batch_end", "\"", ",", "self", ".", "_current_train_return", ",", "batch", ",", "batch_idx", ")", "if", "should_optim_step", ":", "self", ".", "step_scheduler", "(", "model", ",", "scheduler_cfg", ",", "level", "=", "\"", "step", "\"", ",", "current_value", "=", "self", ".", "global_step", ")", "self", ".", "_format_iterable", "(", "iterable", ",", "self", ".", "_current_train_return", ",", "\"", "train", "\"", ")", "self", ".", "global_step", "+", "=", "int", "(", "should_optim_step", ")", "if", "self", ".", "max_steps", "is", "not", "None", "and", "self", ".", "global_step", ">", "=", "self", ".", "max_steps", ":", "self", ".", "should_stop", "=", "True", "break", "self", ".", "fabric", ".", "call", "(", "\"", "on_train_epoch_end", "\"", ")"], "docstring": "The training loop running a single training epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to train\r\n            optimizer: the optimizer, optimizing the LightningModule.\r\n            train_loader: The dataloader yielding the training batches.\r\n            limit_batches: Limits the batches during this training epoch.\r\n                If greater than the number of batches in the ``train_loader``, this has no effect.\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`\r\n                for supported values.", "docstring_tokens": ["the", "training", "loop", "running", "a", "single", "training", "epoch", "args", "model", "the", "lightningmodule", "to", "train", "optimizer", "the", "optimizer", "optimizing", "the", "lightningmodule", "train_loader", "the", "dataloader", "yielding", "the", "training", "batches", "limit_batches", "limits", "the", "batches", "during", "this", "training", "epoch", "if", "greater", "than", "the", "number", "of", "batches", "in", "the", "train_loader", "this", "has", "no", "effect", "scheduler_cfg", "the", "learning", "rate", "scheduler", "configuration", "have", "a", "look", "at", "meth", "lightning", "pytorch", "core", "lightningmodule", "configure_optimizers", "for", "supported", "values"], "docstring_summary": "The training loop running a single training epoch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 193, "end_line": 259, "hash": "7dd4ca71b837d6f688303a3e5eb5fe82", "complexity": 8, "parameters": ["model", "optimizer", "train_loader", "limit_batches", "float]", "scheduler_cfg", "Union[L.fabric.utilities.types.LRScheduler", "bool", "str", "int]]]"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "val_loop", "original_string": "def val_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        val_loader: Optional[torch.utils.data.DataLoader],\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n    ):\r\n        \"\"\"The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.\r\n\r\n        \"\"\"\r\n        # no validation if val_loader wasn't passed\r\n        if val_loader is None:\r\n            return\r\n\r\n        # no validation but warning if val_loader was passed, but validation_step not implemented\r\n        if val_loader is not None and not is_overridden(\"validation_step\", _unwrap_objects(model)):\r\n            L.fabric.utilities.rank_zero_warn(\r\n                \"Your LightningModule does not have a validation_step implemented, \"\r\n                \"but you passed a validation dataloder. Skipping Validation.\"\r\n            )\r\n            return\r\n\r\n        if not is_overridden(\"on_validation_model_eval\", _unwrap_objects(model)):\r\n            model.eval()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_eval\")  # calls `model.eval()`\r\n\r\n        torch.set_grad_enabled(False)\r\n\r\n        self.fabric.call(\"on_validation_epoch_start\")\r\n\r\n        iterable = self.progbar_wrapper(val_loader, total=min(len(val_loader), limit_batches), desc=\"Validation\")\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_validation_batch_start\", batch, batch_idx)\r\n\r\n            out = model.validation_step(batch, batch_idx)\r\n            # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n            out = apply_to_collection(out, torch.Tensor, lambda x: x.detach())\r\n\r\n            self.fabric.call(\"on_validation_batch_end\", out, batch, batch_idx)\r\n            self._current_val_return = out\r\n\r\n            self._format_iterable(iterable, self._current_val_return, \"val\")\r\n\r\n        self.fabric.call(\"on_validation_epoch_end\")\r\n\r\n        if not is_overridden(\"on_validation_model_train\", _unwrap_objects(model)):\r\n            model.train()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_train\")\r\n        torch.set_grad_enabled(True)", "language": "python", "code": "def val_loop(\r\n        self,\r\n        model: L.LightningModule,\r\n        val_loader: Optional[torch.utils.data.DataLoader],\r\n        limit_batches: Union[int, float] = float(\"inf\"),\r\n    ):\r\n        \"\"\"The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.\r\n\r\n        \"\"\"\r\n        # no validation if val_loader wasn't passed\r\n        if val_loader is None:\r\n            return\r\n\r\n        # no validation but warning if val_loader was passed, but validation_step not implemented\r\n        if val_loader is not None and not is_overridden(\"validation_step\", _unwrap_objects(model)):\r\n            L.fabric.utilities.rank_zero_warn(\r\n                \"Your LightningModule does not have a validation_step implemented, \"\r\n                \"but you passed a validation dataloder. Skipping Validation.\"\r\n            )\r\n            return\r\n\r\n        if not is_overridden(\"on_validation_model_eval\", _unwrap_objects(model)):\r\n            model.eval()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_eval\")  # calls `model.eval()`\r\n\r\n        torch.set_grad_enabled(False)\r\n\r\n        self.fabric.call(\"on_validation_epoch_start\")\r\n\r\n        iterable = self.progbar_wrapper(val_loader, total=min(len(val_loader), limit_batches), desc=\"Validation\")\r\n\r\n        for batch_idx, batch in enumerate(iterable):\r\n            # end epoch if stopping training completely or max batches for this epoch reached\r\n            if self.should_stop or batch_idx >= limit_batches:\r\n                break\r\n\r\n            self.fabric.call(\"on_validation_batch_start\", batch, batch_idx)\r\n\r\n            out = model.validation_step(batch, batch_idx)\r\n            # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n            out = apply_to_collection(out, torch.Tensor, lambda x: x.detach())\r\n\r\n            self.fabric.call(\"on_validation_batch_end\", out, batch, batch_idx)\r\n            self._current_val_return = out\r\n\r\n            self._format_iterable(iterable, self._current_val_return, \"val\")\r\n\r\n        self.fabric.call(\"on_validation_epoch_end\")\r\n\r\n        if not is_overridden(\"on_validation_model_train\", _unwrap_objects(model)):\r\n            model.train()\r\n        else:\r\n            self.fabric.call(\"on_validation_model_train\")\r\n        torch.set_grad_enabled(True)", "code_tokens": ["def", "val_loop", "(", "self", ",", "model", ":", "L", ".", "LightningModule", ",", "val_loader", ":", "Optional", "[", "torch", ".", "utils", ".", "data", ".", "DataLoader", "]", ",", "limit_batches", ":", "Union", "[", "int", ",", "float", "]", "=", "float", "(", "\"", "inf", "\"", ")", ",", ")", ":", "\"", "\"", "\"", "The", "validation", "loop", "running", "a", "single", "validation", "epoch", ".", "Args", ":", "model", ":", "the", "LightningModule", "to", "evaluate", "val_loader", ":", "The", "dataloader", "yielding", "the", "validation", "batches", ".", "limit_batches", ":", "Limits", "the", "batches", "during", "this", "validation", "epoch", ".", "If", "greater", "than", "the", "number", "of", "batches", "in", "the", "`", "`", "val_loader", "`", "`", ",", "this", "has", "no", "effect", ".", "\"", "\"", "\"", "if", "val_loader", "is", "None", ":", "return", "if", "val_loader", "is", "not", "None", "and", "not", "is_overridden", "(", "\"", "validation_step", "\"", ",", "_unwrap_objects", "(", "model", ")", ")", ":", "L", ".", "fabric", ".", "utilities", ".", "rank_zero_warn", "(", "\"", "Your", "LightningModule", "does", "not", "have", "a", "validation_step", "implemented", ",", "\"", "\"", "but", "you", "passed", "a", "validation", "dataloder", ".", "Skipping", "Validation", ".", "\"", ")", "return", "if", "not", "is_overridden", "(", "\"", "on_validation_model_eval", "\"", ",", "_unwrap_objects", "(", "model", ")", ")", ":", "model", ".", "eval", "(", ")", "else", ":", "self", ".", "fabric", ".", "call", "(", "\"", "on_validation_model_eval", "\"", ")", "torch", ".", "set_grad_enabled", "(", "False", ")", "self", ".", "fabric", ".", "call", "(", "\"", "on_validation_epoch_start", "\"", ")", "iterable", "=", "self", ".", "progbar_wrapper", "(", "val_loader", ",", "total", "=", "min", "(", "len", "(", "val_loader", ")", ",", "limit_batches", ")", ",", "desc", "=", "\"", "Validation", "\"", ")", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "iterable", ")", ":", "if", "self", ".", "should_stop", "or", "batch_idx", ">", "=", "limit_batches", ":", "break", "self", ".", "fabric", ".", "call", "(", "\"", "on_validation_batch_start", "\"", ",", "batch", ",", "batch_idx", ")", "out", "=", "model", ".", "validation_step", "(", "batch", ",", "batch_idx", ")", "out", "=", "apply_to_collection", "(", "out", ",", "torch", ".", "Tensor", ",", "lambda", "x", ":", "x", ".", "detach", "(", ")", ")", "self", ".", "fabric", ".", "call", "(", "\"", "on_validation_batch_end", "\"", ",", "out", ",", "batch", ",", "batch_idx", ")", "self", ".", "_current_val_return", "=", "out", "self", ".", "_format_iterable", "(", "iterable", ",", "self", ".", "_current_val_return", ",", "\"", "val", "\"", ")", "self", ".", "fabric", ".", "call", "(", "\"", "on_validation_epoch_end", "\"", ")", "if", "not", "is_overridden", "(", "\"", "on_validation_model_train", "\"", ",", "_unwrap_objects", "(", "model", ")", ")", ":", "model", ".", "train", "(", ")", "else", ":", "self", ".", "fabric", ".", "call", "(", "\"", "on_validation_model_train", "\"", ")", "torch", ".", "set_grad_enabled", "(", "True", ")"], "docstring": "The validation loop running a single validation epoch.\r\n\r\n        Args:\r\n            model: the LightningModule to evaluate\r\n            val_loader: The dataloader yielding the validation batches.\r\n            limit_batches: Limits the batches during this validation epoch.\r\n                If greater than the number of batches in the ``val_loader``, this has no effect.", "docstring_tokens": ["the", "validation", "loop", "running", "a", "single", "validation", "epoch", "args", "model", "the", "lightningmodule", "to", "evaluate", "val_loader", "the", "dataloader", "yielding", "the", "validation", "batches", "limit_batches", "limits", "the", "batches", "during", "this", "validation", "epoch", "if", "greater", "than", "the", "number", "of", "batches", "in", "the", "val_loader", "this", "has", "no", "effect"], "docstring_summary": "The validation loop running a single validation epoch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 261, "end_line": 321, "hash": "81622f9eb3301a1e1cd783d25137baf0", "complexity": 9, "parameters": ["model", "val_loader", "limit_batches", "float]"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "training_step", "original_string": "def training_step(self, model: L.LightningModule, batch: Any, batch_idx: int) -> torch.Tensor:\r\n        \"\"\"A single training step, running forward and backward. The optimizer step is called separately, as this is\r\n        given as a closure to the optimizer step.\r\n\r\n        Args:\r\n            model: the lightning module to train\r\n            batch: the batch to run the forward on\r\n            batch_idx: index of the current batch w.r.t the current epoch\r\n\r\n        \"\"\"\r\n        outputs: Union[torch.Tensor, Mapping[str, Any]] = model.training_step(batch, batch_idx=batch_idx)\r\n\r\n        loss = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n\r\n        self.fabric.call(\"on_before_backward\", loss)\r\n        self.fabric.backward(loss)\r\n        self.fabric.call(\"on_after_backward\")\r\n\r\n        # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n        self._current_train_return = apply_to_collection(outputs, dtype=torch.Tensor, function=lambda x: x.detach())\r\n\r\n        return loss", "language": "python", "code": "def training_step(self, model: L.LightningModule, batch: Any, batch_idx: int) -> torch.Tensor:\r\n        \"\"\"A single training step, running forward and backward. The optimizer step is called separately, as this is\r\n        given as a closure to the optimizer step.\r\n\r\n        Args:\r\n            model: the lightning module to train\r\n            batch: the batch to run the forward on\r\n            batch_idx: index of the current batch w.r.t the current epoch\r\n\r\n        \"\"\"\r\n        outputs: Union[torch.Tensor, Mapping[str, Any]] = model.training_step(batch, batch_idx=batch_idx)\r\n\r\n        loss = outputs if isinstance(outputs, torch.Tensor) else outputs[\"loss\"]\r\n\r\n        self.fabric.call(\"on_before_backward\", loss)\r\n        self.fabric.backward(loss)\r\n        self.fabric.call(\"on_after_backward\")\r\n\r\n        # avoid gradients in stored/accumulated values -> prevents potential OOM\r\n        self._current_train_return = apply_to_collection(outputs, dtype=torch.Tensor, function=lambda x: x.detach())\r\n\r\n        return loss", "code_tokens": ["def", "training_step", "(", "self", ",", "model", ":", "L", ".", "LightningModule", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "A", "single", "training", "step", ",", "running", "forward", "and", "backward", ".", "The", "optimizer", "step", "is", "called", "separately", ",", "as", "this", "is", "given", "as", "a", "closure", "to", "the", "optimizer", "step", ".", "Args", ":", "model", ":", "the", "lightning", "module", "to", "train", "batch", ":", "the", "batch", "to", "run", "the", "forward", "on", "batch_idx", ":", "index", "of", "the", "current", "batch", "w", ".", "r", ".", "t", "the", "current", "epoch", "\"", "\"", "\"", "outputs", ":", "Union", "[", "torch", ".", "Tensor", ",", "Mapping", "[", "str", ",", "Any", "]", "]", "=", "model", ".", "training_step", "(", "batch", ",", "batch_idx", "=", "batch_idx", ")", "loss", "=", "outputs", "if", "isinstance", "(", "outputs", ",", "torch", ".", "Tensor", ")", "else", "outputs", "[", "\"", "loss", "\"", "]", "self", ".", "fabric", ".", "call", "(", "\"", "on_before_backward", "\"", ",", "loss", ")", "self", ".", "fabric", ".", "backward", "(", "loss", ")", "self", ".", "fabric", ".", "call", "(", "\"", "on_after_backward", "\"", ")", "self", ".", "_current_train_return", "=", "apply_to_collection", "(", "outputs", ",", "dtype", "=", "torch", ".", "Tensor", ",", "function", "=", "lambda", "x", ":", "x", ".", "detach", "(", ")", ")", "return", "loss"], "docstring": "A single training step, running forward and backward. The optimizer step is called separately, as this is\r\n        given as a closure to the optimizer step.\r\n\r\n        Args:\r\n            model: the lightning module to train\r\n            batch: the batch to run the forward on\r\n            batch_idx: index of the current batch w.r.t the current epoch", "docstring_tokens": ["a", "single", "training", "step", "running", "forward", "and", "backward", "the", "optimizer", "step", "is", "called", "separately", "as", "this", "is", "given", "as", "a", "closure", "to", "the", "optimizer", "step", "args", "model", "the", "lightning", "module", "to", "train", "batch", "the", "batch", "to", "run", "the", "forward", "on", "batch_idx", "index", "of", "the", "current", "batch", "w", "r", "t", "the", "current", "epoch"], "docstring_summary": "A single training step, running forward and backward. The optimizer step is called separately, as this is", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 323, "end_line": 344, "hash": "4b5f46df8382a263e27e1522f29db870", "complexity": 2, "parameters": ["model", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "step_scheduler", "original_string": "def step_scheduler(\r\n        self,\r\n        model: L.LightningModule,\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]],\r\n        level: Literal[\"step\", \"epoch\"],\r\n        current_value: int,\r\n    ) -> None:\r\n        \"\"\"Steps the learning rate scheduler if necessary.\r\n\r\n        Args:\r\n            model: The LightningModule to train\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`lightning.pytorch.LightningModule.configure_optimizers` for supported values.\r\n            level: whether we are trying to step on epoch- or step-level\r\n            current_value: Holds the current_epoch if ``level==epoch``, else holds the ``global_step``\r\n\r\n        \"\"\"\r\n\r\n        # no scheduler\r\n        if scheduler_cfg is None:\r\n            return\r\n\r\n        # wrong interval (step vs. epoch)\r\n        if scheduler_cfg[\"interval\"] != level:\r\n            return\r\n\r\n        # right interval, but wrong step wrt frequency\r\n        if current_value % cast(int, scheduler_cfg[\"frequency\"]) != 0:\r\n            return\r\n\r\n        # assemble potential monitored values\r\n        possible_monitor_vals = {None: None}\r\n        if isinstance(self._current_train_return, torch.Tensor):\r\n            possible_monitor_vals.update(\"train_loss\", self._current_train_return)\r\n        elif isinstance(self._current_train_return, Mapping):\r\n            possible_monitor_vals.update({\"train_\" + k: v for k, v in self._current_train_return.items()})\r\n\r\n        if isinstance(self._current_val_return, torch.Tensor):\r\n            possible_monitor_vals.update(\"val_loss\", self._current_val_return)\r\n        elif isinstance(self._current_val_return, Mapping):\r\n            possible_monitor_vals.update({\"val_\" + k: v for k, v in self._current_val_return.items()})\r\n\r\n        try:\r\n            monitor = possible_monitor_vals[cast(Optional[str], scheduler_cfg[\"monitor\"])]\r\n        except KeyError as ex:\r\n            possible_keys = list(possible_monitor_vals.keys())\r\n            raise KeyError(\r\n                f\"monitor {scheduler_cfg['monitor']} is invalid. Possible values are {possible_keys}.\"\r\n            ) from ex\r\n\r\n        # rely on model hook for actual step\r\n        model.lr_scheduler_step(scheduler_cfg[\"scheduler\"], monitor)", "language": "python", "code": "def step_scheduler(\r\n        self,\r\n        model: L.LightningModule,\r\n        scheduler_cfg: Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]],\r\n        level: Literal[\"step\", \"epoch\"],\r\n        current_value: int,\r\n    ) -> None:\r\n        \"\"\"Steps the learning rate scheduler if necessary.\r\n\r\n        Args:\r\n            model: The LightningModule to train\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`lightning.pytorch.LightningModule.configure_optimizers` for supported values.\r\n            level: whether we are trying to step on epoch- or step-level\r\n            current_value: Holds the current_epoch if ``level==epoch``, else holds the ``global_step``\r\n\r\n        \"\"\"\r\n\r\n        # no scheduler\r\n        if scheduler_cfg is None:\r\n            return\r\n\r\n        # wrong interval (step vs. epoch)\r\n        if scheduler_cfg[\"interval\"] != level:\r\n            return\r\n\r\n        # right interval, but wrong step wrt frequency\r\n        if current_value % cast(int, scheduler_cfg[\"frequency\"]) != 0:\r\n            return\r\n\r\n        # assemble potential monitored values\r\n        possible_monitor_vals = {None: None}\r\n        if isinstance(self._current_train_return, torch.Tensor):\r\n            possible_monitor_vals.update(\"train_loss\", self._current_train_return)\r\n        elif isinstance(self._current_train_return, Mapping):\r\n            possible_monitor_vals.update({\"train_\" + k: v for k, v in self._current_train_return.items()})\r\n\r\n        if isinstance(self._current_val_return, torch.Tensor):\r\n            possible_monitor_vals.update(\"val_loss\", self._current_val_return)\r\n        elif isinstance(self._current_val_return, Mapping):\r\n            possible_monitor_vals.update({\"val_\" + k: v for k, v in self._current_val_return.items()})\r\n\r\n        try:\r\n            monitor = possible_monitor_vals[cast(Optional[str], scheduler_cfg[\"monitor\"])]\r\n        except KeyError as ex:\r\n            possible_keys = list(possible_monitor_vals.keys())\r\n            raise KeyError(\r\n                f\"monitor {scheduler_cfg['monitor']} is invalid. Possible values are {possible_keys}.\"\r\n            ) from ex\r\n\r\n        # rely on model hook for actual step\r\n        model.lr_scheduler_step(scheduler_cfg[\"scheduler\"], monitor)", "code_tokens": ["def", "step_scheduler", "(", "self", ",", "model", ":", "L", ".", "LightningModule", ",", "scheduler_cfg", ":", "Optional", "[", "Mapping", "[", "str", ",", "Union", "[", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "LRScheduler", ",", "bool", ",", "str", ",", "int", "]", "]", "]", ",", "level", ":", "Literal", "[", "\"", "step", "\"", ",", "\"", "epoch", "\"", "]", ",", "current_value", ":", "int", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Steps", "the", "learning", "rate", "scheduler", "if", "necessary", ".", "Args", ":", "model", ":", "The", "LightningModule", "to", "train", "scheduler_cfg", ":", "The", "learning", "rate", "scheduler", "configuration", ".", "Have", "a", "look", "at", ":", "meth", ":", "`", "lightning", ".", "pytorch", ".", "LightningModule", ".", "configure_optimizers", "`", "for", "supported", "values", ".", "level", ":", "whether", "we", "are", "trying", "to", "step", "on", "epoch", "-", "or", "step", "-", "level", "current_value", ":", "Holds", "the", "current_epoch", "if", "`", "`", "level", "=", "=", "epoch", "`", "`", ",", "else", "holds", "the", "`", "`", "global_step", "`", "`", "\"", "\"", "\"", "if", "scheduler_cfg", "is", "None", ":", "return", "if", "scheduler_cfg", "[", "\"", "interval", "\"", "]", "!", "=", "level", ":", "return", "if", "current_value", "%", "cast", "(", "int", ",", "scheduler_cfg", "[", "\"", "frequency", "\"", "]", ")", "!", "=", "0", ":", "return", "possible_monitor_vals", "=", "{", "None", ":", "None", "}", "if", "isinstance", "(", "self", ".", "_current_train_return", ",", "torch", ".", "Tensor", ")", ":", "possible_monitor_vals", ".", "update", "(", "\"", "train_loss", "\"", ",", "self", ".", "_current_train_return", ")", "elif", "isinstance", "(", "self", ".", "_current_train_return", ",", "Mapping", ")", ":", "possible_monitor_vals", ".", "update", "(", "{", "\"", "train_", "\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "_current_train_return", ".", "items", "(", ")", "}", ")", "if", "isinstance", "(", "self", ".", "_current_val_return", ",", "torch", ".", "Tensor", ")", ":", "possible_monitor_vals", ".", "update", "(", "\"", "val_loss", "\"", ",", "self", ".", "_current_val_return", ")", "elif", "isinstance", "(", "self", ".", "_current_val_return", ",", "Mapping", ")", ":", "possible_monitor_vals", ".", "update", "(", "{", "\"", "val_", "\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "_current_val_return", ".", "items", "(", ")", "}", ")", "try", ":", "monitor", "=", "possible_monitor_vals", "[", "cast", "(", "Optional", "[", "str", "]", ",", "scheduler_cfg", "[", "\"", "monitor", "\"", "]", ")", "]", "except", "KeyError", "as", "ex", ":", "possible_keys", "=", "list", "(", "possible_monitor_vals", ".", "keys", "(", ")", ")", "raise", "KeyError", "(", "f", "\"", "monitor", "{", "scheduler_cfg", "[", "'", "monitor", "'", "]", "}", "is", "invalid", ".", "Possible", "values", "are", "{", "possible_keys", "}", ".", "\"", ")", "from", "ex", "model", ".", "lr_scheduler_step", "(", "scheduler_cfg", "[", "\"", "scheduler", "\"", "]", ",", "monitor", ")"], "docstring": "Steps the learning rate scheduler if necessary.\r\n\r\n        Args:\r\n            model: The LightningModule to train\r\n            scheduler_cfg: The learning rate scheduler configuration.\r\n                Have a look at :meth:`lightning.pytorch.LightningModule.configure_optimizers` for supported values.\r\n            level: whether we are trying to step on epoch- or step-level\r\n            current_value: Holds the current_epoch if ``level==epoch``, else holds the ``global_step``", "docstring_tokens": ["steps", "the", "learning", "rate", "scheduler", "if", "necessary", "args", "model", "the", "lightningmodule", "to", "train", "scheduler_cfg", "the", "learning", "rate", "scheduler", "configuration", "have", "a", "look", "at", "meth", "lightning", "pytorch", "lightningmodule", "configure_optimizers", "for", "supported", "values", "level", "whether", "we", "are", "trying", "to", "step", "on", "epoch", "or", "step", "level", "current_value", "holds", "the", "current_epoch", "if", "level", "epoch", "else", "holds", "the", "global_step"], "docstring_summary": "Steps the learning rate scheduler if necessary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 346, "end_line": 397, "hash": "11a53770914a6d57b0086bbbc7c5d6f8", "complexity": 11, "parameters": ["model", "scheduler_cfg", "Union[L.fabric.utilities.types.LRScheduler", "bool", "str", "int]]]", "level", "\"epoch\"]", "current_value"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "progbar_wrapper", "original_string": "def progbar_wrapper(self, iterable: Iterable, total: int, **kwargs: Any):\r\n        \"\"\"Wraps the iterable with tqdm for global rank zero.\r\n\r\n        Args:\r\n            iterable: the iterable to wrap with tqdm\r\n            total: the total length of the iterable, necessary in case the number of batches was limited.\r\n\r\n        \"\"\"\r\n        if self.fabric.is_global_zero:\r\n            return tqdm(iterable, total=total, **kwargs)\r\n        return iterable", "language": "python", "code": "def progbar_wrapper(self, iterable: Iterable, total: int, **kwargs: Any):\r\n        \"\"\"Wraps the iterable with tqdm for global rank zero.\r\n\r\n        Args:\r\n            iterable: the iterable to wrap with tqdm\r\n            total: the total length of the iterable, necessary in case the number of batches was limited.\r\n\r\n        \"\"\"\r\n        if self.fabric.is_global_zero:\r\n            return tqdm(iterable, total=total, **kwargs)\r\n        return iterable", "code_tokens": ["def", "progbar_wrapper", "(", "self", ",", "iterable", ":", "Iterable", ",", "total", ":", "int", ",", "*", "*", "kwargs", ":", "Any", ")", ":", "\"", "\"", "\"", "Wraps", "the", "iterable", "with", "tqdm", "for", "global", "rank", "zero", ".", "Args", ":", "iterable", ":", "the", "iterable", "to", "wrap", "with", "tqdm", "total", ":", "the", "total", "length", "of", "the", "iterable", ",", "necessary", "in", "case", "the", "number", "of", "batches", "was", "limited", ".", "\"", "\"", "\"", "if", "self", ".", "fabric", ".", "is_global_zero", ":", "return", "tqdm", "(", "iterable", ",", "total", "=", "total", ",", "*", "*", "kwargs", ")", "return", "iterable"], "docstring": "Wraps the iterable with tqdm for global rank zero.\r\n\r\n        Args:\r\n            iterable: the iterable to wrap with tqdm\r\n            total: the total length of the iterable, necessary in case the number of batches was limited.", "docstring_tokens": ["wraps", "the", "iterable", "with", "tqdm", "for", "global", "rank", "zero", "args", "iterable", "the", "iterable", "to", "wrap", "with", "tqdm", "total", "the", "total", "length", "of", "the", "iterable", "necessary", "in", "case", "the", "number", "of", "batches", "was", "limited"], "docstring_summary": "Wraps the iterable with tqdm for global rank zero.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 404, "end_line": 414, "hash": "cc483e002ce76ec114931b4d1941aa9a", "complexity": 2, "parameters": ["iterable", "total", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "load", "original_string": "def load(self, state: Optional[Mapping], path: str) -> None:\r\n        \"\"\"Loads a checkpoint from a given file into state.\r\n\r\n        Args:\r\n            state: a mapping containing model, optimizer and lr scheduler\r\n            path: the path to load the checkpoint from\r\n\r\n        \"\"\"\r\n        if state is None:\r\n            state = {}\r\n\r\n        remainder = self.fabric.load(path, state)\r\n        self.global_step = remainder.pop(\"global_step\")\r\n        self.current_epoch = remainder.pop(\"current_epoch\")\r\n\r\n        if remainder:\r\n            raise RuntimeError(f\"Unused Checkpoint Values: {remainder}\")", "language": "python", "code": "def load(self, state: Optional[Mapping], path: str) -> None:\r\n        \"\"\"Loads a checkpoint from a given file into state.\r\n\r\n        Args:\r\n            state: a mapping containing model, optimizer and lr scheduler\r\n            path: the path to load the checkpoint from\r\n\r\n        \"\"\"\r\n        if state is None:\r\n            state = {}\r\n\r\n        remainder = self.fabric.load(path, state)\r\n        self.global_step = remainder.pop(\"global_step\")\r\n        self.current_epoch = remainder.pop(\"current_epoch\")\r\n\r\n        if remainder:\r\n            raise RuntimeError(f\"Unused Checkpoint Values: {remainder}\")", "code_tokens": ["def", "load", "(", "self", ",", "state", ":", "Optional", "[", "Mapping", "]", ",", "path", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Loads", "a", "checkpoint", "from", "a", "given", "file", "into", "state", ".", "Args", ":", "state", ":", "a", "mapping", "containing", "model", ",", "optimizer", "and", "lr", "scheduler", "path", ":", "the", "path", "to", "load", "the", "checkpoint", "from", "\"", "\"", "\"", "if", "state", "is", "None", ":", "state", "=", "{", "}", "remainder", "=", "self", ".", "fabric", ".", "load", "(", "path", ",", "state", ")", "self", ".", "global_step", "=", "remainder", ".", "pop", "(", "\"", "global_step", "\"", ")", "self", ".", "current_epoch", "=", "remainder", ".", "pop", "(", "\"", "current_epoch", "\"", ")", "if", "remainder", ":", "raise", "RuntimeError", "(", "f", "\"", "Unused", "Checkpoint", "Values", ":", "{", "remainder", "}", "\"", ")"], "docstring": "Loads a checkpoint from a given file into state.\r\n\r\n        Args:\r\n            state: a mapping containing model, optimizer and lr scheduler\r\n            path: the path to load the checkpoint from", "docstring_tokens": ["loads", "a", "checkpoint", "from", "a", "given", "file", "into", "state", "args", "state", "a", "mapping", "containing", "model", "optimizer", "and", "lr", "scheduler", "path", "the", "path", "to", "load", "the", "checkpoint", "from"], "docstring_summary": "Loads a checkpoint from a given file into state.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 416, "end_line": 432, "hash": "3c9ff4490ea1c209bf2bd0e1110615ac", "complexity": 3, "parameters": ["state", "path"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "save", "original_string": "def save(self, state: Optional[Mapping]) -> None:\r\n        \"\"\"Saves a checkpoint to the ``checkpoint_dir``\r\n\r\n        Args:\r\n            state: A mapping containing model, optimizer and lr scheduler.\r\n\r\n        \"\"\"\r\n        if state is None:\r\n            state = {}\r\n\r\n        state.update(global_step=self.global_step, current_epoch=self.current_epoch)\r\n\r\n        self.fabric.save(os.path.join(self.checkpoint_dir, f\"epoch-{self.current_epoch:04d}.ckpt\"), state)", "language": "python", "code": "def save(self, state: Optional[Mapping]) -> None:\r\n        \"\"\"Saves a checkpoint to the ``checkpoint_dir``\r\n\r\n        Args:\r\n            state: A mapping containing model, optimizer and lr scheduler.\r\n\r\n        \"\"\"\r\n        if state is None:\r\n            state = {}\r\n\r\n        state.update(global_step=self.global_step, current_epoch=self.current_epoch)\r\n\r\n        self.fabric.save(os.path.join(self.checkpoint_dir, f\"epoch-{self.current_epoch:04d}.ckpt\"), state)", "code_tokens": ["def", "save", "(", "self", ",", "state", ":", "Optional", "[", "Mapping", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Saves", "a", "checkpoint", "to", "the", "`", "`", "checkpoint_dir", "`", "`", "Args", ":", "state", ":", "A", "mapping", "containing", "model", ",", "optimizer", "and", "lr", "scheduler", ".", "\"", "\"", "\"", "if", "state", "is", "None", ":", "state", "=", "{", "}", "state", ".", "update", "(", "global_step", "=", "self", ".", "global_step", ",", "current_epoch", "=", "self", ".", "current_epoch", ")", "self", ".", "fabric", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "f", "\"", "epoch", "-", "{", "self", ".", "current_epoch", ":", "04d", "}", ".", "ckpt", "\"", ")", ",", "state", ")"], "docstring": "Saves a checkpoint to the ``checkpoint_dir``\r\n\r\n        Args:\r\n            state: A mapping containing model, optimizer and lr scheduler.", "docstring_tokens": ["saves", "a", "checkpoint", "to", "the", "checkpoint_dir", "args", "state", "a", "mapping", "containing", "model", "optimizer", "and", "lr", "scheduler"], "docstring_summary": "Saves a checkpoint to the ``checkpoint_dir``", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 434, "end_line": 446, "hash": "c8c0657e369c066f7cd58eb169ba1e92", "complexity": 2, "parameters": ["state"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "get_latest_checkpoint", "original_string": "def get_latest_checkpoint(checkpoint_dir: str) -> Optional[str]:\r\n        \"\"\"Returns the latest checkpoint from the ``checkpoint_dir``\r\n\r\n        Args:\r\n            checkpoint_dir: the directory to search for checkpoints\r\n\r\n        \"\"\"\r\n        if not os.path.isdir(checkpoint_dir):\r\n            return None\r\n\r\n        items = sorted(os.listdir(checkpoint_dir))\r\n\r\n        if not items:\r\n            return None\r\n\r\n        return os.path.join(checkpoint_dir, items[-1])", "language": "python", "code": "def get_latest_checkpoint(checkpoint_dir: str) -> Optional[str]:\r\n        \"\"\"Returns the latest checkpoint from the ``checkpoint_dir``\r\n\r\n        Args:\r\n            checkpoint_dir: the directory to search for checkpoints\r\n\r\n        \"\"\"\r\n        if not os.path.isdir(checkpoint_dir):\r\n            return None\r\n\r\n        items = sorted(os.listdir(checkpoint_dir))\r\n\r\n        if not items:\r\n            return None\r\n\r\n        return os.path.join(checkpoint_dir, items[-1])", "code_tokens": ["def", "get_latest_checkpoint", "(", "checkpoint_dir", ":", "str", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Returns", "the", "latest", "checkpoint", "from", "the", "`", "`", "checkpoint_dir", "`", "`", "Args", ":", "checkpoint_dir", ":", "the", "directory", "to", "search", "for", "checkpoints", "\"", "\"", "\"", "if", "not", "os", ".", "path", ".", "isdir", "(", "checkpoint_dir", ")", ":", "return", "None", "items", "=", "sorted", "(", "os", ".", "listdir", "(", "checkpoint_dir", ")", ")", "if", "not", "items", ":", "return", "None", "return", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "items", "[", "-", "1", "]", ")"], "docstring": "Returns the latest checkpoint from the ``checkpoint_dir``\r\n\r\n        Args:\r\n            checkpoint_dir: the directory to search for checkpoints", "docstring_tokens": ["returns", "the", "latest", "checkpoint", "from", "the", "checkpoint_dir", "args", "checkpoint_dir", "the", "directory", "to", "search", "for", "checkpoints"], "docstring_summary": "Returns the latest checkpoint from the ``checkpoint_dir``", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 449, "end_line": 464, "hash": "9bfc3a58e2b69588b70e4560085e23bb", "complexity": 3, "parameters": ["checkpoint_dir"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "_parse_optimizers_schedulers", "original_string": "def _parse_optimizers_schedulers(\r\n        self, configure_optim_output\r\n    ) -> tuple[\r\n        Optional[L.fabric.utilities.types.Optimizable],\r\n        Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]],\r\n    ]:\r\n        \"\"\"Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        Args:\r\n            configure_optim_output: The output of ``configure_optimizers``.\r\n                For supported values, please refer to :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        _lr_sched_defaults = {\"interval\": \"epoch\", \"frequency\": 1, \"monitor\": \"val_loss\"}\r\n\r\n        # single optimizer\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.Optimizable):\r\n            return configure_optim_output, None\r\n\r\n        # single lr scheduler\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.LRScheduler):\r\n            return None, _lr_sched_defaults.update(scheduler=configure_optim_output)\r\n\r\n        # single lr scheduler config\r\n        if isinstance(configure_optim_output, Mapping):\r\n            _lr_sched_defaults.update(configure_optim_output)\r\n            return None, _lr_sched_defaults\r\n\r\n        # list or tuple\r\n        if isinstance(configure_optim_output, (list, tuple)):\r\n            if all(isinstance(_opt_cand, L.fabric.utilities.types.Optimizable) for _opt_cand in configure_optim_output):\r\n                # single optimizer in list\r\n                if len(configure_optim_output) == 1:\r\n                    return configure_optim_output[0][0], None\r\n\r\n                raise NotImplementedError(\"BYOT only supports a single optimizer\")\r\n\r\n            if all(\r\n                isinstance(_lr_cand, (L.fabric.utilities.types.LRScheduler, Mapping))\r\n                for _lr_cand in configure_optim_output\r\n            ):\r\n                # single scheduler in list\r\n                if len(configure_optim_output) == 1:\r\n                    return None, self._parse_optimizers_schedulers(configure_optim_output[0])[1]\r\n\r\n            # optimizer and lr scheduler\r\n            elif len(configure_optim_output) == 2:\r\n                opt_cands, lr_cands = (\r\n                    self._parse_optimizers_schedulers(configure_optim_output[0])[0],\r\n                    self._parse_optimizers_schedulers(configure_optim_output[1])[1],\r\n                )\r\n                return opt_cands, lr_cands\r\n\r\n        return None, None", "language": "python", "code": "def _parse_optimizers_schedulers(\r\n        self, configure_optim_output\r\n    ) -> tuple[\r\n        Optional[L.fabric.utilities.types.Optimizable],\r\n        Optional[Mapping[str, Union[L.fabric.utilities.types.LRScheduler, bool, str, int]]],\r\n    ]:\r\n        \"\"\"Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        Args:\r\n            configure_optim_output: The output of ``configure_optimizers``.\r\n                For supported values, please refer to :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        _lr_sched_defaults = {\"interval\": \"epoch\", \"frequency\": 1, \"monitor\": \"val_loss\"}\r\n\r\n        # single optimizer\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.Optimizable):\r\n            return configure_optim_output, None\r\n\r\n        # single lr scheduler\r\n        if isinstance(configure_optim_output, L.fabric.utilities.types.LRScheduler):\r\n            return None, _lr_sched_defaults.update(scheduler=configure_optim_output)\r\n\r\n        # single lr scheduler config\r\n        if isinstance(configure_optim_output, Mapping):\r\n            _lr_sched_defaults.update(configure_optim_output)\r\n            return None, _lr_sched_defaults\r\n\r\n        # list or tuple\r\n        if isinstance(configure_optim_output, (list, tuple)):\r\n            if all(isinstance(_opt_cand, L.fabric.utilities.types.Optimizable) for _opt_cand in configure_optim_output):\r\n                # single optimizer in list\r\n                if len(configure_optim_output) == 1:\r\n                    return configure_optim_output[0][0], None\r\n\r\n                raise NotImplementedError(\"BYOT only supports a single optimizer\")\r\n\r\n            if all(\r\n                isinstance(_lr_cand, (L.fabric.utilities.types.LRScheduler, Mapping))\r\n                for _lr_cand in configure_optim_output\r\n            ):\r\n                # single scheduler in list\r\n                if len(configure_optim_output) == 1:\r\n                    return None, self._parse_optimizers_schedulers(configure_optim_output[0])[1]\r\n\r\n            # optimizer and lr scheduler\r\n            elif len(configure_optim_output) == 2:\r\n                opt_cands, lr_cands = (\r\n                    self._parse_optimizers_schedulers(configure_optim_output[0])[0],\r\n                    self._parse_optimizers_schedulers(configure_optim_output[1])[1],\r\n                )\r\n                return opt_cands, lr_cands\r\n\r\n        return None, None", "code_tokens": ["def", "_parse_optimizers_schedulers", "(", "self", ",", "configure_optim_output", ")", "-", ">", "tuple", "[", "Optional", "[", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "Optimizable", "]", ",", "Optional", "[", "Mapping", "[", "str", ",", "Union", "[", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "LRScheduler", ",", "bool", ",", "str", ",", "int", "]", "]", "]", ",", "]", ":", "\"", "\"", "\"", "Recursively", "parses", "the", "output", "of", ":", "meth", ":", "`", "lightning", ".", "pytorch", ".", "LightningModule", ".", "configure_optimizers", "`", ".", "Args", ":", "configure_optim_output", ":", "The", "output", "of", "`", "`", "configure_optimizers", "`", "`", ".", "For", "supported", "values", ",", "please", "refer", "to", ":", "meth", ":", "`", "lightning", ".", "pytorch", ".", "LightningModule", ".", "configure_optimizers", "`", ".", "\"", "\"", "\"", "_lr_sched_defaults", "=", "{", "\"", "interval", "\"", ":", "\"", "epoch", "\"", ",", "\"", "frequency", "\"", ":", "1", ",", "\"", "monitor", "\"", ":", "\"", "val_loss", "\"", "}", "if", "isinstance", "(", "configure_optim_output", ",", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "Optimizable", ")", ":", "return", "configure_optim_output", ",", "None", "if", "isinstance", "(", "configure_optim_output", ",", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "LRScheduler", ")", ":", "return", "None", ",", "_lr_sched_defaults", ".", "update", "(", "scheduler", "=", "configure_optim_output", ")", "if", "isinstance", "(", "configure_optim_output", ",", "Mapping", ")", ":", "_lr_sched_defaults", ".", "update", "(", "configure_optim_output", ")", "return", "None", ",", "_lr_sched_defaults", "if", "isinstance", "(", "configure_optim_output", ",", "(", "list", ",", "tuple", ")", ")", ":", "if", "all", "(", "isinstance", "(", "_opt_cand", ",", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "Optimizable", ")", "for", "_opt_cand", "in", "configure_optim_output", ")", ":", "if", "len", "(", "configure_optim_output", ")", "=", "=", "1", ":", "return", "configure_optim_output", "[", "0", "]", "[", "0", "]", ",", "None", "raise", "NotImplementedError", "(", "\"", "BYOT", "only", "supports", "a", "single", "optimizer", "\"", ")", "if", "all", "(", "isinstance", "(", "_lr_cand", ",", "(", "L", ".", "fabric", ".", "utilities", ".", "types", ".", "LRScheduler", ",", "Mapping", ")", ")", "for", "_lr_cand", "in", "configure_optim_output", ")", ":", "if", "len", "(", "configure_optim_output", ")", "=", "=", "1", ":", "return", "None", ",", "self", ".", "_parse_optimizers_schedulers", "(", "configure_optim_output", "[", "0", "]", ")", "[", "1", "]", "elif", "len", "(", "configure_optim_output", ")", "=", "=", "2", ":", "opt_cands", ",", "lr_cands", "=", "(", "self", ".", "_parse_optimizers_schedulers", "(", "configure_optim_output", "[", "0", "]", ")", "[", "0", "]", ",", "self", ".", "_parse_optimizers_schedulers", "(", "configure_optim_output", "[", "1", "]", ")", "[", "1", "]", ",", ")", "return", "opt_cands", ",", "lr_cands", "return", "None", ",", "None"], "docstring": "Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.\r\n\r\n        Args:\r\n            configure_optim_output: The output of ``configure_optimizers``.\r\n                For supported values, please refer to :meth:`lightning.pytorch.LightningModule.configure_optimizers`.", "docstring_tokens": ["recursively", "parses", "the", "output", "of", "meth", "lightning", "pytorch", "lightningmodule", "configure_optimizers", "args", "configure_optim_output", "the", "output", "of", "configure_optimizers", "for", "supported", "values", "please", "refer", "to", "meth", "lightning", "pytorch", "lightningmodule", "configure_optimizers"], "docstring_summary": "Recursively parses the output of :meth:`lightning.pytorch.LightningModule.configure_optimizers`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 466, "end_line": 519, "hash": "49e88a8be6c27b795bf5a766f60a9454", "complexity": 12, "parameters": ["configure_optim_output"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\build_your_own_trainer\\trainer.py", "func_name": "_format_iterable", "original_string": "def _format_iterable(\r\n        prog_bar, candidates: Optional[Union[torch.Tensor, Mapping[str, Union[torch.Tensor, float, int]]]], prefix: str\r\n    ):\r\n        \"\"\"Adds values as postfix string to progressbar.\r\n\r\n        Args:\r\n            prog_bar: a progressbar (on global rank zero) or an iterable (every other rank).\r\n            candidates: the values to add as postfix strings to the progressbar.\r\n            prefix: the prefix to add to each of these values.\r\n\r\n        \"\"\"\r\n        if isinstance(prog_bar, tqdm) and candidates is not None:\r\n            postfix_str = \"\"\r\n            float_candidates = apply_to_collection(candidates, torch.Tensor, lambda x: x.item())\r\n            if isinstance(candidates, torch.Tensor):\r\n                postfix_str += f\" {prefix}_loss: {float_candidates:.3f}\"\r\n            elif isinstance(candidates, Mapping):\r\n                for k, v in float_candidates.items():\r\n                    postfix_str += f\" {prefix}_{k}: {v:.3f}\"\r\n\r\n            if postfix_str:\r\n                prog_bar.set_postfix_str(postfix_str)", "language": "python", "code": "def _format_iterable(\r\n        prog_bar, candidates: Optional[Union[torch.Tensor, Mapping[str, Union[torch.Tensor, float, int]]]], prefix: str\r\n    ):\r\n        \"\"\"Adds values as postfix string to progressbar.\r\n\r\n        Args:\r\n            prog_bar: a progressbar (on global rank zero) or an iterable (every other rank).\r\n            candidates: the values to add as postfix strings to the progressbar.\r\n            prefix: the prefix to add to each of these values.\r\n\r\n        \"\"\"\r\n        if isinstance(prog_bar, tqdm) and candidates is not None:\r\n            postfix_str = \"\"\r\n            float_candidates = apply_to_collection(candidates, torch.Tensor, lambda x: x.item())\r\n            if isinstance(candidates, torch.Tensor):\r\n                postfix_str += f\" {prefix}_loss: {float_candidates:.3f}\"\r\n            elif isinstance(candidates, Mapping):\r\n                for k, v in float_candidates.items():\r\n                    postfix_str += f\" {prefix}_{k}: {v:.3f}\"\r\n\r\n            if postfix_str:\r\n                prog_bar.set_postfix_str(postfix_str)", "code_tokens": ["def", "_format_iterable", "(", "prog_bar", ",", "candidates", ":", "Optional", "[", "Union", "[", "torch", ".", "Tensor", ",", "Mapping", "[", "str", ",", "Union", "[", "torch", ".", "Tensor", ",", "float", ",", "int", "]", "]", "]", "]", ",", "prefix", ":", "str", ")", ":", "\"", "\"", "\"", "Adds", "values", "as", "postfix", "string", "to", "progressbar", ".", "Args", ":", "prog_bar", ":", "a", "progressbar", "(", "on", "global", "rank", "zero", ")", "or", "an", "iterable", "(", "every", "other", "rank", ")", ".", "candidates", ":", "the", "values", "to", "add", "as", "postfix", "strings", "to", "the", "progressbar", ".", "prefix", ":", "the", "prefix", "to", "add", "to", "each", "of", "these", "values", ".", "\"", "\"", "\"", "if", "isinstance", "(", "prog_bar", ",", "tqdm", ")", "and", "candidates", "is", "not", "None", ":", "postfix_str", "=", "\"", "\"", "float_candidates", "=", "apply_to_collection", "(", "candidates", ",", "torch", ".", "Tensor", ",", "lambda", "x", ":", "x", ".", "item", "(", ")", ")", "if", "isinstance", "(", "candidates", ",", "torch", ".", "Tensor", ")", ":", "postfix_str", "+", "=", "f", "\"", "{", "prefix", "}", "_loss", ":", "{", "float_candidates", ":", ".", "3f", "}", "\"", "elif", "isinstance", "(", "candidates", ",", "Mapping", ")", ":", "for", "k", ",", "v", "in", "float_candidates", ".", "items", "(", ")", ":", "postfix_str", "+", "=", "f", "\"", "{", "prefix", "}", "_", "{", "k", "}", ":", "{", "v", ":", ".", "3f", "}", "\"", "if", "postfix_str", ":", "prog_bar", ".", "set_postfix_str", "(", "postfix_str", ")"], "docstring": "Adds values as postfix string to progressbar.\r\n\r\n        Args:\r\n            prog_bar: a progressbar (on global rank zero) or an iterable (every other rank).\r\n            candidates: the values to add as postfix strings to the progressbar.\r\n            prefix: the prefix to add to each of these values.", "docstring_tokens": ["adds", "values", "as", "postfix", "string", "to", "progressbar", "args", "prog_bar", "a", "progressbar", "on", "global", "rank", "zero", "or", "an", "iterable", "every", "other", "rank", "candidates", "the", "values", "to", "add", "as", "postfix", "strings", "to", "the", "progressbar", "prefix", "the", "prefix", "to", "add", "to", "each", "of", "these", "values"], "docstring_summary": "Adds values as postfix string to progressbar.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\build_your_own_trainer\\trainer.py", "partition": "train", "function_type": "class_method", "class_name": "MyCustomTrainer", "start_line": 522, "end_line": 543, "hash": "6792565ddb4319fbebde493b24dbc061", "complexity": 7, "parameters": ["prog_bar", "candidates", "Mapping[str", "Union[torch.Tensor", "float", "int]]]]", "prefix"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\reinforcement_learning\\rl\\utils.py", "func_name": "strtobool", "original_string": "def strtobool(val):\r\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\r\n\r\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.\r\n    Raises ValueError if 'val' is anything else.\r\n\r\n    Note: taken from distutils after its deprecation.\r\n\r\n    \"\"\"\r\n    val = val.lower()\r\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\r\n        return 1\r\n    if val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\r\n        return 0\r\n    raise ValueError(f\"invalid truth value {val!r}\")", "language": "python", "code": "def strtobool(val):\r\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\r\n\r\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.\r\n    Raises ValueError if 'val' is anything else.\r\n\r\n    Note: taken from distutils after its deprecation.\r\n\r\n    \"\"\"\r\n    val = val.lower()\r\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\r\n        return 1\r\n    if val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\r\n        return 0\r\n    raise ValueError(f\"invalid truth value {val!r}\")", "code_tokens": ["def", "strtobool", "(", "val", ")", ":", "\"", "\"", "\"", "Convert", "a", "string", "representation", "of", "truth", "to", "true", "(", "1", ")", "or", "false", "(", "0", ")", ".", "True", "values", "are", "'", "y", "'", ",", "'", "yes", "'", ",", "'", "t", "'", ",", "'", "true", "'", ",", "'", "on", "'", ",", "and", "'", "1", "'", ";", "false", "values", "are", "'", "n", "'", ",", "'", "no", "'", ",", "'", "f", "'", ",", "'", "false", "'", ",", "'", "off", "'", ",", "and", "'", "0", "'", ".", "Raises", "ValueError", "if", "'", "val", "'", "is", "anything", "else", ".", "Note", ":", "taken", "from", "distutils", "after", "its", "deprecation", ".", "\"", "\"", "\"", "val", "=", "val", ".", "lower", "(", ")", "if", "val", "in", "(", "\"", "y", "\"", ",", "\"", "yes", "\"", ",", "\"", "t", "\"", ",", "\"", "true", "\"", ",", "\"", "on", "\"", ",", "\"", "1", "\"", ")", ":", "return", "1", "if", "val", "in", "(", "\"", "n", "\"", ",", "\"", "no", "\"", ",", "\"", "f", "\"", ",", "\"", "false", "\"", ",", "\"", "off", "\"", ",", "\"", "0", "\"", ")", ":", "return", "0", "raise", "ValueError", "(", "f", "\"", "invalid", "truth", "value", "{", "val", "!", "r", "}", "\"", ")"], "docstring": "Convert a string representation of truth to true (1) or false (0).\r\n\r\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.\r\n    Raises ValueError if 'val' is anything else.\r\n\r\n    Note: taken from distutils after its deprecation.", "docstring_tokens": ["convert", "a", "string", "representation", "of", "truth", "to", "true", "1", "or", "false", "0", "true", "values", "are", "y", "yes", "t", "true", "on", "and", "1", "false", "values", "are", "n", "no", "f", "false", "off", "and", "0", "raises", "valueerror", "if", "val", "is", "anything", "else", "note", "taken", "from", "distutils", "after", "its", "deprecation"], "docstring_summary": "Convert a string representation of truth to true (1) or false (0).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\reinforcement_learning\\rl\\utils.py", "partition": "train", "function_type": "function", "start_line": 13, "end_line": 27, "hash": "e42f1b74d81923f037cc93f1dc70cf1b", "complexity": 3, "parameters": ["val"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "precompute_freqs_cis", "original_string": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\r\n    \"\"\"Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\r\n\r\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\r\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\r\n    The returned tensor contains complex values in complex64 data type.\r\n\r\n    Args:\r\n        dim (int): Dimension of the frequency tensor.\r\n        end (int): End index for precomputing frequencies.\r\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\r\n\r\n    Returns:\r\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\r\n\r\n    \"\"\"\r\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\r\n    t = torch.arange(end, device=freqs.device)\r\n    freqs = torch.outer(t, freqs).float()\r\n    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\r", "language": "python", "code": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\r\n    \"\"\"Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\r\n\r\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\r\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\r\n    The returned tensor contains complex values in complex64 data type.\r\n\r\n    Args:\r\n        dim (int): Dimension of the frequency tensor.\r\n        end (int): End index for precomputing frequencies.\r\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\r\n\r\n    Returns:\r\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\r\n\r\n    \"\"\"\r\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\r\n    t = torch.arange(end, device=freqs.device)\r\n    freqs = torch.outer(t, freqs).float()\r\n    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\r", "code_tokens": ["def", "precompute_freqs_cis", "(", "dim", ":", "int", ",", "end", ":", "int", ",", "theta", ":", "float", "=", "10000", ".", "0", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Precompute", "the", "frequency", "tensor", "for", "complex", "exponentials", "(", "cis", ")", "with", "given", "dimensions", ".", "This", "function", "calculates", "a", "frequency", "tensor", "with", "complex", "exponentials", "using", "the", "given", "dimension", "'", "dim", "'", "and", "the", "end", "index", "'", "end", "'", ".", "The", "'", "theta", "'", "parameter", "scales", "the", "frequencies", ".", "The", "returned", "tensor", "contains", "complex", "values", "in", "complex64", "data", "type", ".", "Args", ":", "dim", "(", "int", ")", ":", "Dimension", "of", "the", "frequency", "tensor", ".", "end", "(", "int", ")", ":", "End", "index", "for", "precomputing", "frequencies", ".", "theta", "(", "float", ",", "optional", ")", ":", "Scaling", "factor", "for", "frequency", "computation", ".", "Defaults", "to", "10000", ".", "0", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Precomputed", "frequency", "tensor", "with", "complex", "exponentials", ".", "\"", "\"", "\"", "freqs", "=", "1", ".", "0", "/", "(", "theta", "*", "*", "(", "torch", ".", "arange", "(", "0", ",", "dim", ",", "2", ")", "[", ":", "(", "dim", "/", "/", "2", ")", "]", ".", "float", "(", ")", "/", "dim", ")", ")", "t", "=", "torch", ".", "arange", "(", "end", ",", "device", "=", "freqs", ".", "device", ")", "freqs", "=", "torch", ".", "outer", "(", "t", ",", "freqs", ")", ".", "float", "(", ")", "return", "torch", ".", "polar", "(", "torch", ".", "ones_like", "(", "freqs", ")", ",", "freqs", ")"], "docstring": "Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\r\n\r\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\r\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\r\n    The returned tensor contains complex values in complex64 data type.\r\n\r\n    Args:\r\n        dim (int): Dimension of the frequency tensor.\r\n        end (int): End index for precomputing frequencies.\r\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\r\n\r\n    Returns:\r\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.", "docstring_tokens": ["precompute", "the", "frequency", "tensor", "for", "complex", "exponentials", "cis", "with", "given", "dimensions", "this", "function", "calculates", "a", "frequency", "tensor", "with", "complex", "exponentials", "using", "the", "given", "dimension", "dim", "and", "the", "end", "index", "end", "the", "theta", "parameter", "scales", "the", "frequencies", "the", "returned", "tensor", "contains", "complex", "values", "in", "complex64", "data", "type", "args", "dim", "int", "dimension", "of", "the", "frequency", "tensor", "end", "int", "end", "index", "for", "precomputing", "frequencies", "theta", "float", "optional", "scaling", "factor", "for", "frequency", "computation", "defaults", "to", "10000", "0", "returns", "torch", "tensor", "precomputed", "frequency", "tensor", "with", "complex", "exponentials"], "docstring_summary": "Precompute the frequency tensor for complex exponentials (cis) with given dimensions.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 37, "end_line": 56, "hash": "e17446729a5934a7d0f538fc43c44565", "complexity": 1, "parameters": ["dim", "end", "theta"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "reshape_for_broadcast", "original_string": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Reshape frequency tensor for broadcasting it with another tensor.\r\n\r\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\r\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\r\n\r\n    The input freqs_cis tensor is assumed to be of shape (max_seqlen, dim),\r\n    and the first seqlen elements will be sliced, but dim must match x.\r\n\r\n    Args:\r\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\r\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\r\n\r\n    Returns:\r\n        torch.Tensor: Reshaped frequency tensor.\r\n\r\n    \"\"\"\r\n    ndim = x.ndim\r\n    assert 0 <= 1 < ndim\r\n    seqlen = x.shape[1]\r\n    freqs_cis = freqs_cis[0:seqlen]\r\n    assert freqs_cis.shape == (seqlen, x.shape[-1])\r\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\r\n    return freqs_cis.view(*shape)", "language": "python", "code": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Reshape frequency tensor for broadcasting it with another tensor.\r\n\r\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\r\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\r\n\r\n    The input freqs_cis tensor is assumed to be of shape (max_seqlen, dim),\r\n    and the first seqlen elements will be sliced, but dim must match x.\r\n\r\n    Args:\r\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\r\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\r\n\r\n    Returns:\r\n        torch.Tensor: Reshaped frequency tensor.\r\n\r\n    \"\"\"\r\n    ndim = x.ndim\r\n    assert 0 <= 1 < ndim\r\n    seqlen = x.shape[1]\r\n    freqs_cis = freqs_cis[0:seqlen]\r\n    assert freqs_cis.shape == (seqlen, x.shape[-1])\r\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\r\n    return freqs_cis.view(*shape)", "code_tokens": ["def", "reshape_for_broadcast", "(", "freqs_cis", ":", "torch", ".", "Tensor", ",", "x", ":", "torch", ".", "Tensor", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Reshape", "frequency", "tensor", "for", "broadcasting", "it", "with", "another", "tensor", ".", "This", "function", "reshapes", "the", "frequency", "tensor", "to", "have", "the", "same", "shape", "as", "the", "target", "tensor", "'", "x", "'", "for", "the", "purpose", "of", "broadcasting", "the", "frequency", "tensor", "during", "element", "-", "wise", "operations", ".", "The", "input", "freqs_cis", "tensor", "is", "assumed", "to", "be", "of", "shape", "(", "max_seqlen", ",", "dim", ")", ",", "and", "the", "first", "seqlen", "elements", "will", "be", "sliced", ",", "but", "dim", "must", "match", "x", ".", "Args", ":", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Frequency", "tensor", "to", "be", "reshaped", ".", "x", "(", "torch", ".", "Tensor", ")", ":", "Target", "tensor", "for", "broadcasting", "compatibility", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Reshaped", "frequency", "tensor", ".", "\"", "\"", "\"", "ndim", "=", "x", ".", "ndim", "assert", "0", "<", "=", "1", "<", "ndim", "seqlen", "=", "x", ".", "shape", "[", "1", "]", "freqs_cis", "=", "freqs_cis", "[", "0", ":", "seqlen", "]", "assert", "freqs_cis", ".", "shape", "=", "=", "(", "seqlen", ",", "x", ".", "shape", "[", "-", "1", "]", ")", "shape", "=", "[", "d", "if", "i", "=", "=", "1", "or", "i", "=", "=", "ndim", "-", "1", "else", "1", "for", "i", ",", "d", "in", "enumerate", "(", "x", ".", "shape", ")", "]", "return", "freqs_cis", ".", "view", "(", "*", "shape", ")"], "docstring": "Reshape frequency tensor for broadcasting it with another tensor.\r\n\r\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\r\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\r\n\r\n    The input freqs_cis tensor is assumed to be of shape (max_seqlen, dim),\r\n    and the first seqlen elements will be sliced, but dim must match x.\r\n\r\n    Args:\r\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\r\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\r\n\r\n    Returns:\r\n        torch.Tensor: Reshaped frequency tensor.", "docstring_tokens": ["reshape", "frequency", "tensor", "for", "broadcasting", "it", "with", "another", "tensor", "this", "function", "reshapes", "the", "frequency", "tensor", "to", "have", "the", "same", "shape", "as", "the", "target", "tensor", "x", "for", "the", "purpose", "of", "broadcasting", "the", "frequency", "tensor", "during", "element", "wise", "operations", "the", "input", "freqs_cis", "tensor", "is", "assumed", "to", "be", "of", "shape", "max_seqlen", "dim", "and", "the", "first", "seqlen", "elements", "will", "be", "sliced", "but", "dim", "must", "match", "x", "args", "freqs_cis", "torch", "tensor", "frequency", "tensor", "to", "be", "reshaped", "x", "torch", "tensor", "target", "tensor", "for", "broadcasting", "compatibility", "returns", "torch", "tensor", "reshaped", "frequency", "tensor"], "docstring_summary": "Reshape frequency tensor for broadcasting it with another tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 59, "end_line": 82, "hash": "acbf19cb572b172511b8846641fbc774", "complexity": 4, "parameters": ["freqs_cis", "x"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "apply_rotary_emb", "original_string": "def apply_rotary_emb(\r\n    xq: torch.Tensor,\r\n    xk: torch.Tensor,\r\n    freqs_cis: torch.Tensor,\r\n) -> tuple[torch.Tensor, torch.Tensor]:\r\n    \"\"\"Apply rotary embeddings to input tensors using the given frequency tensor.\r\n\r\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\r\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\r\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\r\n    returned as real tensors.\r\n\r\n    Args:\r\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\r\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\r\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\r\n\r\n    Returns:\r\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\r\n\r\n    \"\"\"\r\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\r\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\r\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\r\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\r\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\r\n    return xq_out.type_as(xq), xk_out.type_as(xk)", "language": "python", "code": "def apply_rotary_emb(\r\n    xq: torch.Tensor,\r\n    xk: torch.Tensor,\r\n    freqs_cis: torch.Tensor,\r\n) -> tuple[torch.Tensor, torch.Tensor]:\r\n    \"\"\"Apply rotary embeddings to input tensors using the given frequency tensor.\r\n\r\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\r\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\r\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\r\n    returned as real tensors.\r\n\r\n    Args:\r\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\r\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\r\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\r\n\r\n    Returns:\r\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\r\n\r\n    \"\"\"\r\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\r\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\r\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\r\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\r\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\r\n    return xq_out.type_as(xq), xk_out.type_as(xk)", "code_tokens": ["def", "apply_rotary_emb", "(", "xq", ":", "torch", ".", "Tensor", ",", "xk", ":", "torch", ".", "Tensor", ",", "freqs_cis", ":", "torch", ".", "Tensor", ",", ")", "-", ">", "tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\"", "\"", "\"", "Apply", "rotary", "embeddings", "to", "input", "tensors", "using", "the", "given", "frequency", "tensor", ".", "This", "function", "applies", "rotary", "embeddings", "to", "the", "given", "query", "'", "xq", "'", "and", "key", "'", "xk", "'", "tensors", "using", "the", "provided", "frequency", "tensor", "'", "freqs_cis", "'", ".", "The", "input", "tensors", "are", "reshaped", "as", "complex", "numbers", ",", "and", "the", "frequency", "tensor", "is", "reshaped", "for", "broadcasting", "compatibility", ".", "The", "resulting", "tensors", "contain", "rotary", "embeddings", "and", "are", "returned", "as", "real", "tensors", ".", "Args", ":", "xq", "(", "torch", ".", "Tensor", ")", ":", "Query", "tensor", "to", "apply", "rotary", "embeddings", ".", "xk", "(", "torch", ".", "Tensor", ")", ":", "Key", "tensor", "to", "apply", "rotary", "embeddings", ".", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Precomputed", "frequency", "tensor", "for", "complex", "exponentials", ".", "Returns", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "Tuple", "of", "modified", "query", "tensor", "and", "key", "tensor", "with", "rotary", "embeddings", ".", "\"", "\"", "\"", "xq_", "=", "torch", ".", "view_as_complex", "(", "xq", ".", "float", "(", ")", ".", "reshape", "(", "*", "xq", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ",", "2", ")", ")", "xk_", "=", "torch", ".", "view_as_complex", "(", "xk", ".", "float", "(", ")", ".", "reshape", "(", "*", "xk", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ",", "2", ")", ")", "freqs_cis", "=", "reshape_for_broadcast", "(", "freqs_cis", ",", "xq_", ")", "xq_out", "=", "torch", ".", "view_as_real", "(", "xq_", "*", "freqs_cis", ")", ".", "flatten", "(", "3", ")", "xk_out", "=", "torch", ".", "view_as_real", "(", "xk_", "*", "freqs_cis", ")", ".", "flatten", "(", "3", ")", "return", "xq_out", ".", "type_as", "(", "xq", ")", ",", "xk_out", ".", "type_as", "(", "xk", ")"], "docstring": "Apply rotary embeddings to input tensors using the given frequency tensor.\r\n\r\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\r\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\r\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\r\n    returned as real tensors.\r\n\r\n    Args:\r\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\r\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\r\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\r\n\r\n    Returns:\r\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.", "docstring_tokens": ["apply", "rotary", "embeddings", "to", "input", "tensors", "using", "the", "given", "frequency", "tensor", "this", "function", "applies", "rotary", "embeddings", "to", "the", "given", "query", "xq", "and", "key", "xk", "tensors", "using", "the", "provided", "frequency", "tensor", "freqs_cis", "the", "input", "tensors", "are", "reshaped", "as", "complex", "numbers", "and", "the", "frequency", "tensor", "is", "reshaped", "for", "broadcasting", "compatibility", "the", "resulting", "tensors", "contain", "rotary", "embeddings", "and", "are", "returned", "as", "real", "tensors", "args", "xq", "torch", "tensor", "query", "tensor", "to", "apply", "rotary", "embeddings", "xk", "torch", "tensor", "key", "tensor", "to", "apply", "rotary", "embeddings", "freqs_cis", "torch", "tensor", "precomputed", "frequency", "tensor", "for", "complex", "exponentials", "returns", "tuple", "torch", "tensor", "torch", "tensor", "tuple", "of", "modified", "query", "tensor", "and", "key", "tensor", "with", "rotary", "embeddings"], "docstring_summary": "Apply rotary embeddings to input tensors using the given frequency tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 85, "end_line": 111, "hash": "ab332a794fd234d98167bf6395aea8e5", "complexity": 1, "parameters": ["xq", "xk", "freqs_cis"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "repeat_kv", "original_string": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\r\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\r\n    bs, slen, n_kv_heads, head_dim = x.shape\r\n    if n_rep == 1:\r\n        return x\r\n    return (\r\n        x[:, :, :, None, :]\r\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\r\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\r\n    )", "language": "python", "code": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\r\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\r\n    bs, slen, n_kv_heads, head_dim = x.shape\r\n    if n_rep == 1:\r\n        return x\r\n    return (\r\n        x[:, :, :, None, :]\r\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\r\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\r\n    )", "code_tokens": ["def", "repeat_kv", "(", "x", ":", "torch", ".", "Tensor", ",", "n_rep", ":", "int", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "torch", ".", "repeat_interleave", "(", "x", ",", "dim", "=", "2", ",", "repeats", "=", "n_rep", ")", "\"", "\"", "\"", "bs", ",", "slen", ",", "n_kv_heads", ",", "head_dim", "=", "x", ".", "shape", "if", "n_rep", "=", "=", "1", ":", "return", "x", "return", "(", "x", "[", ":", ",", ":", ",", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "bs", ",", "slen", ",", "n_kv_heads", ",", "n_rep", ",", "head_dim", ")", ".", "reshape", "(", "bs", ",", "slen", ",", "n_kv_heads", "*", "n_rep", ",", "head_dim", ")", ")"], "docstring": "torch.repeat_interleave(x, dim=2, repeats=n_rep)", "docstring_tokens": ["torch", "repeat_interleave", "x", "dim", "2", "repeats", "n_rep"], "docstring_summary": "torch.repeat_interleave(x, dim=2, repeats=n_rep)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 114, "end_line": 123, "hash": "0e3fb00e550ade8b6986719bfa4f9e43", "complexity": 2, "parameters": ["x", "n_rep"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "forward", "original_string": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Forward pass of the attention module.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after attention.\r\n\r\n        \"\"\"\r\n        bs, seqlen, _ = x.shape\r\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\r\n\r\n        xq = xq.view(bs, seqlen, self.n_heads, self.head_dim)\r\n        xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n        xv = xv.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n\r\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\r\n\r\n        # repeat k/v heads if n_kv_heads < n_heads\r\n        keys = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n        values = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n\r\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xk = keys.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xv = values.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n\r\n        # we use casual mask for training\r\n        output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)\r\n        output = output.transpose(1, 2).contiguous()  # (bs, seqlen, n_local_heads, head_dim)\r\n        output = output.view(bs, seqlen, -1)\r\n        return self.wo(output)", "language": "python", "code": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Forward pass of the attention module.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after attention.\r\n\r\n        \"\"\"\r\n        bs, seqlen, _ = x.shape\r\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\r\n\r\n        xq = xq.view(bs, seqlen, self.n_heads, self.head_dim)\r\n        xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n        xv = xv.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n\r\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\r\n\r\n        # repeat k/v heads if n_kv_heads < n_heads\r\n        keys = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n        values = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n\r\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xk = keys.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xv = values.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n\r\n        # we use casual mask for training\r\n        output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)\r\n        output = output.transpose(1, 2).contiguous()  # (bs, seqlen, n_local_heads, head_dim)\r\n        output = output.view(bs, seqlen, -1)\r\n        return self.wo(output)", "code_tokens": ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "freqs_cis", ":", "torch", ".", "Tensor", ",", ")", ":", "\"", "\"", "\"", "Forward", "pass", "of", "the", "attention", "module", ".", "Args", ":", "x", "(", "torch", ".", "Tensor", ")", ":", "Input", "tensor", ".", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Precomputed", "frequency", "tensor", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Output", "tensor", "after", "attention", ".", "\"", "\"", "\"", "bs", ",", "seqlen", ",", "_", "=", "x", ".", "shape", "xq", ",", "xk", ",", "xv", "=", "self", ".", "wq", "(", "x", ")", ",", "self", ".", "wk", "(", "x", ")", ",", "self", ".", "wv", "(", "x", ")", "xq", "=", "xq", ".", "view", "(", "bs", ",", "seqlen", ",", "self", ".", "n_heads", ",", "self", ".", "head_dim", ")", "xk", "=", "xk", ".", "view", "(", "bs", ",", "seqlen", ",", "self", ".", "n_kv_heads", ",", "self", ".", "head_dim", ")", "xv", "=", "xv", ".", "view", "(", "bs", ",", "seqlen", ",", "self", ".", "n_kv_heads", ",", "self", ".", "head_dim", ")", "xq", ",", "xk", "=", "apply_rotary_emb", "(", "xq", ",", "xk", ",", "freqs_cis", "=", "freqs_cis", ")", "keys", "=", "repeat_kv", "(", "xk", ",", "self", ".", "n_rep", ")", "values", "=", "repeat_kv", "(", "xv", ",", "self", ".", "n_rep", ")", "xq", "=", "xq", ".", "transpose", "(", "1", ",", "2", ")", "xk", "=", "keys", ".", "transpose", "(", "1", ",", "2", ")", "xv", "=", "values", ".", "transpose", "(", "1", ",", "2", ")", "output", "=", "F", ".", "scaled_dot_product_attention", "(", "xq", ",", "xk", ",", "xv", ",", "is_causal", "=", "True", ")", "output", "=", "output", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "output", "=", "output", ".", "view", "(", "bs", ",", "seqlen", ",", "-", "1", ")", "return", "self", ".", "wo", "(", "output", ")"], "docstring": "Forward pass of the attention module.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after attention.", "docstring_tokens": ["forward", "pass", "of", "the", "attention", "module", "args", "x", "torch", "tensor", "input", "tensor", "freqs_cis", "torch", "tensor", "precomputed", "frequency", "tensor", "returns", "torch", "tensor", "output", "tensor", "after", "attention"], "docstring_summary": "Forward pass of the attention module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Attention", "start_line": 190, "end_line": 226, "hash": "fb748da4c55df2e11dff76a6a6d42d0a", "complexity": 1, "parameters": ["x", "freqs_cis"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "forward", "original_string": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Perform a forward pass through the TransformerBlock.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after applying attention and feedforward layers.\r\n\r\n        \"\"\"\r\n        h = x + self.attention(self.attention_norm(x), freqs_cis)\r\n        return h + self.feed_forward(self.ffn_norm(h))", "language": "python", "code": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Perform a forward pass through the TransformerBlock.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after applying attention and feedforward layers.\r\n\r\n        \"\"\"\r\n        h = x + self.attention(self.attention_norm(x), freqs_cis)\r\n        return h + self.feed_forward(self.ffn_norm(h))", "code_tokens": ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "freqs_cis", ":", "torch", ".", "Tensor", ",", ")", ":", "\"", "\"", "\"", "Perform", "a", "forward", "pass", "through", "the", "TransformerBlock", ".", "Args", ":", "x", "(", "torch", ".", "Tensor", ")", ":", "Input", "tensor", ".", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Precomputed", "cosine", "and", "sine", "frequencies", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Output", "tensor", "after", "applying", "attention", "and", "feedforward", "layers", ".", "\"", "\"", "\"", "h", "=", "x", "+", "self", ".", "attention", "(", "self", ".", "attention_norm", "(", "x", ")", ",", "freqs_cis", ")", "return", "h", "+", "self", ".", "feed_forward", "(", "self", ".", "ffn_norm", "(", "h", ")", ")"], "docstring": "Perform a forward pass through the TransformerBlock.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after applying attention and feedforward layers.", "docstring_tokens": ["perform", "a", "forward", "pass", "through", "the", "transformerblock", "args", "x", "torch", "tensor", "input", "tensor", "freqs_cis", "torch", "tensor", "precomputed", "cosine", "and", "sine", "frequencies", "returns", "torch", "tensor", "output", "tensor", "after", "applying", "attention", "and", "feedforward", "layers"], "docstring_summary": "Perform a forward pass through the TransformerBlock.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "TransformerBlock", "start_line": 313, "end_line": 329, "hash": "06586ac69beb2f07fa14675af1173492", "complexity": 1, "parameters": ["x", "freqs_cis"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "init_weights", "original_string": "def init_weights(self):\r\n        \"\"\"[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.\r\n\r\n        \"\"\"\r\n        with torch.device(self.freqs_cis.device):\r\n            self.freqs_cis = self._precompute_freqs_cis()\r\n        nn.init.normal_(self.tok_embeddings.weight)\r\n        for layer in self.layers.values():\r\n            layer.init_weights()\r\n        self.norm.reset_parameters()\r\n        final_out_std = self.model_args.dim**-0.5\r\n        cutoff_factor = 3\r\n        nn.init.trunc_normal_(\r\n            self.output.weight,\r\n            mean=0.0,\r\n            std=final_out_std,\r\n            a=-cutoff_factor * final_out_std,\r\n            b=cutoff_factor * final_out_std,\r\n        )", "language": "python", "code": "def init_weights(self):\r\n        \"\"\"[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.\r\n\r\n        \"\"\"\r\n        with torch.device(self.freqs_cis.device):\r\n            self.freqs_cis = self._precompute_freqs_cis()\r\n        nn.init.normal_(self.tok_embeddings.weight)\r\n        for layer in self.layers.values():\r\n            layer.init_weights()\r\n        self.norm.reset_parameters()\r\n        final_out_std = self.model_args.dim**-0.5\r\n        cutoff_factor = 3\r\n        nn.init.trunc_normal_(\r\n            self.output.weight,\r\n            mean=0.0,\r\n            std=final_out_std,\r\n            a=-cutoff_factor * final_out_std,\r\n            b=cutoff_factor * final_out_std,\r\n        )", "code_tokens": ["def", "init_weights", "(", "self", ")", ":", "\"", "\"", "\"", "[", "Note", ":", "On", "`", "`", "init_weights", "`", "`", "vs", ".", "`", "`", "reset_parameters", "`", "`", "]", "Modules", "may", "define", "`", "`", "reset_parameters", "`", "`", "to", "initialize", "parameter", "values", ".", "`", "`", "reset_parameters", "`", "`", "is", "meant", "to", "only", "initialize", "directly", "owned", "parameters", "/", "buffers", ",", "not", "those", "of", "their", "child", "modules", ",", "and", "it", "can", "be", "used", "to", "give", "the", "initial", "values", "for", "these", "tensors", ".", "Separately", ",", "users", "may", "want", "custom", "initialization", "for", "their", "modules", ",", "different", "from", "that", "in", "`", "`", "reset_parameters", "`", "`", ".", "For", "this", ",", "we", "define", "`", "`", "init_weights", "`", "`", ".", "We", "only", "call", "it", "in", "the", "constructor", "of", "this", "`", "`", "Transformer", "`", "`", "root", "module", "to", "avoid", "reinitializing", "tensors", ".", "\"", "\"", "\"", "with", "torch", ".", "device", "(", "self", ".", "freqs_cis", ".", "device", ")", ":", "self", ".", "freqs_cis", "=", "self", ".", "_precompute_freqs_cis", "(", ")", "nn", ".", "init", ".", "normal_", "(", "self", ".", "tok_embeddings", ".", "weight", ")", "for", "layer", "in", "self", ".", "layers", ".", "values", "(", ")", ":", "layer", ".", "init_weights", "(", ")", "self", ".", "norm", ".", "reset_parameters", "(", ")", "final_out_std", "=", "self", ".", "model_args", ".", "dim", "*", "*", "-", "0", ".", "5", "cutoff_factor", "=", "3", "nn", ".", "init", ".", "trunc_normal_", "(", "self", ".", "output", ".", "weight", ",", "mean", "=", "0", ".", "0", ",", "std", "=", "final_out_std", ",", "a", "=", "-", "cutoff_factor", "*", "final_out_std", ",", "b", "=", "cutoff_factor", "*", "final_out_std", ",", ")"], "docstring": "[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.", "docstring_tokens": ["note", "on", "init_weights", "vs", "reset_parameters", "modules", "may", "define", "reset_parameters", "to", "initialize", "parameter", "values", "reset_parameters", "is", "meant", "to", "only", "initialize", "directly", "owned", "parameters", "buffers", "not", "those", "of", "their", "child", "modules", "and", "it", "can", "be", "used", "to", "give", "the", "initial", "values", "for", "these", "tensors", "separately", "users", "may", "want", "custom", "initialization", "for", "their", "modules", "different", "from", "that", "in", "reset_parameters", "for", "this", "we", "define", "init_weights", "we", "only", "call", "it", "in", "the", "constructor", "of", "this", "transformer", "root", "module", "to", "avoid", "reinitializing", "tensors"], "docstring_summary": "[Note: On ``init_weights`` vs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 386, "end_line": 414, "hash": "94da5e7255f0ac07771c9b7856d1f3af", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "forward", "original_string": "def forward(self, tokens: torch.Tensor):\r\n        \"\"\"Perform a forward pass through the Transformer model.\r\n\r\n        Args:\r\n            tokens (torch.Tensor): Input token indices.\r\n\r\n        Returns:\r\n            torch.Tensor: Output logits after applying the Transformer model.\r\n\r\n        \"\"\"\r\n        # passthrough for nonexistent layers, allows easy configuration of pipeline parallel stages\r\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\r\n\r\n        for layer in self.layers.values():\r\n            h = layer(h, self.freqs_cis)\r\n\r\n        h = self.norm(h) if self.norm else h\r\n        return self.output(h).float() if self.output else h", "language": "python", "code": "def forward(self, tokens: torch.Tensor):\r\n        \"\"\"Perform a forward pass through the Transformer model.\r\n\r\n        Args:\r\n            tokens (torch.Tensor): Input token indices.\r\n\r\n        Returns:\r\n            torch.Tensor: Output logits after applying the Transformer model.\r\n\r\n        \"\"\"\r\n        # passthrough for nonexistent layers, allows easy configuration of pipeline parallel stages\r\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\r\n\r\n        for layer in self.layers.values():\r\n            h = layer(h, self.freqs_cis)\r\n\r\n        h = self.norm(h) if self.norm else h\r\n        return self.output(h).float() if self.output else h", "code_tokens": ["def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ")", ":", "\"", "\"", "\"", "Perform", "a", "forward", "pass", "through", "the", "Transformer", "model", ".", "Args", ":", "tokens", "(", "torch", ".", "Tensor", ")", ":", "Input", "token", "indices", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Output", "logits", "after", "applying", "the", "Transformer", "model", ".", "\"", "\"", "\"", "h", "=", "self", ".", "tok_embeddings", "(", "tokens", ")", "if", "self", ".", "tok_embeddings", "else", "tokens", "for", "layer", "in", "self", ".", "layers", ".", "values", "(", ")", ":", "h", "=", "layer", "(", "h", ",", "self", ".", "freqs_cis", ")", "h", "=", "self", ".", "norm", "(", "h", ")", "if", "self", ".", "norm", "else", "h", "return", "self", ".", "output", "(", "h", ")", ".", "float", "(", ")", "if", "self", ".", "output", "else", "h"], "docstring": "Perform a forward pass through the Transformer model.\r\n\r\n        Args:\r\n            tokens (torch.Tensor): Input token indices.\r\n\r\n        Returns:\r\n            torch.Tensor: Output logits after applying the Transformer model.", "docstring_tokens": ["perform", "a", "forward", "pass", "through", "the", "transformer", "model", "args", "tokens", "torch", "tensor", "input", "token", "indices", "returns", "torch", "tensor", "output", "logits", "after", "applying", "the", "transformer", "model"], "docstring_summary": "Perform a forward pass through the Transformer model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 425, "end_line": 442, "hash": "abfbf414d576b9e3077c4a3f0899266a", "complexity": 5, "parameters": ["tokens"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\model.py", "func_name": "from_model_args", "original_string": "def from_model_args(cls, model_args: ModelArgs) -> \"Transformer\":\r\n        \"\"\"Initialize a Transformer model from a ModelArgs object.\r\n\r\n        Args:\r\n            model_args (ModelArgs): Model configuration arguments.\r\n\r\n        Returns:\r\n            Transformer: Transformer model.\r\n\r\n        \"\"\"\r\n        return cls(model_args)", "language": "python", "code": "def from_model_args(cls, model_args: ModelArgs) -> \"Transformer\":\r\n        \"\"\"Initialize a Transformer model from a ModelArgs object.\r\n\r\n        Args:\r\n            model_args (ModelArgs): Model configuration arguments.\r\n\r\n        Returns:\r\n            Transformer: Transformer model.\r\n\r\n        \"\"\"\r\n        return cls(model_args)", "code_tokens": ["def", "from_model_args", "(", "cls", ",", "model_args", ":", "ModelArgs", ")", "-", ">", "\"", "Transformer", "\"", ":", "\"", "\"", "\"", "Initialize", "a", "Transformer", "model", "from", "a", "ModelArgs", "object", ".", "Args", ":", "model_args", "(", "ModelArgs", ")", ":", "Model", "configuration", "arguments", ".", "Returns", ":", "Transformer", ":", "Transformer", "model", ".", "\"", "\"", "\"", "return", "cls", "(", "model_args", ")"], "docstring": "Initialize a Transformer model from a ModelArgs object.\r\n\r\n        Args:\r\n            model_args (ModelArgs): Model configuration arguments.\r\n\r\n        Returns:\r\n            Transformer: Transformer model.", "docstring_tokens": ["initialize", "a", "transformer", "model", "from", "a", "modelargs", "object", "args", "model_args", "modelargs", "model", "configuration", "arguments", "returns", "transformer", "transformer", "model"], "docstring_summary": "Initialize a Transformer model from a ModelArgs object.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 445, "end_line": 455, "hash": "1bb7c9a9a78cd29ef878e007645a2f49", "complexity": 1, "parameters": ["model_args"]}
{"repo": "pytorch-lightning", "path": "examples\\fabric\\tensor_parallel\\parallelism.py", "func_name": "parallelize", "original_string": "def parallelize(model: Transformer, device_mesh: DeviceMesh) -> Transformer:\r\n    \"\"\"Apply parallelisms and activation checkpointing to the model.\r\n\r\n    NOTE: The passed-in model preferably should be on meta device. Otherwise,\r\n    the model must fit on GPU or CPU memory.\r\n\r\n    \"\"\"\r\n\r\n    dp_mesh = device_mesh[\"data_parallel\"]\r\n    tp_mesh = device_mesh[\"tensor_parallel\"]\r\n\r\n    if tp_mesh.size() > 1:\r\n        # 1. Parallelize the first embedding and the last linear proj layer\r\n        # 2. Parallelize the root norm layer over the sequence dim\r\n        # 3. Shard the first transformer block's inputs\r\n\r\n        # Parallelize the first embedding and the last linear out projection\r\n        plan = {\r\n            \"tok_embeddings\": RowwiseParallel(input_layouts=Replicate()),\r\n            \"output\": ColwiseParallel(\r\n                input_layouts=Shard(1),\r\n                # Optional: Shard the output along the class dimension to compute the loss in parallel.\r\n                # See `loss_parallel` in `train.py`\r\n                output_layouts=Shard(-1),\r\n                use_local_output=False,\r\n            ),\r\n            \"norm\": SequenceParallel(),\r\n            \"layers.0\": PrepareModuleInput(\r\n                input_layouts=(Replicate(), None),\r\n                desired_input_layouts=(Shard(1), None),\r\n                use_local_output=True,\r\n            ),\r\n        }\r\n        model = parallelize_module(model, tp_mesh, plan)\r\n\r\n        # Parallelize each transformer block\r\n        for transformer_block in model.layers.values():\r\n            plan = {\r\n                \"attention\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1), None),\r\n                    desired_input_layouts=(Replicate(), None),\r\n                ),\r\n                \"attention.wq\": ColwiseParallel(),\r\n                \"attention.wk\": ColwiseParallel(),\r\n                \"attention.wv\": ColwiseParallel(),\r\n                \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"attention_norm\": SequenceParallel(),\r\n                \"feed_forward\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1),),\r\n                    desired_input_layouts=(Replicate(),),\r\n                ),\r\n                \"feed_forward.w1\": ColwiseParallel(),\r\n                \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"feed_forward.w3\": ColwiseParallel(),\r\n                \"ffn_norm\": SequenceParallel(),\r\n            }\r\n\r\n            # Adjust attention module to use the local number of heads\r\n            attn_layer = transformer_block.attention\r\n            attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size()\r\n            attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size()\r\n\r\n            # Apply the plan for the current transformer block\r\n            parallelize_module(transformer_block, tp_mesh, plan)\r\n\r\n    if dp_mesh.size() > 1:\r\n        assert dp_mesh.ndim == 1  # Hybrid-sharding not supported\r\n\r\n        # NOTE: Currently, the user is required to manually handle precision settings such as the `mp_policy` here\r\n        # because the model parallel strategy does not respect all settings of `Fabric(precision=...)` at the moment.\r\n        mp_policy = MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32)\r\n\r\n        fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\r\n        for layer_id, transformer_block in model.layers.items():\r\n            # Apply activation checkpointing\r\n            transformer_block = checkpoint_wrapper(transformer_block)\r\n            # As an optimization, do not reshard after forward for the last\r\n            # transformer block since FSDP would prefetch it immediately\r\n            reshard_after_forward = int(layer_id) < len(model.layers) - 1\r\n            fully_shard(\r\n                transformer_block,\r\n                **fsdp_config,\r\n                reshard_after_forward=reshard_after_forward,\r\n            )\r\n            model.layers[layer_id] = transformer_block\r\n        model = fully_shard(model, **fsdp_config)\r\n\r\n    return model", "language": "python", "code": "def parallelize(model: Transformer, device_mesh: DeviceMesh) -> Transformer:\r\n    \"\"\"Apply parallelisms and activation checkpointing to the model.\r\n\r\n    NOTE: The passed-in model preferably should be on meta device. Otherwise,\r\n    the model must fit on GPU or CPU memory.\r\n\r\n    \"\"\"\r\n\r\n    dp_mesh = device_mesh[\"data_parallel\"]\r\n    tp_mesh = device_mesh[\"tensor_parallel\"]\r\n\r\n    if tp_mesh.size() > 1:\r\n        # 1. Parallelize the first embedding and the last linear proj layer\r\n        # 2. Parallelize the root norm layer over the sequence dim\r\n        # 3. Shard the first transformer block's inputs\r\n\r\n        # Parallelize the first embedding and the last linear out projection\r\n        plan = {\r\n            \"tok_embeddings\": RowwiseParallel(input_layouts=Replicate()),\r\n            \"output\": ColwiseParallel(\r\n                input_layouts=Shard(1),\r\n                # Optional: Shard the output along the class dimension to compute the loss in parallel.\r\n                # See `loss_parallel` in `train.py`\r\n                output_layouts=Shard(-1),\r\n                use_local_output=False,\r\n            ),\r\n            \"norm\": SequenceParallel(),\r\n            \"layers.0\": PrepareModuleInput(\r\n                input_layouts=(Replicate(), None),\r\n                desired_input_layouts=(Shard(1), None),\r\n                use_local_output=True,\r\n            ),\r\n        }\r\n        model = parallelize_module(model, tp_mesh, plan)\r\n\r\n        # Parallelize each transformer block\r\n        for transformer_block in model.layers.values():\r\n            plan = {\r\n                \"attention\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1), None),\r\n                    desired_input_layouts=(Replicate(), None),\r\n                ),\r\n                \"attention.wq\": ColwiseParallel(),\r\n                \"attention.wk\": ColwiseParallel(),\r\n                \"attention.wv\": ColwiseParallel(),\r\n                \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"attention_norm\": SequenceParallel(),\r\n                \"feed_forward\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1),),\r\n                    desired_input_layouts=(Replicate(),),\r\n                ),\r\n                \"feed_forward.w1\": ColwiseParallel(),\r\n                \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"feed_forward.w3\": ColwiseParallel(),\r\n                \"ffn_norm\": SequenceParallel(),\r\n            }\r\n\r\n            # Adjust attention module to use the local number of heads\r\n            attn_layer = transformer_block.attention\r\n            attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size()\r\n            attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size()\r\n\r\n            # Apply the plan for the current transformer block\r\n            parallelize_module(transformer_block, tp_mesh, plan)\r\n\r\n    if dp_mesh.size() > 1:\r\n        assert dp_mesh.ndim == 1  # Hybrid-sharding not supported\r\n\r\n        # NOTE: Currently, the user is required to manually handle precision settings such as the `mp_policy` here\r\n        # because the model parallel strategy does not respect all settings of `Fabric(precision=...)` at the moment.\r\n        mp_policy = MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32)\r\n\r\n        fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\r\n        for layer_id, transformer_block in model.layers.items():\r\n            # Apply activation checkpointing\r\n            transformer_block = checkpoint_wrapper(transformer_block)\r\n            # As an optimization, do not reshard after forward for the last\r\n            # transformer block since FSDP would prefetch it immediately\r\n            reshard_after_forward = int(layer_id) < len(model.layers) - 1\r\n            fully_shard(\r\n                transformer_block,\r\n                **fsdp_config,\r\n                reshard_after_forward=reshard_after_forward,\r\n            )\r\n            model.layers[layer_id] = transformer_block\r\n        model = fully_shard(model, **fsdp_config)\r\n\r\n    return model", "code_tokens": ["def", "parallelize", "(", "model", ":", "Transformer", ",", "device_mesh", ":", "DeviceMesh", ")", "-", ">", "Transformer", ":", "\"", "\"", "\"", "Apply", "parallelisms", "and", "activation", "checkpointing", "to", "the", "model", ".", "NOTE", ":", "The", "passed", "-", "in", "model", "preferably", "should", "be", "on", "meta", "device", ".", "Otherwise", ",", "the", "model", "must", "fit", "on", "GPU", "or", "CPU", "memory", ".", "\"", "\"", "\"", "dp_mesh", "=", "device_mesh", "[", "\"", "data_parallel", "\"", "]", "tp_mesh", "=", "device_mesh", "[", "\"", "tensor_parallel", "\"", "]", "if", "tp_mesh", ".", "size", "(", ")", ">", "1", ":", "plan", "=", "{", "\"", "tok_embeddings", "\"", ":", "RowwiseParallel", "(", "input_layouts", "=", "Replicate", "(", ")", ")", ",", "\"", "output", "\"", ":", "ColwiseParallel", "(", "input_layouts", "=", "Shard", "(", "1", ")", ",", "output_layouts", "=", "Shard", "(", "-", "1", ")", ",", "use_local_output", "=", "False", ",", ")", ",", "\"", "norm", "\"", ":", "SequenceParallel", "(", ")", ",", "\"", "layers", ".", "0", "\"", ":", "PrepareModuleInput", "(", "input_layouts", "=", "(", "Replicate", "(", ")", ",", "None", ")", ",", "desired_input_layouts", "=", "(", "Shard", "(", "1", ")", ",", "None", ")", ",", "use_local_output", "=", "True", ",", ")", ",", "}", "model", "=", "parallelize_module", "(", "model", ",", "tp_mesh", ",", "plan", ")", "for", "transformer_block", "in", "model", ".", "layers", ".", "values", "(", ")", ":", "plan", "=", "{", "\"", "attention", "\"", ":", "PrepareModuleInput", "(", "input_layouts", "=", "(", "Shard", "(", "1", ")", ",", "None", ")", ",", "desired_input_layouts", "=", "(", "Replicate", "(", ")", ",", "None", ")", ",", ")", ",", "\"", "attention", ".", "wq", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "attention", ".", "wk", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "attention", ".", "wv", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "attention", ".", "wo", "\"", ":", "RowwiseParallel", "(", "output_layouts", "=", "Shard", "(", "1", ")", ")", ",", "\"", "attention_norm", "\"", ":", "SequenceParallel", "(", ")", ",", "\"", "feed_forward", "\"", ":", "PrepareModuleInput", "(", "input_layouts", "=", "(", "Shard", "(", "1", ")", ",", ")", ",", "desired_input_layouts", "=", "(", "Replicate", "(", ")", ",", ")", ",", ")", ",", "\"", "feed_forward", ".", "w1", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "feed_forward", ".", "w2", "\"", ":", "RowwiseParallel", "(", "output_layouts", "=", "Shard", "(", "1", ")", ")", ",", "\"", "feed_forward", ".", "w3", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "ffn_norm", "\"", ":", "SequenceParallel", "(", ")", ",", "}", "attn_layer", "=", "transformer_block", ".", "attention", "attn_layer", ".", "n_heads", "=", "attn_layer", ".", "n_heads", "/", "/", "tp_mesh", ".", "size", "(", ")", "attn_layer", ".", "n_kv_heads", "=", "attn_layer", ".", "n_kv_heads", "/", "/", "tp_mesh", ".", "size", "(", ")", "parallelize_module", "(", "transformer_block", ",", "tp_mesh", ",", "plan", ")", "if", "dp_mesh", ".", "size", "(", ")", ">", "1", ":", "assert", "dp_mesh", ".", "ndim", "=", "=", "1", "mp_policy", "=", "MixedPrecisionPolicy", "(", "param_dtype", "=", "torch", ".", "bfloat16", ",", "reduce_dtype", "=", "torch", ".", "float32", ")", "fsdp_config", "=", "{", "\"", "mesh", "\"", ":", "dp_mesh", ",", "\"", "mp_policy", "\"", ":", "mp_policy", "}", "for", "layer_id", ",", "transformer_block", "in", "model", ".", "layers", ".", "items", "(", ")", ":", "transformer_block", "=", "checkpoint_wrapper", "(", "transformer_block", ")", "reshard_after_forward", "=", "int", "(", "layer_id", ")", "<", "len", "(", "model", ".", "layers", ")", "-", "1", "fully_shard", "(", "transformer_block", ",", "*", "*", "fsdp_config", ",", "reshard_after_forward", "=", "reshard_after_forward", ",", ")", "model", ".", "layers", "[", "layer_id", "]", "=", "transformer_block", "model", "=", "fully_shard", "(", "model", ",", "*", "*", "fsdp_config", ")", "return", "model"], "docstring": "Apply parallelisms and activation checkpointing to the model.\r\n\r\n    NOTE: The passed-in model preferably should be on meta device. Otherwise,\r\n    the model must fit on GPU or CPU memory.", "docstring_tokens": ["apply", "parallelisms", "and", "activation", "checkpointing", "to", "the", "model", "note", "the", "passed", "in", "model", "preferably", "should", "be", "on", "meta", "device", "otherwise", "the", "model", "must", "fit", "on", "gpu", "or", "cpu", "memory"], "docstring_summary": "Apply parallelisms and activation checkpointing to the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\fabric\\tensor_parallel\\parallelism.py", "partition": "train", "function_type": "function", "start_line": 18, "end_line": 105, "hash": "6684c41993c9851f3b02bf6aacb86948", "complexity": 5, "parameters": ["model", "device_mesh"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\basics\\autoencoder.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        num_samples: int = 3,\r\n        nrow: int = 8,\r\n        padding: int = 2,\r\n        normalize: bool = True,\r\n        value_range: Optional[tuple[int, int]] = None,\r\n        scale_each: bool = False,\r\n        pad_value: int = 0,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            num_samples: Number of images displayed in the grid. Default: ``3``.\r\n            nrow: Number of images displayed in each row of the grid.\r\n                The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\r\n            padding: Amount of padding. Default: ``2``.\r\n            normalize: If ``True``, shift the image to the range (0, 1),\r\n                by the min and max values specified by :attr:`range`. Default: ``False``.\r\n            value_range: Tuple (min, max) where min and max are numbers,\r\n                then these numbers are used to normalize the image. By default, min and max\r\n                are computed from the tensor.\r\n            scale_each: If ``True``, scale each image in the batch of\r\n                images separately rather than the (min, max) over all images. Default: ``False``.\r\n            pad_value: Value for the padded pixels. Default: ``0``.\r\n        \"\"\"\r\n        if not _TORCHVISION_AVAILABLE:  # pragma: no cover\r\n            raise ModuleNotFoundError(\"You want to use `torchvision` which is not installed yet.\")\r\n\r\n        super().__init__()\r\n        self.num_samples = num_samples\r\n        self.nrow = nrow\r\n        self.padding = padding\r\n        self.normalize = normalize\r\n        self.value_range = value_range\r\n        self.scale_each = scale_each\r\n        self.pad_value = pad_value", "language": "python", "code": "def __init__(\r\n        self,\r\n        num_samples: int = 3,\r\n        nrow: int = 8,\r\n        padding: int = 2,\r\n        normalize: bool = True,\r\n        value_range: Optional[tuple[int, int]] = None,\r\n        scale_each: bool = False,\r\n        pad_value: int = 0,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            num_samples: Number of images displayed in the grid. Default: ``3``.\r\n            nrow: Number of images displayed in each row of the grid.\r\n                The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\r\n            padding: Amount of padding. Default: ``2``.\r\n            normalize: If ``True``, shift the image to the range (0, 1),\r\n                by the min and max values specified by :attr:`range`. Default: ``False``.\r\n            value_range: Tuple (min, max) where min and max are numbers,\r\n                then these numbers are used to normalize the image. By default, min and max\r\n                are computed from the tensor.\r\n            scale_each: If ``True``, scale each image in the batch of\r\n                images separately rather than the (min, max) over all images. Default: ``False``.\r\n            pad_value: Value for the padded pixels. Default: ``0``.\r\n        \"\"\"\r\n        if not _TORCHVISION_AVAILABLE:  # pragma: no cover\r\n            raise ModuleNotFoundError(\"You want to use `torchvision` which is not installed yet.\")\r\n\r\n        super().__init__()\r\n        self.num_samples = num_samples\r\n        self.nrow = nrow\r\n        self.padding = padding\r\n        self.normalize = normalize\r\n        self.value_range = value_range\r\n        self.scale_each = scale_each\r\n        self.pad_value = pad_value", "code_tokens": ["def", "__init__", "(", "self", ",", "num_samples", ":", "int", "=", "3", ",", "nrow", ":", "int", "=", "8", ",", "padding", ":", "int", "=", "2", ",", "normalize", ":", "bool", "=", "True", ",", "value_range", ":", "Optional", "[", "tuple", "[", "int", ",", "int", "]", "]", "=", "None", ",", "scale_each", ":", "bool", "=", "False", ",", "pad_value", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "num_samples", ":", "Number", "of", "images", "displayed", "in", "the", "grid", ".", "Default", ":", "`", "`", "3", "`", "`", ".", "nrow", ":", "Number", "of", "images", "displayed", "in", "each", "row", "of", "the", "grid", ".", "The", "final", "grid", "size", "is", "`", "`", "(", "B", "/", "nrow", ",", "nrow", ")", "`", "`", ".", "Default", ":", "`", "`", "8", "`", "`", ".", "padding", ":", "Amount", "of", "padding", ".", "Default", ":", "`", "`", "2", "`", "`", ".", "normalize", ":", "If", "`", "`", "True", "`", "`", ",", "shift", "the", "image", "to", "the", "range", "(", "0", ",", "1", ")", ",", "by", "the", "min", "and", "max", "values", "specified", "by", ":", "attr", ":", "`", "range", "`", ".", "Default", ":", "`", "`", "False", "`", "`", ".", "value_range", ":", "Tuple", "(", "min", ",", "max", ")", "where", "min", "and", "max", "are", "numbers", ",", "then", "these", "numbers", "are", "used", "to", "normalize", "the", "image", ".", "By", "default", ",", "min", "and", "max", "are", "computed", "from", "the", "tensor", ".", "scale_each", ":", "If", "`", "`", "True", "`", "`", ",", "scale", "each", "image", "in", "the", "batch", "of", "images", "separately", "rather", "than", "the", "(", "min", ",", "max", ")", "over", "all", "images", ".", "Default", ":", "`", "`", "False", "`", "`", ".", "pad_value", ":", "Value", "for", "the", "padded", "pixels", ".", "Default", ":", "`", "`", "0", "`", "`", ".", "\"", "\"", "\"", "if", "not", "_TORCHVISION_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "\"", "You", "want", "to", "use", "`", "torchvision", "`", "which", "is", "not", "installed", "yet", ".", "\"", ")", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "num_samples", "=", "num_samples", "self", ".", "nrow", "=", "nrow", "self", ".", "padding", "=", "padding", "self", ".", "normalize", "=", "normalize", "self", ".", "value_range", "=", "value_range", "self", ".", "scale_each", "=", "scale_each", "self", ".", "pad_value", "=", "pad_value"], "docstring": "Args:\r\n            num_samples: Number of images displayed in the grid. Default: ``3``.\r\n            nrow: Number of images displayed in each row of the grid.\r\n                The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\r\n            padding: Amount of padding. Default: ``2``.\r\n            normalize: If ``True``, shift the image to the range (0, 1),\r\n                by the min and max values specified by :attr:`range`. Default: ``False``.\r\n            value_range: Tuple (min, max) where min and max are numbers,\r\n                then these numbers are used to normalize the image. By default, min and max\r\n                are computed from the tensor.\r\n            scale_each: If ``True``, scale each image in the batch of\r\n                images separately rather than the (min, max) over all images. Default: ``False``.\r\n            pad_value: Value for the padded pixels. Default: ``0``.", "docstring_tokens": ["args", "num_samples", "number", "of", "images", "displayed", "in", "the", "grid", "default", "3", "nrow", "number", "of", "images", "displayed", "in", "each", "row", "of", "the", "grid", "the", "final", "grid", "size", "is", "b", "nrow", "nrow", "default", "8", "padding", "amount", "of", "padding", "default", "2", "normalize", "if", "true", "shift", "the", "image", "to", "the", "range", "0", "1", "by", "the", "min", "and", "max", "values", "specified", "by", "attr", "range", "default", "false", "value_range", "tuple", "min", "max", "where", "min", "and", "max", "are", "numbers", "then", "these", "numbers", "are", "used", "to", "normalize", "the", "image", "by", "default", "min", "and", "max", "are", "computed", "from", "the", "tensor", "scale_each", "if", "true", "scale", "each", "image", "in", "the", "batch", "of", "images", "separately", "rather", "than", "the", "min", "max", "over", "all", "images", "default", "false", "pad_value", "value", "for", "the", "padded", "pixels", "default", "0"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\basics\\autoencoder.py", "partition": "train", "function_type": "class_method", "class_name": "ImageSampler", "start_line": 42, "end_line": 77, "hash": "b79a05e4adf4d55da36392b5e1216881", "complexity": 2, "parameters": ["num_samples", "nrow", "padding", "normalize", "value_range", "int]]", "scale_each", "pad_value"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "func_name": "__init__", "original_string": "def __init__(self, dl_path: Union[str, Path] = \"data\", num_workers: int = 0, batch_size: int = 8):\r\n        \"\"\"CatDogImageDataModule.\r\n\r\n        Args:\r\n            dl_path: root directory where to download the data\r\n            num_workers: number of CPU workers\r\n            batch_size: number of sample in a batch\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        self._dl_path = dl_path\r\n        self._num_workers = num_workers\r\n        self._batch_size = batch_size", "language": "python", "code": "def __init__(self, dl_path: Union[str, Path] = \"data\", num_workers: int = 0, batch_size: int = 8):\r\n        \"\"\"CatDogImageDataModule.\r\n\r\n        Args:\r\n            dl_path: root directory where to download the data\r\n            num_workers: number of CPU workers\r\n            batch_size: number of sample in a batch\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        self._dl_path = dl_path\r\n        self._num_workers = num_workers\r\n        self._batch_size = batch_size", "code_tokens": ["def", "__init__", "(", "self", ",", "dl_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "\"", "data", "\"", ",", "num_workers", ":", "int", "=", "0", ",", "batch_size", ":", "int", "=", "8", ")", ":", "\"", "\"", "\"", "CatDogImageDataModule", ".", "Args", ":", "dl_path", ":", "root", "directory", "where", "to", "download", "the", "data", "num_workers", ":", "number", "of", "CPU", "workers", "batch_size", ":", "number", "of", "sample", "in", "a", "batch", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "_dl_path", "=", "dl_path", "self", ".", "_num_workers", "=", "num_workers", "self", ".", "_batch_size", "=", "batch_size"], "docstring": "CatDogImageDataModule.\r\n\r\n        Args:\r\n            dl_path: root directory where to download the data\r\n            num_workers: number of CPU workers\r\n            batch_size: number of sample in a batch", "docstring_tokens": ["catdogimagedatamodule", "args", "dl_path", "root", "directory", "where", "to", "download", "the", "data", "num_workers", "number", "of", "cpu", "workers", "batch_size", "number", "of", "sample", "in", "a", "batch"], "docstring_summary": "CatDogImageDataModule.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "partition": "train", "function_type": "class_method", "class_name": "CatDogImageDataModule", "start_line": 93, "end_line": 106, "hash": "561fb11545df34587aad445e1556a688", "complexity": 1, "parameters": ["dl_path", "Path]", "num_workers", "batch_size"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "func_name": "__dataloader", "original_string": "def __dataloader(self, train: bool):\r\n        \"\"\"Train/validation loaders.\"\"\"\r\n        if train:\r\n            dataset = self.create_dataset(self.data_path.joinpath(\"train\"), self.train_transform)\r\n        else:\r\n            dataset = self.create_dataset(self.data_path.joinpath(\"validation\"), self.valid_transform)\r\n        return DataLoader(dataset=dataset, batch_size=self._batch_size, num_workers=self._num_workers, shuffle=train)", "language": "python", "code": "def __dataloader(self, train: bool):\r\n        \"\"\"Train/validation loaders.\"\"\"\r\n        if train:\r\n            dataset = self.create_dataset(self.data_path.joinpath(\"train\"), self.train_transform)\r\n        else:\r\n            dataset = self.create_dataset(self.data_path.joinpath(\"validation\"), self.valid_transform)\r\n        return DataLoader(dataset=dataset, batch_size=self._batch_size, num_workers=self._num_workers, shuffle=train)", "code_tokens": ["def", "__dataloader", "(", "self", ",", "train", ":", "bool", ")", ":", "\"", "\"", "\"", "Train", "/", "validation", "loaders", ".", "\"", "\"", "\"", "if", "train", ":", "dataset", "=", "self", ".", "create_dataset", "(", "self", ".", "data_path", ".", "joinpath", "(", "\"", "train", "\"", ")", ",", "self", ".", "train_transform", ")", "else", ":", "dataset", "=", "self", ".", "create_dataset", "(", "self", ".", "data_path", ".", "joinpath", "(", "\"", "validation", "\"", ")", ",", "self", ".", "valid_transform", ")", "return", "DataLoader", "(", "dataset", "=", "dataset", ",", "batch_size", "=", "self", ".", "_batch_size", ",", "num_workers", "=", "self", ".", "_num_workers", ",", "shuffle", "=", "train", ")"], "docstring": "Train/validation loaders.", "docstring_tokens": ["train", "validation", "loaders"], "docstring_summary": "Train/validation loaders.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "partition": "train", "function_type": "class_method", "class_name": "CatDogImageDataModule", "start_line": 136, "end_line": 142, "hash": "726226aef80c6270b895586098fa43c1", "complexity": 2, "parameters": ["train"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        backbone: str = \"resnet50\",\r\n        train_bn: bool = False,\r\n        milestones: tuple = (2, 4),\r\n        batch_size: int = 32,\r\n        lr: float = 1e-3,\r\n        lr_scheduler_gamma: float = 1e-1,\r\n        num_workers: int = 6,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"TransferLearningModel.\r\n\r\n        Args:\r\n            backbone: Name (as in ``torchvision.models``) of the feature extractor\r\n            train_bn: Whether the BatchNorm layers should be trainable\r\n            milestones: List of two epochs milestones\r\n            lr: Initial learning rate\r\n            lr_scheduler_gamma: Factor by which the learning rate is reduced at each milestone\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.backbone = backbone\r\n        self.train_bn = train_bn\r\n        self.milestones = milestones\r\n        self.batch_size = batch_size\r\n        self.lr = lr\r\n        self.lr_scheduler_gamma = lr_scheduler_gamma\r\n        self.num_workers = num_workers\r\n\r\n        self.__build_model()\r\n\r\n        self.train_acc = Accuracy(task=\"binary\")\r\n        self.valid_acc = Accuracy(task=\"binary\")\r\n        self.save_hyperparameters()", "language": "python", "code": "def __init__(\r\n        self,\r\n        backbone: str = \"resnet50\",\r\n        train_bn: bool = False,\r\n        milestones: tuple = (2, 4),\r\n        batch_size: int = 32,\r\n        lr: float = 1e-3,\r\n        lr_scheduler_gamma: float = 1e-1,\r\n        num_workers: int = 6,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"TransferLearningModel.\r\n\r\n        Args:\r\n            backbone: Name (as in ``torchvision.models``) of the feature extractor\r\n            train_bn: Whether the BatchNorm layers should be trainable\r\n            milestones: List of two epochs milestones\r\n            lr: Initial learning rate\r\n            lr_scheduler_gamma: Factor by which the learning rate is reduced at each milestone\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.backbone = backbone\r\n        self.train_bn = train_bn\r\n        self.milestones = milestones\r\n        self.batch_size = batch_size\r\n        self.lr = lr\r\n        self.lr_scheduler_gamma = lr_scheduler_gamma\r\n        self.num_workers = num_workers\r\n\r\n        self.__build_model()\r\n\r\n        self.train_acc = Accuracy(task=\"binary\")\r\n        self.valid_acc = Accuracy(task=\"binary\")\r\n        self.save_hyperparameters()", "code_tokens": ["def", "__init__", "(", "self", ",", "backbone", ":", "str", "=", "\"", "resnet50", "\"", ",", "train_bn", ":", "bool", "=", "False", ",", "milestones", ":", "tuple", "=", "(", "2", ",", "4", ")", ",", "batch_size", ":", "int", "=", "32", ",", "lr", ":", "float", "=", "1e", "-", "3", ",", "lr_scheduler_gamma", ":", "float", "=", "1e", "-", "1", ",", "num_workers", ":", "int", "=", "6", ",", "*", "*", "kwargs", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "TransferLearningModel", ".", "Args", ":", "backbone", ":", "Name", "(", "as", "in", "`", "`", "torchvision", ".", "models", "`", "`", ")", "of", "the", "feature", "extractor", "train_bn", ":", "Whether", "the", "BatchNorm", "layers", "should", "be", "trainable", "milestones", ":", "List", "of", "two", "epochs", "milestones", "lr", ":", "Initial", "learning", "rate", "lr_scheduler_gamma", ":", "Factor", "by", "which", "the", "learning", "rate", "is", "reduced", "at", "each", "milestone", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "backbone", "=", "backbone", "self", ".", "train_bn", "=", "train_bn", "self", ".", "milestones", "=", "milestones", "self", ".", "batch_size", "=", "batch_size", "self", ".", "lr", "=", "lr", "self", ".", "lr_scheduler_gamma", "=", "lr_scheduler_gamma", "self", ".", "num_workers", "=", "num_workers", "self", ".", "__build_model", "(", ")", "self", ".", "train_acc", "=", "Accuracy", "(", "task", "=", "\"", "binary", "\"", ")", "self", ".", "valid_acc", "=", "Accuracy", "(", "task", "=", "\"", "binary", "\"", ")", "self", ".", "save_hyperparameters", "(", ")"], "docstring": "TransferLearningModel.\r\n\r\n        Args:\r\n            backbone: Name (as in ``torchvision.models``) of the feature extractor\r\n            train_bn: Whether the BatchNorm layers should be trainable\r\n            milestones: List of two epochs milestones\r\n            lr: Initial learning rate\r\n            lr_scheduler_gamma: Factor by which the learning rate is reduced at each milestone", "docstring_tokens": ["transferlearningmodel", "args", "backbone", "name", "as", "in", "torchvision", "models", "of", "the", "feature", "extractor", "train_bn", "whether", "the", "batchnorm", "layers", "should", "be", "trainable", "milestones", "list", "of", "two", "epochs", "milestones", "lr", "initial", "learning", "rate", "lr_scheduler_gamma", "factor", "by", "which", "the", "learning", "rate", "is", "reduced", "at", "each", "milestone"], "docstring_summary": "TransferLearningModel.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "partition": "train", "function_type": "class_method", "class_name": "TransferLearningModel", "start_line": 157, "end_line": 191, "hash": "5bacb5f03af982d2ab44ba40266440d7", "complexity": 1, "parameters": ["backbone", "train_bn", "milestones", "4)", "batch_size", "lr", "lr_scheduler_gamma", "num_workers", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "func_name": "__build_model", "original_string": "def __build_model(self):\r\n        \"\"\"Define model layers & loss.\"\"\"\r\n        # 1. Load pre-trained network:\r\n        backbone = get_torchvision_model(self.backbone, weights=\"DEFAULT\")\r\n\r\n        _layers = list(backbone.children())[:-1]\r\n        self.feature_extractor = nn.Sequential(*_layers)\r\n\r\n        # 2. Classifier:\r\n        _fc_layers = [nn.Linear(2048, 256), nn.ReLU(), nn.Linear(256, 32), nn.Linear(32, 1)]\r\n        self.fc = nn.Sequential(*_fc_layers)\r\n\r\n        # 3. Loss:\r\n        self.loss_func = F.binary_cross_entropy_with_logits", "language": "python", "code": "def __build_model(self):\r\n        \"\"\"Define model layers & loss.\"\"\"\r\n        # 1. Load pre-trained network:\r\n        backbone = get_torchvision_model(self.backbone, weights=\"DEFAULT\")\r\n\r\n        _layers = list(backbone.children())[:-1]\r\n        self.feature_extractor = nn.Sequential(*_layers)\r\n\r\n        # 2. Classifier:\r\n        _fc_layers = [nn.Linear(2048, 256), nn.ReLU(), nn.Linear(256, 32), nn.Linear(32, 1)]\r\n        self.fc = nn.Sequential(*_fc_layers)\r\n\r\n        # 3. Loss:\r\n        self.loss_func = F.binary_cross_entropy_with_logits", "code_tokens": ["def", "__build_model", "(", "self", ")", ":", "\"", "\"", "\"", "Define", "model", "layers", "&", "loss", ".", "\"", "\"", "\"", "backbone", "=", "get_torchvision_model", "(", "self", ".", "backbone", ",", "weights", "=", "\"", "DEFAULT", "\"", ")", "_layers", "=", "list", "(", "backbone", ".", "children", "(", ")", ")", "[", ":", "-", "1", "]", "self", ".", "feature_extractor", "=", "nn", ".", "Sequential", "(", "*", "_layers", ")", "_fc_layers", "=", "[", "nn", ".", "Linear", "(", "2048", ",", "256", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "256", ",", "32", ")", ",", "nn", ".", "Linear", "(", "32", ",", "1", ")", "]", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "*", "_fc_layers", ")", "self", ".", "loss_func", "=", "F", ".", "binary_cross_entropy_with_logits"], "docstring": "Define model layers & loss.", "docstring_tokens": ["define", "model", "layers", "loss"], "docstring_summary": "Define model layers & loss.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "partition": "train", "function_type": "class_method", "class_name": "TransferLearningModel", "start_line": 193, "end_line": 206, "hash": "65189245d5da270244387334207e28c8", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "func_name": "forward", "original_string": "def forward(self, x):\r\n        \"\"\"Forward pass.\r\n\r\n        Returns logits.\r\n\r\n        \"\"\"\r\n        # 1. Feature extraction:\r\n        x = self.feature_extractor(x)\r\n        x = x.squeeze(-1).squeeze(-1)\r\n\r\n        # 2. Classifier (returns logits):\r\n        return self.fc(x)", "language": "python", "code": "def forward(self, x):\r\n        \"\"\"Forward pass.\r\n\r\n        Returns logits.\r\n\r\n        \"\"\"\r\n        # 1. Feature extraction:\r\n        x = self.feature_extractor(x)\r\n        x = x.squeeze(-1).squeeze(-1)\r\n\r\n        # 2. Classifier (returns logits):\r\n        return self.fc(x)", "code_tokens": ["def", "forward", "(", "self", ",", "x", ")", ":", "\"", "\"", "\"", "Forward", "pass", ".", "Returns", "logits", ".", "\"", "\"", "\"", "x", "=", "self", ".", "feature_extractor", "(", "x", ")", "x", "=", "x", ".", "squeeze", "(", "-", "1", ")", ".", "squeeze", "(", "-", "1", ")", "return", "self", ".", "fc", "(", "x", ")"], "docstring": "Forward pass.\r\n\r\n        Returns logits.", "docstring_tokens": ["forward", "pass", "returns", "logits"], "docstring_summary": "Forward pass.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\computer_vision_fine_tuning.py", "partition": "train", "function_type": "class_method", "class_name": "TransferLearningModel", "start_line": 208, "end_line": 219, "hash": "88401faa2964dffffedb4c40dede8ba4", "complexity": 1, "parameters": ["x"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "create_mlp", "original_string": "def create_mlp(input_shape: tuple[int], n_actions: int, hidden_size: int = 128):\r\n    \"\"\"Simple Multi-Layer Perceptron network.\"\"\"\r\n    return nn.Sequential(\r\n        nn.Linear(input_shape[0], hidden_size),\r\n        nn.ReLU(),\r\n        nn.Linear(hidden_size, hidden_size),\r\n        nn.ReLU(),\r\n        nn.Linear(hidden_size, n_actions),\r\n    )", "language": "python", "code": "def create_mlp(input_shape: tuple[int], n_actions: int, hidden_size: int = 128):\r\n    \"\"\"Simple Multi-Layer Perceptron network.\"\"\"\r\n    return nn.Sequential(\r\n        nn.Linear(input_shape[0], hidden_size),\r\n        nn.ReLU(),\r\n        nn.Linear(hidden_size, hidden_size),\r\n        nn.ReLU(),\r\n        nn.Linear(hidden_size, n_actions),\r\n    )", "code_tokens": ["def", "create_mlp", "(", "input_shape", ":", "tuple", "[", "int", "]", ",", "n_actions", ":", "int", ",", "hidden_size", ":", "int", "=", "128", ")", ":", "\"", "\"", "\"", "Simple", "Multi", "-", "Layer", "Perceptron", "network", ".", "\"", "\"", "\"", "return", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "input_shape", "[", "0", "]", ",", "hidden_size", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "hidden_size", ",", "n_actions", ")", ",", ")"], "docstring": "Simple Multi-Layer Perceptron network.", "docstring_tokens": ["simple", "multi", "layer", "perceptron", "network"], "docstring_summary": "Simple Multi-Layer Perceptron network.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "function", "start_line": 45, "end_line": 53, "hash": "fc8c0c7c253510fae2526a16ff445c13", "complexity": 1, "parameters": ["input_shape", "n_actions", "hidden_size"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "__init__", "original_string": "def __init__(self, actor_net):\r\n        \"\"\"\r\n        Args:\r\n            input_shape: observation shape of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        self.actor_net = actor_net", "language": "python", "code": "def __init__(self, actor_net):\r\n        \"\"\"\r\n        Args:\r\n            input_shape: observation shape of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        self.actor_net = actor_net", "code_tokens": ["def", "__init__", "(", "self", ",", "actor_net", ")", ":", "\"", "\"", "\"", "Args", ":", "input_shape", ":", "observation", "shape", "of", "the", "environment", "n_actions", ":", "number", "of", "discrete", "actions", "available", "in", "the", "environment", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "actor_net", "=", "actor_net"], "docstring": "Args:\r\n            input_shape: observation shape of the environment\r\n            n_actions: number of discrete actions available in the environment", "docstring_tokens": ["args", "input_shape", "observation", "shape", "of", "the", "environment", "n_actions", "number", "of", "discrete", "actions", "available", "in", "the", "environment"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "ActorCategorical", "start_line": 59, "end_line": 67, "hash": "5da01bb2f66251a6fdb48ead9191c24f", "complexity": 1, "parameters": ["actor_net"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "get_log_prob", "original_string": "def get_log_prob(self, pi: Categorical, actions: torch.Tensor):\r\n        \"\"\"Takes in a distribution and actions and returns log prob of actions under the distribution.\r\n\r\n        Args:\r\n            pi: torch distribution\r\n            actions: actions taken by distribution\r\n\r\n        Returns:\r\n            log probability of the action under pi\r\n\r\n        \"\"\"\r\n        return pi.log_prob(actions)", "language": "python", "code": "def get_log_prob(self, pi: Categorical, actions: torch.Tensor):\r\n        \"\"\"Takes in a distribution and actions and returns log prob of actions under the distribution.\r\n\r\n        Args:\r\n            pi: torch distribution\r\n            actions: actions taken by distribution\r\n\r\n        Returns:\r\n            log probability of the action under pi\r\n\r\n        \"\"\"\r\n        return pi.log_prob(actions)", "code_tokens": ["def", "get_log_prob", "(", "self", ",", "pi", ":", "Categorical", ",", "actions", ":", "torch", ".", "Tensor", ")", ":", "\"", "\"", "\"", "Takes", "in", "a", "distribution", "and", "actions", "and", "returns", "log", "prob", "of", "actions", "under", "the", "distribution", ".", "Args", ":", "pi", ":", "torch", "distribution", "actions", ":", "actions", "taken", "by", "distribution", "Returns", ":", "log", "probability", "of", "the", "action", "under", "pi", "\"", "\"", "\"", "return", "pi", ".", "log_prob", "(", "actions", ")"], "docstring": "Takes in a distribution and actions and returns log prob of actions under the distribution.\r\n\r\n        Args:\r\n            pi: torch distribution\r\n            actions: actions taken by distribution\r\n\r\n        Returns:\r\n            log probability of the action under pi", "docstring_tokens": ["takes", "in", "a", "distribution", "and", "actions", "and", "returns", "log", "prob", "of", "actions", "under", "the", "distribution", "args", "pi", "torch", "distribution", "actions", "actions", "taken", "by", "distribution", "returns", "log", "probability", "of", "the", "action", "under", "pi"], "docstring_summary": "Takes in a distribution and actions and returns log prob of actions under the distribution.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "ActorCategorical", "start_line": 76, "end_line": 87, "hash": "6e9c1f3abcad3075a3628734f5052606", "complexity": 1, "parameters": ["pi", "actions"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "__init__", "original_string": "def __init__(self, actor_net, act_dim):\r\n        \"\"\"\r\n        Args:\r\n            input_shape: observation shape of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n        \"\"\"\r\n        super().__init__()\r\n        self.actor_net = actor_net\r\n        log_std = -0.5 * torch.ones(act_dim, dtype=torch.float)\r\n        self.log_std = nn.Parameter(log_std)", "language": "python", "code": "def __init__(self, actor_net, act_dim):\r\n        \"\"\"\r\n        Args:\r\n            input_shape: observation shape of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n        \"\"\"\r\n        super().__init__()\r\n        self.actor_net = actor_net\r\n        log_std = -0.5 * torch.ones(act_dim, dtype=torch.float)\r\n        self.log_std = nn.Parameter(log_std)", "code_tokens": ["def", "__init__", "(", "self", ",", "actor_net", ",", "act_dim", ")", ":", "\"", "\"", "\"", "Args", ":", "input_shape", ":", "observation", "shape", "of", "the", "environment", "n_actions", ":", "number", "of", "discrete", "actions", "available", "in", "the", "environment", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "actor_net", "=", "actor_net", "log_std", "=", "-", "0", ".", "5", "*", "torch", ".", "ones", "(", "act_dim", ",", "dtype", "=", "torch", ".", "float", ")", "self", ".", "log_std", "=", "nn", ".", "Parameter", "(", "log_std", ")"], "docstring": "Args:\r\n            input_shape: observation shape of the environment\r\n            n_actions: number of discrete actions available in the environment", "docstring_tokens": ["args", "input_shape", "observation", "shape", "of", "the", "environment", "n_actions", "number", "of", "discrete", "actions", "available", "in", "the", "environment"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "ActorContinuous", "start_line": 94, "end_line": 103, "hash": "8f87a4f8dfcda1a07b35b4090c587ba5", "complexity": 1, "parameters": ["actor_net", "act_dim"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "get_log_prob", "original_string": "def get_log_prob(self, pi: Normal, actions: torch.Tensor):\r\n        \"\"\"Takes in a distribution and actions and returns log prob of actions under the distribution.\r\n\r\n        Args:\r\n            pi: torch distribution\r\n            actions: actions taken by distribution\r\n\r\n        Returns:\r\n            log probability of the action under pi\r\n\r\n        \"\"\"\r\n        return pi.log_prob(actions).sum(axis=-1)", "language": "python", "code": "def get_log_prob(self, pi: Normal, actions: torch.Tensor):\r\n        \"\"\"Takes in a distribution and actions and returns log prob of actions under the distribution.\r\n\r\n        Args:\r\n            pi: torch distribution\r\n            actions: actions taken by distribution\r\n\r\n        Returns:\r\n            log probability of the action under pi\r\n\r\n        \"\"\"\r\n        return pi.log_prob(actions).sum(axis=-1)", "code_tokens": ["def", "get_log_prob", "(", "self", ",", "pi", ":", "Normal", ",", "actions", ":", "torch", ".", "Tensor", ")", ":", "\"", "\"", "\"", "Takes", "in", "a", "distribution", "and", "actions", "and", "returns", "log", "prob", "of", "actions", "under", "the", "distribution", ".", "Args", ":", "pi", ":", "torch", "distribution", "actions", ":", "actions", "taken", "by", "distribution", "Returns", ":", "log", "probability", "of", "the", "action", "under", "pi", "\"", "\"", "\"", "return", "pi", ".", "log_prob", "(", "actions", ")", ".", "sum", "(", "axis", "=", "-", "1", ")"], "docstring": "Takes in a distribution and actions and returns log prob of actions under the distribution.\r\n\r\n        Args:\r\n            pi: torch distribution\r\n            actions: actions taken by distribution\r\n\r\n        Returns:\r\n            log probability of the action under pi", "docstring_tokens": ["takes", "in", "a", "distribution", "and", "actions", "and", "returns", "log", "prob", "of", "actions", "under", "the", "distribution", "args", "pi", "torch", "distribution", "actions", "actions", "taken", "by", "distribution", "returns", "log", "probability", "of", "the", "action", "under", "pi"], "docstring_summary": "Takes in a distribution and actions and returns log prob of actions under the distribution.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "ActorContinuous", "start_line": 113, "end_line": 124, "hash": "774b76c6e39db9777d7d11ea974bdae2", "complexity": 1, "parameters": ["pi", "actions"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        env: str,\r\n        gamma: float = 0.99,\r\n        lam: float = 0.95,\r\n        lr_actor: float = 3e-4,\r\n        lr_critic: float = 1e-3,\r\n        max_episode_len: float = 200,\r\n        batch_size: int = 512,\r\n        steps_per_epoch: int = 2048,\r\n        nb_optim_iters: int = 4,\r\n        clip_ratio: float = 0.2,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Hyperparameters\r\n        self.lr_actor = lr_actor\r\n        self.lr_critic = lr_critic\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.nb_optim_iters = nb_optim_iters\r\n        self.batch_size = batch_size\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.max_episode_len = max_episode_len\r\n        self.clip_ratio = clip_ratio\r\n        self.save_hyperparameters()\r\n\r\n        self.automatic_optimization = False\r\n\r\n        self.env = gym.make(env)\r\n        # value network\r\n        self.critic = create_mlp(self.env.observation_space.shape, 1)\r\n        # policy network (agent)\r\n        if isinstance(self.env.action_space, gym.spaces.box.Box):\r\n            act_dim = self.env.action_space.shape[0]\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, act_dim)\r\n            self.actor = ActorContinuous(actor_mlp, act_dim)\r\n        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\r\n            self.actor = ActorCategorical(actor_mlp)\r\n        else:\r\n            raise NotImplementedError(\r\n                \"Env action space should be of type Box (continuous) or Discrete (categorical).\"\r\n                f\" Got type: {type(self.env.action_space)}\"\r\n            )\r\n\r\n        self.batch_states = []\r\n        self.batch_actions = []\r\n        self.batch_adv = []\r\n        self.batch_qvals = []\r\n        self.batch_logp = []\r\n\r\n        self.ep_rewards = []\r\n        self.ep_values = []\r\n        self.epoch_rewards = []\r\n\r\n        self.episode_step = 0\r\n        self.avg_ep_reward = 0\r\n        self.avg_ep_len = 0\r\n        self.avg_reward = 0\r\n\r\n        self.state = torch.FloatTensor(self.env.reset())", "language": "python", "code": "def __init__(\r\n        self,\r\n        env: str,\r\n        gamma: float = 0.99,\r\n        lam: float = 0.95,\r\n        lr_actor: float = 3e-4,\r\n        lr_critic: float = 1e-3,\r\n        max_episode_len: float = 200,\r\n        batch_size: int = 512,\r\n        steps_per_epoch: int = 2048,\r\n        nb_optim_iters: int = 4,\r\n        clip_ratio: float = 0.2,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        # Hyperparameters\r\n        self.lr_actor = lr_actor\r\n        self.lr_critic = lr_critic\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.nb_optim_iters = nb_optim_iters\r\n        self.batch_size = batch_size\r\n        self.gamma = gamma\r\n        self.lam = lam\r\n        self.max_episode_len = max_episode_len\r\n        self.clip_ratio = clip_ratio\r\n        self.save_hyperparameters()\r\n\r\n        self.automatic_optimization = False\r\n\r\n        self.env = gym.make(env)\r\n        # value network\r\n        self.critic = create_mlp(self.env.observation_space.shape, 1)\r\n        # policy network (agent)\r\n        if isinstance(self.env.action_space, gym.spaces.box.Box):\r\n            act_dim = self.env.action_space.shape[0]\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, act_dim)\r\n            self.actor = ActorContinuous(actor_mlp, act_dim)\r\n        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\r\n            actor_mlp = create_mlp(self.env.observation_space.shape, self.env.action_space.n)\r\n            self.actor = ActorCategorical(actor_mlp)\r\n        else:\r\n            raise NotImplementedError(\r\n                \"Env action space should be of type Box (continuous) or Discrete (categorical).\"\r\n                f\" Got type: {type(self.env.action_space)}\"\r\n            )\r\n\r\n        self.batch_states = []\r\n        self.batch_actions = []\r\n        self.batch_adv = []\r\n        self.batch_qvals = []\r\n        self.batch_logp = []\r\n\r\n        self.ep_rewards = []\r\n        self.ep_values = []\r\n        self.epoch_rewards = []\r\n\r\n        self.episode_step = 0\r\n        self.avg_ep_reward = 0\r\n        self.avg_ep_len = 0\r\n        self.avg_reward = 0\r\n\r\n        self.state = torch.FloatTensor(self.env.reset())", "code_tokens": ["def", "__init__", "(", "self", ",", "env", ":", "str", ",", "gamma", ":", "float", "=", "0", ".", "99", ",", "lam", ":", "float", "=", "0", ".", "95", ",", "lr_actor", ":", "float", "=", "3e", "-", "4", ",", "lr_critic", ":", "float", "=", "1e", "-", "3", ",", "max_episode_len", ":", "float", "=", "200", ",", "batch_size", ":", "int", "=", "512", ",", "steps_per_epoch", ":", "int", "=", "2048", ",", "nb_optim_iters", ":", "int", "=", "4", ",", "clip_ratio", ":", "float", "=", "0", ".", "2", ",", "*", "*", "kwargs", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "env", ":", "gym", "environment", "tag", "gamma", ":", "discount", "factor", "lam", ":", "advantage", "discount", "factor", "(", "lambda", "in", "the", "paper", ")", "lr_actor", ":", "learning", "rate", "of", "actor", "network", "lr_critic", ":", "learning", "rate", "of", "critic", "network", "max_episode_len", ":", "maximum", "number", "interactions", "(", "actions", ")", "in", "an", "episode", "batch_size", ":", "batch_size", "when", "training", "network", "-", "can", "simulate", "number", "of", "policy", "updates", "performed", "per", "epoch", "steps_per_epoch", ":", "how", "many", "action", "-", "state", "pairs", "to", "rollout", "for", "trajectory", "collection", "per", "epoch", "nb_optim_iters", ":", "how", "many", "steps", "of", "gradient", "descent", "to", "perform", "on", "each", "batch", "clip_ratio", ":", "hyperparameter", "for", "clipping", "in", "the", "policy", "objective", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "lr_actor", "=", "lr_actor", "self", ".", "lr_critic", "=", "lr_critic", "self", ".", "steps_per_epoch", "=", "steps_per_epoch", "self", ".", "nb_optim_iters", "=", "nb_optim_iters", "self", ".", "batch_size", "=", "batch_size", "self", ".", "gamma", "=", "gamma", "self", ".", "lam", "=", "lam", "self", ".", "max_episode_len", "=", "max_episode_len", "self", ".", "clip_ratio", "=", "clip_ratio", "self", ".", "save_hyperparameters", "(", ")", "self", ".", "automatic_optimization", "=", "False", "self", ".", "env", "=", "gym", ".", "make", "(", "env", ")", "self", ".", "critic", "=", "create_mlp", "(", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "1", ")", "if", "isinstance", "(", "self", ".", "env", ".", "action_space", ",", "gym", ".", "spaces", ".", "box", ".", "Box", ")", ":", "act_dim", "=", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "actor_mlp", "=", "create_mlp", "(", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "act_dim", ")", "self", ".", "actor", "=", "ActorContinuous", "(", "actor_mlp", ",", "act_dim", ")", "elif", "isinstance", "(", "self", ".", "env", ".", "action_space", ",", "gym", ".", "spaces", ".", "discrete", ".", "Discrete", ")", ":", "actor_mlp", "=", "create_mlp", "(", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "self", ".", "env", ".", "action_space", ".", "n", ")", "self", ".", "actor", "=", "ActorCategorical", "(", "actor_mlp", ")", "else", ":", "raise", "NotImplementedError", "(", "\"", "Env", "action", "space", "should", "be", "of", "type", "Box", "(", "continuous", ")", "or", "Discrete", "(", "categorical", ")", ".", "\"", "f", "\"", "Got", "type", ":", "{", "type", "(", "self", ".", "env", ".", "action_space", ")", "}", "\"", ")", "self", ".", "batch_states", "=", "[", "]", "self", ".", "batch_actions", "=", "[", "]", "self", ".", "batch_adv", "=", "[", "]", "self", ".", "batch_qvals", "=", "[", "]", "self", ".", "batch_logp", "=", "[", "]", "self", ".", "ep_rewards", "=", "[", "]", "self", ".", "ep_values", "=", "[", "]", "self", ".", "epoch_rewards", "=", "[", "]", "self", ".", "episode_step", "=", "0", "self", ".", "avg_ep_reward", "=", "0", "self", ".", "avg_ep_len", "=", "0", "self", ".", "avg_reward", "=", "0", "self", ".", "state", "=", "torch", ".", "FloatTensor", "(", "self", ".", "env", ".", "reset", "(", ")", ")"], "docstring": "Args:\r\n            env: gym environment tag\r\n            gamma: discount factor\r\n            lam: advantage discount factor (lambda in the paper)\r\n            lr_actor: learning rate of actor network\r\n            lr_critic: learning rate of critic network\r\n            max_episode_len: maximum number interactions (actions) in an episode\r\n            batch_size:  batch_size when training network- can simulate number of policy updates performed per epoch\r\n            steps_per_epoch: how many action-state pairs to rollout for trajectory collection per epoch\r\n            nb_optim_iters: how many steps of gradient descent to perform on each batch\r\n            clip_ratio: hyperparameter for clipping in the policy objective", "docstring_tokens": ["args", "env", "gym", "environment", "tag", "gamma", "discount", "factor", "lam", "advantage", "discount", "factor", "lambda", "in", "the", "paper", "lr_actor", "learning", "rate", "of", "actor", "network", "lr_critic", "learning", "rate", "of", "critic", "network", "max_episode_len", "maximum", "number", "interactions", "actions", "in", "an", "episode", "batch_size", "batch_size", "when", "training", "network", "can", "simulate", "number", "of", "policy", "updates", "performed", "per", "epoch", "steps_per_epoch", "how", "many", "action", "state", "pairs", "to", "rollout", "for", "trajectory", "collection", "per", "epoch", "nb_optim_iters", "how", "many", "steps", "of", "gradient", "descent", "to", "perform", "on", "each", "batch", "clip_ratio", "hyperparameter", "for", "clipping", "in", "the", "policy", "objective"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 154, "end_line": 229, "hash": "4c413b034484f7c6c2f3c02d3031e0ef", "complexity": 3, "parameters": ["env", "gamma", "lam", "lr_actor", "lr_critic", "max_episode_len", "batch_size", "steps_per_epoch", "nb_optim_iters", "clip_ratio", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "forward", "original_string": "def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\r\n        \"\"\"Passes in a state x through the network and returns the policy and a sampled action.\r\n\r\n        Args:\r\n            x: environment state\r\n\r\n        Returns:\r\n            Tuple of policy and action\r\n\r\n        \"\"\"\r\n        pi, action = self.actor(x)\r\n        value = self.critic(x)\r\n\r\n        return pi, action, value", "language": "python", "code": "def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\r\n        \"\"\"Passes in a state x through the network and returns the policy and a sampled action.\r\n\r\n        Args:\r\n            x: environment state\r\n\r\n        Returns:\r\n            Tuple of policy and action\r\n\r\n        \"\"\"\r\n        pi, action = self.actor(x)\r\n        value = self.critic(x)\r\n\r\n        return pi, action, value", "code_tokens": ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "-", ">", "tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\"", "\"", "\"", "Passes", "in", "a", "state", "x", "through", "the", "network", "and", "returns", "the", "policy", "and", "a", "sampled", "action", ".", "Args", ":", "x", ":", "environment", "state", "Returns", ":", "Tuple", "of", "policy", "and", "action", "\"", "\"", "\"", "pi", ",", "action", "=", "self", ".", "actor", "(", "x", ")", "value", "=", "self", ".", "critic", "(", "x", ")", "return", "pi", ",", "action", ",", "value"], "docstring": "Passes in a state x through the network and returns the policy and a sampled action.\r\n\r\n        Args:\r\n            x: environment state\r\n\r\n        Returns:\r\n            Tuple of policy and action", "docstring_tokens": ["passes", "in", "a", "state", "x", "through", "the", "network", "and", "returns", "the", "policy", "and", "a", "sampled", "action", "args", "x", "environment", "state", "returns", "tuple", "of", "policy", "and", "action"], "docstring_summary": "Passes in a state x through the network and returns the policy and a sampled action.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 231, "end_line": 244, "hash": "b5dc407440d464a0b27103375b895bb1", "complexity": 1, "parameters": ["x"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "discount_rewards", "original_string": "def discount_rewards(self, rewards: list[float], discount: float) -> list[float]:\r\n        \"\"\"Calculate the discounted rewards of all rewards in list.\r\n\r\n        Args:\r\n            rewards: list of rewards/advantages\r\n\r\n        Returns:\r\n            list of discounted rewards/advantages\r\n\r\n        \"\"\"\r\n        assert isinstance(rewards[0], float)\r\n\r\n        cumul_reward = []\r\n        sum_r = 0.0\r\n\r\n        for r in reversed(rewards):\r\n            sum_r = (sum_r * discount) + r\r\n            cumul_reward.append(sum_r)\r\n\r\n        return list(reversed(cumul_reward))", "language": "python", "code": "def discount_rewards(self, rewards: list[float], discount: float) -> list[float]:\r\n        \"\"\"Calculate the discounted rewards of all rewards in list.\r\n\r\n        Args:\r\n            rewards: list of rewards/advantages\r\n\r\n        Returns:\r\n            list of discounted rewards/advantages\r\n\r\n        \"\"\"\r\n        assert isinstance(rewards[0], float)\r\n\r\n        cumul_reward = []\r\n        sum_r = 0.0\r\n\r\n        for r in reversed(rewards):\r\n            sum_r = (sum_r * discount) + r\r\n            cumul_reward.append(sum_r)\r\n\r\n        return list(reversed(cumul_reward))", "code_tokens": ["def", "discount_rewards", "(", "self", ",", "rewards", ":", "list", "[", "float", "]", ",", "discount", ":", "float", ")", "-", ">", "list", "[", "float", "]", ":", "\"", "\"", "\"", "Calculate", "the", "discounted", "rewards", "of", "all", "rewards", "in", "list", ".", "Args", ":", "rewards", ":", "list", "of", "rewards", "/", "advantages", "Returns", ":", "list", "of", "discounted", "rewards", "/", "advantages", "\"", "\"", "\"", "assert", "isinstance", "(", "rewards", "[", "0", "]", ",", "float", ")", "cumul_reward", "=", "[", "]", "sum_r", "=", "0", ".", "0", "for", "r", "in", "reversed", "(", "rewards", ")", ":", "sum_r", "=", "(", "sum_r", "*", "discount", ")", "+", "r", "cumul_reward", ".", "append", "(", "sum_r", ")", "return", "list", "(", "reversed", "(", "cumul_reward", ")", ")"], "docstring": "Calculate the discounted rewards of all rewards in list.\r\n\r\n        Args:\r\n            rewards: list of rewards/advantages\r\n\r\n        Returns:\r\n            list of discounted rewards/advantages", "docstring_tokens": ["calculate", "the", "discounted", "rewards", "of", "all", "rewards", "in", "list", "args", "rewards", "list", "of", "rewards", "advantages", "returns", "list", "of", "discounted", "rewards", "advantages"], "docstring_summary": "Calculate the discounted rewards of all rewards in list.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 246, "end_line": 265, "hash": "5085812900472bc45b8e5f0fea5bb20c", "complexity": 2, "parameters": ["rewards", "discount"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "calc_advantage", "original_string": "def calc_advantage(self, rewards: list[float], values: list[float], last_value: float) -> list[float]:\r\n        \"\"\"Calculate the advantage given rewards, state values, and the last value of episode.\r\n\r\n        Args:\r\n            rewards: list of episode rewards\r\n            values: list of state values from critic\r\n            last_value: value of last state of episode\r\n\r\n        Returns:\r\n            list of advantages\r\n\r\n        \"\"\"\r\n        rews = rewards + [last_value]\r\n        vals = values + [last_value]\r\n        # GAE\r\n        delta = [rews[i] + self.gamma * vals[i + 1] - vals[i] for i in range(len(rews) - 1)]\r\n        return self.discount_rewards(delta, self.gamma * self.lam)", "language": "python", "code": "def calc_advantage(self, rewards: list[float], values: list[float], last_value: float) -> list[float]:\r\n        \"\"\"Calculate the advantage given rewards, state values, and the last value of episode.\r\n\r\n        Args:\r\n            rewards: list of episode rewards\r\n            values: list of state values from critic\r\n            last_value: value of last state of episode\r\n\r\n        Returns:\r\n            list of advantages\r\n\r\n        \"\"\"\r\n        rews = rewards + [last_value]\r\n        vals = values + [last_value]\r\n        # GAE\r\n        delta = [rews[i] + self.gamma * vals[i + 1] - vals[i] for i in range(len(rews) - 1)]\r\n        return self.discount_rewards(delta, self.gamma * self.lam)", "code_tokens": ["def", "calc_advantage", "(", "self", ",", "rewards", ":", "list", "[", "float", "]", ",", "values", ":", "list", "[", "float", "]", ",", "last_value", ":", "float", ")", "-", ">", "list", "[", "float", "]", ":", "\"", "\"", "\"", "Calculate", "the", "advantage", "given", "rewards", ",", "state", "values", ",", "and", "the", "last", "value", "of", "episode", ".", "Args", ":", "rewards", ":", "list", "of", "episode", "rewards", "values", ":", "list", "of", "state", "values", "from", "critic", "last_value", ":", "value", "of", "last", "state", "of", "episode", "Returns", ":", "list", "of", "advantages", "\"", "\"", "\"", "rews", "=", "rewards", "+", "[", "last_value", "]", "vals", "=", "values", "+", "[", "last_value", "]", "delta", "=", "[", "rews", "[", "i", "]", "+", "self", ".", "gamma", "*", "vals", "[", "i", "+", "1", "]", "-", "vals", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "rews", ")", "-", "1", ")", "]", "return", "self", ".", "discount_rewards", "(", "delta", ",", "self", ".", "gamma", "*", "self", ".", "lam", ")"], "docstring": "Calculate the advantage given rewards, state values, and the last value of episode.\r\n\r\n        Args:\r\n            rewards: list of episode rewards\r\n            values: list of state values from critic\r\n            last_value: value of last state of episode\r\n\r\n        Returns:\r\n            list of advantages", "docstring_tokens": ["calculate", "the", "advantage", "given", "rewards", "state", "values", "and", "the", "last", "value", "of", "episode", "args", "rewards", "list", "of", "episode", "rewards", "values", "list", "of", "state", "values", "from", "critic", "last_value", "value", "of", "last", "state", "of", "episode", "returns", "list", "of", "advantages"], "docstring_summary": "Calculate the advantage given rewards, state values, and the last value of episode.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 267, "end_line": 283, "hash": "f50509dfa012e7cf2ba8bba4cf09ab0e", "complexity": 2, "parameters": ["rewards", "values", "last_value"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "generate_trajectory_samples", "original_string": "def generate_trajectory_samples(self) -> tuple[list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]]:\r\n        \"\"\"\r\n        Contains the logic for generating trajectory data to train policy and value network\r\n        Yield:\r\n           Tuple of Lists containing tensors for states, actions, log probs, qvals and advantage\r\n        \"\"\"\r\n        for step in range(self.steps_per_epoch):\r\n            self.state = self.state.to(device=self.device)\r\n\r\n            with torch.no_grad():\r\n                pi, action, value = self(self.state)\r\n                log_prob = self.actor.get_log_prob(pi, action)\r\n\r\n            next_state, reward, done, _ = self.env.step(action.cpu().numpy())\r\n\r\n            self.episode_step += 1\r\n\r\n            self.batch_states.append(self.state)\r\n            self.batch_actions.append(action)\r\n            self.batch_logp.append(log_prob)\r\n\r\n            self.ep_rewards.append(reward)\r\n            self.ep_values.append(value.item())\r\n\r\n            self.state = torch.FloatTensor(next_state)\r\n\r\n            epoch_end = step == (self.steps_per_epoch - 1)\r\n            terminal = len(self.ep_rewards) == self.max_episode_len\r\n\r\n            if epoch_end or done or terminal:\r\n                # if trajectory ends abtruptly, bootstrap value of next state\r\n                if (terminal or epoch_end) and not done:\r\n                    self.state = self.state.to(device=self.device)\r\n                    with torch.no_grad():\r\n                        _, _, value = self(self.state)\r\n                        last_value = value.item()\r\n                        steps_before_cutoff = self.episode_step\r\n                else:\r\n                    last_value = 0\r\n                    steps_before_cutoff = 0\r\n\r\n                # discounted cumulative reward\r\n                self.batch_qvals += self.discount_rewards(self.ep_rewards + [last_value], self.gamma)[:-1]\r\n                # advantage\r\n                self.batch_adv += self.calc_advantage(self.ep_rewards, self.ep_values, last_value)\r\n                # logs\r\n                self.epoch_rewards.append(sum(self.ep_rewards))\r\n                # reset params\r\n                self.ep_rewards = []\r\n                self.ep_values = []\r\n                self.episode_step = 0\r\n                self.state = torch.FloatTensor(self.env.reset())\r\n\r\n            if epoch_end:\r\n                train_data = zip(\r\n                    self.batch_states, self.batch_actions, self.batch_logp, self.batch_qvals, self.batch_adv\r\n                )\r\n\r\n                for state, action, logp_old, qval, adv in train_data:\r\n                    yield state, action, logp_old, qval, adv\r\n\r\n                self.batch_states.clear()\r\n                self.batch_actions.clear()\r\n                self.batch_adv.clear()\r\n                self.batch_logp.clear()\r\n                self.batch_qvals.clear()\r\n\r\n                # logging\r\n                self.avg_reward = sum(self.epoch_rewards) / self.steps_per_epoch\r\n\r\n                # if epoch ended abruptly, exclude last cut-short episode to prevent stats skewness\r\n                epoch_rewards = self.epoch_rewards\r\n                if not done:\r\n                    epoch_rewards = epoch_rewards[:-1]\r\n\r\n                total_epoch_reward = sum(epoch_rewards)\r\n                nb_episodes = len(epoch_rewards)\r\n\r\n                self.avg_ep_reward = total_epoch_reward / nb_episodes\r\n                self.avg_ep_len = (self.steps_per_epoch - steps_before_cutoff) / nb_episodes\r\n\r\n                self.epoch_rewards.clear()", "language": "python", "code": "def generate_trajectory_samples(self) -> tuple[list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]]:\r\n        \"\"\"\r\n        Contains the logic for generating trajectory data to train policy and value network\r\n        Yield:\r\n           Tuple of Lists containing tensors for states, actions, log probs, qvals and advantage\r\n        \"\"\"\r\n        for step in range(self.steps_per_epoch):\r\n            self.state = self.state.to(device=self.device)\r\n\r\n            with torch.no_grad():\r\n                pi, action, value = self(self.state)\r\n                log_prob = self.actor.get_log_prob(pi, action)\r\n\r\n            next_state, reward, done, _ = self.env.step(action.cpu().numpy())\r\n\r\n            self.episode_step += 1\r\n\r\n            self.batch_states.append(self.state)\r\n            self.batch_actions.append(action)\r\n            self.batch_logp.append(log_prob)\r\n\r\n            self.ep_rewards.append(reward)\r\n            self.ep_values.append(value.item())\r\n\r\n            self.state = torch.FloatTensor(next_state)\r\n\r\n            epoch_end = step == (self.steps_per_epoch - 1)\r\n            terminal = len(self.ep_rewards) == self.max_episode_len\r\n\r\n            if epoch_end or done or terminal:\r\n                # if trajectory ends abtruptly, bootstrap value of next state\r\n                if (terminal or epoch_end) and not done:\r\n                    self.state = self.state.to(device=self.device)\r\n                    with torch.no_grad():\r\n                        _, _, value = self(self.state)\r\n                        last_value = value.item()\r\n                        steps_before_cutoff = self.episode_step\r\n                else:\r\n                    last_value = 0\r\n                    steps_before_cutoff = 0\r\n\r\n                # discounted cumulative reward\r\n                self.batch_qvals += self.discount_rewards(self.ep_rewards + [last_value], self.gamma)[:-1]\r\n                # advantage\r\n                self.batch_adv += self.calc_advantage(self.ep_rewards, self.ep_values, last_value)\r\n                # logs\r\n                self.epoch_rewards.append(sum(self.ep_rewards))\r\n                # reset params\r\n                self.ep_rewards = []\r\n                self.ep_values = []\r\n                self.episode_step = 0\r\n                self.state = torch.FloatTensor(self.env.reset())\r\n\r\n            if epoch_end:\r\n                train_data = zip(\r\n                    self.batch_states, self.batch_actions, self.batch_logp, self.batch_qvals, self.batch_adv\r\n                )\r\n\r\n                for state, action, logp_old, qval, adv in train_data:\r\n                    yield state, action, logp_old, qval, adv\r\n\r\n                self.batch_states.clear()\r\n                self.batch_actions.clear()\r\n                self.batch_adv.clear()\r\n                self.batch_logp.clear()\r\n                self.batch_qvals.clear()\r\n\r\n                # logging\r\n                self.avg_reward = sum(self.epoch_rewards) / self.steps_per_epoch\r\n\r\n                # if epoch ended abruptly, exclude last cut-short episode to prevent stats skewness\r\n                epoch_rewards = self.epoch_rewards\r\n                if not done:\r\n                    epoch_rewards = epoch_rewards[:-1]\r\n\r\n                total_epoch_reward = sum(epoch_rewards)\r\n                nb_episodes = len(epoch_rewards)\r\n\r\n                self.avg_ep_reward = total_epoch_reward / nb_episodes\r\n                self.avg_ep_len = (self.steps_per_epoch - steps_before_cutoff) / nb_episodes\r\n\r\n                self.epoch_rewards.clear()", "code_tokens": ["def", "generate_trajectory_samples", "(", "self", ")", "-", ">", "tuple", "[", "list", "[", "torch", ".", "Tensor", "]", ",", "list", "[", "torch", ".", "Tensor", "]", ",", "list", "[", "torch", ".", "Tensor", "]", "]", ":", "\"", "\"", "\"", "Contains", "the", "logic", "for", "generating", "trajectory", "data", "to", "train", "policy", "and", "value", "network", "Yield", ":", "Tuple", "of", "Lists", "containing", "tensors", "for", "states", ",", "actions", ",", "log", "probs", ",", "qvals", "and", "advantage", "\"", "\"", "\"", "for", "step", "in", "range", "(", "self", ".", "steps_per_epoch", ")", ":", "self", ".", "state", "=", "self", ".", "state", ".", "to", "(", "device", "=", "self", ".", "device", ")", "with", "torch", ".", "no_grad", "(", ")", ":", "pi", ",", "action", ",", "value", "=", "self", "(", "self", ".", "state", ")", "log_prob", "=", "self", ".", "actor", ".", "get_log_prob", "(", "pi", ",", "action", ")", "next_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "self", ".", "episode_step", "+", "=", "1", "self", ".", "batch_states", ".", "append", "(", "self", ".", "state", ")", "self", ".", "batch_actions", ".", "append", "(", "action", ")", "self", ".", "batch_logp", ".", "append", "(", "log_prob", ")", "self", ".", "ep_rewards", ".", "append", "(", "reward", ")", "self", ".", "ep_values", ".", "append", "(", "value", ".", "item", "(", ")", ")", "self", ".", "state", "=", "torch", ".", "FloatTensor", "(", "next_state", ")", "epoch_end", "=", "step", "=", "=", "(", "self", ".", "steps_per_epoch", "-", "1", ")", "terminal", "=", "len", "(", "self", ".", "ep_rewards", ")", "=", "=", "self", ".", "max_episode_len", "if", "epoch_end", "or", "done", "or", "terminal", ":", "if", "(", "terminal", "or", "epoch_end", ")", "and", "not", "done", ":", "self", ".", "state", "=", "self", ".", "state", ".", "to", "(", "device", "=", "self", ".", "device", ")", "with", "torch", ".", "no_grad", "(", ")", ":", "_", ",", "_", ",", "value", "=", "self", "(", "self", ".", "state", ")", "last_value", "=", "value", ".", "item", "(", ")", "steps_before_cutoff", "=", "self", ".", "episode_step", "else", ":", "last_value", "=", "0", "steps_before_cutoff", "=", "0", "self", ".", "batch_qvals", "+", "=", "self", ".", "discount_rewards", "(", "self", ".", "ep_rewards", "+", "[", "last_value", "]", ",", "self", ".", "gamma", ")", "[", ":", "-", "1", "]", "self", ".", "batch_adv", "+", "=", "self", ".", "calc_advantage", "(", "self", ".", "ep_rewards", ",", "self", ".", "ep_values", ",", "last_value", ")", "self", ".", "epoch_rewards", ".", "append", "(", "sum", "(", "self", ".", "ep_rewards", ")", ")", "self", ".", "ep_rewards", "=", "[", "]", "self", ".", "ep_values", "=", "[", "]", "self", ".", "episode_step", "=", "0", "self", ".", "state", "=", "torch", ".", "FloatTensor", "(", "self", ".", "env", ".", "reset", "(", ")", ")", "if", "epoch_end", ":", "train_data", "=", "zip", "(", "self", ".", "batch_states", ",", "self", ".", "batch_actions", ",", "self", ".", "batch_logp", ",", "self", ".", "batch_qvals", ",", "self", ".", "batch_adv", ")", "for", "state", ",", "action", ",", "logp_old", ",", "qval", ",", "adv", "in", "train_data", ":", "yield", "state", ",", "action", ",", "logp_old", ",", "qval", ",", "adv", "self", ".", "batch_states", ".", "clear", "(", ")", "self", ".", "batch_actions", ".", "clear", "(", ")", "self", ".", "batch_adv", ".", "clear", "(", ")", "self", ".", "batch_logp", ".", "clear", "(", ")", "self", ".", "batch_qvals", ".", "clear", "(", ")", "self", ".", "avg_reward", "=", "sum", "(", "self", ".", "epoch_rewards", ")", "/", "self", ".", "steps_per_epoch", "epoch_rewards", "=", "self", ".", "epoch_rewards", "if", "not", "done", ":", "epoch_rewards", "=", "epoch_rewards", "[", ":", "-", "1", "]", "total_epoch_reward", "=", "sum", "(", "epoch_rewards", ")", "nb_episodes", "=", "len", "(", "epoch_rewards", ")", "self", ".", "avg_ep_reward", "=", "total_epoch_reward", "/", "nb_episodes", "self", ".", "avg_ep_len", "=", "(", "self", ".", "steps_per_epoch", "-", "steps_before_cutoff", ")", "/", "nb_episodes", "self", ".", "epoch_rewards", ".", "clear", "(", ")"], "docstring": "Contains the logic for generating trajectory data to train policy and value network\r\n        Yield:\r\n           Tuple of Lists containing tensors for states, actions, log probs, qvals and advantage", "docstring_tokens": ["contains", "the", "logic", "for", "generating", "trajectory", "data", "to", "train", "policy", "and", "value", "network", "yield", "tuple", "of", "lists", "containing", "tensors", "for", "states", "actions", "log", "probs", "qvals", "and", "advantage"], "docstring_summary": "Contains the logic for generating trajectory data to train policy and value network", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 285, "end_line": 366, "hash": "81a1bc2e304fe468de53e07860f972e5", "complexity": 13, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "training_step", "original_string": "def training_step(self, batch: tuple[torch.Tensor, torch.Tensor]):\r\n        \"\"\"Carries out a single update to actor and critic network from a batch of replay buffer.\r\n\r\n        Args:\r\n            batch: batch of replay buffer/trajectory data\r\n\r\n        \"\"\"\r\n        state, action, old_logp, qval, adv = batch\r\n\r\n        # normalize advantages\r\n        adv = (adv - adv.mean()) / adv.std()\r\n\r\n        self.log(\"avg_ep_len\", self.avg_ep_len, prog_bar=True, on_step=False, on_epoch=True)\r\n        self.log(\"avg_ep_reward\", self.avg_ep_reward, prog_bar=True, on_step=False, on_epoch=True)\r\n        self.log(\"avg_reward\", self.avg_reward, prog_bar=True, on_step=False, on_epoch=True)\r\n\r\n        optimizer_actor, optimizer_critic = self.optimizers()\r\n\r\n        loss_actor = self.actor_loss(state, action, old_logp, qval, adv)\r\n        self.manual_backward(loss_actor)\r\n        optimizer_actor.step()\r\n        optimizer_actor.zero_grad()\r\n\r\n        loss_critic = self.critic_loss(state, action, old_logp, qval, adv)\r\n        self.manual_backward(loss_critic)\r\n        optimizer_critic.step()\r\n        optimizer_critic.zero_grad()\r\n\r\n        self.log(\"loss_critic\", loss_critic, on_step=False, on_epoch=True, prog_bar=False, logger=True)\r\n        self.log(\"loss_actor\", loss_actor, on_step=False, on_epoch=True, prog_bar=True, logger=True)", "language": "python", "code": "def training_step(self, batch: tuple[torch.Tensor, torch.Tensor]):\r\n        \"\"\"Carries out a single update to actor and critic network from a batch of replay buffer.\r\n\r\n        Args:\r\n            batch: batch of replay buffer/trajectory data\r\n\r\n        \"\"\"\r\n        state, action, old_logp, qval, adv = batch\r\n\r\n        # normalize advantages\r\n        adv = (adv - adv.mean()) / adv.std()\r\n\r\n        self.log(\"avg_ep_len\", self.avg_ep_len, prog_bar=True, on_step=False, on_epoch=True)\r\n        self.log(\"avg_ep_reward\", self.avg_ep_reward, prog_bar=True, on_step=False, on_epoch=True)\r\n        self.log(\"avg_reward\", self.avg_reward, prog_bar=True, on_step=False, on_epoch=True)\r\n\r\n        optimizer_actor, optimizer_critic = self.optimizers()\r\n\r\n        loss_actor = self.actor_loss(state, action, old_logp, qval, adv)\r\n        self.manual_backward(loss_actor)\r\n        optimizer_actor.step()\r\n        optimizer_actor.zero_grad()\r\n\r\n        loss_critic = self.critic_loss(state, action, old_logp, qval, adv)\r\n        self.manual_backward(loss_critic)\r\n        optimizer_critic.step()\r\n        optimizer_critic.zero_grad()\r\n\r\n        self.log(\"loss_critic\", loss_critic, on_step=False, on_epoch=True, prog_bar=False, logger=True)\r\n        self.log(\"loss_actor\", loss_actor, on_step=False, on_epoch=True, prog_bar=True, logger=True)", "code_tokens": ["def", "training_step", "(", "self", ",", "batch", ":", "tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ")", ":", "\"", "\"", "\"", "Carries", "out", "a", "single", "update", "to", "actor", "and", "critic", "network", "from", "a", "batch", "of", "replay", "buffer", ".", "Args", ":", "batch", ":", "batch", "of", "replay", "buffer", "/", "trajectory", "data", "\"", "\"", "\"", "state", ",", "action", ",", "old_logp", ",", "qval", ",", "adv", "=", "batch", "adv", "=", "(", "adv", "-", "adv", ".", "mean", "(", ")", ")", "/", "adv", ".", "std", "(", ")", "self", ".", "log", "(", "\"", "avg_ep_len", "\"", ",", "self", ".", "avg_ep_len", ",", "prog_bar", "=", "True", ",", "on_step", "=", "False", ",", "on_epoch", "=", "True", ")", "self", ".", "log", "(", "\"", "avg_ep_reward", "\"", ",", "self", ".", "avg_ep_reward", ",", "prog_bar", "=", "True", ",", "on_step", "=", "False", ",", "on_epoch", "=", "True", ")", "self", ".", "log", "(", "\"", "avg_reward", "\"", ",", "self", ".", "avg_reward", ",", "prog_bar", "=", "True", ",", "on_step", "=", "False", ",", "on_epoch", "=", "True", ")", "optimizer_actor", ",", "optimizer_critic", "=", "self", ".", "optimizers", "(", ")", "loss_actor", "=", "self", ".", "actor_loss", "(", "state", ",", "action", ",", "old_logp", ",", "qval", ",", "adv", ")", "self", ".", "manual_backward", "(", "loss_actor", ")", "optimizer_actor", ".", "step", "(", ")", "optimizer_actor", ".", "zero_grad", "(", ")", "loss_critic", "=", "self", ".", "critic_loss", "(", "state", ",", "action", ",", "old_logp", ",", "qval", ",", "adv", ")", "self", ".", "manual_backward", "(", "loss_critic", ")", "optimizer_critic", ".", "step", "(", ")", "optimizer_critic", ".", "zero_grad", "(", ")", "self", ".", "log", "(", "\"", "loss_critic", "\"", ",", "loss_critic", ",", "on_step", "=", "False", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "False", ",", "logger", "=", "True", ")", "self", ".", "log", "(", "\"", "loss_actor", "\"", ",", "loss_actor", ",", "on_step", "=", "False", ",", "on_epoch", "=", "True", ",", "prog_bar", "=", "True", ",", "logger", "=", "True", ")"], "docstring": "Carries out a single update to actor and critic network from a batch of replay buffer.\r\n\r\n        Args:\r\n            batch: batch of replay buffer/trajectory data", "docstring_tokens": ["carries", "out", "a", "single", "update", "to", "actor", "and", "critic", "network", "from", "a", "batch", "of", "replay", "buffer", "args", "batch", "batch", "of", "replay", "buffer", "trajectory", "data"], "docstring_summary": "Carries out a single update to actor and critic network from a batch of replay buffer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 379, "end_line": 408, "hash": "8fca503059b942c60d534af12d86c613", "complexity": 1, "parameters": ["batch", "torch.Tensor]"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "configure_optimizers", "original_string": "def configure_optimizers(self) -> list[Optimizer]:\r\n        \"\"\"Initialize Adam optimizer.\"\"\"\r\n        optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\r\n        optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic)\r\n        return optimizer_actor, optimizer_critic", "language": "python", "code": "def configure_optimizers(self) -> list[Optimizer]:\r\n        \"\"\"Initialize Adam optimizer.\"\"\"\r\n        optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\r\n        optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic)\r\n        return optimizer_actor, optimizer_critic", "code_tokens": ["def", "configure_optimizers", "(", "self", ")", "-", ">", "list", "[", "Optimizer", "]", ":", "\"", "\"", "\"", "Initialize", "Adam", "optimizer", ".", "\"", "\"", "\"", "optimizer_actor", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "actor", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr_actor", ")", "optimizer_critic", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr_critic", ")", "return", "optimizer_actor", ",", "optimizer_critic"], "docstring": "Initialize Adam optimizer.", "docstring_tokens": ["initialize", "adam", "optimizer"], "docstring_summary": "Initialize Adam optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 410, "end_line": 414, "hash": "9a00f92e15e1404f2d7043594854868c", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(self, *args, **kwargs):\r\n        \"\"\"Run 'nb_optim_iters' number of iterations of gradient descent on actor and critic for each data sample.\"\"\"\r\n        for _ in range(self.nb_optim_iters):\r\n            super().optimizer_step(*args, **kwargs)", "language": "python", "code": "def optimizer_step(self, *args, **kwargs):\r\n        \"\"\"Run 'nb_optim_iters' number of iterations of gradient descent on actor and critic for each data sample.\"\"\"\r\n        for _ in range(self.nb_optim_iters):\r\n            super().optimizer_step(*args, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "\"", "\"", "\"", "Run", "'", "nb_optim_iters", "'", "number", "of", "iterations", "of", "gradient", "descent", "on", "actor", "and", "critic", "for", "each", "data", "sample", ".", "\"", "\"", "\"", "for", "_", "in", "range", "(", "self", ".", "nb_optim_iters", ")", ":", "super", "(", ")", ".", "optimizer_step", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Run 'nb_optim_iters' number of iterations of gradient descent on actor and critic for each data sample.", "docstring_tokens": ["run", "nb_optim_iters", "number", "of", "iterations", "of", "gradient", "descent", "on", "actor", "and", "critic", "for", "each", "data", "sample"], "docstring_summary": "Run 'nb_optim_iters' number of iterations of gradient descent on actor and critic for each data sample.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 416, "end_line": 419, "hash": "94091fa85fee7b8390af1658fe0ada73", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "func_name": "_dataloader", "original_string": "def _dataloader(self) -> DataLoader:\r\n        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\r\n        dataset = ExperienceSourceDataset(self.generate_trajectory_samples)\r\n        return DataLoader(dataset=dataset, batch_size=self.batch_size)", "language": "python", "code": "def _dataloader(self) -> DataLoader:\r\n        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\r\n        dataset = ExperienceSourceDataset(self.generate_trajectory_samples)\r\n        return DataLoader(dataset=dataset, batch_size=self.batch_size)", "code_tokens": ["def", "_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "\"", "\"", "\"", "Initialize", "the", "Replay", "Buffer", "dataset", "used", "for", "retrieving", "experiences", ".", "\"", "\"", "\"", "dataset", "=", "ExperienceSourceDataset", "(", "self", ".", "generate_trajectory_samples", ")", "return", "DataLoader", "(", "dataset", "=", "dataset", ",", "batch_size", "=", "self", ".", "batch_size", ")"], "docstring": "Initialize the Replay Buffer dataset used for retrieving experiences.", "docstring_tokens": ["initialize", "the", "replay", "buffer", "dataset", "used", "for", "retrieving", "experiences"], "docstring_summary": "Initialize the Replay Buffer dataset used for retrieving experiences.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_ppo.py", "partition": "train", "function_type": "class_method", "class_name": "PPOLightning", "start_line": 421, "end_line": 424, "hash": "51ad9e338555c4073b4e49e5171d8e1d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "__init__", "original_string": "def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\r\n        \"\"\"\r\n        Args:\r\n            obs_size: observation/state size of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n            hidden_size: size of hidden layers\r\n        \"\"\"\r\n        super().__init__()\r\n        self.net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions))", "language": "python", "code": "def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\r\n        \"\"\"\r\n        Args:\r\n            obs_size: observation/state size of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n            hidden_size: size of hidden layers\r\n        \"\"\"\r\n        super().__init__()\r\n        self.net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions))", "code_tokens": ["def", "__init__", "(", "self", ",", "obs_size", ":", "int", ",", "n_actions", ":", "int", ",", "hidden_size", ":", "int", "=", "128", ")", ":", "\"", "\"", "\"", "Args", ":", "obs_size", ":", "observation", "/", "state", "size", "of", "the", "environment", "n_actions", ":", "number", "of", "discrete", "actions", "available", "in", "the", "environment", "hidden_size", ":", "size", "of", "hidden", "layers", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "obs_size", ",", "hidden_size", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "hidden_size", ",", "n_actions", ")", ")"], "docstring": "Args:\r\n            obs_size: observation/state size of the environment\r\n            n_actions: number of discrete actions available in the environment\r\n            hidden_size: size of hidden layers", "docstring_tokens": ["args", "obs_size", "observation", "state", "size", "of", "the", "environment", "n_actions", "number", "of", "discrete", "actions", "available", "in", "the", "environment", "hidden_size", "size", "of", "hidden", "layers"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQN", "start_line": 60, "end_line": 68, "hash": "addf2f8ed0193f9f37aac0b943be0498", "complexity": 1, "parameters": ["obs_size", "n_actions", "hidden_size"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "__init__", "original_string": "def __init__(self, capacity: int) -> None:\r\n        \"\"\"\r\n        Args:\r\n            capacity: size of the buffer\r\n        \"\"\"\r\n        self.buffer = deque(maxlen=capacity)", "language": "python", "code": "def __init__(self, capacity: int) -> None:\r\n        \"\"\"\r\n        Args:\r\n            capacity: size of the buffer\r\n        \"\"\"\r\n        self.buffer = deque(maxlen=capacity)", "code_tokens": ["def", "__init__", "(", "self", ",", "capacity", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "capacity", ":", "size", "of", "the", "buffer", "\"", "\"", "\"", "self", ".", "buffer", "=", "deque", "(", "maxlen", "=", "capacity", ")"], "docstring": "Args:\r\n            capacity: size of the buffer", "docstring_tokens": ["args", "capacity", "size", "of", "the", "buffer"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "ReplayBuffer", "start_line": 86, "end_line": 91, "hash": "c4cfb4f5871b6eb556f9dbafed0e1157", "complexity": 1, "parameters": ["capacity"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "append", "original_string": "def append(self, experience: Experience) -> None:\r\n        \"\"\"Add experience to the buffer.\r\n\r\n        Args:\r\n            experience: tuple (state, action, reward, done, new_state)\r\n\r\n        \"\"\"\r\n        self.buffer.append(experience)", "language": "python", "code": "def append(self, experience: Experience) -> None:\r\n        \"\"\"Add experience to the buffer.\r\n\r\n        Args:\r\n            experience: tuple (state, action, reward, done, new_state)\r\n\r\n        \"\"\"\r\n        self.buffer.append(experience)", "code_tokens": ["def", "append", "(", "self", ",", "experience", ":", "Experience", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Add", "experience", "to", "the", "buffer", ".", "Args", ":", "experience", ":", "tuple", "(", "state", ",", "action", ",", "reward", ",", "done", ",", "new_state", ")", "\"", "\"", "\"", "self", ".", "buffer", ".", "append", "(", "experience", ")"], "docstring": "Add experience to the buffer.\r\n\r\n        Args:\r\n            experience: tuple (state, action, reward, done, new_state)", "docstring_tokens": ["add", "experience", "to", "the", "buffer", "args", "experience", "tuple", "state", "action", "reward", "done", "new_state"], "docstring_summary": "Add experience to the buffer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "ReplayBuffer", "start_line": 96, "end_line": 103, "hash": "e47beef8fddca03527ea4f49aa907834", "complexity": 1, "parameters": ["experience"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "__init__", "original_string": "def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\r\n        \"\"\"\r\n        Args:\r\n            buffer: replay buffer\r\n            sample_size: number of experiences to sample at a time\r\n        \"\"\"\r\n        self.buffer = buffer\r\n        self.sample_size = sample_size", "language": "python", "code": "def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\r\n        \"\"\"\r\n        Args:\r\n            buffer: replay buffer\r\n            sample_size: number of experiences to sample at a time\r\n        \"\"\"\r\n        self.buffer = buffer\r\n        self.sample_size = sample_size", "code_tokens": ["def", "__init__", "(", "self", ",", "buffer", ":", "ReplayBuffer", ",", "sample_size", ":", "int", "=", "200", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "buffer", ":", "replay", "buffer", "sample_size", ":", "number", "of", "experiences", "to", "sample", "at", "a", "time", "\"", "\"", "\"", "self", ".", "buffer", "=", "buffer", "self", ".", "sample_size", "=", "sample_size"], "docstring": "Args:\r\n            buffer: replay buffer\r\n            sample_size: number of experiences to sample at a time", "docstring_tokens": ["args", "buffer", "replay", "buffer", "sample_size", "number", "of", "experiences", "to", "sample", "at", "a", "time"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "RLDataset", "start_line": 126, "end_line": 133, "hash": "fb96b84a6cbeb892dce4cb9258fe7d1b", "complexity": 1, "parameters": ["buffer", "sample_size"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "__init__", "original_string": "def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: training environment\r\n            replay_buffer: replay buffer storing experiences\r\n        \"\"\"\r\n        self.env = env\r\n        self.replay_buffer = replay_buffer\r\n        self.reset()\r\n        self.state = self.env.reset()", "language": "python", "code": "def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\r\n        \"\"\"\r\n        Args:\r\n            env: training environment\r\n            replay_buffer: replay buffer storing experiences\r\n        \"\"\"\r\n        self.env = env\r\n        self.replay_buffer = replay_buffer\r\n        self.reset()\r\n        self.state = self.env.reset()", "code_tokens": ["def", "__init__", "(", "self", ",", "env", ":", "gym", ".", "Env", ",", "replay_buffer", ":", "ReplayBuffer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "env", ":", "training", "environment", "replay_buffer", ":", "replay", "buffer", "storing", "experiences", "\"", "\"", "\"", "self", ".", "env", "=", "env", "self", ".", "replay_buffer", "=", "replay_buffer", "self", ".", "reset", "(", ")", "self", ".", "state", "=", "self", ".", "env", ".", "reset", "(", ")"], "docstring": "Args:\r\n            env: training environment\r\n            replay_buffer: replay buffer storing experiences", "docstring_tokens": ["args", "env", "training", "environment", "replay_buffer", "replay", "buffer", "storing", "experiences"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "Agent", "start_line": 151, "end_line": 160, "hash": "79069f9f41d429d84d5d8c3a7e489527", "complexity": 1, "parameters": ["env", "replay_buffer"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "get_action", "original_string": "def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\r\n        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\r\n\r\n        Args:\r\n            net: DQN network\r\n            epsilon: value to determine likelihood of taking a random action\r\n            device: current device\r\n\r\n        Returns:\r\n            action\r\n\r\n        \"\"\"\r\n        if random.random() < epsilon:\r\n            action = self.env.action_space.sample()\r\n        else:\r\n            state = torch.tensor([self.state])\r\n\r\n            if device not in [\"cpu\"]:\r\n                state = state.cuda(device)\r\n\r\n            q_values = net(state)\r\n            _, action = torch.max(q_values, dim=1)\r\n            action = int(action.item())\r\n\r\n        return action", "language": "python", "code": "def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\r\n        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\r\n\r\n        Args:\r\n            net: DQN network\r\n            epsilon: value to determine likelihood of taking a random action\r\n            device: current device\r\n\r\n        Returns:\r\n            action\r\n\r\n        \"\"\"\r\n        if random.random() < epsilon:\r\n            action = self.env.action_space.sample()\r\n        else:\r\n            state = torch.tensor([self.state])\r\n\r\n            if device not in [\"cpu\"]:\r\n                state = state.cuda(device)\r\n\r\n            q_values = net(state)\r\n            _, action = torch.max(q_values, dim=1)\r\n            action = int(action.item())\r\n\r\n        return action", "code_tokens": ["def", "get_action", "(", "self", ",", "net", ":", "nn", ".", "Module", ",", "epsilon", ":", "float", ",", "device", ":", "str", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Using", "the", "given", "network", ",", "decide", "what", "action", "to", "carry", "out", "using", "an", "epsilon", "-", "greedy", "policy", ".", "Args", ":", "net", ":", "DQN", "network", "epsilon", ":", "value", "to", "determine", "likelihood", "of", "taking", "a", "random", "action", "device", ":", "current", "device", "Returns", ":", "action", "\"", "\"", "\"", "if", "random", ".", "random", "(", ")", "<", "epsilon", ":", "action", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", "else", ":", "state", "=", "torch", ".", "tensor", "(", "[", "self", ".", "state", "]", ")", "if", "device", "not", "in", "[", "\"", "cpu", "\"", "]", ":", "state", "=", "state", ".", "cuda", "(", "device", ")", "q_values", "=", "net", "(", "state", ")", "_", ",", "action", "=", "torch", ".", "max", "(", "q_values", ",", "dim", "=", "1", ")", "action", "=", "int", "(", "action", ".", "item", "(", ")", ")", "return", "action"], "docstring": "Using the given network, decide what action to carry out using an epsilon-greedy policy.\r\n\r\n        Args:\r\n            net: DQN network\r\n            epsilon: value to determine likelihood of taking a random action\r\n            device: current device\r\n\r\n        Returns:\r\n            action", "docstring_tokens": ["using", "the", "given", "network", "decide", "what", "action", "to", "carry", "out", "using", "an", "epsilon", "greedy", "policy", "args", "net", "dqn", "network", "epsilon", "value", "to", "determine", "likelihood", "of", "taking", "a", "random", "action", "device", "current", "device", "returns", "action"], "docstring_summary": "Using the given network, decide what action to carry out using an epsilon-greedy policy.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "Agent", "start_line": 166, "end_line": 190, "hash": "169dd8784b175b341d94e259ac47f6ea", "complexity": 3, "parameters": ["net", "epsilon", "device"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "play_step", "original_string": "def play_step(self, net: nn.Module, epsilon: float = 0.0, device: str = \"cpu\") -> tuple[float, bool]:\r\n        \"\"\"Carries out a single interaction step between the agent and the environment.\r\n\r\n        Args:\r\n            net: DQN network\r\n            epsilon: value to determine likelihood of taking a random action\r\n            device: current device\r\n\r\n        Returns:\r\n            reward, done\r\n\r\n        \"\"\"\r\n        action = self.get_action(net, epsilon, device)\r\n\r\n        # do step in the environment\r\n        new_state, reward, done, _ = self.env.step(action)\r\n\r\n        exp = Experience(self.state, action, reward, done, new_state)\r\n\r\n        self.replay_buffer.append(exp)\r\n\r\n        self.state = new_state\r\n        if done:\r\n            self.reset()\r\n        return reward, done", "language": "python", "code": "def play_step(self, net: nn.Module, epsilon: float = 0.0, device: str = \"cpu\") -> tuple[float, bool]:\r\n        \"\"\"Carries out a single interaction step between the agent and the environment.\r\n\r\n        Args:\r\n            net: DQN network\r\n            epsilon: value to determine likelihood of taking a random action\r\n            device: current device\r\n\r\n        Returns:\r\n            reward, done\r\n\r\n        \"\"\"\r\n        action = self.get_action(net, epsilon, device)\r\n\r\n        # do step in the environment\r\n        new_state, reward, done, _ = self.env.step(action)\r\n\r\n        exp = Experience(self.state, action, reward, done, new_state)\r\n\r\n        self.replay_buffer.append(exp)\r\n\r\n        self.state = new_state\r\n        if done:\r\n            self.reset()\r\n        return reward, done", "code_tokens": ["def", "play_step", "(", "self", ",", "net", ":", "nn", ".", "Module", ",", "epsilon", ":", "float", "=", "0", ".", "0", ",", "device", ":", "str", "=", "\"", "cpu", "\"", ")", "-", ">", "tuple", "[", "float", ",", "bool", "]", ":", "\"", "\"", "\"", "Carries", "out", "a", "single", "interaction", "step", "between", "the", "agent", "and", "the", "environment", ".", "Args", ":", "net", ":", "DQN", "network", "epsilon", ":", "value", "to", "determine", "likelihood", "of", "taking", "a", "random", "action", "device", ":", "current", "device", "Returns", ":", "reward", ",", "done", "\"", "\"", "\"", "action", "=", "self", ".", "get_action", "(", "net", ",", "epsilon", ",", "device", ")", "new_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "exp", "=", "Experience", "(", "self", ".", "state", ",", "action", ",", "reward", ",", "done", ",", "new_state", ")", "self", ".", "replay_buffer", ".", "append", "(", "exp", ")", "self", ".", "state", "=", "new_state", "if", "done", ":", "self", ".", "reset", "(", ")", "return", "reward", ",", "done"], "docstring": "Carries out a single interaction step between the agent and the environment.\r\n\r\n        Args:\r\n            net: DQN network\r\n            epsilon: value to determine likelihood of taking a random action\r\n            device: current device\r\n\r\n        Returns:\r\n            reward, done", "docstring_tokens": ["carries", "out", "a", "single", "interaction", "step", "between", "the", "agent", "and", "the", "environment", "args", "net", "dqn", "network", "epsilon", "value", "to", "determine", "likelihood", "of", "taking", "a", "random", "action", "device", "current", "device", "returns", "reward", "done"], "docstring_summary": "Carries out a single interaction step between the agent and the environment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "Agent", "start_line": 193, "end_line": 217, "hash": "7251f027bdac7b54f5757158807d00ef", "complexity": 2, "parameters": ["net", "epsilon", "device"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "populate", "original_string": "def populate(self, steps: int = 1000) -> None:\r\n        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\r\n        experiences.\r\n\r\n        Args:\r\n            steps: number of random steps to populate the buffer with\r\n\r\n        \"\"\"\r\n        for i in range(steps):\r\n            self.agent.play_step(self.net, epsilon=1.0)", "language": "python", "code": "def populate(self, steps: int = 1000) -> None:\r\n        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\r\n        experiences.\r\n\r\n        Args:\r\n            steps: number of random steps to populate the buffer with\r\n\r\n        \"\"\"\r\n        for i in range(steps):\r\n            self.agent.play_step(self.net, epsilon=1.0)", "code_tokens": ["def", "populate", "(", "self", ",", "steps", ":", "int", "=", "1000", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Carries", "out", "several", "random", "steps", "through", "the", "environment", "to", "initially", "fill", "up", "the", "replay", "buffer", "with", "experiences", ".", "Args", ":", "steps", ":", "number", "of", "random", "steps", "to", "populate", "the", "buffer", "with", "\"", "\"", "\"", "for", "i", "in", "range", "(", "steps", ")", ":", "self", ".", "agent", ".", "play_step", "(", "self", ".", "net", ",", "epsilon", "=", "1", ".", "0", ")"], "docstring": "Carries out several random steps through the environment to initially fill up the replay buffer with\r\n        experiences.\r\n\r\n        Args:\r\n            steps: number of random steps to populate the buffer with", "docstring_tokens": ["carries", "out", "several", "random", "steps", "through", "the", "environment", "to", "initially", "fill", "up", "the", "replay", "buffer", "with", "experiences", "args", "steps", "number", "of", "random", "steps", "to", "populate", "the", "buffer", "with"], "docstring_summary": "Carries out several random steps through the environment to initially fill up the replay buffer with", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQNLightning", "start_line": 275, "end_line": 284, "hash": "2527e3394c2620dce71aba1682e441ae", "complexity": 2, "parameters": ["steps"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "forward", "original_string": "def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n        \"\"\"Passes in a state `x` through the network and gets the `q_values` of each action as an output.\r\n\r\n        Args:\r\n            x: environment state\r\n\r\n        Returns:\r\n            q values\r\n\r\n        \"\"\"\r\n        return self.net(x)", "language": "python", "code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n        \"\"\"Passes in a state `x` through the network and gets the `q_values` of each action as an output.\r\n\r\n        Args:\r\n            x: environment state\r\n\r\n        Returns:\r\n            q values\r\n\r\n        \"\"\"\r\n        return self.net(x)", "code_tokens": ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Passes", "in", "a", "state", "`", "x", "`", "through", "the", "network", "and", "gets", "the", "`", "q_values", "`", "of", "each", "action", "as", "an", "output", ".", "Args", ":", "x", ":", "environment", "state", "Returns", ":", "q", "values", "\"", "\"", "\"", "return", "self", ".", "net", "(", "x", ")"], "docstring": "Passes in a state `x` through the network and gets the `q_values` of each action as an output.\r\n\r\n        Args:\r\n            x: environment state\r\n\r\n        Returns:\r\n            q values", "docstring_tokens": ["passes", "in", "a", "state", "x", "through", "the", "network", "and", "gets", "the", "q_values", "of", "each", "action", "as", "an", "output", "args", "x", "environment", "state", "returns", "q", "values"], "docstring_summary": "Passes in a state `x` through the network and gets the `q_values` of each action as an output.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQNLightning", "start_line": 286, "end_line": 296, "hash": "c99c6821be9510d196b37ee8b847ba02", "complexity": 1, "parameters": ["x"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "dqn_mse_loss", "original_string": "def dqn_mse_loss(self, batch: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\r\n        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n\r\n        Returns:\r\n            loss\r\n\r\n        \"\"\"\r\n        states, actions, rewards, dones, next_states = batch\r\n\r\n        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\r\n\r\n        with torch.no_grad():\r\n            next_state_values = self.target_net(next_states).max(1)[0]\r\n            next_state_values[dones] = 0.0\r\n            next_state_values = next_state_values.detach()\r\n\r\n        expected_state_action_values = next_state_values * self.gamma + rewards\r\n\r\n        return nn.MSELoss()(state_action_values, expected_state_action_values)", "language": "python", "code": "def dqn_mse_loss(self, batch: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\r\n        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n\r\n        Returns:\r\n            loss\r\n\r\n        \"\"\"\r\n        states, actions, rewards, dones, next_states = batch\r\n\r\n        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\r\n\r\n        with torch.no_grad():\r\n            next_state_values = self.target_net(next_states).max(1)[0]\r\n            next_state_values[dones] = 0.0\r\n            next_state_values = next_state_values.detach()\r\n\r\n        expected_state_action_values = next_state_values * self.gamma + rewards\r\n\r\n        return nn.MSELoss()(state_action_values, expected_state_action_values)", "code_tokens": ["def", "dqn_mse_loss", "(", "self", ",", "batch", ":", "tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Calculates", "the", "mse", "loss", "using", "a", "mini", "batch", "from", "the", "replay", "buffer", ".", "Args", ":", "batch", ":", "current", "mini", "batch", "of", "replay", "data", "Returns", ":", "loss", "\"", "\"", "\"", "states", ",", "actions", ",", "rewards", ",", "dones", ",", "next_states", "=", "batch", "state_action_values", "=", "self", ".", "net", "(", "states", ")", ".", "gather", "(", "1", ",", "actions", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "with", "torch", ".", "no_grad", "(", ")", ":", "next_state_values", "=", "self", ".", "target_net", "(", "next_states", ")", ".", "max", "(", "1", ")", "[", "0", "]", "next_state_values", "[", "dones", "]", "=", "0", ".", "0", "next_state_values", "=", "next_state_values", ".", "detach", "(", ")", "expected_state_action_values", "=", "next_state_values", "*", "self", ".", "gamma", "+", "rewards", "return", "nn", ".", "MSELoss", "(", ")", "(", "state_action_values", ",", "expected_state_action_values", ")"], "docstring": "Calculates the mse loss using a mini batch from the replay buffer.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n\r\n        Returns:\r\n            loss", "docstring_tokens": ["calculates", "the", "mse", "loss", "using", "a", "mini", "batch", "from", "the", "replay", "buffer", "args", "batch", "current", "mini", "batch", "of", "replay", "data", "returns", "loss"], "docstring_summary": "Calculates the mse loss using a mini batch from the replay buffer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQNLightning", "start_line": 298, "end_line": 319, "hash": "0e907977d8d0ed6b8547ec4ad955ec51", "complexity": 2, "parameters": ["batch", "torch.Tensor]"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "training_step", "original_string": "def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], nb_batch) -> OrderedDict:\r\n        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss based on\r\n        the minibatch received.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n            nb_batch: batch number\r\n\r\n        Returns:\r\n            Training loss and log metrics\r\n\r\n        \"\"\"\r\n        device = self.get_device(batch)\r\n        epsilon = max(self.eps_end, self.eps_start - (self.global_step + 1) / self.eps_last_frame)\r\n\r\n        # step through environment with agent\r\n        reward, done = self.agent.play_step(self.net, epsilon, device)\r\n        self.episode_reward += reward\r\n\r\n        # calculates training loss\r\n        loss = self.dqn_mse_loss(batch)\r\n\r\n        if done:\r\n            self.total_reward = self.episode_reward\r\n            self.episode_reward = 0\r\n\r\n        # Soft update of target network\r\n        if self.global_step % self.sync_rate == 0:\r\n            self.target_net.load_state_dict(self.net.state_dict())\r\n\r\n        log = {\r\n            \"total_reward\": torch.tensor(self.total_reward).to(device),\r\n            \"reward\": torch.tensor(reward).to(device),\r\n            \"steps\": torch.tensor(self.global_step).to(device),\r\n        }\r\n\r\n        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": log})", "language": "python", "code": "def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], nb_batch) -> OrderedDict:\r\n        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss based on\r\n        the minibatch received.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n            nb_batch: batch number\r\n\r\n        Returns:\r\n            Training loss and log metrics\r\n\r\n        \"\"\"\r\n        device = self.get_device(batch)\r\n        epsilon = max(self.eps_end, self.eps_start - (self.global_step + 1) / self.eps_last_frame)\r\n\r\n        # step through environment with agent\r\n        reward, done = self.agent.play_step(self.net, epsilon, device)\r\n        self.episode_reward += reward\r\n\r\n        # calculates training loss\r\n        loss = self.dqn_mse_loss(batch)\r\n\r\n        if done:\r\n            self.total_reward = self.episode_reward\r\n            self.episode_reward = 0\r\n\r\n        # Soft update of target network\r\n        if self.global_step % self.sync_rate == 0:\r\n            self.target_net.load_state_dict(self.net.state_dict())\r\n\r\n        log = {\r\n            \"total_reward\": torch.tensor(self.total_reward).to(device),\r\n            \"reward\": torch.tensor(reward).to(device),\r\n            \"steps\": torch.tensor(self.global_step).to(device),\r\n        }\r\n\r\n        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": log})", "code_tokens": ["def", "training_step", "(", "self", ",", "batch", ":", "tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ",", "nb_batch", ")", "-", ">", "OrderedDict", ":", "\"", "\"", "\"", "Carries", "out", "a", "single", "step", "through", "the", "environment", "to", "update", "the", "replay", "buffer", ".", "Then", "calculates", "loss", "based", "on", "the", "minibatch", "received", ".", "Args", ":", "batch", ":", "current", "mini", "batch", "of", "replay", "data", "nb_batch", ":", "batch", "number", "Returns", ":", "Training", "loss", "and", "log", "metrics", "\"", "\"", "\"", "device", "=", "self", ".", "get_device", "(", "batch", ")", "epsilon", "=", "max", "(", "self", ".", "eps_end", ",", "self", ".", "eps_start", "-", "(", "self", ".", "global_step", "+", "1", ")", "/", "self", ".", "eps_last_frame", ")", "reward", ",", "done", "=", "self", ".", "agent", ".", "play_step", "(", "self", ".", "net", ",", "epsilon", ",", "device", ")", "self", ".", "episode_reward", "+", "=", "reward", "loss", "=", "self", ".", "dqn_mse_loss", "(", "batch", ")", "if", "done", ":", "self", ".", "total_reward", "=", "self", ".", "episode_reward", "self", ".", "episode_reward", "=", "0", "if", "self", ".", "global_step", "%", "self", ".", "sync_rate", "=", "=", "0", ":", "self", ".", "target_net", ".", "load_state_dict", "(", "self", ".", "net", ".", "state_dict", "(", ")", ")", "log", "=", "{", "\"", "total_reward", "\"", ":", "torch", ".", "tensor", "(", "self", ".", "total_reward", ")", ".", "to", "(", "device", ")", ",", "\"", "reward", "\"", ":", "torch", ".", "tensor", "(", "reward", ")", ".", "to", "(", "device", ")", ",", "\"", "steps", "\"", ":", "torch", ".", "tensor", "(", "self", ".", "global_step", ")", ".", "to", "(", "device", ")", ",", "}", "return", "OrderedDict", "(", "{", "\"", "loss", "\"", ":", "loss", ",", "\"", "log", "\"", ":", "log", ",", "\"", "progress_bar", "\"", ":", "log", "}", ")"], "docstring": "Carries out a single step through the environment to update the replay buffer. Then calculates loss based on\r\n        the minibatch received.\r\n\r\n        Args:\r\n            batch: current mini batch of replay data\r\n            nb_batch: batch number\r\n\r\n        Returns:\r\n            Training loss and log metrics", "docstring_tokens": ["carries", "out", "a", "single", "step", "through", "the", "environment", "to", "update", "the", "replay", "buffer", "then", "calculates", "loss", "based", "on", "the", "minibatch", "received", "args", "batch", "current", "mini", "batch", "of", "replay", "data", "nb_batch", "batch", "number", "returns", "training", "loss", "and", "log", "metrics"], "docstring_summary": "Carries out a single step through the environment to update the replay buffer. Then calculates loss based on", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQNLightning", "start_line": 321, "end_line": 357, "hash": "84b3939a1240065a02cb6e28f5ccfedb", "complexity": 3, "parameters": ["batch", "torch.Tensor]", "nb_batch"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "configure_optimizers", "original_string": "def configure_optimizers(self) -> list[Optimizer]:\r\n        \"\"\"Initialize Adam optimizer.\"\"\"\r\n        optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\r\n        return [optimizer]", "language": "python", "code": "def configure_optimizers(self) -> list[Optimizer]:\r\n        \"\"\"Initialize Adam optimizer.\"\"\"\r\n        optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\r\n        return [optimizer]", "code_tokens": ["def", "configure_optimizers", "(", "self", ")", "-", ">", "list", "[", "Optimizer", "]", ":", "\"", "\"", "\"", "Initialize", "Adam", "optimizer", ".", "\"", "\"", "\"", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "net", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr", ")", "return", "[", "optimizer", "]"], "docstring": "Initialize Adam optimizer.", "docstring_tokens": ["initialize", "adam", "optimizer"], "docstring_summary": "Initialize Adam optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQNLightning", "start_line": 359, "end_line": 362, "hash": "82c3764240ac6083dde84d3397714b39", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "func_name": "__dataloader", "original_string": "def __dataloader(self) -> DataLoader:\r\n        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\r\n        dataset = RLDataset(self.buffer, self.episode_length)\r\n        return DataLoader(dataset=dataset, batch_size=self.batch_size, sampler=None)", "language": "python", "code": "def __dataloader(self) -> DataLoader:\r\n        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\r\n        dataset = RLDataset(self.buffer, self.episode_length)\r\n        return DataLoader(dataset=dataset, batch_size=self.batch_size, sampler=None)", "code_tokens": ["def", "__dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "\"", "\"", "\"", "Initialize", "the", "Replay", "Buffer", "dataset", "used", "for", "retrieving", "experiences", ".", "\"", "\"", "\"", "dataset", "=", "RLDataset", "(", "self", ".", "buffer", ",", "self", ".", "episode_length", ")", "return", "DataLoader", "(", "dataset", "=", "dataset", ",", "batch_size", "=", "self", ".", "batch_size", ",", "sampler", "=", "None", ")"], "docstring": "Initialize the Replay Buffer dataset used for retrieving experiences.", "docstring_tokens": ["initialize", "the", "replay", "buffer", "dataset", "used", "for", "retrieving", "experiences"], "docstring_summary": "Initialize the Replay Buffer dataset used for retrieving experiences.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\reinforce_learn_Qnet.py", "partition": "train", "function_type": "class_method", "class_name": "DQNLightning", "start_line": 364, "end_line": 367, "hash": "ad2f65027fa37bad6b9945e49b6838a6", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\semantic_segmentation.py", "func_name": "_create_synth_kitti_dataset", "original_string": "def _create_synth_kitti_dataset(path_dir: str, image_dims: tuple = (1024, 512)):\r\n    \"\"\"Create synthetic dataset with random images, just to simulate that the dataset have been already downloaded.\"\"\"\r\n    path_dir_images = os.path.join(path_dir, KITTI.IMAGE_PATH)\r\n    path_dir_masks = os.path.join(path_dir, KITTI.MASK_PATH)\r\n    for p_dir in (path_dir_images, path_dir_masks):\r\n        os.makedirs(p_dir, exist_ok=True)\r\n    for i in range(3):\r\n        path_img = os.path.join(path_dir_images, f\"dummy_kitti_{i}.png\")\r\n        Image.new(\"RGB\", image_dims).save(path_img)\r\n        path_mask = os.path.join(path_dir_masks, f\"dummy_kitti_{i}.png\")\r\n        Image.new(\"L\", image_dims).save(path_mask)", "language": "python", "code": "def _create_synth_kitti_dataset(path_dir: str, image_dims: tuple = (1024, 512)):\r\n    \"\"\"Create synthetic dataset with random images, just to simulate that the dataset have been already downloaded.\"\"\"\r\n    path_dir_images = os.path.join(path_dir, KITTI.IMAGE_PATH)\r\n    path_dir_masks = os.path.join(path_dir, KITTI.MASK_PATH)\r\n    for p_dir in (path_dir_images, path_dir_masks):\r\n        os.makedirs(p_dir, exist_ok=True)\r\n    for i in range(3):\r\n        path_img = os.path.join(path_dir_images, f\"dummy_kitti_{i}.png\")\r\n        Image.new(\"RGB\", image_dims).save(path_img)\r\n        path_mask = os.path.join(path_dir_masks, f\"dummy_kitti_{i}.png\")\r\n        Image.new(\"L\", image_dims).save(path_mask)", "code_tokens": ["def", "_create_synth_kitti_dataset", "(", "path_dir", ":", "str", ",", "image_dims", ":", "tuple", "=", "(", "1024", ",", "512", ")", ")", ":", "\"", "\"", "\"", "Create", "synthetic", "dataset", "with", "random", "images", ",", "just", "to", "simulate", "that", "the", "dataset", "have", "been", "already", "downloaded", ".", "\"", "\"", "\"", "path_dir_images", "=", "os", ".", "path", ".", "join", "(", "path_dir", ",", "KITTI", ".", "IMAGE_PATH", ")", "path_dir_masks", "=", "os", ".", "path", ".", "join", "(", "path_dir", ",", "KITTI", ".", "MASK_PATH", ")", "for", "p_dir", "in", "(", "path_dir_images", ",", "path_dir_masks", ")", ":", "os", ".", "makedirs", "(", "p_dir", ",", "exist_ok", "=", "True", ")", "for", "i", "in", "range", "(", "3", ")", ":", "path_img", "=", "os", ".", "path", ".", "join", "(", "path_dir_images", ",", "f", "\"", "dummy_kitti_", "{", "i", "}", ".", "png", "\"", ")", "Image", ".", "new", "(", "\"", "RGB", "\"", ",", "image_dims", ")", ".", "save", "(", "path_img", ")", "path_mask", "=", "os", ".", "path", ".", "join", "(", "path_dir_masks", ",", "f", "\"", "dummy_kitti_", "{", "i", "}", ".", "png", "\"", ")", "Image", ".", "new", "(", "\"", "L", "\"", ",", "image_dims", ")", ".", "save", "(", "path_mask", ")"], "docstring": "Create synthetic dataset with random images, just to simulate that the dataset have been already downloaded.", "docstring_tokens": ["create", "synthetic", "dataset", "with", "random", "images", "just", "to", "simulate", "that", "the", "dataset", "have", "been", "already", "downloaded"], "docstring_summary": "Create synthetic dataset with random images, just to simulate that the dataset have been already downloaded.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\semantic_segmentation.py", "partition": "train", "function_type": "function", "start_line": 31, "end_line": 41, "hash": "3e9a253681d0356a92693436b57f7f6e", "complexity": 3, "parameters": ["path_dir", "image_dims", "512"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\semantic_segmentation.py", "func_name": "encode_segmap", "original_string": "def encode_segmap(self, mask):\r\n        \"\"\"Sets void classes to zero so they won't be considered for training.\"\"\"\r\n        for voidc in self.void_labels:\r\n            mask[mask == voidc] = self.ignore_index\r\n        for validc in self.valid_labels:\r\n            mask[mask == validc] = self.class_map[validc]\r\n        # remove extra idxs from updated dataset\r\n        mask[mask > 18] = self.ignore_index\r\n        return mask", "language": "python", "code": "def encode_segmap(self, mask):\r\n        \"\"\"Sets void classes to zero so they won't be considered for training.\"\"\"\r\n        for voidc in self.void_labels:\r\n            mask[mask == voidc] = self.ignore_index\r\n        for validc in self.valid_labels:\r\n            mask[mask == validc] = self.class_map[validc]\r\n        # remove extra idxs from updated dataset\r\n        mask[mask > 18] = self.ignore_index\r\n        return mask", "code_tokens": ["def", "encode_segmap", "(", "self", ",", "mask", ")", ":", "\"", "\"", "\"", "Sets", "void", "classes", "to", "zero", "so", "they", "won", "'", "t", "be", "considered", "for", "training", ".", "\"", "\"", "\"", "for", "voidc", "in", "self", ".", "void_labels", ":", "mask", "[", "mask", "=", "=", "voidc", "]", "=", "self", ".", "ignore_index", "for", "validc", "in", "self", ".", "valid_labels", ":", "mask", "[", "mask", "=", "=", "validc", "]", "=", "self", ".", "class_map", "[", "validc", "]", "mask", "[", "mask", ">", "18", "]", "=", "self", ".", "ignore_index", "return", "mask"], "docstring": "Sets void classes to zero so they won't be considered for training.", "docstring_tokens": ["sets", "void", "classes", "to", "zero", "so", "they", "won", "t", "be", "considered", "for", "training"], "docstring_summary": "Sets void classes to zero so they won't be considered for training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\semantic_segmentation.py", "partition": "train", "function_type": "class_method", "class_name": "KITTI", "start_line": 121, "end_line": 129, "hash": "2e269e678f61ae13a696332ca14cae65", "complexity": 3, "parameters": ["mask"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\semantic_segmentation.py", "func_name": "get_filenames", "original_string": "def get_filenames(self, path):\r\n        \"\"\"Returns a list of absolute paths to images inside given `path`\"\"\"\r\n        files_list = []\r\n        for filename in os.listdir(path):\r\n            files_list.append(os.path.join(path, filename))\r\n        return files_list", "language": "python", "code": "def get_filenames(self, path):\r\n        \"\"\"Returns a list of absolute paths to images inside given `path`\"\"\"\r\n        files_list = []\r\n        for filename in os.listdir(path):\r\n            files_list.append(os.path.join(path, filename))\r\n        return files_list", "code_tokens": ["def", "get_filenames", "(", "self", ",", "path", ")", ":", "\"", "\"", "\"", "Returns", "a", "list", "of", "absolute", "paths", "to", "images", "inside", "given", "`", "path", "`", "\"", "\"", "\"", "files_list", "=", "[", "]", "for", "filename", "in", "os", ".", "listdir", "(", "path", ")", ":", "files_list", ".", "append", "(", "os", ".", "path", ".", "join", "(", "path", ",", "filename", ")", ")", "return", "files_list"], "docstring": "Returns a list of absolute paths to images inside given `path`", "docstring_tokens": ["returns", "a", "list", "of", "absolute", "paths", "to", "images", "inside", "given", "path"], "docstring_summary": "Returns a list of absolute paths to images inside given `path`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\semantic_segmentation.py", "partition": "train", "function_type": "class_method", "class_name": "KITTI", "start_line": 131, "end_line": 136, "hash": "6d7486ff7ab1eecc15b3b45f787da51a", "complexity": 2, "parameters": ["path"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\domain_templates\\semantic_segmentation.py", "func_name": "__init__", "original_string": "def __init__(self, num_classes: int = 19, num_layers: int = 5, features_start: int = 64, bilinear: bool = False):\r\n        \"\"\"\r\n        Args:\r\n            num_classes: Number of output classes required (default 19 for KITTI dataset)\r\n            num_layers: Number of layers in each side of U-net\r\n            features_start: Number of features in first layer\r\n            bilinear: Whether to use bilinear interpolation or transposed convolutions for upsampling.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.num_layers = num_layers\r\n\r\n        layers = [DoubleConv(3, features_start)]\r\n\r\n        feats = features_start\r\n        for _ in range(num_layers - 1):\r\n            layers.append(Down(feats, feats * 2))\r\n            feats *= 2\r\n\r\n        for _ in range(num_layers - 1):\r\n            layers.append(Up(feats, feats // 2, bilinear))\r\n            feats //= 2\r\n\r\n        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\r\n\r\n        self.layers = nn.ModuleList(layers)", "language": "python", "code": "def __init__(self, num_classes: int = 19, num_layers: int = 5, features_start: int = 64, bilinear: bool = False):\r\n        \"\"\"\r\n        Args:\r\n            num_classes: Number of output classes required (default 19 for KITTI dataset)\r\n            num_layers: Number of layers in each side of U-net\r\n            features_start: Number of features in first layer\r\n            bilinear: Whether to use bilinear interpolation or transposed convolutions for upsampling.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.num_layers = num_layers\r\n\r\n        layers = [DoubleConv(3, features_start)]\r\n\r\n        feats = features_start\r\n        for _ in range(num_layers - 1):\r\n            layers.append(Down(feats, feats * 2))\r\n            feats *= 2\r\n\r\n        for _ in range(num_layers - 1):\r\n            layers.append(Up(feats, feats // 2, bilinear))\r\n            feats //= 2\r\n\r\n        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\r\n\r\n        self.layers = nn.ModuleList(layers)", "code_tokens": ["def", "__init__", "(", "self", ",", "num_classes", ":", "int", "=", "19", ",", "num_layers", ":", "int", "=", "5", ",", "features_start", ":", "int", "=", "64", ",", "bilinear", ":", "bool", "=", "False", ")", ":", "\"", "\"", "\"", "Args", ":", "num_classes", ":", "Number", "of", "output", "classes", "required", "(", "default", "19", "for", "KITTI", "dataset", ")", "num_layers", ":", "Number", "of", "layers", "in", "each", "side", "of", "U", "-", "net", "features_start", ":", "Number", "of", "features", "in", "first", "layer", "bilinear", ":", "Whether", "to", "use", "bilinear", "interpolation", "or", "transposed", "convolutions", "for", "upsampling", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "num_layers", "=", "num_layers", "layers", "=", "[", "DoubleConv", "(", "3", ",", "features_start", ")", "]", "feats", "=", "features_start", "for", "_", "in", "range", "(", "num_layers", "-", "1", ")", ":", "layers", ".", "append", "(", "Down", "(", "feats", ",", "feats", "*", "2", ")", ")", "feats", "*", "=", "2", "for", "_", "in", "range", "(", "num_layers", "-", "1", ")", ":", "layers", ".", "append", "(", "Up", "(", "feats", ",", "feats", "/", "/", "2", ",", "bilinear", ")", ")", "feats", "/", "/", "=", "2", "layers", ".", "append", "(", "nn", ".", "Conv2d", "(", "feats", ",", "num_classes", ",", "kernel_size", "=", "1", ")", ")", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "layers", ")"], "docstring": "Args:\r\n            num_classes: Number of output classes required (default 19 for KITTI dataset)\r\n            num_layers: Number of layers in each side of U-net\r\n            features_start: Number of features in first layer\r\n            bilinear: Whether to use bilinear interpolation or transposed convolutions for upsampling.", "docstring_tokens": ["args", "num_classes", "number", "of", "output", "classes", "required", "default", "19", "for", "kitti", "dataset", "num_layers", "number", "of", "layers", "in", "each", "side", "of", "u", "net", "features_start", "number", "of", "features", "in", "first", "layer", "bilinear", "whether", "to", "use", "bilinear", "interpolation", "or", "transposed", "convolutions", "for", "upsampling"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\domain_templates\\semantic_segmentation.py", "partition": "train", "function_type": "class_method", "class_name": "UNet", "start_line": 158, "end_line": 182, "hash": "c60796b8a88f9aa314cb4020a789103d", "complexity": 3, "parameters": ["num_classes", "num_layers", "features_start", "bilinear"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "precompute_freqs_cis", "original_string": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\r\n    \"\"\"Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\r\n\r\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\r\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\r\n    The returned tensor contains complex values in complex64 data type.\r\n\r\n    Args:\r\n        dim (int): Dimension of the frequency tensor.\r\n        end (int): End index for precomputing frequencies.\r\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\r\n\r\n    Returns:\r\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\r\n\r\n    \"\"\"\r\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\r\n    t = torch.arange(end, device=freqs.device)\r\n    freqs = torch.outer(t, freqs).float()\r\n    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\r", "language": "python", "code": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\r\n    \"\"\"Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\r\n\r\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\r\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\r\n    The returned tensor contains complex values in complex64 data type.\r\n\r\n    Args:\r\n        dim (int): Dimension of the frequency tensor.\r\n        end (int): End index for precomputing frequencies.\r\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\r\n\r\n    Returns:\r\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\r\n\r\n    \"\"\"\r\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\r\n    t = torch.arange(end, device=freqs.device)\r\n    freqs = torch.outer(t, freqs).float()\r\n    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\r", "code_tokens": ["def", "precompute_freqs_cis", "(", "dim", ":", "int", ",", "end", ":", "int", ",", "theta", ":", "float", "=", "10000", ".", "0", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Precompute", "the", "frequency", "tensor", "for", "complex", "exponentials", "(", "cis", ")", "with", "given", "dimensions", ".", "This", "function", "calculates", "a", "frequency", "tensor", "with", "complex", "exponentials", "using", "the", "given", "dimension", "'", "dim", "'", "and", "the", "end", "index", "'", "end", "'", ".", "The", "'", "theta", "'", "parameter", "scales", "the", "frequencies", ".", "The", "returned", "tensor", "contains", "complex", "values", "in", "complex64", "data", "type", ".", "Args", ":", "dim", "(", "int", ")", ":", "Dimension", "of", "the", "frequency", "tensor", ".", "end", "(", "int", ")", ":", "End", "index", "for", "precomputing", "frequencies", ".", "theta", "(", "float", ",", "optional", ")", ":", "Scaling", "factor", "for", "frequency", "computation", ".", "Defaults", "to", "10000", ".", "0", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Precomputed", "frequency", "tensor", "with", "complex", "exponentials", ".", "\"", "\"", "\"", "freqs", "=", "1", ".", "0", "/", "(", "theta", "*", "*", "(", "torch", ".", "arange", "(", "0", ",", "dim", ",", "2", ")", "[", ":", "(", "dim", "/", "/", "2", ")", "]", ".", "float", "(", ")", "/", "dim", ")", ")", "t", "=", "torch", ".", "arange", "(", "end", ",", "device", "=", "freqs", ".", "device", ")", "freqs", "=", "torch", ".", "outer", "(", "t", ",", "freqs", ")", ".", "float", "(", ")", "return", "torch", ".", "polar", "(", "torch", ".", "ones_like", "(", "freqs", ")", ",", "freqs", ")"], "docstring": "Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\r\n\r\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\r\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\r\n    The returned tensor contains complex values in complex64 data type.\r\n\r\n    Args:\r\n        dim (int): Dimension of the frequency tensor.\r\n        end (int): End index for precomputing frequencies.\r\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\r\n\r\n    Returns:\r\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.", "docstring_tokens": ["precompute", "the", "frequency", "tensor", "for", "complex", "exponentials", "cis", "with", "given", "dimensions", "this", "function", "calculates", "a", "frequency", "tensor", "with", "complex", "exponentials", "using", "the", "given", "dimension", "dim", "and", "the", "end", "index", "end", "the", "theta", "parameter", "scales", "the", "frequencies", "the", "returned", "tensor", "contains", "complex", "values", "in", "complex64", "data", "type", "args", "dim", "int", "dimension", "of", "the", "frequency", "tensor", "end", "int", "end", "index", "for", "precomputing", "frequencies", "theta", "float", "optional", "scaling", "factor", "for", "frequency", "computation", "defaults", "to", "10000", "0", "returns", "torch", "tensor", "precomputed", "frequency", "tensor", "with", "complex", "exponentials"], "docstring_summary": "Precompute the frequency tensor for complex exponentials (cis) with given dimensions.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 37, "end_line": 56, "hash": "e17446729a5934a7d0f538fc43c44565", "complexity": 1, "parameters": ["dim", "end", "theta"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "reshape_for_broadcast", "original_string": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Reshape frequency tensor for broadcasting it with another tensor.\r\n\r\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\r\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\r\n\r\n    The input freqs_cis tensor is assumed to be of shape (max_seqlen, dim),\r\n    and the first seqlen elements will be sliced, but dim must match x.\r\n\r\n    Args:\r\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\r\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\r\n\r\n    Returns:\r\n        torch.Tensor: Reshaped frequency tensor.\r\n\r\n    \"\"\"\r\n    ndim = x.ndim\r\n    assert 0 <= 1 < ndim\r\n    seqlen = x.shape[1]\r\n    freqs_cis = freqs_cis[0:seqlen]\r\n    assert freqs_cis.shape == (seqlen, x.shape[-1])\r\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\r\n    return freqs_cis.view(*shape)", "language": "python", "code": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Reshape frequency tensor for broadcasting it with another tensor.\r\n\r\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\r\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\r\n\r\n    The input freqs_cis tensor is assumed to be of shape (max_seqlen, dim),\r\n    and the first seqlen elements will be sliced, but dim must match x.\r\n\r\n    Args:\r\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\r\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\r\n\r\n    Returns:\r\n        torch.Tensor: Reshaped frequency tensor.\r\n\r\n    \"\"\"\r\n    ndim = x.ndim\r\n    assert 0 <= 1 < ndim\r\n    seqlen = x.shape[1]\r\n    freqs_cis = freqs_cis[0:seqlen]\r\n    assert freqs_cis.shape == (seqlen, x.shape[-1])\r\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\r\n    return freqs_cis.view(*shape)", "code_tokens": ["def", "reshape_for_broadcast", "(", "freqs_cis", ":", "torch", ".", "Tensor", ",", "x", ":", "torch", ".", "Tensor", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Reshape", "frequency", "tensor", "for", "broadcasting", "it", "with", "another", "tensor", ".", "This", "function", "reshapes", "the", "frequency", "tensor", "to", "have", "the", "same", "shape", "as", "the", "target", "tensor", "'", "x", "'", "for", "the", "purpose", "of", "broadcasting", "the", "frequency", "tensor", "during", "element", "-", "wise", "operations", ".", "The", "input", "freqs_cis", "tensor", "is", "assumed", "to", "be", "of", "shape", "(", "max_seqlen", ",", "dim", ")", ",", "and", "the", "first", "seqlen", "elements", "will", "be", "sliced", ",", "but", "dim", "must", "match", "x", ".", "Args", ":", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Frequency", "tensor", "to", "be", "reshaped", ".", "x", "(", "torch", ".", "Tensor", ")", ":", "Target", "tensor", "for", "broadcasting", "compatibility", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Reshaped", "frequency", "tensor", ".", "\"", "\"", "\"", "ndim", "=", "x", ".", "ndim", "assert", "0", "<", "=", "1", "<", "ndim", "seqlen", "=", "x", ".", "shape", "[", "1", "]", "freqs_cis", "=", "freqs_cis", "[", "0", ":", "seqlen", "]", "assert", "freqs_cis", ".", "shape", "=", "=", "(", "seqlen", ",", "x", ".", "shape", "[", "-", "1", "]", ")", "shape", "=", "[", "d", "if", "i", "=", "=", "1", "or", "i", "=", "=", "ndim", "-", "1", "else", "1", "for", "i", ",", "d", "in", "enumerate", "(", "x", ".", "shape", ")", "]", "return", "freqs_cis", ".", "view", "(", "*", "shape", ")"], "docstring": "Reshape frequency tensor for broadcasting it with another tensor.\r\n\r\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\r\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\r\n\r\n    The input freqs_cis tensor is assumed to be of shape (max_seqlen, dim),\r\n    and the first seqlen elements will be sliced, but dim must match x.\r\n\r\n    Args:\r\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\r\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\r\n\r\n    Returns:\r\n        torch.Tensor: Reshaped frequency tensor.", "docstring_tokens": ["reshape", "frequency", "tensor", "for", "broadcasting", "it", "with", "another", "tensor", "this", "function", "reshapes", "the", "frequency", "tensor", "to", "have", "the", "same", "shape", "as", "the", "target", "tensor", "x", "for", "the", "purpose", "of", "broadcasting", "the", "frequency", "tensor", "during", "element", "wise", "operations", "the", "input", "freqs_cis", "tensor", "is", "assumed", "to", "be", "of", "shape", "max_seqlen", "dim", "and", "the", "first", "seqlen", "elements", "will", "be", "sliced", "but", "dim", "must", "match", "x", "args", "freqs_cis", "torch", "tensor", "frequency", "tensor", "to", "be", "reshaped", "x", "torch", "tensor", "target", "tensor", "for", "broadcasting", "compatibility", "returns", "torch", "tensor", "reshaped", "frequency", "tensor"], "docstring_summary": "Reshape frequency tensor for broadcasting it with another tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 59, "end_line": 82, "hash": "acbf19cb572b172511b8846641fbc774", "complexity": 4, "parameters": ["freqs_cis", "x"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "apply_rotary_emb", "original_string": "def apply_rotary_emb(\r\n    xq: torch.Tensor,\r\n    xk: torch.Tensor,\r\n    freqs_cis: torch.Tensor,\r\n) -> tuple[torch.Tensor, torch.Tensor]:\r\n    \"\"\"Apply rotary embeddings to input tensors using the given frequency tensor.\r\n\r\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\r\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\r\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\r\n    returned as real tensors.\r\n\r\n    Args:\r\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\r\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\r\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\r\n\r\n    Returns:\r\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\r\n\r\n    \"\"\"\r\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\r\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\r\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\r\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\r\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\r\n    return xq_out.type_as(xq), xk_out.type_as(xk)", "language": "python", "code": "def apply_rotary_emb(\r\n    xq: torch.Tensor,\r\n    xk: torch.Tensor,\r\n    freqs_cis: torch.Tensor,\r\n) -> tuple[torch.Tensor, torch.Tensor]:\r\n    \"\"\"Apply rotary embeddings to input tensors using the given frequency tensor.\r\n\r\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\r\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\r\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\r\n    returned as real tensors.\r\n\r\n    Args:\r\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\r\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\r\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\r\n\r\n    Returns:\r\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\r\n\r\n    \"\"\"\r\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\r\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\r\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\r\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\r\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\r\n    return xq_out.type_as(xq), xk_out.type_as(xk)", "code_tokens": ["def", "apply_rotary_emb", "(", "xq", ":", "torch", ".", "Tensor", ",", "xk", ":", "torch", ".", "Tensor", ",", "freqs_cis", ":", "torch", ".", "Tensor", ",", ")", "-", ">", "tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\"", "\"", "\"", "Apply", "rotary", "embeddings", "to", "input", "tensors", "using", "the", "given", "frequency", "tensor", ".", "This", "function", "applies", "rotary", "embeddings", "to", "the", "given", "query", "'", "xq", "'", "and", "key", "'", "xk", "'", "tensors", "using", "the", "provided", "frequency", "tensor", "'", "freqs_cis", "'", ".", "The", "input", "tensors", "are", "reshaped", "as", "complex", "numbers", ",", "and", "the", "frequency", "tensor", "is", "reshaped", "for", "broadcasting", "compatibility", ".", "The", "resulting", "tensors", "contain", "rotary", "embeddings", "and", "are", "returned", "as", "real", "tensors", ".", "Args", ":", "xq", "(", "torch", ".", "Tensor", ")", ":", "Query", "tensor", "to", "apply", "rotary", "embeddings", ".", "xk", "(", "torch", ".", "Tensor", ")", ":", "Key", "tensor", "to", "apply", "rotary", "embeddings", ".", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Precomputed", "frequency", "tensor", "for", "complex", "exponentials", ".", "Returns", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "Tuple", "of", "modified", "query", "tensor", "and", "key", "tensor", "with", "rotary", "embeddings", ".", "\"", "\"", "\"", "xq_", "=", "torch", ".", "view_as_complex", "(", "xq", ".", "float", "(", ")", ".", "reshape", "(", "*", "xq", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ",", "2", ")", ")", "xk_", "=", "torch", ".", "view_as_complex", "(", "xk", ".", "float", "(", ")", ".", "reshape", "(", "*", "xk", ".", "shape", "[", ":", "-", "1", "]", ",", "-", "1", ",", "2", ")", ")", "freqs_cis", "=", "reshape_for_broadcast", "(", "freqs_cis", ",", "xq_", ")", "xq_out", "=", "torch", ".", "view_as_real", "(", "xq_", "*", "freqs_cis", ")", ".", "flatten", "(", "3", ")", "xk_out", "=", "torch", ".", "view_as_real", "(", "xk_", "*", "freqs_cis", ")", ".", "flatten", "(", "3", ")", "return", "xq_out", ".", "type_as", "(", "xq", ")", ",", "xk_out", ".", "type_as", "(", "xk", ")"], "docstring": "Apply rotary embeddings to input tensors using the given frequency tensor.\r\n\r\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\r\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\r\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\r\n    returned as real tensors.\r\n\r\n    Args:\r\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\r\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\r\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\r\n\r\n    Returns:\r\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.", "docstring_tokens": ["apply", "rotary", "embeddings", "to", "input", "tensors", "using", "the", "given", "frequency", "tensor", "this", "function", "applies", "rotary", "embeddings", "to", "the", "given", "query", "xq", "and", "key", "xk", "tensors", "using", "the", "provided", "frequency", "tensor", "freqs_cis", "the", "input", "tensors", "are", "reshaped", "as", "complex", "numbers", "and", "the", "frequency", "tensor", "is", "reshaped", "for", "broadcasting", "compatibility", "the", "resulting", "tensors", "contain", "rotary", "embeddings", "and", "are", "returned", "as", "real", "tensors", "args", "xq", "torch", "tensor", "query", "tensor", "to", "apply", "rotary", "embeddings", "xk", "torch", "tensor", "key", "tensor", "to", "apply", "rotary", "embeddings", "freqs_cis", "torch", "tensor", "precomputed", "frequency", "tensor", "for", "complex", "exponentials", "returns", "tuple", "torch", "tensor", "torch", "tensor", "tuple", "of", "modified", "query", "tensor", "and", "key", "tensor", "with", "rotary", "embeddings"], "docstring_summary": "Apply rotary embeddings to input tensors using the given frequency tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 85, "end_line": 111, "hash": "ab332a794fd234d98167bf6395aea8e5", "complexity": 1, "parameters": ["xq", "xk", "freqs_cis"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "repeat_kv", "original_string": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\r\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\r\n    bs, slen, n_kv_heads, head_dim = x.shape\r\n    if n_rep == 1:\r\n        return x\r\n    return (\r\n        x[:, :, :, None, :]\r\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\r\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\r\n    )", "language": "python", "code": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\r\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\r\n    bs, slen, n_kv_heads, head_dim = x.shape\r\n    if n_rep == 1:\r\n        return x\r\n    return (\r\n        x[:, :, :, None, :]\r\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\r\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\r\n    )", "code_tokens": ["def", "repeat_kv", "(", "x", ":", "torch", ".", "Tensor", ",", "n_rep", ":", "int", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "torch", ".", "repeat_interleave", "(", "x", ",", "dim", "=", "2", ",", "repeats", "=", "n_rep", ")", "\"", "\"", "\"", "bs", ",", "slen", ",", "n_kv_heads", ",", "head_dim", "=", "x", ".", "shape", "if", "n_rep", "=", "=", "1", ":", "return", "x", "return", "(", "x", "[", ":", ",", ":", ",", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "bs", ",", "slen", ",", "n_kv_heads", ",", "n_rep", ",", "head_dim", ")", ".", "reshape", "(", "bs", ",", "slen", ",", "n_kv_heads", "*", "n_rep", ",", "head_dim", ")", ")"], "docstring": "torch.repeat_interleave(x, dim=2, repeats=n_rep)", "docstring_tokens": ["torch", "repeat_interleave", "x", "dim", "2", "repeats", "n_rep"], "docstring_summary": "torch.repeat_interleave(x, dim=2, repeats=n_rep)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "function", "start_line": 114, "end_line": 123, "hash": "0e3fb00e550ade8b6986719bfa4f9e43", "complexity": 2, "parameters": ["x", "n_rep"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "forward", "original_string": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Forward pass of the attention module.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after attention.\r\n\r\n        \"\"\"\r\n        bs, seqlen, _ = x.shape\r\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\r\n\r\n        xq = xq.view(bs, seqlen, self.n_heads, self.head_dim)\r\n        xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n        xv = xv.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n\r\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\r\n\r\n        # repeat k/v heads if n_kv_heads < n_heads\r\n        keys = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n        values = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n\r\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xk = keys.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xv = values.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n\r\n        # we use casual mask for training\r\n        output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)\r\n        output = output.transpose(1, 2).contiguous()  # (bs, seqlen, n_local_heads, head_dim)\r\n        output = output.view(bs, seqlen, -1)\r\n        return self.wo(output)", "language": "python", "code": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Forward pass of the attention module.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after attention.\r\n\r\n        \"\"\"\r\n        bs, seqlen, _ = x.shape\r\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\r\n\r\n        xq = xq.view(bs, seqlen, self.n_heads, self.head_dim)\r\n        xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n        xv = xv.view(bs, seqlen, self.n_kv_heads, self.head_dim)\r\n\r\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\r\n\r\n        # repeat k/v heads if n_kv_heads < n_heads\r\n        keys = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n        values = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\r\n\r\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xk = keys.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n        xv = values.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\r\n\r\n        # we use casual mask for training\r\n        output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)\r\n        output = output.transpose(1, 2).contiguous()  # (bs, seqlen, n_local_heads, head_dim)\r\n        output = output.view(bs, seqlen, -1)\r\n        return self.wo(output)", "code_tokens": ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "freqs_cis", ":", "torch", ".", "Tensor", ",", ")", ":", "\"", "\"", "\"", "Forward", "pass", "of", "the", "attention", "module", ".", "Args", ":", "x", "(", "torch", ".", "Tensor", ")", ":", "Input", "tensor", ".", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Precomputed", "frequency", "tensor", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Output", "tensor", "after", "attention", ".", "\"", "\"", "\"", "bs", ",", "seqlen", ",", "_", "=", "x", ".", "shape", "xq", ",", "xk", ",", "xv", "=", "self", ".", "wq", "(", "x", ")", ",", "self", ".", "wk", "(", "x", ")", ",", "self", ".", "wv", "(", "x", ")", "xq", "=", "xq", ".", "view", "(", "bs", ",", "seqlen", ",", "self", ".", "n_heads", ",", "self", ".", "head_dim", ")", "xk", "=", "xk", ".", "view", "(", "bs", ",", "seqlen", ",", "self", ".", "n_kv_heads", ",", "self", ".", "head_dim", ")", "xv", "=", "xv", ".", "view", "(", "bs", ",", "seqlen", ",", "self", ".", "n_kv_heads", ",", "self", ".", "head_dim", ")", "xq", ",", "xk", "=", "apply_rotary_emb", "(", "xq", ",", "xk", ",", "freqs_cis", "=", "freqs_cis", ")", "keys", "=", "repeat_kv", "(", "xk", ",", "self", ".", "n_rep", ")", "values", "=", "repeat_kv", "(", "xv", ",", "self", ".", "n_rep", ")", "xq", "=", "xq", ".", "transpose", "(", "1", ",", "2", ")", "xk", "=", "keys", ".", "transpose", "(", "1", ",", "2", ")", "xv", "=", "values", ".", "transpose", "(", "1", ",", "2", ")", "output", "=", "F", ".", "scaled_dot_product_attention", "(", "xq", ",", "xk", ",", "xv", ",", "is_causal", "=", "True", ")", "output", "=", "output", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "output", "=", "output", ".", "view", "(", "bs", ",", "seqlen", ",", "-", "1", ")", "return", "self", ".", "wo", "(", "output", ")"], "docstring": "Forward pass of the attention module.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after attention.", "docstring_tokens": ["forward", "pass", "of", "the", "attention", "module", "args", "x", "torch", "tensor", "input", "tensor", "freqs_cis", "torch", "tensor", "precomputed", "frequency", "tensor", "returns", "torch", "tensor", "output", "tensor", "after", "attention"], "docstring_summary": "Forward pass of the attention module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Attention", "start_line": 190, "end_line": 226, "hash": "fb748da4c55df2e11dff76a6a6d42d0a", "complexity": 1, "parameters": ["x", "freqs_cis"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "forward", "original_string": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Perform a forward pass through the TransformerBlock.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after applying attention and feedforward layers.\r\n\r\n        \"\"\"\r\n        h = x + self.attention(self.attention_norm(x), freqs_cis)\r\n        return h + self.feed_forward(self.ffn_norm(h))", "language": "python", "code": "def forward(\r\n        self,\r\n        x: torch.Tensor,\r\n        freqs_cis: torch.Tensor,\r\n    ):\r\n        \"\"\"Perform a forward pass through the TransformerBlock.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after applying attention and feedforward layers.\r\n\r\n        \"\"\"\r\n        h = x + self.attention(self.attention_norm(x), freqs_cis)\r\n        return h + self.feed_forward(self.ffn_norm(h))", "code_tokens": ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "freqs_cis", ":", "torch", ".", "Tensor", ",", ")", ":", "\"", "\"", "\"", "Perform", "a", "forward", "pass", "through", "the", "TransformerBlock", ".", "Args", ":", "x", "(", "torch", ".", "Tensor", ")", ":", "Input", "tensor", ".", "freqs_cis", "(", "torch", ".", "Tensor", ")", ":", "Precomputed", "cosine", "and", "sine", "frequencies", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Output", "tensor", "after", "applying", "attention", "and", "feedforward", "layers", ".", "\"", "\"", "\"", "h", "=", "x", "+", "self", ".", "attention", "(", "self", ".", "attention_norm", "(", "x", ")", ",", "freqs_cis", ")", "return", "h", "+", "self", ".", "feed_forward", "(", "self", ".", "ffn_norm", "(", "h", ")", ")"], "docstring": "Perform a forward pass through the TransformerBlock.\r\n\r\n        Args:\r\n            x (torch.Tensor): Input tensor.\r\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\r\n\r\n        Returns:\r\n            torch.Tensor: Output tensor after applying attention and feedforward layers.", "docstring_tokens": ["perform", "a", "forward", "pass", "through", "the", "transformerblock", "args", "x", "torch", "tensor", "input", "tensor", "freqs_cis", "torch", "tensor", "precomputed", "cosine", "and", "sine", "frequencies", "returns", "torch", "tensor", "output", "tensor", "after", "applying", "attention", "and", "feedforward", "layers"], "docstring_summary": "Perform a forward pass through the TransformerBlock.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "TransformerBlock", "start_line": 313, "end_line": 329, "hash": "06586ac69beb2f07fa14675af1173492", "complexity": 1, "parameters": ["x", "freqs_cis"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "init_weights", "original_string": "def init_weights(self):\r\n        \"\"\"[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.\r\n\r\n        \"\"\"\r\n        with torch.device(self.freqs_cis.device):\r\n            self.freqs_cis = self._precompute_freqs_cis()\r\n        nn.init.normal_(self.tok_embeddings.weight)\r\n        for layer in self.layers.values():\r\n            layer.init_weights()\r\n        self.norm.reset_parameters()\r\n        final_out_std = self.model_args.dim**-0.5\r\n        cutoff_factor = 3\r\n        nn.init.trunc_normal_(\r\n            self.output.weight,\r\n            mean=0.0,\r\n            std=final_out_std,\r\n            a=-cutoff_factor * final_out_std,\r\n            b=cutoff_factor * final_out_std,\r\n        )", "language": "python", "code": "def init_weights(self):\r\n        \"\"\"[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.\r\n\r\n        \"\"\"\r\n        with torch.device(self.freqs_cis.device):\r\n            self.freqs_cis = self._precompute_freqs_cis()\r\n        nn.init.normal_(self.tok_embeddings.weight)\r\n        for layer in self.layers.values():\r\n            layer.init_weights()\r\n        self.norm.reset_parameters()\r\n        final_out_std = self.model_args.dim**-0.5\r\n        cutoff_factor = 3\r\n        nn.init.trunc_normal_(\r\n            self.output.weight,\r\n            mean=0.0,\r\n            std=final_out_std,\r\n            a=-cutoff_factor * final_out_std,\r\n            b=cutoff_factor * final_out_std,\r\n        )", "code_tokens": ["def", "init_weights", "(", "self", ")", ":", "\"", "\"", "\"", "[", "Note", ":", "On", "`", "`", "init_weights", "`", "`", "vs", ".", "`", "`", "reset_parameters", "`", "`", "]", "Modules", "may", "define", "`", "`", "reset_parameters", "`", "`", "to", "initialize", "parameter", "values", ".", "`", "`", "reset_parameters", "`", "`", "is", "meant", "to", "only", "initialize", "directly", "owned", "parameters", "/", "buffers", ",", "not", "those", "of", "their", "child", "modules", ",", "and", "it", "can", "be", "used", "to", "give", "the", "initial", "values", "for", "these", "tensors", ".", "Separately", ",", "users", "may", "want", "custom", "initialization", "for", "their", "modules", ",", "different", "from", "that", "in", "`", "`", "reset_parameters", "`", "`", ".", "For", "this", ",", "we", "define", "`", "`", "init_weights", "`", "`", ".", "We", "only", "call", "it", "in", "the", "constructor", "of", "this", "`", "`", "Transformer", "`", "`", "root", "module", "to", "avoid", "reinitializing", "tensors", ".", "\"", "\"", "\"", "with", "torch", ".", "device", "(", "self", ".", "freqs_cis", ".", "device", ")", ":", "self", ".", "freqs_cis", "=", "self", ".", "_precompute_freqs_cis", "(", ")", "nn", ".", "init", ".", "normal_", "(", "self", ".", "tok_embeddings", ".", "weight", ")", "for", "layer", "in", "self", ".", "layers", ".", "values", "(", ")", ":", "layer", ".", "init_weights", "(", ")", "self", ".", "norm", ".", "reset_parameters", "(", ")", "final_out_std", "=", "self", ".", "model_args", ".", "dim", "*", "*", "-", "0", ".", "5", "cutoff_factor", "=", "3", "nn", ".", "init", ".", "trunc_normal_", "(", "self", ".", "output", ".", "weight", ",", "mean", "=", "0", ".", "0", ",", "std", "=", "final_out_std", ",", "a", "=", "-", "cutoff_factor", "*", "final_out_std", ",", "b", "=", "cutoff_factor", "*", "final_out_std", ",", ")"], "docstring": "[Note: On ``init_weights`` vs.\r\n\r\n        ``reset_parameters``]\r\n        Modules may define ``reset_parameters`` to initialize parameter values.\r\n        ``reset_parameters`` is meant to only initialize directly owned\r\n        parameters/buffers, not those of their child modules, and it can be\r\n        used to give the initial values for these tensors.\r\n        Separately, users may want custom initialization for their modules,\r\n        different from that in ``reset_parameters``. For this, we define\r\n        ``init_weights``. We only call it in the constructor of this\r\n        ``Transformer`` root module to avoid reinitializing tensors.", "docstring_tokens": ["note", "on", "init_weights", "vs", "reset_parameters", "modules", "may", "define", "reset_parameters", "to", "initialize", "parameter", "values", "reset_parameters", "is", "meant", "to", "only", "initialize", "directly", "owned", "parameters", "buffers", "not", "those", "of", "their", "child", "modules", "and", "it", "can", "be", "used", "to", "give", "the", "initial", "values", "for", "these", "tensors", "separately", "users", "may", "want", "custom", "initialization", "for", "their", "modules", "different", "from", "that", "in", "reset_parameters", "for", "this", "we", "define", "init_weights", "we", "only", "call", "it", "in", "the", "constructor", "of", "this", "transformer", "root", "module", "to", "avoid", "reinitializing", "tensors"], "docstring_summary": "[Note: On ``init_weights`` vs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 386, "end_line": 414, "hash": "94da5e7255f0ac07771c9b7856d1f3af", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "forward", "original_string": "def forward(self, tokens: torch.Tensor):\r\n        \"\"\"Perform a forward pass through the Transformer model.\r\n\r\n        Args:\r\n            tokens (torch.Tensor): Input token indices.\r\n\r\n        Returns:\r\n            torch.Tensor: Output logits after applying the Transformer model.\r\n\r\n        \"\"\"\r\n        # passthrough for nonexistent layers, allows easy configuration of pipeline parallel stages\r\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\r\n\r\n        for layer in self.layers.values():\r\n            h = layer(h, self.freqs_cis)\r\n\r\n        h = self.norm(h) if self.norm else h\r\n        return self.output(h).float() if self.output else h", "language": "python", "code": "def forward(self, tokens: torch.Tensor):\r\n        \"\"\"Perform a forward pass through the Transformer model.\r\n\r\n        Args:\r\n            tokens (torch.Tensor): Input token indices.\r\n\r\n        Returns:\r\n            torch.Tensor: Output logits after applying the Transformer model.\r\n\r\n        \"\"\"\r\n        # passthrough for nonexistent layers, allows easy configuration of pipeline parallel stages\r\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\r\n\r\n        for layer in self.layers.values():\r\n            h = layer(h, self.freqs_cis)\r\n\r\n        h = self.norm(h) if self.norm else h\r\n        return self.output(h).float() if self.output else h", "code_tokens": ["def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ")", ":", "\"", "\"", "\"", "Perform", "a", "forward", "pass", "through", "the", "Transformer", "model", ".", "Args", ":", "tokens", "(", "torch", ".", "Tensor", ")", ":", "Input", "token", "indices", ".", "Returns", ":", "torch", ".", "Tensor", ":", "Output", "logits", "after", "applying", "the", "Transformer", "model", ".", "\"", "\"", "\"", "h", "=", "self", ".", "tok_embeddings", "(", "tokens", ")", "if", "self", ".", "tok_embeddings", "else", "tokens", "for", "layer", "in", "self", ".", "layers", ".", "values", "(", ")", ":", "h", "=", "layer", "(", "h", ",", "self", ".", "freqs_cis", ")", "h", "=", "self", ".", "norm", "(", "h", ")", "if", "self", ".", "norm", "else", "h", "return", "self", ".", "output", "(", "h", ")", ".", "float", "(", ")", "if", "self", ".", "output", "else", "h"], "docstring": "Perform a forward pass through the Transformer model.\r\n\r\n        Args:\r\n            tokens (torch.Tensor): Input token indices.\r\n\r\n        Returns:\r\n            torch.Tensor: Output logits after applying the Transformer model.", "docstring_tokens": ["perform", "a", "forward", "pass", "through", "the", "transformer", "model", "args", "tokens", "torch", "tensor", "input", "token", "indices", "returns", "torch", "tensor", "output", "logits", "after", "applying", "the", "transformer", "model"], "docstring_summary": "Perform a forward pass through the Transformer model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 425, "end_line": 442, "hash": "abfbf414d576b9e3077c4a3f0899266a", "complexity": 5, "parameters": ["tokens"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\model.py", "func_name": "from_model_args", "original_string": "def from_model_args(cls, model_args: ModelArgs) -> \"Transformer\":\r\n        \"\"\"Initialize a Transformer model from a ModelArgs object.\r\n\r\n        Args:\r\n            model_args (ModelArgs): Model configuration arguments.\r\n\r\n        Returns:\r\n            Transformer: Transformer model.\r\n\r\n        \"\"\"\r\n        return cls(model_args)", "language": "python", "code": "def from_model_args(cls, model_args: ModelArgs) -> \"Transformer\":\r\n        \"\"\"Initialize a Transformer model from a ModelArgs object.\r\n\r\n        Args:\r\n            model_args (ModelArgs): Model configuration arguments.\r\n\r\n        Returns:\r\n            Transformer: Transformer model.\r\n\r\n        \"\"\"\r\n        return cls(model_args)", "code_tokens": ["def", "from_model_args", "(", "cls", ",", "model_args", ":", "ModelArgs", ")", "-", ">", "\"", "Transformer", "\"", ":", "\"", "\"", "\"", "Initialize", "a", "Transformer", "model", "from", "a", "ModelArgs", "object", ".", "Args", ":", "model_args", "(", "ModelArgs", ")", ":", "Model", "configuration", "arguments", ".", "Returns", ":", "Transformer", ":", "Transformer", "model", ".", "\"", "\"", "\"", "return", "cls", "(", "model_args", ")"], "docstring": "Initialize a Transformer model from a ModelArgs object.\r\n\r\n        Args:\r\n            model_args (ModelArgs): Model configuration arguments.\r\n\r\n        Returns:\r\n            Transformer: Transformer model.", "docstring_tokens": ["initialize", "a", "transformer", "model", "from", "a", "modelargs", "object", "args", "model_args", "modelargs", "model", "configuration", "arguments", "returns", "transformer", "transformer", "model"], "docstring_summary": "Initialize a Transformer model from a ModelArgs object.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\model.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 445, "end_line": 455, "hash": "1bb7c9a9a78cd29ef878e007645a2f49", "complexity": 1, "parameters": ["model_args"]}
{"repo": "pytorch-lightning", "path": "examples\\pytorch\\tensor_parallel\\parallelism.py", "func_name": "parallelize", "original_string": "def parallelize(model: Transformer, device_mesh: DeviceMesh) -> Transformer:\r\n    \"\"\"Apply parallelisms and activation checkpointing to the model.\r\n\r\n    NOTE: The passed-in model preferably should be on meta device. Otherwise,\r\n    the model must fit on GPU or CPU memory.\r\n\r\n    \"\"\"\r\n\r\n    dp_mesh = device_mesh[\"data_parallel\"]\r\n    tp_mesh = device_mesh[\"tensor_parallel\"]\r\n\r\n    if tp_mesh.size() > 1:\r\n        # 1. Parallelize the first embedding and the last linear proj layer\r\n        # 2. Parallelize the root norm layer over the sequence dim\r\n        # 3. Shard the first transformer block's inputs\r\n\r\n        # Parallelize the first embedding and the last linear out projection\r\n        plan = {\r\n            \"tok_embeddings\": RowwiseParallel(input_layouts=Replicate()),\r\n            \"output\": ColwiseParallel(\r\n                input_layouts=Shard(1),\r\n                # Optional: Shard the output along the class dimension to compute the loss in parallel.\r\n                # See `loss_parallel` in `train.py`\r\n                output_layouts=Shard(-1),\r\n                use_local_output=False,\r\n            ),\r\n            \"norm\": SequenceParallel(),\r\n            \"layers.0\": PrepareModuleInput(\r\n                input_layouts=(Replicate(), None),\r\n                desired_input_layouts=(Shard(1), None),\r\n                use_local_output=True,\r\n            ),\r\n        }\r\n        model = parallelize_module(model, tp_mesh, plan)\r\n\r\n        # Parallelize each transformer block\r\n        for transformer_block in model.layers.values():\r\n            plan = {\r\n                \"attention\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1), None),\r\n                    desired_input_layouts=(Replicate(), None),\r\n                ),\r\n                \"attention.wq\": ColwiseParallel(),\r\n                \"attention.wk\": ColwiseParallel(),\r\n                \"attention.wv\": ColwiseParallel(),\r\n                \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"attention_norm\": SequenceParallel(),\r\n                \"feed_forward\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1),),\r\n                    desired_input_layouts=(Replicate(),),\r\n                ),\r\n                \"feed_forward.w1\": ColwiseParallel(),\r\n                \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"feed_forward.w3\": ColwiseParallel(),\r\n                \"ffn_norm\": SequenceParallel(),\r\n            }\r\n\r\n            # Adjust attention module to use the local number of heads\r\n            attn_layer = transformer_block.attention\r\n            attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size()\r\n            attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size()\r\n\r\n            # Apply the plan for the current transformer block\r\n            parallelize_module(transformer_block, tp_mesh, plan)\r\n\r\n    if dp_mesh.size() > 1:\r\n        assert dp_mesh.ndim == 1  # Hybrid-sharding not supported\r\n\r\n        # NOTE: Currently, the user is required to manually handle precision settings such as the `mp_policy` here\r\n        # because the model parallel strategy does not respect all settings of `Fabric(precision=...)` at the moment.\r\n        mp_policy = MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32)\r\n\r\n        fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\r\n        for layer_id, transformer_block in model.layers.items():\r\n            # Apply activation checkpointing\r\n            transformer_block = checkpoint_wrapper(transformer_block)\r\n            # As an optimization, do not reshard after forward for the last\r\n            # transformer block since FSDP would prefetch it immediately\r\n            reshard_after_forward = int(layer_id) < len(model.layers) - 1\r\n            fully_shard(\r\n                transformer_block,\r\n                **fsdp_config,\r\n                reshard_after_forward=reshard_after_forward,\r\n            )\r\n            model.layers[layer_id] = transformer_block\r\n        model = fully_shard(model, **fsdp_config)\r\n\r\n    return model", "language": "python", "code": "def parallelize(model: Transformer, device_mesh: DeviceMesh) -> Transformer:\r\n    \"\"\"Apply parallelisms and activation checkpointing to the model.\r\n\r\n    NOTE: The passed-in model preferably should be on meta device. Otherwise,\r\n    the model must fit on GPU or CPU memory.\r\n\r\n    \"\"\"\r\n\r\n    dp_mesh = device_mesh[\"data_parallel\"]\r\n    tp_mesh = device_mesh[\"tensor_parallel\"]\r\n\r\n    if tp_mesh.size() > 1:\r\n        # 1. Parallelize the first embedding and the last linear proj layer\r\n        # 2. Parallelize the root norm layer over the sequence dim\r\n        # 3. Shard the first transformer block's inputs\r\n\r\n        # Parallelize the first embedding and the last linear out projection\r\n        plan = {\r\n            \"tok_embeddings\": RowwiseParallel(input_layouts=Replicate()),\r\n            \"output\": ColwiseParallel(\r\n                input_layouts=Shard(1),\r\n                # Optional: Shard the output along the class dimension to compute the loss in parallel.\r\n                # See `loss_parallel` in `train.py`\r\n                output_layouts=Shard(-1),\r\n                use_local_output=False,\r\n            ),\r\n            \"norm\": SequenceParallel(),\r\n            \"layers.0\": PrepareModuleInput(\r\n                input_layouts=(Replicate(), None),\r\n                desired_input_layouts=(Shard(1), None),\r\n                use_local_output=True,\r\n            ),\r\n        }\r\n        model = parallelize_module(model, tp_mesh, plan)\r\n\r\n        # Parallelize each transformer block\r\n        for transformer_block in model.layers.values():\r\n            plan = {\r\n                \"attention\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1), None),\r\n                    desired_input_layouts=(Replicate(), None),\r\n                ),\r\n                \"attention.wq\": ColwiseParallel(),\r\n                \"attention.wk\": ColwiseParallel(),\r\n                \"attention.wv\": ColwiseParallel(),\r\n                \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"attention_norm\": SequenceParallel(),\r\n                \"feed_forward\": PrepareModuleInput(\r\n                    input_layouts=(Shard(1),),\r\n                    desired_input_layouts=(Replicate(),),\r\n                ),\r\n                \"feed_forward.w1\": ColwiseParallel(),\r\n                \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\r\n                \"feed_forward.w3\": ColwiseParallel(),\r\n                \"ffn_norm\": SequenceParallel(),\r\n            }\r\n\r\n            # Adjust attention module to use the local number of heads\r\n            attn_layer = transformer_block.attention\r\n            attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size()\r\n            attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size()\r\n\r\n            # Apply the plan for the current transformer block\r\n            parallelize_module(transformer_block, tp_mesh, plan)\r\n\r\n    if dp_mesh.size() > 1:\r\n        assert dp_mesh.ndim == 1  # Hybrid-sharding not supported\r\n\r\n        # NOTE: Currently, the user is required to manually handle precision settings such as the `mp_policy` here\r\n        # because the model parallel strategy does not respect all settings of `Fabric(precision=...)` at the moment.\r\n        mp_policy = MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32)\r\n\r\n        fsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\r\n        for layer_id, transformer_block in model.layers.items():\r\n            # Apply activation checkpointing\r\n            transformer_block = checkpoint_wrapper(transformer_block)\r\n            # As an optimization, do not reshard after forward for the last\r\n            # transformer block since FSDP would prefetch it immediately\r\n            reshard_after_forward = int(layer_id) < len(model.layers) - 1\r\n            fully_shard(\r\n                transformer_block,\r\n                **fsdp_config,\r\n                reshard_after_forward=reshard_after_forward,\r\n            )\r\n            model.layers[layer_id] = transformer_block\r\n        model = fully_shard(model, **fsdp_config)\r\n\r\n    return model", "code_tokens": ["def", "parallelize", "(", "model", ":", "Transformer", ",", "device_mesh", ":", "DeviceMesh", ")", "-", ">", "Transformer", ":", "\"", "\"", "\"", "Apply", "parallelisms", "and", "activation", "checkpointing", "to", "the", "model", ".", "NOTE", ":", "The", "passed", "-", "in", "model", "preferably", "should", "be", "on", "meta", "device", ".", "Otherwise", ",", "the", "model", "must", "fit", "on", "GPU", "or", "CPU", "memory", ".", "\"", "\"", "\"", "dp_mesh", "=", "device_mesh", "[", "\"", "data_parallel", "\"", "]", "tp_mesh", "=", "device_mesh", "[", "\"", "tensor_parallel", "\"", "]", "if", "tp_mesh", ".", "size", "(", ")", ">", "1", ":", "plan", "=", "{", "\"", "tok_embeddings", "\"", ":", "RowwiseParallel", "(", "input_layouts", "=", "Replicate", "(", ")", ")", ",", "\"", "output", "\"", ":", "ColwiseParallel", "(", "input_layouts", "=", "Shard", "(", "1", ")", ",", "output_layouts", "=", "Shard", "(", "-", "1", ")", ",", "use_local_output", "=", "False", ",", ")", ",", "\"", "norm", "\"", ":", "SequenceParallel", "(", ")", ",", "\"", "layers", ".", "0", "\"", ":", "PrepareModuleInput", "(", "input_layouts", "=", "(", "Replicate", "(", ")", ",", "None", ")", ",", "desired_input_layouts", "=", "(", "Shard", "(", "1", ")", ",", "None", ")", ",", "use_local_output", "=", "True", ",", ")", ",", "}", "model", "=", "parallelize_module", "(", "model", ",", "tp_mesh", ",", "plan", ")", "for", "transformer_block", "in", "model", ".", "layers", ".", "values", "(", ")", ":", "plan", "=", "{", "\"", "attention", "\"", ":", "PrepareModuleInput", "(", "input_layouts", "=", "(", "Shard", "(", "1", ")", ",", "None", ")", ",", "desired_input_layouts", "=", "(", "Replicate", "(", ")", ",", "None", ")", ",", ")", ",", "\"", "attention", ".", "wq", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "attention", ".", "wk", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "attention", ".", "wv", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "attention", ".", "wo", "\"", ":", "RowwiseParallel", "(", "output_layouts", "=", "Shard", "(", "1", ")", ")", ",", "\"", "attention_norm", "\"", ":", "SequenceParallel", "(", ")", ",", "\"", "feed_forward", "\"", ":", "PrepareModuleInput", "(", "input_layouts", "=", "(", "Shard", "(", "1", ")", ",", ")", ",", "desired_input_layouts", "=", "(", "Replicate", "(", ")", ",", ")", ",", ")", ",", "\"", "feed_forward", ".", "w1", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "feed_forward", ".", "w2", "\"", ":", "RowwiseParallel", "(", "output_layouts", "=", "Shard", "(", "1", ")", ")", ",", "\"", "feed_forward", ".", "w3", "\"", ":", "ColwiseParallel", "(", ")", ",", "\"", "ffn_norm", "\"", ":", "SequenceParallel", "(", ")", ",", "}", "attn_layer", "=", "transformer_block", ".", "attention", "attn_layer", ".", "n_heads", "=", "attn_layer", ".", "n_heads", "/", "/", "tp_mesh", ".", "size", "(", ")", "attn_layer", ".", "n_kv_heads", "=", "attn_layer", ".", "n_kv_heads", "/", "/", "tp_mesh", ".", "size", "(", ")", "parallelize_module", "(", "transformer_block", ",", "tp_mesh", ",", "plan", ")", "if", "dp_mesh", ".", "size", "(", ")", ">", "1", ":", "assert", "dp_mesh", ".", "ndim", "=", "=", "1", "mp_policy", "=", "MixedPrecisionPolicy", "(", "param_dtype", "=", "torch", ".", "bfloat16", ",", "reduce_dtype", "=", "torch", ".", "float32", ")", "fsdp_config", "=", "{", "\"", "mesh", "\"", ":", "dp_mesh", ",", "\"", "mp_policy", "\"", ":", "mp_policy", "}", "for", "layer_id", ",", "transformer_block", "in", "model", ".", "layers", ".", "items", "(", ")", ":", "transformer_block", "=", "checkpoint_wrapper", "(", "transformer_block", ")", "reshard_after_forward", "=", "int", "(", "layer_id", ")", "<", "len", "(", "model", ".", "layers", ")", "-", "1", "fully_shard", "(", "transformer_block", ",", "*", "*", "fsdp_config", ",", "reshard_after_forward", "=", "reshard_after_forward", ",", ")", "model", ".", "layers", "[", "layer_id", "]", "=", "transformer_block", "model", "=", "fully_shard", "(", "model", ",", "*", "*", "fsdp_config", ")", "return", "model"], "docstring": "Apply parallelisms and activation checkpointing to the model.\r\n\r\n    NOTE: The passed-in model preferably should be on meta device. Otherwise,\r\n    the model must fit on GPU or CPU memory.", "docstring_tokens": ["apply", "parallelisms", "and", "activation", "checkpointing", "to", "the", "model", "note", "the", "passed", "in", "model", "preferably", "should", "be", "on", "meta", "device", "otherwise", "the", "model", "must", "fit", "on", "gpu", "or", "cpu", "memory"], "docstring_summary": "Apply parallelisms and activation checkpointing to the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/examples\\pytorch\\tensor_parallel\\parallelism.py", "partition": "train", "function_type": "function", "start_line": 18, "end_line": 105, "hash": "6684c41993c9851f3b02bf6aacb86948", "complexity": 5, "parameters": ["model", "device_mesh"]}
{"repo": "pytorch-lightning", "path": "requirements\\collect_env_details.py", "func_name": "info_packages", "original_string": "def info_packages() -> dict:\r\n    \"\"\"Get name and version of all installed packages.\"\"\"\r\n    packages = {}\r\n    for dist in pkg_resources.working_set:\r\n        package = dist.as_requirement()\r\n        packages[package.key] = package.specs[0][1]\r\n    return packages", "language": "python", "code": "def info_packages() -> dict:\r\n    \"\"\"Get name and version of all installed packages.\"\"\"\r\n    packages = {}\r\n    for dist in pkg_resources.working_set:\r\n        package = dist.as_requirement()\r\n        packages[package.key] = package.specs[0][1]\r\n    return packages", "code_tokens": ["def", "info_packages", "(", ")", "-", ">", "dict", ":", "\"", "\"", "\"", "Get", "name", "and", "version", "of", "all", "installed", "packages", ".", "\"", "\"", "\"", "packages", "=", "{", "}", "for", "dist", "in", "pkg_resources", ".", "working_set", ":", "package", "=", "dist", ".", "as_requirement", "(", ")", "packages", "[", "package", ".", "key", "]", "=", "package", ".", "specs", "[", "0", "]", "[", "1", "]", "return", "packages"], "docstring": "Get name and version of all installed packages.", "docstring_tokens": ["get", "name", "and", "version", "of", "all", "installed", "packages"], "docstring_summary": "Get name and version of all installed packages.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/requirements\\collect_env_details.py", "partition": "train", "function_type": "function", "start_line": 52, "end_line": 58, "hash": "0f635f078a7f15cc7c543f79e10e0ea5", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "_get_supported_strategies", "original_string": "def _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]", "language": "python", "code": "def _get_supported_strategies() -> list[str]:\r\n    \"\"\"Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.\"\"\"\r\n    available_strategies = STRATEGY_REGISTRY.available_strategies()\r\n    excluded = r\".*(spawn|fork|notebook|xla|tpu|offload).*\"\r\n    return [strategy for strategy in available_strategies if not re.match(excluded, strategy)]", "code_tokens": ["def", "_get_supported_strategies", "(", ")", "-", ">", "list", "[", "str", "]", ":", "\"", "\"", "\"", "Returns", "strategy", "choices", "from", "the", "registry", ",", "with", "the", "ones", "removed", "that", "are", "incompatible", "to", "be", "launched", "from", "the", "CLI", "or", "ones", "that", "require", "further", "configuration", "by", "the", "user", ".", "\"", "\"", "\"", "available_strategies", "=", "STRATEGY_REGISTRY", ".", "available_strategies", "(", ")", "excluded", "=", "r", "\"", ".", "*", "(", "spawn", "|", "fork", "|", "notebook", "|", "xla", "|", "tpu", "|", "offload", ")", ".", "*", "\"", "return", "[", "strategy", "for", "strategy", "in", "available_strategies", "if", "not", "re", ".", "match", "(", "excluded", ",", "strategy", ")", "]"], "docstring": "Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the\r\n    CLI or ones that require further configuration by the user.", "docstring_tokens": ["returns", "strategy", "choices", "from", "the", "registry", "with", "the", "ones", "removed", "that", "are", "incompatible", "to", "be", "launched", "from", "the", "cli", "or", "ones", "that", "require", "further", "configuration", "by", "the", "user"], "docstring_summary": "Returns strategy choices from the registry, with the ones removed that are incompatible to be launched from the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\cli.py", "partition": "train", "function_type": "function", "start_line": 39, "end_line": 44, "hash": "046e4042f9fb91a6de62ae9b37347f2d", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "_run", "original_string": "def _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)", "language": "python", "code": "def _run(**kwargs: Any) -> None:\r\n        \"\"\"Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.\r\n\r\n        \"\"\"\r\n        script_args = list(kwargs.pop(\"script_args\", []))\r\n        main(args=Namespace(**kwargs), script_args=script_args)", "code_tokens": ["def", "_run", "(", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Run", "a", "Lightning", "Fabric", "script", ".", "SCRIPT", "is", "the", "path", "to", "the", "Python", "script", "with", "the", "code", "to", "run", ".", "The", "script", "must", "contain", "a", "Fabric", "object", ".", "SCRIPT_ARGS", "are", "the", "remaining", "arguments", "that", "you", "can", "pass", "to", "the", "script", "itself", "and", "are", "expected", "to", "be", "parsed", "there", ".", "\"", "\"", "\"", "script_args", "=", "list", "(", "kwargs", ".", "pop", "(", "\"", "script_args", "\"", ",", "[", "]", ")", ")", "main", "(", "args", "=", "Namespace", "(", "*", "*", "kwargs", ")", ",", "script_args", "=", "script_args", ")"], "docstring": "Run a Lightning Fabric script.\r\n\r\n        SCRIPT is the path to the Python script with the code to run. The script must contain a Fabric object.\r\n\r\n        SCRIPT_ARGS are the remaining arguments that you can pass to the script itself and are expected to be parsed\r\n        there.", "docstring_tokens": ["run", "a", "lightning", "fabric", "script", "script", "is", "the", "path", "to", "the", "python", "script", "with", "the", "code", "to", "run", "the", "script", "must", "contain", "a", "fabric", "object", "script_args", "are", "the", "remaining", "arguments", "that", "you", "can", "pass", "to", "the", "script", "itself", "and", "are", "expected", "to", "be", "parsed", "there"], "docstring_summary": "Run a Lightning Fabric script.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\cli.py", "partition": "train", "function_type": "function", "start_line": 126, "end_line": 136, "hash": "5916701a855ff88a774745f8ca336a9a", "complexity": 1, "parameters": ["**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "_consolidate", "original_string": "def _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)", "language": "python", "code": "def _consolidate(checkpoint_folder: str, output_file: Optional[str]) -> None:\r\n        \"\"\"Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.\r\n\r\n        \"\"\"\r\n        args = Namespace(checkpoint_folder=checkpoint_folder, output_file=output_file)\r\n        config = _process_cli_args(args)\r\n        checkpoint = _load_distributed_checkpoint(config.checkpoint_folder)\r\n        torch.save(checkpoint, config.output_file)", "code_tokens": ["def", "_consolidate", "(", "checkpoint_folder", ":", "str", ",", "output_file", ":", "Optional", "[", "str", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Convert", "a", "distributed", "/", "sharded", "checkpoint", "into", "a", "single", "file", "that", "can", "be", "loaded", "with", "`", "torch", ".", "load", "(", ")", "`", ".", "Only", "supports", "FSDP", "sharded", "checkpoints", "at", "the", "moment", ".", "\"", "\"", "\"", "args", "=", "Namespace", "(", "checkpoint_folder", "=", "checkpoint_folder", ",", "output_file", "=", "output_file", ")", "config", "=", "_process_cli_args", "(", "args", ")", "checkpoint", "=", "_load_distributed_checkpoint", "(", "config", ".", "checkpoint_folder", ")", "torch", ".", "save", "(", "checkpoint", ",", "config", ".", "output_file", ")"], "docstring": "Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.\r\n\r\n        Only supports FSDP sharded checkpoints at the moment.", "docstring_tokens": ["convert", "a", "distributed", "sharded", "checkpoint", "into", "a", "single", "file", "that", "can", "be", "loaded", "with", "torch", "load", "only", "supports", "fsdp", "sharded", "checkpoints", "at", "the", "moment"], "docstring_summary": "Convert a distributed/sharded checkpoint into a single file that can be loaded with `torch.load()`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\cli.py", "partition": "train", "function_type": "function", "start_line": 158, "end_line": 167, "hash": "7fe837b00b7d3ad820c46965a3186c7c", "complexity": 1, "parameters": ["checkpoint_folder", "output_file"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "_set_env_variables", "original_string": "def _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)", "language": "python", "code": "def _set_env_variables(args: Namespace) -> None:\r\n    \"\"\"Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.\r\n\r\n    \"\"\"\r\n    os.environ[\"LT_CLI_USED\"] = \"1\"\r\n    if args.accelerator is not None:\r\n        os.environ[\"LT_ACCELERATOR\"] = str(args.accelerator)\r\n    if args.strategy is not None:\r\n        os.environ[\"LT_STRATEGY\"] = str(args.strategy)\r\n    os.environ[\"LT_DEVICES\"] = str(args.devices)\r\n    os.environ[\"LT_NUM_NODES\"] = str(args.num_nodes)\r\n    if args.precision is not None:\r\n        os.environ[\"LT_PRECISION\"] = str(args.precision)", "code_tokens": ["def", "_set_env_variables", "(", "args", ":", "Namespace", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Set", "the", "environment", "variables", "for", "the", "new", "processes", ".", "The", "Fabric", "connector", "will", "parse", "the", "arguments", "set", "here", ".", "\"", "\"", "\"", "os", ".", "environ", "[", "\"", "LT_CLI_USED", "\"", "]", "=", "\"", "1", "\"", "if", "args", ".", "accelerator", "is", "not", "None", ":", "os", ".", "environ", "[", "\"", "LT_ACCELERATOR", "\"", "]", "=", "str", "(", "args", ".", "accelerator", ")", "if", "args", ".", "strategy", "is", "not", "None", ":", "os", ".", "environ", "[", "\"", "LT_STRATEGY", "\"", "]", "=", "str", "(", "args", ".", "strategy", ")", "os", ".", "environ", "[", "\"", "LT_DEVICES", "\"", "]", "=", "str", "(", "args", ".", "devices", ")", "os", ".", "environ", "[", "\"", "LT_NUM_NODES", "\"", "]", "=", "str", "(", "args", ".", "num_nodes", ")", "if", "args", ".", "precision", "is", "not", "None", ":", "os", ".", "environ", "[", "\"", "LT_PRECISION", "\"", "]", "=", "str", "(", "args", ".", "precision", ")"], "docstring": "Set the environment variables for the new processes.\r\n\r\n    The Fabric connector will parse the arguments set here.", "docstring_tokens": ["set", "the", "environment", "variables", "for", "the", "new", "processes", "the", "fabric", "connector", "will", "parse", "the", "arguments", "set", "here"], "docstring_summary": "Set the environment variables for the new processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\cli.py", "partition": "train", "function_type": "function", "start_line": 170, "end_line": 184, "hash": "fe7ab9046a75c3e2254b9b9c2154b80b", "complexity": 4, "parameters": ["args"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "_get_num_processes", "original_string": "def _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0", "language": "python", "code": "def _get_num_processes(accelerator: str, devices: str) -> int:\r\n    \"\"\"Parse the `devices` argument to determine how many processes need to be launched on the current machine.\"\"\"\r\n\r\n    if accelerator == \"auto\" or accelerator is None:\r\n        accelerator = _select_auto_accelerator()\r\n    if devices == \"auto\":\r\n        if accelerator == \"cuda\" or accelerator == \"mps\" or accelerator == \"cpu\":\r\n            devices = \"1\"\r\n        else:\r\n            raise ValueError(f\"Cannot default to '1' device for accelerator='{accelerator}'\")\r\n    if accelerator == \"gpu\":\r\n        parsed_devices = _parse_gpu_ids(devices, include_cuda=True, include_mps=True)\r\n    elif accelerator == \"cuda\":\r\n        parsed_devices = CUDAAccelerator.parse_devices(devices)\r\n    elif accelerator == \"mps\":\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n    elif accelerator == \"tpu\":\r\n        raise ValueError(\"Launching processes for TPU through the CLI is not supported.\")\r\n    else:\r\n        return CPUAccelerator.parse_devices(devices)\r\n    return len(parsed_devices) if parsed_devices is not None else 0", "code_tokens": ["def", "_get_num_processes", "(", "accelerator", ":", "str", ",", "devices", ":", "str", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Parse", "the", "`", "devices", "`", "argument", "to", "determine", "how", "many", "processes", "need", "to", "be", "launched", "on", "the", "current", "machine", ".", "\"", "\"", "\"", "if", "accelerator", "=", "=", "\"", "auto", "\"", "or", "accelerator", "is", "None", ":", "accelerator", "=", "_select_auto_accelerator", "(", ")", "if", "devices", "=", "=", "\"", "auto", "\"", ":", "if", "accelerator", "=", "=", "\"", "cuda", "\"", "or", "accelerator", "=", "=", "\"", "mps", "\"", "or", "accelerator", "=", "=", "\"", "cpu", "\"", ":", "devices", "=", "\"", "1", "\"", "else", ":", "raise", "ValueError", "(", "f", "\"", "Cannot", "default", "to", "'", "1", "'", "device", "for", "accelerator", "=", "'", "{", "accelerator", "}", "'", "\"", ")", "if", "accelerator", "=", "=", "\"", "gpu", "\"", ":", "parsed_devices", "=", "_parse_gpu_ids", "(", "devices", ",", "include_cuda", "=", "True", ",", "include_mps", "=", "True", ")", "elif", "accelerator", "=", "=", "\"", "cuda", "\"", ":", "parsed_devices", "=", "CUDAAccelerator", ".", "parse_devices", "(", "devices", ")", "elif", "accelerator", "=", "=", "\"", "mps", "\"", ":", "parsed_devices", "=", "MPSAccelerator", ".", "parse_devices", "(", "devices", ")", "elif", "accelerator", "=", "=", "\"", "tpu", "\"", ":", "raise", "ValueError", "(", "\"", "Launching", "processes", "for", "TPU", "through", "the", "CLI", "is", "not", "supported", ".", "\"", ")", "else", ":", "return", "CPUAccelerator", ".", "parse_devices", "(", "devices", ")", "return", "len", "(", "parsed_devices", ")", "if", "parsed_devices", "is", "not", "None", "else", "0"], "docstring": "Parse the `devices` argument to determine how many processes need to be launched on the current machine.", "docstring_tokens": ["parse", "the", "devices", "argument", "to", "determine", "how", "many", "processes", "need", "to", "be", "launched", "on", "the", "current", "machine"], "docstring_summary": "Parse the `devices` argument to determine how many processes need to be launched on the current machine.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\cli.py", "partition": "train", "function_type": "function", "start_line": 187, "end_line": 207, "hash": "a862892a33c8f5b1bb8d72b22888e9a9", "complexity": 12, "parameters": ["accelerator", "devices"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\cli.py", "func_name": "_torchrun_launch", "original_string": "def _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    # set a good default number of threads for OMP to avoid warnings being emitted to the user\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)", "language": "python", "code": "def _torchrun_launch(args: Namespace, script_args: list[str]) -> None:\r\n    \"\"\"This will invoke `torchrun` programmatically to launch the given script in new processes.\"\"\"\r\n    import torch.distributed.run as torchrun\r\n\r\n    num_processes = 1 if args.strategy == \"dp\" else _get_num_processes(args.accelerator, args.devices)\r\n\r\n    torchrun_args = [\r\n        f\"--nproc_per_node={num_processes}\",\r\n        f\"--nnodes={args.num_nodes}\",\r\n        f\"--node_rank={args.node_rank}\",\r\n        f\"--master_addr={args.main_address}\",\r\n        f\"--master_port={args.main_port}\",\r\n        args.script,\r\n    ]\r\n    torchrun_args.extend(script_args)\r\n\r\n    # set a good default number of threads for OMP to avoid warnings being emitted to the user\r\n    os.environ.setdefault(\"OMP_NUM_THREADS\", str(_suggested_max_num_threads()))\r\n    torchrun.main(torchrun_args)", "code_tokens": ["def", "_torchrun_launch", "(", "args", ":", "Namespace", ",", "script_args", ":", "list", "[", "str", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "will", "invoke", "`", "torchrun", "`", "programmatically", "to", "launch", "the", "given", "script", "in", "new", "processes", ".", "\"", "\"", "\"", "import", "torch", ".", "distributed", ".", "run", "as", "torchrun", "num_processes", "=", "1", "if", "args", ".", "strategy", "=", "=", "\"", "dp", "\"", "else", "_get_num_processes", "(", "args", ".", "accelerator", ",", "args", ".", "devices", ")", "torchrun_args", "=", "[", "f", "\"", "-", "-", "nproc_per_node", "=", "{", "num_processes", "}", "\"", ",", "f", "\"", "-", "-", "nnodes", "=", "{", "args", ".", "num_nodes", "}", "\"", ",", "f", "\"", "-", "-", "node_rank", "=", "{", "args", ".", "node_rank", "}", "\"", ",", "f", "\"", "-", "-", "master_addr", "=", "{", "args", ".", "main_address", "}", "\"", ",", "f", "\"", "-", "-", "master_port", "=", "{", "args", ".", "main_port", "}", "\"", ",", "args", ".", "script", ",", "]", "torchrun_args", ".", "extend", "(", "script_args", ")", "os", ".", "environ", ".", "setdefault", "(", "\"", "OMP_NUM_THREADS", "\"", ",", "str", "(", "_suggested_max_num_threads", "(", ")", ")", ")", "torchrun", ".", "main", "(", "torchrun_args", ")"], "docstring": "This will invoke `torchrun` programmatically to launch the given script in new processes.", "docstring_tokens": ["this", "will", "invoke", "torchrun", "programmatically", "to", "launch", "the", "given", "script", "in", "new", "processes"], "docstring_summary": "This will invoke `torchrun` programmatically to launch the given script in new processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\cli.py", "partition": "train", "function_type": "function", "start_line": 210, "end_line": 228, "hash": "33ff5e1d442d0b52dc83e76abb67d410", "complexity": 2, "parameters": ["args", "script_args"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "_choose_auto_accelerator", "original_string": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"", "language": "python", "code": "def _choose_auto_accelerator() -> str:\r\n        \"\"\"Choose the accelerator type (str) based on availability when ``accelerator='auto'``.\"\"\"\r\n        if XLAAccelerator.is_available():\r\n            return \"tpu\"\r\n        if MPSAccelerator.is_available():\r\n            return \"mps\"\r\n        if CUDAAccelerator.is_available():\r\n            return \"cuda\"\r\n        return \"cpu\"", "code_tokens": ["def", "_choose_auto_accelerator", "(", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Choose", "the", "accelerator", "type", "(", "str", ")", "based", "on", "availability", "when", "`", "`", "accelerator", "=", "'", "auto", "'", "`", "`", ".", "\"", "\"", "\"", "if", "XLAAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "tpu", "\"", "if", "MPSAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "mps", "\"", "if", "CUDAAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "cuda", "\"", "return", "\"", "cpu", "\""], "docstring": "Choose the accelerator type (str) based on availability when ``accelerator='auto'``.", "docstring_tokens": ["choose", "the", "accelerator", "type", "str", "based", "on", "availability", "when", "accelerator", "auto"], "docstring_summary": "Choose the accelerator type (str) based on availability when ``accelerator='auto'``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\connector.py", "partition": "train", "function_type": "class_method", "class_name": "_Connector", "start_line": 316, "end_line": 324, "hash": "df04b5404c68bc50579911be49a7b2b5", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "_check_strategy_and_fallback", "original_string": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        # Change fsdp to xla_fsdp if using TPU\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "language": "python", "code": "def _check_strategy_and_fallback(self) -> None:\r\n        \"\"\"Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.\"\"\"\r\n        # current fallback and check logic only apply to user pass in str config and object config\r\n        # TODO this logic should apply to both str and object config\r\n        strategy_flag = \"\" if isinstance(self._strategy_flag, Strategy) else self._strategy_flag\r\n\r\n        # Change fsdp to xla_fsdp if using TPU\r\n        if strategy_flag == \"fsdp\" and self._accelerator_flag == \"tpu\":\r\n            strategy_flag = \"xla_fsdp\"\r\n        if strategy_flag == \"dp\" and self._accelerator_flag == \"cpu\":\r\n            rank_zero_warn(f\"{strategy_flag!r} is not supported on CPUs, hence setting `strategy='ddp'`.\")\r\n            strategy_flag = \"ddp\"\r\n        if strategy_flag in _DDP_FORK_ALIASES and \"fork\" not in torch.multiprocessing.get_all_start_methods():\r\n            raise ValueError(\r\n                f\"You selected `Fabric(strategy='{strategy_flag}')` but process forking is not supported on this\"\r\n                f\" platform. We recommend `Fabric(strategy='ddp_spawn')` instead.\"\r\n            )\r\n        if (\r\n            strategy_flag in _FSDP_ALIASES or type(self._strategy_flag) is FSDPStrategy\r\n        ) and self._accelerator_flag not in (\"cuda\", \"gpu\"):\r\n            raise ValueError(\r\n                \"You selected the FSDP strategy but FSDP is only available on GPU. Set `Fabric(accelerator='gpu', ...)`\"\r\n                \" to continue or select a different strategy.\"\r\n            )\r\n        if strategy_flag:\r\n            self._strategy_flag = strategy_flag", "code_tokens": ["def", "_check_strategy_and_fallback", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "edge", "cases", "when", "the", "strategy", "selection", "was", "a", "string", "input", ",", "and", "we", "need", "to", "fall", "back", "to", "a", "different", "choice", "depending", "on", "other", "parameters", "or", "the", "environment", ".", "\"", "\"", "\"", "strategy_flag", "=", "\"", "\"", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "Strategy", ")", "else", "self", ".", "_strategy_flag", "if", "strategy_flag", "=", "=", "\"", "fsdp", "\"", "and", "self", ".", "_accelerator_flag", "=", "=", "\"", "tpu", "\"", ":", "strategy_flag", "=", "\"", "xla_fsdp", "\"", "if", "strategy_flag", "=", "=", "\"", "dp", "\"", "and", "self", ".", "_accelerator_flag", "=", "=", "\"", "cpu", "\"", ":", "rank_zero_warn", "(", "f", "\"", "{", "strategy_flag", "!", "r", "}", "is", "not", "supported", "on", "CPUs", ",", "hence", "setting", "`", "strategy", "=", "'", "ddp", "'", "`", ".", "\"", ")", "strategy_flag", "=", "\"", "ddp", "\"", "if", "strategy_flag", "in", "_DDP_FORK_ALIASES", "and", "\"", "fork", "\"", "not", "in", "torch", ".", "multiprocessing", ".", "get_all_start_methods", "(", ")", ":", "raise", "ValueError", "(", "f", "\"", "You", "selected", "`", "Fabric", "(", "strategy", "=", "'", "{", "strategy_flag", "}", "'", ")", "`", "but", "process", "forking", "is", "not", "supported", "on", "this", "\"", "f", "\"", "platform", ".", "We", "recommend", "`", "Fabric", "(", "strategy", "=", "'", "ddp_spawn", "'", ")", "`", "instead", ".", "\"", ")", "if", "(", "strategy_flag", "in", "_FSDP_ALIASES", "or", "type", "(", "self", ".", "_strategy_flag", ")", "is", "FSDPStrategy", ")", "and", "self", ".", "_accelerator_flag", "not", "in", "(", "\"", "cuda", "\"", ",", "\"", "gpu", "\"", ")", ":", "raise", "ValueError", "(", "\"", "You", "selected", "the", "FSDP", "strategy", "but", "FSDP", "is", "only", "available", "on", "GPU", ".", "Set", "`", "Fabric", "(", "accelerator", "=", "'", "gpu", "'", ",", ".", ".", ".", ")", "`", "\"", "\"", "to", "continue", "or", "select", "a", "different", "strategy", ".", "\"", ")", "if", "strategy_flag", ":", "self", ".", "_strategy_flag", "=", "strategy_flag"], "docstring": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different\r\n        choice depending on other parameters or the environment.", "docstring_tokens": ["checks", "edge", "cases", "when", "the", "strategy", "selection", "was", "a", "string", "input", "and", "we", "need", "to", "fall", "back", "to", "a", "different", "choice", "depending", "on", "other", "parameters", "or", "the", "environment"], "docstring_summary": "Checks edge cases when the strategy selection was a string input, and we need to fall back to a different", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\connector.py", "partition": "train", "function_type": "class_method", "class_name": "_Connector", "start_line": 414, "end_line": 440, "hash": "199bc92fcc4ea61a3de6cedb0602bcb9", "complexity": 12, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "_init_strategy", "original_string": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "language": "python", "code": "def _init_strategy(self) -> None:\r\n        \"\"\"Instantiate the Strategy given depending on the setting of ``_strategy_flag``.\"\"\"\r\n        # The validation of `_strategy_flag` already happened earlier on in the connector\r\n        assert isinstance(self._strategy_flag, (str, Strategy))\r\n        if isinstance(self._strategy_flag, str):\r\n            self.strategy = STRATEGY_REGISTRY.get(self._strategy_flag)\r\n        else:\r\n            self.strategy = self._strategy_flag", "code_tokens": ["def", "_init_strategy", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Instantiate", "the", "Strategy", "given", "depending", "on", "the", "setting", "of", "`", "`", "_strategy_flag", "`", "`", ".", "\"", "\"", "\"", "assert", "isinstance", "(", "self", ".", "_strategy_flag", ",", "(", "str", ",", "Strategy", ")", ")", "if", "isinstance", "(", "self", ".", "_strategy_flag", ",", "str", ")", ":", "self", ".", "strategy", "=", "STRATEGY_REGISTRY", ".", "get", "(", "self", ".", "_strategy_flag", ")", "else", ":", "self", ".", "strategy", "=", "self", ".", "_strategy_flag"], "docstring": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.", "docstring_tokens": ["instantiate", "the", "strategy", "given", "depending", "on", "the", "setting", "of", "_strategy_flag"], "docstring_summary": "Instantiate the Strategy given depending on the setting of ``_strategy_flag``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\connector.py", "partition": "train", "function_type": "class_method", "class_name": "_Connector", "start_line": 442, "end_line": 449, "hash": "1b0593f8aef3dc934dfc968afb6537fb", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\connector.py", "func_name": "_lazy_init_strategy", "original_string": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        # TODO: should be moved to _check_strategy_and_fallback().\r\n        # Current test check precision first, so keep this check here to meet error order\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )", "language": "python", "code": "def _lazy_init_strategy(self) -> None:\r\n        \"\"\"Lazily set missing attributes on the previously instantiated strategy.\"\"\"\r\n        self.strategy.accelerator = self.accelerator\r\n        if self.precision:\r\n            self.strategy.precision = self.precision\r\n        if self.checkpoint_io:\r\n            self.strategy.checkpoint_io = self.checkpoint_io\r\n        if hasattr(self.strategy, \"cluster_environment\"):\r\n            if self.strategy.cluster_environment is None:\r\n                self.strategy.cluster_environment = self.cluster_environment\r\n            self.cluster_environment = self.strategy.cluster_environment\r\n        if hasattr(self.strategy, \"parallel_devices\"):\r\n            if self.strategy.parallel_devices:\r\n                self._parallel_devices = self.strategy.parallel_devices\r\n            else:\r\n                self.strategy.parallel_devices = self._parallel_devices\r\n        if hasattr(self.strategy, \"num_nodes\"):\r\n            self.strategy._num_nodes = self._num_nodes_flag\r\n        if hasattr(self.strategy, \"_set_world_ranks\"):\r\n            self.strategy._set_world_ranks()\r\n        self.strategy._configure_launcher()\r\n\r\n        if _IS_INTERACTIVE and self.strategy.launcher and not self.strategy.launcher.is_interactive_compatible:\r\n            raise RuntimeError(\r\n                f\"`Fabric(strategy={self._strategy_flag!r})` is not compatible with an interactive\"\r\n                \" environment. Run your code as a script, or choose one of the compatible strategies:\"\r\n                f\" `Fabric(strategy='dp'|'ddp_notebook')`.\"\r\n                \" In case you are spawning processes yourself, make sure to include the Fabric\"\r\n                \" creation inside the worker function.\"\r\n            )\r\n\r\n        # TODO: should be moved to _check_strategy_and_fallback().\r\n        # Current test check precision first, so keep this check here to meet error order\r\n        if isinstance(self.accelerator, XLAAccelerator) and not isinstance(\r\n            self.strategy, (SingleDeviceXLAStrategy, XLAStrategy, XLAFSDPStrategy)\r\n        ):\r\n            raise ValueError(\r\n                \"The `XLAAccelerator` can only be used with a `SingleDeviceXLAStrategy`, `XLAStrategy`, or\"\r\n                f\" `XLAFSDPStrategy`. Found {self.strategy.__class__.__name__}.\"\r\n            )", "code_tokens": ["def", "_lazy_init_strategy", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Lazily", "set", "missing", "attributes", "on", "the", "previously", "instantiated", "strategy", ".", "\"", "\"", "\"", "self", ".", "strategy", ".", "accelerator", "=", "self", ".", "accelerator", "if", "self", ".", "precision", ":", "self", ".", "strategy", ".", "precision", "=", "self", ".", "precision", "if", "self", ".", "checkpoint_io", ":", "self", ".", "strategy", ".", "checkpoint_io", "=", "self", ".", "checkpoint_io", "if", "hasattr", "(", "self", ".", "strategy", ",", "\"", "cluster_environment", "\"", ")", ":", "if", "self", ".", "strategy", ".", "cluster_environment", "is", "None", ":", "self", ".", "strategy", ".", "cluster_environment", "=", "self", ".", "cluster_environment", "self", ".", "cluster_environment", "=", "self", ".", "strategy", ".", "cluster_environment", "if", "hasattr", "(", "self", ".", "strategy", ",", "\"", "parallel_devices", "\"", ")", ":", "if", "self", ".", "strategy", ".", "parallel_devices", ":", "self", ".", "_parallel_devices", "=", "self", ".", "strategy", ".", "parallel_devices", "else", ":", "self", ".", "strategy", ".", "parallel_devices", "=", "self", ".", "_parallel_devices", "if", "hasattr", "(", "self", ".", "strategy", ",", "\"", "num_nodes", "\"", ")", ":", "self", ".", "strategy", ".", "_num_nodes", "=", "self", ".", "_num_nodes_flag", "if", "hasattr", "(", "self", ".", "strategy", ",", "\"", "_set_world_ranks", "\"", ")", ":", "self", ".", "strategy", ".", "_set_world_ranks", "(", ")", "self", ".", "strategy", ".", "_configure_launcher", "(", ")", "if", "_IS_INTERACTIVE", "and", "self", ".", "strategy", ".", "launcher", "and", "not", "self", ".", "strategy", ".", "launcher", ".", "is_interactive_compatible", ":", "raise", "RuntimeError", "(", "f", "\"", "`", "Fabric", "(", "strategy", "=", "{", "self", ".", "_strategy_flag", "!", "r", "}", ")", "`", "is", "not", "compatible", "with", "an", "interactive", "\"", "\"", "environment", ".", "Run", "your", "code", "as", "a", "script", ",", "or", "choose", "one", "of", "the", "compatible", "strategies", ":", "\"", "f", "\"", "`", "Fabric", "(", "strategy", "=", "'", "dp", "'", "|", "'", "ddp_notebook", "'", ")", "`", ".", "\"", "\"", "In", "case", "you", "are", "spawning", "processes", "yourself", ",", "make", "sure", "to", "include", "the", "Fabric", "\"", "\"", "creation", "inside", "the", "worker", "function", ".", "\"", ")", "if", "isinstance", "(", "self", ".", "accelerator", ",", "XLAAccelerator", ")", "and", "not", "isinstance", "(", "self", ".", "strategy", ",", "(", "SingleDeviceXLAStrategy", ",", "XLAStrategy", ",", "XLAFSDPStrategy", ")", ")", ":", "raise", "ValueError", "(", "\"", "The", "`", "XLAAccelerator", "`", "can", "only", "be", "used", "with", "a", "`", "SingleDeviceXLAStrategy", "`", ",", "`", "XLAStrategy", "`", ",", "or", "\"", "f", "\"", "`", "XLAFSDPStrategy", "`", ".", "Found", "{", "self", ".", "strategy", ".", "__class__", ".", "__name__", "}", ".", "\"", ")"], "docstring": "Lazily set missing attributes on the previously instantiated strategy.", "docstring_tokens": ["lazily", "set", "missing", "attributes", "on", "the", "previously", "instantiated", "strategy"], "docstring_summary": "Lazily set missing attributes on the previously instantiated strategy.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\connector.py", "partition": "train", "function_type": "class_method", "class_name": "_Connector", "start_line": 499, "end_line": 538, "hash": "9f2c382a413076f881097abd72310e26", "complexity": 14, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "device", "original_string": "def device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device", "language": "python", "code": "def device(self) -> torch.device:\r\n        \"\"\"The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.\r\n\r\n        \"\"\"\r\n        return self._strategy.root_device", "code_tokens": ["def", "device", "(", "self", ")", "-", ">", "torch", ".", "device", ":", "\"", "\"", "\"", "The", "current", "device", "this", "process", "runs", "on", ".", "Use", "this", "to", "create", "tensors", "directly", "on", "the", "device", "if", "needed", ".", "\"", "\"", "\"", "return", "self", ".", "_strategy", ".", "root_device"], "docstring": "The current device this process runs on.\r\n\r\n        Use this to create tensors directly on the device if needed.", "docstring_tokens": ["the", "current", "device", "this", "process", "runs", "on", "use", "this", "to", "create", "tensors", "directly", "on", "the", "device", "if", "needed"], "docstring_summary": "The current device this process runs on.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 178, "end_line": 184, "hash": "19387412d6e4bc043eb4c231ed79c819", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "run", "original_string": "def run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"", "language": "python", "code": "def run(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.\r\n\r\n        \"\"\"", "code_tokens": ["def", "run", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "All", "the", "code", "inside", "this", "run", "method", "gets", "accelerated", "by", "Fabric", ".", "You", "can", "pass", "arbitrary", "arguments", "to", "this", "function", "when", "overriding", "it", ".", "\"", "\"", "\""], "docstring": "All the code inside this run method gets accelerated by Fabric.\r\n\r\n        You can pass arbitrary arguments to this function when overriding it.", "docstring_tokens": ["all", "the", "code", "inside", "this", "run", "method", "gets", "accelerated", "by", "fabric", "you", "can", "pass", "arbitrary", "arguments", "to", "this", "function", "when", "overriding", "it"], "docstring_summary": "All the code inside this run method gets accelerated by Fabric.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 221, "end_line": 226, "hash": "1c67f6da3fce4497431f5b0cf2141d51", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "setup", "original_string": "def setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            # Basic usage\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            # With multiple optimizers and scheduler\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            # Model only\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        # Let accelerator/plugin wrap and connect the models and optimizers\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            # join both types in a tuple for API convenience\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module", "language": "python", "code": "def setup(\r\n        self,\r\n        module: nn.Module,\r\n        *optimizers: Optimizer,\r\n        scheduler: Optional[\"_LRScheduler\"] = None,\r\n        move_to_device: bool = True,\r\n        _reapply_compile: bool = True,\r\n    ) -> Any:  # no specific return because the way we want our API to look does not play well with mypy\r\n        r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            # Basic usage\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            # With multiple optimizers and scheduler\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            # Model only\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"\r\n        self._validate_setup(module, optimizers)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=list(optimizers))\r\n\r\n        # Let accelerator/plugin wrap and connect the models and optimizers\r\n        if optimizers:\r\n            module, optimizers, scheduler = self._strategy.setup_module_and_optimizers(  # type: ignore[assignment]\r\n                module, list(optimizers), scheduler\r\n            )\r\n        else:\r\n            module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        optimizers = [_FabricOptimizer(optimizer, self._strategy, self._callbacks) for optimizer in optimizers]\r\n\r\n        self._models_setup += 1\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            original_module._fabric_optimizers = optimizers\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self.call(\"on_after_setup\", fabric=self, module=module)\r\n\r\n        if optimizers:\r\n            # join both types in a tuple for API convenience\r\n            return (module, *optimizers, scheduler) if scheduler is not None else (module, *optimizers)\r\n        return module", "code_tokens": ["def", "setup", "(", "self", ",", "module", ":", "nn", ".", "Module", ",", "*", "optimizers", ":", "Optimizer", ",", "scheduler", ":", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "=", "None", ",", "move_to_device", ":", "bool", "=", "True", ",", "_reapply_compile", ":", "bool", "=", "True", ",", ")", "-", ">", "Any", ":", "r", "\"", "\"", "\"", "Set", "up", "a", "model", "and", "its", "optimizers", "for", "accelerated", "training", ".", "Args", ":", "module", ":", "A", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", "to", "set", "up", ".", "*", "optimizers", ":", "The", "optimizer", "(", "s", ")", "to", "set", "up", ".", "Can", "be", "zero", "or", "more", "optimizers", ".", "scheduler", ":", "An", "optional", "learning", "rate", "scheduler", "to", "set", "up", ".", "Must", "be", "provided", "after", "optimizers", "if", "used", ".", "move_to_device", ":", "If", "set", "`", "`", "True", "`", "`", "(", "default", ")", ",", "moves", "the", "model", "to", "the", "correct", "device", ".", "Set", "this", "to", "`", "`", "False", "`", "`", "and", "alternatively", "use", ":", "meth", ":", "`", "to_device", "`", "manually", ".", "_reapply_compile", ":", "If", "`", "`", "True", "`", "`", "(", "default", ")", ",", "and", "the", "model", "was", "`", "`", "torch", ".", "compile", "`", "`", "d", "before", ",", "the", "corresponding", ":", "class", ":", "`", "~", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", "wrapper", "will", "be", "removed", "and", "reapplied", "with", "the", "same", "settings", "after", "the", "model", "was", "set", "up", "by", "the", "strategy", "(", "e", ".", "g", ".", ",", "after", "the", "model", "was", "wrapped", "by", "DDP", ",", "FSDP", "etc", ".", ")", ".", "Set", "it", "to", "`", "`", "False", "`", "`", "if", "compiling", "DDP", "/", "FSDP", "is", "causing", "issues", ".", "Returns", ":", "If", "no", "optimizers", "are", "passed", ",", "returns", "the", "wrapped", "module", ".", "If", "optimizers", "are", "passed", ",", "returns", "a", "tuple", "containing", "the", "wrapped", "module", "and", "optimizers", ",", "and", "optionally", "the", "scheduler", "if", "provided", ",", "in", "the", "same", "order", "they", "were", "passed", "in", ".", "Note", ":", "For", "certain", "strategies", "like", "FSDP", ",", "you", "may", "need", "to", "set", "up", "the", "model", "first", "using", ":", "meth", ":", "`", "setup_module", "`", ",", "then", "create", "the", "optimizer", ",", "and", "finally", "set", "up", "the", "optimizer", "using", ":", "meth", ":", "`", "setup_optimizers", "`", ".", "Example", ":", ":", "model", ",", "optimizer", "=", "fabric", ".", "setup", "(", "model", ",", "optimizer", ")", "model", ",", "opt1", ",", "opt2", ",", "scheduler", "=", "fabric", ".", "setup", "(", "model", ",", "opt1", ",", "opt2", ",", "scheduler", "=", "scheduler", ")", "model", "=", "fabric", ".", "setup", "(", "model", ")", "\"", "\"", "\"", "self", ".", "_validate_setup", "(", "module", ",", "optimizers", ")", "module", ",", "compile_kwargs", "=", "_unwrap_compiled", "(", "module", ")", "if", "_reapply_compile", "else", "(", "module", ",", "None", ")", "original_module", "=", "module", "module", "=", "self", ".", "_precision", ".", "convert_module", "(", "module", ")", "if", "move_to_device", ":", "module", "=", "self", ".", "_move_model_to_device", "(", "model", "=", "module", ",", "optimizers", "=", "list", "(", "optimizers", ")", ")", "if", "optimizers", ":", "module", ",", "optimizers", ",", "scheduler", "=", "self", ".", "_strategy", ".", "setup_module_and_optimizers", "(", "module", ",", "list", "(", "optimizers", ")", ",", "scheduler", ")", "else", ":", "module", "=", "self", ".", "_strategy", ".", "setup_module", "(", "module", ")", "if", "compile_kwargs", "is", "not", "None", ":", "module", "=", "_to_compiled", "(", "module", ",", "compile_kwargs", ")", "module", "=", "_FabricModule", "(", "module", ",", "self", ".", "_strategy", ",", "original_module", "=", "original_module", ")", "_update_properties", "(", "module", ",", "device", "=", "self", ".", "device", "if", "move_to_device", "else", "next", "(", "module", ".", "parameters", "(", ")", ",", "torch", ".", "tensor", "(", "0", ")", ")", ".", "device", ")", "optimizers", "=", "[", "_FabricOptimizer", "(", "optimizer", ",", "self", ".", "_strategy", ",", "self", ".", "_callbacks", ")", "for", "optimizer", "in", "optimizers", "]", "self", ".", "_models_setup", "+", "=", "1", "if", "hasattr", "(", "original_module", ",", "\"", "_fabric", "\"", ")", ":", "original_module", ".", "_fabric", "=", "self", "original_module", ".", "_fabric_optimizers", "=", "optimizers", "if", "original_module", "not", "in", "self", ".", "_callbacks", ":", "self", ".", "_callbacks", ".", "append", "(", "original_module", ")", "self", ".", "call", "(", "\"", "on_after_setup", "\"", ",", "fabric", "=", "self", ",", "module", "=", "module", ")", "if", "optimizers", ":", "return", "(", "module", ",", "*", "optimizers", ",", "scheduler", ")", "if", "scheduler", "is", "not", "None", "else", "(", "module", ",", "*", "optimizers", ")", "return", "module"], "docstring": "r\"\"\"Set up a model and its optimizers for accelerated training.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            *optimizers: The optimizer(s) to set up. Can be zero or more optimizers.\r\n            scheduler: An optional learning rate scheduler to set up. Must be provided after optimizers if used.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            If no optimizers are passed, returns the wrapped module. If optimizers are passed, returns a tuple\r\n            containing the wrapped module and optimizers, and optionally the scheduler if provided, in the same\r\n            order they were passed in.\r\n\r\n        Note:\r\n            For certain strategies like FSDP, you may need to set up the model first using :meth:`setup_module`,\r\n            then create the optimizer, and finally set up the optimizer using :meth:`setup_optimizers`.\r\n\r\n        Example::\r\n\r\n            # Basic usage\r\n            model, optimizer = fabric.setup(model, optimizer)\r\n\r\n            # With multiple optimizers and scheduler\r\n            model, opt1, opt2, scheduler = fabric.setup(model, opt1, opt2, scheduler=scheduler)\r\n\r\n            # Model only\r\n            model = fabric.setup(model)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "set", "up", "a", "model", "and", "its", "optimizers", "for", "accelerated", "training", "args", "module", "a", "class", "torch", "nn", "module", "to", "set", "up", "optimizers", "the", "optimizer", "s", "to", "set", "up", "can", "be", "zero", "or", "more", "optimizers", "scheduler", "an", "optional", "learning", "rate", "scheduler", "to", "set", "up", "must", "be", "provided", "after", "optimizers", "if", "used", "move_to_device", "if", "set", "true", "default", "moves", "the", "model", "to", "the", "correct", "device", "set", "this", "to", "false", "and", "alternatively", "use", "meth", "to_device", "manually", "_reapply_compile", "if", "true", "default", "and", "the", "model", "was", "torch", "compile", "d", "before", "the", "corresponding", "class", "torch", "_dynamo", "optimizedmodule", "wrapper", "will", "be", "removed", "and", "reapplied", "with", "the", "same", "settings", "after", "the", "model", "was", "set", "up", "by", "the", "strategy", "e", "g", "after", "the", "model", "was", "wrapped", "by", "ddp", "fsdp", "etc", "set", "it", "to", "false", "if", "compiling", "ddp", "fsdp", "is", "causing", "issues", "returns", "if", "no", "optimizers", "are", "passed", "returns", "the", "wrapped", "module", "if", "optimizers", "are", "passed", "returns", "a", "tuple", "containing", "the", "wrapped", "module", "and", "optimizers", "and", "optionally", "the", "scheduler", "if", "provided", "in", "the", "same", "order", "they", "were", "passed", "in", "note", "for", "certain", "strategies", "like", "fsdp", "you", "may", "need", "to", "set", "up", "the", "model", "first", "using", "meth", "setup_module", "then", "create", "the", "optimizer", "and", "finally", "set", "up", "the", "optimizer", "using", "meth", "setup_optimizers", "example", "basic", "usage", "model", "optimizer", "fabric", "setup", "model", "optimizer", "with", "multiple", "optimizers", "and", "scheduler", "model", "opt1", "opt2", "scheduler", "fabric", "setup", "model", "opt1", "opt2", "scheduler", "scheduler", "model", "only", "model", "fabric", "setup", "model"], "docstring_summary": "r\"\"\"Set up a model and its optimizers for accelerated training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 228, "end_line": 312, "hash": "9f2773e66fe073529790fa73088d921f", "complexity": 11, "parameters": ["module", "*optimizers", "scheduler", "move_to_device", "_reapply_compile"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "setup_module", "original_string": "def setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            # Set up model first (useful for FSDP)\r\n            model = fabric.setup_module(model)\r\n\r\n            # Then create and set up optimizer\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        # Let strategy wrap and connect the module alone\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module", "language": "python", "code": "def setup_module(\r\n        self, module: nn.Module, move_to_device: bool = True, _reapply_compile: bool = True\r\n    ) -> _FabricModule:\r\n        r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            # Set up model first (useful for FSDP)\r\n            model = fabric.setup_module(model)\r\n\r\n            # Then create and set up optimizer\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"\r\n        self._validate_setup_module(module)\r\n        module, compile_kwargs = _unwrap_compiled(module) if _reapply_compile else (module, None)\r\n        original_module = module\r\n\r\n        module = self._precision.convert_module(module)\r\n\r\n        if move_to_device:\r\n            module = self._move_model_to_device(model=module, optimizers=[])\r\n\r\n        # Let strategy wrap and connect the module alone\r\n        module = self._strategy.setup_module(module)\r\n\r\n        if compile_kwargs is not None:\r\n            module = _to_compiled(module, compile_kwargs)\r\n        module = _FabricModule(module, self._strategy, original_module=original_module)\r\n\r\n        # Update the _DeviceDtypeModuleMixin's device parameter\r\n        # NOTE: for sharded strategies or manual device placement, there's no single root device\r\n        _update_properties(\r\n            module, device=self.device if move_to_device else next(module.parameters(), torch.tensor(0)).device\r\n        )\r\n\r\n        if hasattr(original_module, \"_fabric\"):  # this is probably a LightningModule\r\n            original_module._fabric = self\r\n            if original_module not in self._callbacks:\r\n                self._callbacks.append(original_module)\r\n\r\n        self._models_setup += 1\r\n        return module", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "nn", ".", "Module", ",", "move_to_device", ":", "bool", "=", "True", ",", "_reapply_compile", ":", "bool", "=", "True", ")", "-", ">", "_FabricModule", ":", "r", "\"", "\"", "\"", "Set", "up", "a", "model", "for", "accelerated", "training", "or", "inference", ".", "This", "is", "the", "same", "as", "calling", "`", "`", ".", "setup", "(", "model", ")", "`", "`", "with", "no", "optimizers", ".", "It", "is", "useful", "for", "inference", "or", "for", "certain", "strategies", "like", "`", "FSDP", "`", "that", "require", "setting", "up", "the", "module", "before", "the", "optimizer", "can", "be", "created", "and", "set", "up", ".", "See", "also", ":", "meth", ":", "`", "setup_optimizers", "`", ".", "Args", ":", "module", ":", "A", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", "to", "set", "up", ".", "move_to_device", ":", "If", "set", "`", "`", "True", "`", "`", "(", "default", ")", ",", "moves", "the", "model", "to", "the", "correct", "device", ".", "Set", "this", "to", "`", "`", "False", "`", "`", "and", "alternatively", "use", ":", "meth", ":", "`", "to_device", "`", "manually", ".", "_reapply_compile", ":", "If", "`", "`", "True", "`", "`", "(", "default", ")", ",", "and", "the", "model", "was", "`", "`", "torch", ".", "compile", "`", "`", "d", "before", ",", "the", "corresponding", ":", "class", ":", "`", "~", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", "wrapper", "will", "be", "removed", "and", "reapplied", "with", "the", "same", "settings", "after", "the", "model", "was", "set", "up", "by", "the", "strategy", "(", "e", ".", "g", ".", ",", "after", "the", "model", "was", "wrapped", "by", "DDP", ",", "FSDP", "etc", ".", ")", ".", "Set", "it", "to", "`", "`", "False", "`", "`", "if", "compiling", "DDP", "/", "FSDP", "is", "causing", "issues", ".", "Returns", ":", "The", "wrapped", "model", "as", "a", ":", "class", ":", "`", "~", "lightning", ".", "fabric", ".", "wrappers", ".", "_FabricModule", "`", ".", "Example", ":", ":", "model", "=", "fabric", ".", "setup_module", "(", "model", ")", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ")", "optimizer", "=", "fabric", ".", "setup_optimizers", "(", "optimizer", ")", "\"", "\"", "\"", "self", ".", "_validate_setup_module", "(", "module", ")", "module", ",", "compile_kwargs", "=", "_unwrap_compiled", "(", "module", ")", "if", "_reapply_compile", "else", "(", "module", ",", "None", ")", "original_module", "=", "module", "module", "=", "self", ".", "_precision", ".", "convert_module", "(", "module", ")", "if", "move_to_device", ":", "module", "=", "self", ".", "_move_model_to_device", "(", "model", "=", "module", ",", "optimizers", "=", "[", "]", ")", "module", "=", "self", ".", "_strategy", ".", "setup_module", "(", "module", ")", "if", "compile_kwargs", "is", "not", "None", ":", "module", "=", "_to_compiled", "(", "module", ",", "compile_kwargs", ")", "module", "=", "_FabricModule", "(", "module", ",", "self", ".", "_strategy", ",", "original_module", "=", "original_module", ")", "_update_properties", "(", "module", ",", "device", "=", "self", ".", "device", "if", "move_to_device", "else", "next", "(", "module", ".", "parameters", "(", ")", ",", "torch", ".", "tensor", "(", "0", ")", ")", ".", "device", ")", "if", "hasattr", "(", "original_module", ",", "\"", "_fabric", "\"", ")", ":", "original_module", ".", "_fabric", "=", "self", "if", "original_module", "not", "in", "self", ".", "_callbacks", ":", "self", ".", "_callbacks", ".", "append", "(", "original_module", ")", "self", ".", "_models_setup", "+", "=", "1", "return", "module"], "docstring": "r\"\"\"Set up a model for accelerated training or inference.\r\n\r\n        This is the same as calling ``.setup(model)`` with no optimizers. It is useful for inference or for certain\r\n        strategies like `FSDP` that require setting up the module before the optimizer can be created and set up.\r\n        See also :meth:`setup_optimizers`.\r\n\r\n        Args:\r\n            module: A :class:`torch.nn.Module` to set up.\r\n            move_to_device: If set ``True`` (default), moves the model to the correct device. Set this to ``False``\r\n                and alternatively use :meth:`to_device` manually.\r\n            _reapply_compile: If ``True`` (default), and the model was ``torch.compile``d before, the\r\n                corresponding :class:`~torch._dynamo.OptimizedModule` wrapper will be removed and reapplied with the\r\n                same settings after the model was set up by the strategy (e.g., after the model was wrapped by DDP,\r\n                FSDP etc.). Set it to ``False`` if compiling DDP/FSDP is causing issues.\r\n\r\n        Returns:\r\n            The wrapped model as a :class:`~lightning.fabric.wrappers._FabricModule`.\r\n\r\n        Example::\r\n\r\n            # Set up model first (useful for FSDP)\r\n            model = fabric.setup_module(model)\r\n\r\n            # Then create and set up optimizer\r\n            optimizer = torch.optim.Adam(model.parameters())\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "set", "up", "a", "model", "for", "accelerated", "training", "or", "inference", "this", "is", "the", "same", "as", "calling", "setup", "model", "with", "no", "optimizers", "it", "is", "useful", "for", "inference", "or", "for", "certain", "strategies", "like", "fsdp", "that", "require", "setting", "up", "the", "module", "before", "the", "optimizer", "can", "be", "created", "and", "set", "up", "see", "also", "meth", "setup_optimizers", "args", "module", "a", "class", "torch", "nn", "module", "to", "set", "up", "move_to_device", "if", "set", "true", "default", "moves", "the", "model", "to", "the", "correct", "device", "set", "this", "to", "false", "and", "alternatively", "use", "meth", "to_device", "manually", "_reapply_compile", "if", "true", "default", "and", "the", "model", "was", "torch", "compile", "d", "before", "the", "corresponding", "class", "torch", "_dynamo", "optimizedmodule", "wrapper", "will", "be", "removed", "and", "reapplied", "with", "the", "same", "settings", "after", "the", "model", "was", "set", "up", "by", "the", "strategy", "e", "g", "after", "the", "model", "was", "wrapped", "by", "ddp", "fsdp", "etc", "set", "it", "to", "false", "if", "compiling", "ddp", "fsdp", "is", "causing", "issues", "returns", "the", "wrapped", "model", "as", "a", "class", "lightning", "fabric", "wrappers", "_fabricmodule", "example", "set", "up", "model", "first", "useful", "for", "fsdp", "model", "fabric", "setup_module", "model", "then", "create", "and", "set", "up", "optimizer", "optimizer", "torch", "optim", "adam", "model", "parameters", "optimizer", "fabric", "setup_optimizers", "optimizer"], "docstring_summary": "r\"\"\"Set up a model for accelerated training or inference.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 314, "end_line": 373, "hash": "5501e71ea072c42fe0bef2831562734f", "complexity": 7, "parameters": ["module", "move_to_device", "_reapply_compile"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "setup_optimizers", "original_string": "def setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            # Single optimizer\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            # Multiple optimizers\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)", "language": "python", "code": "def setup_optimizers(self, *optimizers: Optimizer) -> Union[_FabricOptimizer, tuple[_FabricOptimizer, ...]]:\r\n        r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            # Single optimizer\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            # Multiple optimizers\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"\r\n        self._validate_setup_optimizers(optimizers)\r\n        optimizers = [self._strategy.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        optimizers = [\r\n            _FabricOptimizer(optimizer=optimizer, strategy=self._strategy, callbacks=self._callbacks)\r\n            for optimizer in optimizers\r\n        ]\r\n        return optimizers[0] if len(optimizers) == 1 else tuple(optimizers)", "code_tokens": ["def", "setup_optimizers", "(", "self", ",", "*", "optimizers", ":", "Optimizer", ")", "-", ">", "Union", "[", "_FabricOptimizer", ",", "tuple", "[", "_FabricOptimizer", ",", ".", ".", ".", "]", "]", ":", "r", "\"", "\"", "\"", "Set", "up", "one", "or", "more", "optimizers", "for", "accelerated", "training", ".", "Some", "strategies", "do", "not", "allow", "setting", "up", "model", "and", "optimizer", "independently", ".", "For", "them", ",", "you", "should", "call", "`", "`", ".", "setup", "(", "model", ",", "optimizer", ",", ".", ".", ".", ")", "`", "`", "instead", "to", "jointly", "set", "them", "up", ".", "Args", ":", "*", "optimizers", ":", "One", "or", "more", "optimizers", "to", "set", "up", ".", "Must", "provide", "at", "least", "one", "optimizer", ".", "Returns", ":", "If", "a", "single", "optimizer", "is", "passed", ",", "returns", "the", "wrapped", "optimizer", ".", "If", "multiple", "optimizers", "are", "passed", ",", "returns", "a", "tuple", "of", "wrapped", "optimizers", "in", "the", "same", "order", "they", "were", "passed", "in", ".", "Raises", ":", "RuntimeError", ":", "If", "using", "DeepSpeed", "or", "XLA", "strategies", ",", "which", "require", "joint", "model", "-", "optimizer", "setup", ".", "Note", ":", "This", "method", "cannot", "be", "used", "with", "DeepSpeed", "or", "XLA", "strategies", ".", "Use", ":", "meth", ":", "`", "setup", "`", "instead", "for", "those", "strategies", ".", "Example", ":", ":", "optimizer", "=", "fabric", ".", "setup_optimizers", "(", "optimizer", ")", "opt1", ",", "opt2", "=", "fabric", ".", "setup_optimizers", "(", "opt1", ",", "opt2", ")", "\"", "\"", "\"", "self", ".", "_validate_setup_optimizers", "(", "optimizers", ")", "optimizers", "=", "[", "self", ".", "_strategy", ".", "setup_optimizer", "(", "optimizer", ")", "for", "optimizer", "in", "optimizers", "]", "optimizers", "=", "[", "_FabricOptimizer", "(", "optimizer", "=", "optimizer", ",", "strategy", "=", "self", ".", "_strategy", ",", "callbacks", "=", "self", ".", "_callbacks", ")", "for", "optimizer", "in", "optimizers", "]", "return", "optimizers", "[", "0", "]", "if", "len", "(", "optimizers", ")", "=", "=", "1", "else", "tuple", "(", "optimizers", ")"], "docstring": "r\"\"\"Set up one or more optimizers for accelerated training.\r\n\r\n        Some strategies do not allow setting up model and optimizer independently. For them, you should call\r\n        ``.setup(model, optimizer, ...)`` instead to jointly set them up.\r\n\r\n        Args:\r\n            *optimizers: One or more optimizers to set up. Must provide at least one optimizer.\r\n\r\n        Returns:\r\n            If a single optimizer is passed, returns the wrapped optimizer. If multiple optimizers are passed,\r\n            returns a tuple of wrapped optimizers in the same order they were passed in.\r\n\r\n        Raises:\r\n            RuntimeError: If using DeepSpeed or XLA strategies, which require joint model-optimizer setup.\r\n\r\n        Note:\r\n            This method cannot be used with DeepSpeed or XLA strategies. Use :meth:`setup` instead for those strategies.\r\n\r\n        Example::\r\n\r\n            # Single optimizer\r\n            optimizer = fabric.setup_optimizers(optimizer)\r\n\r\n            # Multiple optimizers\r\n            opt1, opt2 = fabric.setup_optimizers(opt1, opt2)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "set", "up", "one", "or", "more", "optimizers", "for", "accelerated", "training", "some", "strategies", "do", "not", "allow", "setting", "up", "model", "and", "optimizer", "independently", "for", "them", "you", "should", "call", "setup", "model", "optimizer", "instead", "to", "jointly", "set", "them", "up", "args", "optimizers", "one", "or", "more", "optimizers", "to", "set", "up", "must", "provide", "at", "least", "one", "optimizer", "returns", "if", "a", "single", "optimizer", "is", "passed", "returns", "the", "wrapped", "optimizer", "if", "multiple", "optimizers", "are", "passed", "returns", "a", "tuple", "of", "wrapped", "optimizers", "in", "the", "same", "order", "they", "were", "passed", "in", "raises", "runtimeerror", "if", "using", "deepspeed", "or", "xla", "strategies", "which", "require", "joint", "model", "optimizer", "setup", "note", "this", "method", "cannot", "be", "used", "with", "deepspeed", "or", "xla", "strategies", "use", "meth", "setup", "instead", "for", "those", "strategies", "example", "single", "optimizer", "optimizer", "fabric", "setup_optimizers", "optimizer", "multiple", "optimizers", "opt1", "opt2", "fabric", "setup_optimizers", "opt1", "opt2"], "docstring_summary": "r\"\"\"Set up one or more optimizers for accelerated training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 375, "end_line": 409, "hash": "a02743cc8f6c713e102acd495b81b3ff", "complexity": 4, "parameters": ["*optimizers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "setup_dataloaders", "original_string": "def setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            # Single dataloader\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            # Multiple dataloaders\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders", "language": "python", "code": "def setup_dataloaders(\r\n        self, *dataloaders: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> Union[DataLoader, list[DataLoader]]:\r\n        r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            # Single dataloader\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            # Multiple dataloaders\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"\r\n        self._validate_setup_dataloaders(dataloaders)\r\n        dataloaders = [\r\n            self._setup_dataloader(\r\n                dataloader, use_distributed_sampler=use_distributed_sampler, move_to_device=move_to_device\r\n            )\r\n            for dataloader in dataloaders\r\n        ]\r\n        return dataloaders[0] if len(dataloaders) == 1 else dataloaders", "code_tokens": ["def", "setup_dataloaders", "(", "self", ",", "*", "dataloaders", ":", "DataLoader", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "move_to_device", ":", "bool", "=", "True", ")", "-", ">", "Union", "[", "DataLoader", ",", "list", "[", "DataLoader", "]", "]", ":", "r", "\"", "\"", "\"", "Set", "up", "one", "or", "multiple", "dataloaders", "for", "accelerated", "training", ".", "If", "you", "need", "different", "settings", "for", "each", "dataloader", ",", "call", "this", "method", "individually", "for", "each", "one", ".", "Args", ":", "*", "dataloaders", ":", "One", "or", "more", "PyTorch", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "instances", "to", "set", "up", ".", "use_distributed_sampler", ":", "If", "set", "`", "`", "True", "`", "`", "(", "default", ")", ",", "automatically", "wraps", "or", "replaces", "the", "sampler", "on", "the", "dataloader", "(", "s", ")", "for", "distributed", "training", ".", "If", "you", "have", "a", "custom", "sampler", "defined", ",", "set", "this", "argument", "to", "`", "`", "False", "`", "`", ".", "move_to_device", ":", "If", "set", "`", "`", "True", "`", "`", "(", "default", ")", ",", "moves", "the", "data", "returned", "by", "the", "dataloader", "(", "s", ")", "automatically", "to", "the", "correct", "device", ".", "Set", "this", "to", "`", "`", "False", "`", "`", "and", "alternatively", "use", ":", "meth", ":", "`", "to_device", "`", "manually", "on", "the", "returned", "data", ".", "Returns", ":", "If", "a", "single", "dataloader", "is", "passed", ",", "returns", "the", "wrapped", "dataloader", ".", "If", "multiple", "dataloaders", "are", "passed", ",", "returns", "a", "list", "of", "wrapped", "dataloaders", "in", "the", "same", "order", "they", "were", "passed", "in", ".", "Example", ":", ":", "train_loader", "=", "fabric", ".", "setup_dataloaders", "(", "train_loader", ")", "train_loader", ",", "val_loader", "=", "fabric", ".", "setup_dataloaders", "(", "train_loader", ",", "val_loader", ")", "\"", "\"", "\"", "self", ".", "_validate_setup_dataloaders", "(", "dataloaders", ")", "dataloaders", "=", "[", "self", ".", "_setup_dataloader", "(", "dataloader", ",", "use_distributed_sampler", "=", "use_distributed_sampler", ",", "move_to_device", "=", "move_to_device", ")", "for", "dataloader", "in", "dataloaders", "]", "return", "dataloaders", "[", "0", "]", "if", "len", "(", "dataloaders", ")", "=", "=", "1", "else", "dataloaders"], "docstring": "r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each\r\n        dataloader, call this method individually for each one.\r\n\r\n        Args:\r\n            *dataloaders: One or more PyTorch :class:`~torch.utils.data.DataLoader` instances to set up.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader(s) for distributed training. If you have a custom sampler defined, set this argument\r\n                to ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader(s) automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            If a single dataloader is passed, returns the wrapped dataloader. If multiple dataloaders are passed,\r\n            returns a list of wrapped dataloaders in the same order they were passed in.\r\n\r\n        Example::\r\n\r\n            # Single dataloader\r\n            train_loader = fabric.setup_dataloaders(train_loader)\r\n\r\n            # Multiple dataloaders\r\n            train_loader, val_loader = fabric.setup_dataloaders(train_loader, val_loader)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "set", "up", "one", "or", "multiple", "dataloaders", "for", "accelerated", "training", "if", "you", "need", "different", "settings", "for", "each", "dataloader", "call", "this", "method", "individually", "for", "each", "one", "args", "dataloaders", "one", "or", "more", "pytorch", "class", "torch", "utils", "data", "dataloader", "instances", "to", "set", "up", "use_distributed_sampler", "if", "set", "true", "default", "automatically", "wraps", "or", "replaces", "the", "sampler", "on", "the", "dataloader", "s", "for", "distributed", "training", "if", "you", "have", "a", "custom", "sampler", "defined", "set", "this", "argument", "to", "false", "move_to_device", "if", "set", "true", "default", "moves", "the", "data", "returned", "by", "the", "dataloader", "s", "automatically", "to", "the", "correct", "device", "set", "this", "to", "false", "and", "alternatively", "use", "meth", "to_device", "manually", "on", "the", "returned", "data", "returns", "if", "a", "single", "dataloader", "is", "passed", "returns", "the", "wrapped", "dataloader", "if", "multiple", "dataloaders", "are", "passed", "returns", "a", "list", "of", "wrapped", "dataloaders", "in", "the", "same", "order", "they", "were", "passed", "in", "example", "single", "dataloader", "train_loader", "fabric", "setup_dataloaders", "train_loader", "multiple", "dataloaders", "train_loader", "val_loader", "fabric", "setup_dataloaders", "train_loader", "val_loader"], "docstring_summary": "r\"\"\"Set up one or multiple dataloaders for accelerated training. If you need different settings for each", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 411, "end_line": 446, "hash": "42272265e95bd0e4c30f973e675ae497", "complexity": 3, "parameters": ["*dataloaders", "use_distributed_sampler", "move_to_device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "_setup_dataloader", "original_string": "def _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            # the dataloader needs to be re-instantiated because we want to update the sampler\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        # add worker_init_fn for correct seeding in worker processes\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader", "language": "python", "code": "def _setup_dataloader(\r\n        self, dataloader: DataLoader, use_distributed_sampler: bool = True, move_to_device: bool = True\r\n    ) -> DataLoader:\r\n        r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"\r\n        if use_distributed_sampler and self._requires_distributed_sampler(dataloader):\r\n            sampler = self._get_distributed_sampler(dataloader, **self._strategy.distributed_sampler_kwargs)\r\n\r\n            # the dataloader needs to be re-instantiated because we want to update the sampler\r\n            dataloader = _update_dataloader(dataloader, sampler)\r\n\r\n        # add worker_init_fn for correct seeding in worker processes\r\n        _auto_add_worker_init_fn(dataloader, self.global_rank)\r\n\r\n        dataloader = self._strategy.process_dataloader(dataloader)\r\n        device = self.device if move_to_device and not isinstance(self._strategy, XLAStrategy) else None\r\n        fabric_dataloader = _FabricDataLoader(dataloader=dataloader, device=device)\r\n        fabric_dataloader = cast(DataLoader, fabric_dataloader)\r\n        return fabric_dataloader", "code_tokens": ["def", "_setup_dataloader", "(", "self", ",", "dataloader", ":", "DataLoader", ",", "use_distributed_sampler", ":", "bool", "=", "True", ",", "move_to_device", ":", "bool", "=", "True", ")", "-", ">", "DataLoader", ":", "r", "\"", "\"", "\"", "Set", "up", "a", "single", "dataloader", "for", "accelerated", "training", ".", "Args", ":", "dataloader", ":", "The", "dataloader", "to", "accelerate", ".", "use_distributed_sampler", ":", "If", "set", "`", "`", "True", "`", "`", "(", "default", ")", ",", "automatically", "wraps", "or", "replaces", "the", "sampler", "on", "the", "dataloader", "for", "distributed", "training", ".", "If", "you", "have", "a", "custom", "sampler", "defined", ",", "set", "this", "argument", "to", "`", "`", "False", "`", "`", ".", "move_to_device", ":", "If", "set", "`", "`", "True", "`", "`", "(", "default", ")", ",", "moves", "the", "data", "returned", "by", "the", "dataloader", "automatically", "to", "the", "correct", "device", ".", "Set", "this", "to", "`", "`", "False", "`", "`", "and", "alternatively", "use", ":", "meth", ":", "`", "to_device", "`", "manually", "on", "the", "returned", "data", ".", "Returns", ":", "The", "wrapped", "dataloader", ".", "\"", "\"", "\"", "if", "use_distributed_sampler", "and", "self", ".", "_requires_distributed_sampler", "(", "dataloader", ")", ":", "sampler", "=", "self", ".", "_get_distributed_sampler", "(", "dataloader", ",", "*", "*", "self", ".", "_strategy", ".", "distributed_sampler_kwargs", ")", "dataloader", "=", "_update_dataloader", "(", "dataloader", ",", "sampler", ")", "_auto_add_worker_init_fn", "(", "dataloader", ",", "self", ".", "global_rank", ")", "dataloader", "=", "self", ".", "_strategy", ".", "process_dataloader", "(", "dataloader", ")", "device", "=", "self", ".", "device", "if", "move_to_device", "and", "not", "isinstance", "(", "self", ".", "_strategy", ",", "XLAStrategy", ")", "else", "None", "fabric_dataloader", "=", "_FabricDataLoader", "(", "dataloader", "=", "dataloader", ",", "device", "=", "device", ")", "fabric_dataloader", "=", "cast", "(", "DataLoader", ",", "fabric_dataloader", ")", "return", "fabric_dataloader"], "docstring": "r\"\"\"Set up a single dataloader for accelerated training.\r\n\r\n        Args:\r\n            dataloader: The dataloader to accelerate.\r\n            use_distributed_sampler: If set ``True`` (default), automatically wraps or replaces the sampler on the\r\n                dataloader for distributed training. If you have a custom sampler defined, set this argument to\r\n                ``False``.\r\n            move_to_device: If set ``True`` (default), moves the data returned by the dataloader automatically to\r\n                the correct device. Set this to ``False`` and alternatively use :meth:`to_device` manually on the\r\n                returned data.\r\n\r\n        Returns:\r\n            The wrapped dataloader.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "set", "up", "a", "single", "dataloader", "for", "accelerated", "training", "args", "dataloader", "the", "dataloader", "to", "accelerate", "use_distributed_sampler", "if", "set", "true", "default", "automatically", "wraps", "or", "replaces", "the", "sampler", "on", "the", "dataloader", "for", "distributed", "training", "if", "you", "have", "a", "custom", "sampler", "defined", "set", "this", "argument", "to", "false", "move_to_device", "if", "set", "true", "default", "moves", "the", "data", "returned", "by", "the", "dataloader", "automatically", "to", "the", "correct", "device", "set", "this", "to", "false", "and", "alternatively", "use", "meth", "to_device", "manually", "on", "the", "returned", "data", "returns", "the", "wrapped", "dataloader"], "docstring_summary": "r\"\"\"Set up a single dataloader for accelerated training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 448, "end_line": 479, "hash": "90c6dc77ff39431e1360eaa1523306e1", "complexity": 5, "parameters": ["dataloader", "use_distributed_sampler", "move_to_device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "backward", "original_string": "def backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            # With DeepSpeed and multiple models\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                # requires to attach the current `DeepSpeedEngine` for the `_FabricOptimizer.step` call.\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False", "language": "python", "code": "def backward(self, tensor: Tensor, *args: Any, model: Optional[_FabricModule] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            # With DeepSpeed and multiple models\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"\r\n        module = model._forward_module if model is not None else model\r\n        module, _ = _unwrap_compiled(module)\r\n        if isinstance(self._strategy, DeepSpeedStrategy):\r\n            if model is None:\r\n                if self._models_setup == 0:\r\n                    raise RuntimeError(\"No models were set up for backward. Did you forget to call `fabric.setup()`?\")\r\n                if self._models_setup > 1:\r\n                    raise ValueError(\r\n                        \"When using multiple models + deepspeed, please provide the model used to perform\"\r\n                        \" the optimization: `self.backward(loss, model=model)`\"\r\n                    )\r\n                module = self._strategy.model\r\n            else:\r\n                # requires to attach the current `DeepSpeedEngine` for the `_FabricOptimizer.step` call.\r\n                self._strategy._deepspeed_engine = module\r\n\r\n        lightning.fabric.wrappers._in_fabric_backward = True\r\n        try:\r\n            self._strategy.backward(tensor, module, *args, **kwargs)\r\n        finally:\r\n            lightning.fabric.wrappers._in_fabric_backward = False", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "*", "args", ":", "Any", ",", "model", ":", "Optional", "[", "_FabricModule", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Replaces", "`", "`", "loss", ".", "backward", "(", ")", "`", "`", "in", "your", "training", "loop", ".", "Handles", "precision", "automatically", "for", "you", ".", "Args", ":", "tensor", ":", "The", "tensor", "(", "loss", ")", "to", "back", "-", "propagate", "gradients", "from", ".", "*", "args", ":", "Optional", "positional", "arguments", "passed", "to", "the", "underlying", "backward", "function", ".", "model", ":", "Optional", "model", "instance", "for", "plugins", "that", "require", "the", "model", "for", "backward", "(", ")", ".", "Required", "when", "using", "DeepSpeed", "strategy", "with", "multiple", "models", ".", "*", "*", "kwargs", ":", "Optional", "named", "keyword", "arguments", "passed", "to", "the", "underlying", "backward", "function", ".", "Note", ":", "When", "using", "`", "`", "strategy", "=", "\"", "deepspeed", "\"", "`", "`", "and", "multiple", "models", "were", "set", "up", ",", "it", "is", "required", "to", "pass", "in", "the", "model", "as", "argument", "here", ".", "Example", ":", ":", "loss", "=", "criterion", "(", "output", ",", "target", ")", "fabric", ".", "backward", "(", "loss", ")", "fabric", ".", "backward", "(", "loss", ",", "model", "=", "model", ")", "\"", "\"", "\"", "module", "=", "model", ".", "_forward_module", "if", "model", "is", "not", "None", "else", "model", "module", ",", "_", "=", "_unwrap_compiled", "(", "module", ")", "if", "isinstance", "(", "self", ".", "_strategy", ",", "DeepSpeedStrategy", ")", ":", "if", "model", "is", "None", ":", "if", "self", ".", "_models_setup", "=", "=", "0", ":", "raise", "RuntimeError", "(", "\"", "No", "models", "were", "set", "up", "for", "backward", ".", "Did", "you", "forget", "to", "call", "`", "fabric", ".", "setup", "(", ")", "`", "?", "\"", ")", "if", "self", ".", "_models_setup", ">", "1", ":", "raise", "ValueError", "(", "\"", "When", "using", "multiple", "models", "+", "deepspeed", ",", "please", "provide", "the", "model", "used", "to", "perform", "\"", "\"", "the", "optimization", ":", "`", "self", ".", "backward", "(", "loss", ",", "model", "=", "model", ")", "`", "\"", ")", "module", "=", "self", ".", "_strategy", ".", "model", "else", ":", "self", ".", "_strategy", ".", "_deepspeed_engine", "=", "module", "lightning", ".", "fabric", ".", "wrappers", ".", "_in_fabric_backward", "=", "True", "try", ":", "self", ".", "_strategy", ".", "backward", "(", "tensor", ",", "module", ",", "*", "args", ",", "*", "*", "kwargs", ")", "finally", ":", "lightning", ".", "fabric", ".", "wrappers", ".", "_in_fabric_backward", "=", "False"], "docstring": "r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.\r\n\r\n        Args:\r\n            tensor: The tensor (loss) to back-propagate gradients from.\r\n            *args: Optional positional arguments passed to the underlying backward function.\r\n            model: Optional model instance for plugins that require the model for backward(). Required when using\r\n                DeepSpeed strategy with multiple models.\r\n            **kwargs: Optional named keyword arguments passed to the underlying backward function.\r\n\r\n        Note:\r\n            When using ``strategy=\"deepspeed\"`` and multiple models were set up, it is required to pass in the\r\n            model as argument here.\r\n\r\n        Example::\r\n\r\n            loss = criterion(output, target)\r\n            fabric.backward(loss)\r\n\r\n            # With DeepSpeed and multiple models\r\n            fabric.backward(loss, model=model)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "replaces", "loss", "backward", "in", "your", "training", "loop", "handles", "precision", "automatically", "for", "you", "args", "tensor", "the", "tensor", "loss", "to", "back", "propagate", "gradients", "from", "args", "optional", "positional", "arguments", "passed", "to", "the", "underlying", "backward", "function", "model", "optional", "model", "instance", "for", "plugins", "that", "require", "the", "model", "for", "backward", "required", "when", "using", "deepspeed", "strategy", "with", "multiple", "models", "kwargs", "optional", "named", "keyword", "arguments", "passed", "to", "the", "underlying", "backward", "function", "note", "when", "using", "strategy", "deepspeed", "and", "multiple", "models", "were", "set", "up", "it", "is", "required", "to", "pass", "in", "the", "model", "as", "argument", "here", "example", "loss", "criterion", "output", "target", "fabric", "backward", "loss", "with", "deepspeed", "and", "multiple", "models", "fabric", "backward", "loss", "model", "model"], "docstring_summary": "r\"\"\"Replaces ``loss.backward()`` in your training loop. Handles precision automatically for you.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 481, "end_line": 524, "hash": "cbb5823e050a4699064a7ae86ed1ccd5", "complexity": 6, "parameters": ["tensor", "*args", "model", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "clip_gradients", "original_string": "def clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            # Clip by value\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            # Clip by norm\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        module: Union[torch.nn.Module, _FabricModule],\r\n        optimizer: Union[Optimizer, _FabricOptimizer],\r\n        clip_val: Optional[Union[float, int]] = None,\r\n        max_norm: Optional[Union[float, int]] = None,\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Optional[torch.Tensor]:\r\n        \"\"\"Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            # Clip by value\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            # Clip by norm\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)\r\n\r\n        \"\"\"\r\n        if clip_val is not None and max_norm is not None:\r\n            raise ValueError(\r\n                \"Only one of `clip_val` or `max_norm` can be set as this specifies the underlying clipping algorithm!\"\r\n            )\r\n\r\n        if clip_val is not None:\r\n            self.strategy.clip_gradients_value(_unwrap_objects(module), _unwrap_objects(optimizer), clip_val=clip_val)\r\n            return None\r\n        if max_norm is not None:\r\n            return self.strategy.clip_gradients_norm(\r\n                _unwrap_objects(module),\r\n                _unwrap_objects(optimizer),\r\n                max_norm=max_norm,\r\n                norm_type=norm_type,\r\n                error_if_nonfinite=error_if_nonfinite,\r\n            )\r\n        raise ValueError(\"You have to specify either `clip_val` or `max_norm` to do gradient clipping!\")", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "module", ":", "Union", "[", "torch", ".", "nn", ".", "Module", ",", "_FabricModule", "]", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "_FabricOptimizer", "]", ",", "clip_val", ":", "Optional", "[", "Union", "[", "float", ",", "int", "]", "]", "=", "None", ",", "max_norm", ":", "Optional", "[", "Union", "[", "float", ",", "int", "]", "]", "=", "None", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "Optional", "[", "torch", ".", "Tensor", "]", ":", "\"", "\"", "\"", "Clip", "the", "gradients", "of", "the", "model", "to", "a", "given", "max", "value", "or", "max", "norm", ".", "Args", ":", "module", ":", "The", "module", "whose", "parameters", "should", "be", "clipped", ".", "optimizer", ":", "The", "optimizer", "referencing", "the", "parameters", "to", "be", "clipped", ".", "clip_val", ":", "If", "passed", ",", "gradients", "will", "be", "clipped", "to", "this", "value", ".", "Cannot", "be", "used", "together", "with", "`", "`", "max_norm", "`", "`", ".", "max_norm", ":", "If", "passed", ",", "clips", "the", "gradients", "in", "such", "a", "way", "that", "the", "p", "-", "norm", "of", "the", "resulting", "parameters", "is", "no", "larger", "than", "the", "given", "value", ".", "Cannot", "be", "used", "together", "with", "`", "`", "clip_val", "`", "`", ".", "norm_type", ":", "The", "type", "of", "norm", "if", "`", "`", "max_norm", "`", "`", "was", "passed", ".", "Can", "be", "`", "`", "'", "inf", "'", "`", "`", "for", "infinity", "norm", ".", "Defaults", "to", "2", "-", "norm", ".", "error_if_nonfinite", ":", "An", "error", "is", "raised", "if", "the", "total", "norm", "of", "the", "gradients", "is", "NaN", "or", "infinite", ".", "Only", "applies", "when", "`", "`", "max_norm", "`", "`", "is", "used", ".", "Returns", ":", "The", "total", "norm", "of", "the", "gradients", "(", "before", "clipping", "was", "applied", ")", "as", "a", "scalar", "tensor", "if", "`", "`", "max_norm", "`", "`", "was", "passed", ",", "otherwise", "`", "`", "None", "`", "`", ".", "Raises", ":", "ValueError", ":", "If", "both", "`", "`", "clip_val", "`", "`", "and", "`", "`", "max_norm", "`", "`", "are", "provided", ",", "or", "if", "neither", "is", "provided", ".", "Example", ":", ":", "fabric", ".", "clip_gradients", "(", "model", ",", "optimizer", ",", "clip_val", "=", "1", ".", "0", ")", "total_norm", "=", "fabric", ".", "clip_gradients", "(", "model", ",", "optimizer", ",", "max_norm", "=", "1", ".", "0", ")", "\"", "\"", "\"", "if", "clip_val", "is", "not", "None", "and", "max_norm", "is", "not", "None", ":", "raise", "ValueError", "(", "\"", "Only", "one", "of", "`", "clip_val", "`", "or", "`", "max_norm", "`", "can", "be", "set", "as", "this", "specifies", "the", "underlying", "clipping", "algorithm", "!", "\"", ")", "if", "clip_val", "is", "not", "None", ":", "self", ".", "strategy", ".", "clip_gradients_value", "(", "_unwrap_objects", "(", "module", ")", ",", "_unwrap_objects", "(", "optimizer", ")", ",", "clip_val", "=", "clip_val", ")", "return", "None", "if", "max_norm", "is", "not", "None", ":", "return", "self", ".", "strategy", ".", "clip_gradients_norm", "(", "_unwrap_objects", "(", "module", ")", ",", "_unwrap_objects", "(", "optimizer", ")", ",", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ",", "error_if_nonfinite", "=", "error_if_nonfinite", ",", ")", "raise", "ValueError", "(", "\"", "You", "have", "to", "specify", "either", "`", "clip_val", "`", "or", "`", "max_norm", "`", "to", "do", "gradient", "clipping", "!", "\"", ")"], "docstring": "Clip the gradients of the model to a given max value or max norm.\r\n\r\n        Args:\r\n            module: The module whose parameters should be clipped.\r\n            optimizer: The optimizer referencing the parameters to be clipped.\r\n            clip_val: If passed, gradients will be clipped to this value. Cannot be used together with ``max_norm``.\r\n            max_norm: If passed, clips the gradients in such a way that the p-norm of the resulting parameters is\r\n                no larger than the given value. Cannot be used together with ``clip_val``.\r\n            norm_type: The type of norm if ``max_norm`` was passed. Can be ``'inf'`` for infinity norm.\r\n                Defaults to 2-norm.\r\n            error_if_nonfinite: An error is raised if the total norm of the gradients is NaN or infinite.\r\n                Only applies when ``max_norm`` is used.\r\n\r\n        Returns:\r\n            The total norm of the gradients (before clipping was applied) as a scalar tensor if ``max_norm`` was\r\n            passed, otherwise ``None``.\r\n\r\n        Raises:\r\n            ValueError: If both ``clip_val`` and ``max_norm`` are provided, or if neither is provided.\r\n\r\n        Example::\r\n\r\n            # Clip by value\r\n            fabric.clip_gradients(model, optimizer, clip_val=1.0)\r\n\r\n            # Clip by norm\r\n            total_norm = fabric.clip_gradients(model, optimizer, max_norm=1.0)", "docstring_tokens": ["clip", "the", "gradients", "of", "the", "model", "to", "a", "given", "max", "value", "or", "max", "norm", "args", "module", "the", "module", "whose", "parameters", "should", "be", "clipped", "optimizer", "the", "optimizer", "referencing", "the", "parameters", "to", "be", "clipped", "clip_val", "if", "passed", "gradients", "will", "be", "clipped", "to", "this", "value", "cannot", "be", "used", "together", "with", "max_norm", "max_norm", "if", "passed", "clips", "the", "gradients", "in", "such", "a", "way", "that", "the", "p", "norm", "of", "the", "resulting", "parameters", "is", "no", "larger", "than", "the", "given", "value", "cannot", "be", "used", "together", "with", "clip_val", "norm_type", "the", "type", "of", "norm", "if", "max_norm", "was", "passed", "can", "be", "inf", "for", "infinity", "norm", "defaults", "to", "2", "norm", "error_if_nonfinite", "an", "error", "is", "raised", "if", "the", "total", "norm", "of", "the", "gradients", "is", "nan", "or", "infinite", "only", "applies", "when", "max_norm", "is", "used", "returns", "the", "total", "norm", "of", "the", "gradients", "before", "clipping", "was", "applied", "as", "a", "scalar", "tensor", "if", "max_norm", "was", "passed", "otherwise", "none", "raises", "valueerror", "if", "both", "clip_val", "and", "max_norm", "are", "provided", "or", "if", "neither", "is", "provided", "example", "clip", "by", "value", "fabric", "clip_gradients", "model", "optimizer", "clip_val", "1", "0", "clip", "by", "norm", "total_norm", "fabric", "clip_gradients", "model", "optimizer", "max_norm", "1", "0"], "docstring_summary": "Clip the gradients of the model to a given max value or max norm.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 526, "end_line": 580, "hash": "4e02d79ad9e241c93b0bba54d1110d42", "complexity": 5, "parameters": ["module", "_FabricModule]", "optimizer", "_FabricOptimizer]", "clip_val", "int]]", "max_norm", "int]]", "norm_type", "int]", "error_if_nonfinite"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "autocast", "original_string": "def autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()", "language": "python", "code": "def autocast(self) -> AbstractContextManager:\r\n        \"\"\"A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.\r\n\r\n        \"\"\"\r\n        return self._precision.forward_context()", "code_tokens": ["def", "autocast", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "A", "context", "manager", "to", "automatically", "convert", "operations", "for", "the", "chosen", "precision", ".", "Use", "this", "only", "if", "the", "`", "forward", "`", "method", "of", "your", "model", "does", "not", "cover", "all", "operations", "you", "wish", "to", "run", "with", "the", "chosen", "precision", "setting", ".", "\"", "\"", "\"", "return", "self", ".", "_precision", ".", "forward_context", "(", ")"], "docstring": "A context manager to automatically convert operations for the chosen precision.\r\n\r\n        Use this only if the `forward` method of your model does not cover all operations you wish to run with the\r\n        chosen precision setting.", "docstring_tokens": ["a", "context", "manager", "to", "automatically", "convert", "operations", "for", "the", "chosen", "precision", "use", "this", "only", "if", "the", "forward", "method", "of", "your", "model", "does", "not", "cover", "all", "operations", "you", "wish", "to", "run", "with", "the", "chosen", "precision", "setting"], "docstring_summary": "A context manager to automatically convert operations for the chosen precision.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 582, "end_line": 589, "hash": "58bbfa741ac6e1d52b356bb19379d6e3", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "to_device", "original_string": "def to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)", "language": "python", "code": "def to_device(self, obj: Union[nn.Module, Tensor, Any]) -> Union[nn.Module, Tensor, Any]:\r\n        r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"\r\n        if isinstance(obj, nn.Module):\r\n            self._accelerator.setup_device(self.device)\r\n            self._strategy.module_to_device(obj)\r\n            return obj\r\n        return move_data_to_device(obj, device=self.device)", "code_tokens": ["def", "to_device", "(", "self", ",", "obj", ":", "Union", "[", "nn", ".", "Module", ",", "Tensor", ",", "Any", "]", ")", "-", ">", "Union", "[", "nn", ".", "Module", ",", "Tensor", ",", "Any", "]", ":", "r", "\"", "\"", "\"", "Move", "a", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", "or", "a", "collection", "of", "tensors", "to", "the", "current", "device", ",", "if", "it", "is", "not", "already", "on", "that", "device", ".", "Args", ":", "obj", ":", "An", "object", "to", "move", "to", "the", "device", ".", "Can", "be", "an", "instance", "of", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", ",", "a", "tensor", ",", "or", "a", "(", "nested", ")", "collection", "of", "tensors", "(", "e", ".", "g", ".", ",", "a", "dictionary", ")", ".", "Returns", ":", "A", "reference", "to", "the", "object", "that", "was", "moved", "to", "the", "new", "device", ".", "\"", "\"", "\"", "if", "isinstance", "(", "obj", ",", "nn", ".", "Module", ")", ":", "self", ".", "_accelerator", ".", "setup_device", "(", "self", ".", "device", ")", "self", ".", "_strategy", ".", "module_to_device", "(", "obj", ")", "return", "obj", "return", "move_data_to_device", "(", "obj", ",", "device", "=", "self", ".", "device", ")"], "docstring": "r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on\r\n        that device.\r\n\r\n        Args:\r\n            obj: An object to move to the device. Can be an instance of :class:`torch.nn.Module`, a tensor, or a\r\n                 (nested) collection of tensors (e.g., a dictionary).\r\n\r\n        Returns:\r\n            A reference to the object that was moved to the new device.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "move", "a", "class", "torch", "nn", "module", "or", "a", "collection", "of", "tensors", "to", "the", "current", "device", "if", "it", "is", "not", "already", "on", "that", "device", "args", "obj", "an", "object", "to", "move", "to", "the", "device", "can", "be", "an", "instance", "of", "class", "torch", "nn", "module", "a", "tensor", "or", "a", "nested", "collection", "of", "tensors", "e", "g", "a", "dictionary", "returns", "a", "reference", "to", "the", "object", "that", "was", "moved", "to", "the", "new", "device"], "docstring_summary": "r\"\"\"Move a :class:`torch.nn.Module` or a collection of tensors to the current device, if it is not already on", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 600, "end_line": 616, "hash": "c3b8b36dd9009bc3bbdba74ab43d0c33", "complexity": 2, "parameters": ["obj", "Tensor", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "print", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"\r\n        if self.local_rank == 0:\r\n            print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Print", "something", "only", "on", "the", "first", "process", ".", "If", "running", "on", "multiple", "machines", ",", "it", "will", "print", "from", "the", "first", "process", "in", "each", "machine", ".", "Arguments", "passed", "to", "this", "method", "are", "forwarded", "to", "the", "Python", "built", "-", "in", ":", "func", ":", "`", "print", "`", "function", ".", "\"", "\"", "\"", "if", "self", ".", "local_rank", "=", "=", "0", ":", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first\r\n        process in each machine.\r\n\r\n        Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "print", "something", "only", "on", "the", "first", "process", "if", "running", "on", "multiple", "machines", "it", "will", "print", "from", "the", "first", "process", "in", "each", "machine", "arguments", "passed", "to", "this", "method", "are", "forwarded", "to", "the", "python", "built", "in", "func", "print", "function"], "docstring_summary": "r\"\"\"Print something only on the first process. If running on multiple machines, it will print from the first", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 618, "end_line": 626, "hash": "a2acff912d4c82784474ae0c309c39c9", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "barrier", "original_string": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)", "language": "python", "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        self._strategy.barrier(name=name)", "code_tokens": ["def", "barrier", "(", "self", ",", "name", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Wait", "for", "all", "processes", "to", "enter", "this", "call", ".", "Use", "this", "to", "synchronize", "all", "parallel", "processes", ",", "but", "only", "if", "necessary", ",", "otherwise", "the", "overhead", "of", "synchronization", "will", "cause", "your", "program", "to", "slow", "down", ".", "This", "method", "needs", "to", "be", "called", "on", "all", "processes", ".", "Failing", "to", "do", "so", "will", "cause", "your", "program", "to", "stall", "forever", ".", "\"", "\"", "\"", "self", ".", "_validate_launched", "(", ")", "self", ".", "_strategy", ".", "barrier", "(", "name", "=", "name", ")"], "docstring": "Wait for all processes to enter this call.\r\n\r\n        Use this to synchronize all parallel processes, but only if necessary, otherwise the overhead of synchronization\r\n        will cause your program to slow down. This method needs to be called on all processes. Failing to do so will\r\n        cause your program to stall forever.", "docstring_tokens": ["wait", "for", "all", "processes", "to", "enter", "this", "call", "use", "this", "to", "synchronize", "all", "parallel", "processes", "but", "only", "if", "necessary", "otherwise", "the", "overhead", "of", "synchronization", "will", "cause", "your", "program", "to", "slow", "down", "this", "method", "needs", "to", "be", "called", "on", "all", "processes", "failing", "to", "do", "so", "will", "cause", "your", "program", "to", "stall", "forever"], "docstring_summary": "Wait for all processes to enter this call.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 628, "end_line": 637, "hash": "840fcaa9e0312e4d14c51df8c6947486", "complexity": 1, "parameters": ["name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "broadcast", "original_string": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)", "language": "python", "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.broadcast(obj, src=src)", "code_tokens": ["def", "broadcast", "(", "self", ",", "obj", ":", "TBroadcast", ",", "src", ":", "int", "=", "0", ")", "-", ">", "TBroadcast", ":", "r", "\"", "\"", "\"", "Send", "a", "tensor", "from", "one", "process", "to", "all", "others", ".", "This", "method", "needs", "to", "be", "called", "on", "all", "processes", ".", "Failing", "to", "do", "so", "will", "cause", "your", "program", "to", "stall", "forever", ".", "Args", ":", "obj", ":", "The", "object", "to", "broadcast", "to", "all", "other", "members", ".", "Any", "serializable", "object", "is", "supported", ",", "but", "it", "is", "most", "efficient", "with", "the", "object", "being", "a", ":", "class", ":", "`", "~", "torch", ".", "Tensor", "`", ".", "src", ":", "The", "(", "global", ")", "rank", "of", "the", "process", "that", "should", "send", "the", "data", "to", "all", "others", ".", "Return", ":", "The", "transferred", "data", ",", "the", "same", "value", "on", "every", "rank", ".", "\"", "\"", "\"", "self", ".", "_validate_launched", "(", ")", "return", "self", ".", "_strategy", ".", "broadcast", "(", "obj", ",", "src", "=", "src", ")"], "docstring": "r\"\"\"Send a tensor from one process to all others.\r\n\r\n        This method needs to be called on all processes. Failing to do so will cause your program to stall forever.\r\n\r\n        Args:\r\n            obj: The object to broadcast to all other members. Any serializable object is supported, but it is\r\n                most efficient with the object being a :class:`~torch.Tensor`.\r\n            src: The (global) rank of the process that should send the data to all others.\r\n\r\n        Return:\r\n            The transferred data, the same value on every rank.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "send", "a", "tensor", "from", "one", "process", "to", "all", "others", "this", "method", "needs", "to", "be", "called", "on", "all", "processes", "failing", "to", "do", "so", "will", "cause", "your", "program", "to", "stall", "forever", "args", "obj", "the", "object", "to", "broadcast", "to", "all", "other", "members", "any", "serializable", "object", "is", "supported", "but", "it", "is", "most", "efficient", "with", "the", "object", "being", "a", "class", "torch", "tensor", "src", "the", "global", "rank", "of", "the", "process", "that", "should", "send", "the", "data", "to", "all", "others", "return", "the", "transferred", "data", "the", "same", "value", "on", "every", "rank"], "docstring_summary": "r\"\"\"Send a tensor from one process to all others.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 639, "end_line": 654, "hash": "ebc2297485c22987eb6388143867467f", "complexity": 1, "parameters": ["obj", "src"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "all_gather", "original_string": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)", "language": "python", "code": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_gather, group=group, sync_grads=sync_grads)", "code_tokens": ["def", "all_gather", "(", "self", ",", "data", ":", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ":", "\"", "\"", "\"", "Gather", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes", ".", "This", "method", "needs", "to", "be", "called", "on", "all", "processes", "and", "the", "tensors", "need", "to", "have", "the", "same", "shape", "across", "all", "processes", ",", "otherwise", "your", "program", "will", "stall", "forever", ".", "Args", ":", "data", ":", "int", ",", "float", ",", "tensor", "of", "shape", "(", "batch", ",", ".", ".", ".", ")", ",", "or", "a", "(", "possibly", "nested", ")", "collection", "thereof", ".", "group", ":", "the", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", ".", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "`", "`", "all_gather", "`", "`", "operation", "Return", ":", "A", "tensor", "of", "shape", "(", "world_size", ",", "batch", ",", ".", ".", ".", ")", ",", "or", "if", "the", "input", "was", "a", "collection", "the", "output", "will", "also", "be", "a", "collection", "with", "tensors", "of", "this", "shape", ".", "For", "the", "special", "case", "where", "world_size", "is", "1", ",", "no", "additional", "dimension", "is", "added", "to", "the", "tensor", "(", "s", ")", ".", "\"", "\"", "\"", "self", ".", "_validate_launched", "(", ")", "group", "=", "group", "if", "group", "is", "not", "None", "else", "torch", ".", "distributed", ".", "group", ".", "WORLD", "data", "=", "convert_to_tensors", "(", "data", ",", "device", "=", "self", ".", "device", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "self", ".", "_strategy", ".", "all_gather", ",", "group", "=", "group", ",", "sync_grads", "=", "sync_grads", ")"], "docstring": "Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world).\r\n            sync_grads: flag that allows users to synchronize gradients for the ``all_gather`` operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).", "docstring_tokens": ["gather", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes", "this", "method", "needs", "to", "be", "called", "on", "all", "processes", "and", "the", "tensors", "need", "to", "have", "the", "same", "shape", "across", "all", "processes", "otherwise", "your", "program", "will", "stall", "forever", "args", "data", "int", "float", "tensor", "of", "shape", "batch", "or", "a", "possibly", "nested", "collection", "thereof", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all_gather", "operation", "return", "a", "tensor", "of", "shape", "world_size", "batch", "or", "if", "the", "input", "was", "a", "collection", "the", "output", "will", "also", "be", "a", "collection", "with", "tensors", "of", "this", "shape", "for", "the", "special", "case", "where", "world_size", "is", "1", "no", "additional", "dimension", "is", "added", "to", "the", "tensor", "s"], "docstring_summary": "Gather tensors or collections of tensors from multiple processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 656, "end_line": 678, "hash": "0d70befd28f86d07b85067ee602cd2be", "complexity": 2, "parameters": ["data", "dict", "list", "tuple]", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "all_reduce", "original_string": "def all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)", "language": "python", "code": "def all_reduce(\r\n        self,\r\n        data: Union[Tensor, dict, list, tuple],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        \"\"\"Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, self._strategy.all_reduce, group=group, reduce_op=reduce_op)", "code_tokens": ["def", "all_reduce", "(", "self", ",", "data", ":", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "\"", "mean", "\"", ",", ")", "-", ">", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ":", "\"", "\"", "\"", "Reduce", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes", ".", "The", "reduction", "on", "tensors", "is", "applied", "in", "-", "place", ",", "meaning", "the", "result", "will", "be", "placed", "back", "into", "the", "input", "tensor", ".", "This", "method", "needs", "to", "be", "called", "on", "all", "processes", "and", "the", "tensors", "need", "to", "have", "the", "same", "shape", "across", "all", "processes", ",", "otherwise", "your", "program", "will", "stall", "forever", ".", "Args", ":", "data", ":", "int", ",", "float", ",", "tensor", "of", "shape", "(", "batch", ",", ".", ".", ".", ")", ",", "or", "a", "(", "possibly", "nested", ")", "collection", "thereof", ".", "Tensor", "will", "be", "modified", "in", "-", "place", ".", "group", ":", "the", "process", "group", "to", "reduce", "results", "across", ".", "Defaults", "to", "all", "processes", "(", "world", ")", ".", "reduce_op", ":", "the", "reduction", "operation", ".", "Defaults", "to", "'", "mean", "'", ".", "Can", "also", "be", "a", "string", "'", "sum", "'", "or", "ReduceOp", ".", "Some", "strategies", "may", "limit", "the", "choices", "here", ".", "Return", ":", "A", "tensor", "of", "the", "same", "shape", "as", "the", "input", "with", "values", "reduced", "pointwise", "across", "processes", ".", "The", "same", "is", "applied", "to", "tensors", "in", "a", "collection", "if", "a", "collection", "is", "given", "as", "input", ".", "\"", "\"", "\"", "self", ".", "_validate_launched", "(", ")", "group", "=", "group", "if", "group", "is", "not", "None", "else", "torch", ".", "distributed", ".", "group", ".", "WORLD", "data", "=", "convert_to_tensors", "(", "data", ",", "device", "=", "self", ".", "device", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "self", ".", "_strategy", ".", "all_reduce", ",", "group", "=", "group", ",", "reduce_op", "=", "reduce_op", ")"], "docstring": "Reduce tensors or collections of tensors from multiple processes.\r\n\r\n        The reduction on tensors is applied in-place, meaning the result will be placed back into the input tensor.\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. Tensor will be\r\n                modified in-place.\r\n            group: the process group to reduce results across. Defaults to all processes (world).\r\n            reduce_op: the reduction operation. Defaults to 'mean'. Can also be a string 'sum' or ReduceOp.\r\n                Some strategies may limit the choices here.\r\n\r\n        Return:\r\n            A tensor of the same shape as the input with values reduced pointwise across processes. The same is\r\n            applied to tensors in a collection if a collection is given as input.", "docstring_tokens": ["reduce", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes", "the", "reduction", "on", "tensors", "is", "applied", "in", "place", "meaning", "the", "result", "will", "be", "placed", "back", "into", "the", "input", "tensor", "this", "method", "needs", "to", "be", "called", "on", "all", "processes", "and", "the", "tensors", "need", "to", "have", "the", "same", "shape", "across", "all", "processes", "otherwise", "your", "program", "will", "stall", "forever", "args", "data", "int", "float", "tensor", "of", "shape", "batch", "or", "a", "possibly", "nested", "collection", "thereof", "tensor", "will", "be", "modified", "in", "place", "group", "the", "process", "group", "to", "reduce", "results", "across", "defaults", "to", "all", "processes", "world", "reduce_op", "the", "reduction", "operation", "defaults", "to", "mean", "can", "also", "be", "a", "string", "sum", "or", "reduceop", "some", "strategies", "may", "limit", "the", "choices", "here", "return", "a", "tensor", "of", "the", "same", "shape", "as", "the", "input", "with", "values", "reduced", "pointwise", "across", "processes", "the", "same", "is", "applied", "to", "tensors", "in", "a", "collection", "if", "a", "collection", "is", "given", "as", "input"], "docstring_summary": "Reduce tensors or collections of tensors from multiple processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 680, "end_line": 707, "hash": "781e71c7a22d7e15059438e455757bee", "complexity": 2, "parameters": ["data", "dict", "list", "tuple]", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "rank_zero_first", "original_string": "def rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()", "language": "python", "code": "def rank_zero_first(self, local: bool = False) -> Generator:\r\n        r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"\r\n        rank = self.local_rank if local else self.global_rank\r\n        with _InfiniteBarrier() as barrier:\r\n            if rank > 0:\r\n                barrier()\r\n            yield\r\n            if rank == 0:\r\n                barrier()", "code_tokens": ["def", "rank_zero_first", "(", "self", ",", "local", ":", "bool", "=", "False", ")", "-", ">", "Generator", ":", "r", "\"", "\"", "\"", "The", "code", "block", "under", "this", "context", "manager", "gets", "executed", "first", "on", "the", "main", "process", "(", "rank", "0", ")", "and", "only", "when", "completed", ",", "the", "other", "processes", "get", "to", "run", "the", "code", "in", "parallel", ".", "Args", ":", "local", ":", "Set", "this", "to", "`", "`", "True", "`", "`", "if", "the", "*", "*", "local", "*", "*", "rank", "should", "be", "the", "one", "going", "first", ".", "Useful", "if", "you", "are", "downloading", "data", "and", "the", "filesystem", "isn", "'", "t", "shared", "between", "the", "nodes", ".", "Example", ":", ":", "with", "fabric", ".", "rank_zero_first", "(", ")", ":", "dataset", "=", "MNIST", "(", "\"", "datasets", "/", "\"", ",", "download", "=", "True", ")", "\"", "\"", "\"", "rank", "=", "self", ".", "local_rank", "if", "local", "else", "self", ".", "global_rank", "with", "_InfiniteBarrier", "(", ")", "as", "barrier", ":", "if", "rank", ">", "0", ":", "barrier", "(", ")", "yield", "if", "rank", "=", "=", "0", ":", "barrier", "(", ")"], "docstring": "r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when\r\n        completed, the other processes get to run the code in parallel.\r\n\r\n        Args:\r\n            local: Set this to ``True`` if the **local** rank should be the one going first. Useful if you are\r\n                downloading data and the filesystem isn't shared between the nodes.\r\n\r\n        Example::\r\n\r\n            with fabric.rank_zero_first():\r\n                dataset = MNIST(\"datasets/\", download=True)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "the", "code", "block", "under", "this", "context", "manager", "gets", "executed", "first", "on", "the", "main", "process", "rank", "0", "and", "only", "when", "completed", "the", "other", "processes", "get", "to", "run", "the", "code", "in", "parallel", "args", "local", "set", "this", "to", "true", "if", "the", "local", "rank", "should", "be", "the", "one", "going", "first", "useful", "if", "you", "are", "downloading", "data", "and", "the", "filesystem", "isn", "t", "shared", "between", "the", "nodes", "example", "with", "fabric", "rank_zero_first", "dataset", "mnist", "datasets", "download", "true"], "docstring_summary": "r\"\"\"The code block under this context manager gets executed first on the main process (rank 0) and only when", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 710, "end_line": 730, "hash": "0312f161ffb41e68592d117b7fa0c802", "complexity": 5, "parameters": ["local"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "no_backward_sync", "original_string": "def no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            # Accumulate gradients over 8 batches\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)", "language": "python", "code": "def no_backward_sync(self, module: _FabricModule, enabled: bool = True) -> AbstractContextManager:\r\n        r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            # Accumulate gradients over 8 batches\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"\r\n        module, _ = _unwrap_compiled(module)\r\n        if not isinstance(module, _FabricModule):\r\n            raise TypeError(\r\n                \"You need to set up the model first before you can call `fabric.no_backward_sync()`:\"\r\n                \" `model = fabric.setup(model, ...)`\"\r\n            )\r\n        if isinstance(self._strategy, (SingleDeviceStrategy, XLAStrategy)):\r\n            return nullcontext()\r\n        if self._strategy._backward_sync_control is None:\r\n            rank_zero_warn(\r\n                f\"The `{self._strategy.__class__.__name__}` does not support skipping the gradient synchronization.\"\r\n                f\" Remove `.no_backward_sync()` from your code or choose a different strategy.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            return nullcontext()\r\n\r\n        forward_module, _ = _unwrap_compiled(module._forward_module)\r\n        return self._strategy._backward_sync_control.no_backward_sync(forward_module, enabled)", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "_FabricModule", ",", "enabled", ":", "bool", "=", "True", ")", "-", ">", "AbstractContextManager", ":", "r", "\"", "\"", "\"", "Skip", "gradient", "synchronization", "during", "backward", "to", "avoid", "redundant", "communication", "overhead", ".", "Use", "this", "context", "manager", "when", "performing", "gradient", "accumulation", "to", "speed", "up", "training", "with", "multiple", "devices", ".", "Both", "the", "model", "'", "s", "`", "`", ".", "forward", "(", ")", "`", "`", "and", "the", "`", "`", "fabric", ".", "backward", "(", ")", "`", "`", "call", "need", "to", "run", "under", "this", "context", ".", "Args", ":", "module", ":", "The", "module", "for", "which", "to", "control", "the", "gradient", "synchronization", ".", "Must", "be", "a", "module", "that", "was", "set", "up", "with", ":", "meth", ":", "`", "setup", "`", "or", ":", "meth", ":", "`", "setup_module", "`", ".", "enabled", ":", "Whether", "the", "context", "manager", "is", "enabled", "or", "not", ".", "`", "`", "True", "`", "`", "means", "skip", "the", "sync", ",", "`", "`", "False", "`", "`", "means", "do", "not", "skip", ".", "Returns", ":", "A", "context", "manager", "that", "controls", "gradient", "synchronization", ".", "Raises", ":", "TypeError", ":", "If", "the", "module", "was", "not", "set", "up", "with", "Fabric", "first", ".", "Note", ":", "For", "strategies", "that", "don", "'", "t", "support", "gradient", "sync", "control", ",", "a", "warning", "is", "emitted", "and", "the", "context", "manager", "becomes", "a", "no", "-", "op", ".", "For", "single", "-", "device", "strategies", ",", "it", "is", "always", "a", "no", "-", "op", ".", "Example", ":", ":", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "dataloader", ")", ":", "with", "fabric", ".", "no_backward_sync", "(", "model", ",", "enabled", "=", "(", "batch_idx", "%", "8", "!", "=", "0", ")", ")", ":", "output", "=", "model", "(", "batch", ")", "loss", "=", "criterion", "(", "output", ",", "target", ")", "fabric", ".", "backward", "(", "loss", ")", "if", "batch_idx", "%", "8", "=", "=", "0", ":", "optimizer", ".", "step", "(", ")", "optimizer", ".", "zero_grad", "(", ")", "\"", "\"", "\"", "module", ",", "_", "=", "_unwrap_compiled", "(", "module", ")", "if", "not", "isinstance", "(", "module", ",", "_FabricModule", ")", ":", "raise", "TypeError", "(", "\"", "You", "need", "to", "set", "up", "the", "model", "first", "before", "you", "can", "call", "`", "fabric", ".", "no_backward_sync", "(", ")", "`", ":", "\"", "\"", "`", "model", "=", "fabric", ".", "setup", "(", "model", ",", ".", ".", ".", ")", "`", "\"", ")", "if", "isinstance", "(", "self", ".", "_strategy", ",", "(", "SingleDeviceStrategy", ",", "XLAStrategy", ")", ")", ":", "return", "nullcontext", "(", ")", "if", "self", ".", "_strategy", ".", "_backward_sync_control", "is", "None", ":", "rank_zero_warn", "(", "f", "\"", "The", "`", "{", "self", ".", "_strategy", ".", "__class__", ".", "__name__", "}", "`", "does", "not", "support", "skipping", "the", "gradient", "synchronization", ".", "\"", "f", "\"", "Remove", "`", ".", "no_backward_sync", "(", ")", "`", "from", "your", "code", "or", "choose", "a", "different", "strategy", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "return", "nullcontext", "(", ")", "forward_module", ",", "_", "=", "_unwrap_compiled", "(", "module", ".", "_forward_module", ")", "return", "self", ".", "_strategy", ".", "_backward_sync_control", ".", "no_backward_sync", "(", "forward_module", ",", "enabled", ")"], "docstring": "r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.\r\n\r\n        Use this context manager when performing gradient accumulation to speed up training with multiple devices.\r\n        Both the model's ``.forward()`` and the ``fabric.backward()`` call need to run under this context.\r\n\r\n        Args:\r\n            module: The module for which to control the gradient synchronization. Must be a module that was\r\n                set up with :meth:`setup` or :meth:`setup_module`.\r\n            enabled: Whether the context manager is enabled or not. ``True`` means skip the sync, ``False`` means do not\r\n                skip.\r\n\r\n        Returns:\r\n            A context manager that controls gradient synchronization.\r\n\r\n        Raises:\r\n            TypeError: If the module was not set up with Fabric first.\r\n\r\n        Note:\r\n            For strategies that don't support gradient sync control, a warning is emitted and the context manager\r\n            becomes a no-op. For single-device strategies, it is always a no-op.\r\n\r\n        Example::\r\n\r\n            # Accumulate gradients over 8 batches\r\n            for batch_idx, batch in enumerate(dataloader):\r\n                with fabric.no_backward_sync(model, enabled=(batch_idx % 8 != 0)):\r\n                    output = model(batch)\r\n                    loss = criterion(output, target)\r\n                    fabric.backward(loss)\r\n\r\n                if batch_idx % 8 == 0:\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "skip", "gradient", "synchronization", "during", "backward", "to", "avoid", "redundant", "communication", "overhead", "use", "this", "context", "manager", "when", "performing", "gradient", "accumulation", "to", "speed", "up", "training", "with", "multiple", "devices", "both", "the", "model", "s", "forward", "and", "the", "fabric", "backward", "call", "need", "to", "run", "under", "this", "context", "args", "module", "the", "module", "for", "which", "to", "control", "the", "gradient", "synchronization", "must", "be", "a", "module", "that", "was", "set", "up", "with", "meth", "setup", "or", "meth", "setup_module", "enabled", "whether", "the", "context", "manager", "is", "enabled", "or", "not", "true", "means", "skip", "the", "sync", "false", "means", "do", "not", "skip", "returns", "a", "context", "manager", "that", "controls", "gradient", "synchronization", "raises", "typeerror", "if", "the", "module", "was", "not", "set", "up", "with", "fabric", "first", "note", "for", "strategies", "that", "don", "t", "support", "gradient", "sync", "control", "a", "warning", "is", "emitted", "and", "the", "context", "manager", "becomes", "a", "no", "op", "for", "single", "device", "strategies", "it", "is", "always", "a", "no", "op", "example", "accumulate", "gradients", "over", "8", "batches", "for", "batch_idx", "batch", "in", "enumerate", "dataloader", "with", "fabric", "no_backward_sync", "model", "enabled", "batch_idx", "8", "0", "output", "model", "batch", "loss", "criterion", "output", "target", "fabric", "backward", "loss", "if", "batch_idx", "8", "0", "optimizer", "step", "optimizer", "zero_grad"], "docstring_summary": "r\"\"\"Skip gradient synchronization during backward to avoid redundant communication overhead.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 732, "end_line": 785, "hash": "e444c4224597f6f2169202c18cee78a3", "complexity": 4, "parameters": ["module", "enabled"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "sharded_model", "original_string": "def sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()", "language": "python", "code": "def sharded_model(self) -> AbstractContextManager:\r\n        r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"\r\n        rank_zero_deprecation(\"`Fabric.sharded_model()` is deprecated in favor of `Fabric.init_module()`.\")\r\n        self._validate_launched()\r\n        if isinstance(self.strategy, _Sharded):\r\n            return self.strategy.module_sharded_context()\r\n        return nullcontext()", "code_tokens": ["def", "sharded_model", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "r", "\"", "\"", "\"", "Instantiate", "a", "model", "under", "this", "context", "manager", "to", "prepare", "it", "for", "model", "-", "parallel", "sharding", ".", ".", ".", "deprecated", ":", ":", "This", "context", "manager", "is", "deprecated", "in", "favor", "of", ":", "meth", ":", "`", "init_module", "`", ",", "use", "it", "instead", ".", "\"", "\"", "\"", "rank_zero_deprecation", "(", "\"", "`", "Fabric", ".", "sharded_model", "(", ")", "`", "is", "deprecated", "in", "favor", "of", "`", "Fabric", ".", "init_module", "(", ")", "`", ".", "\"", ")", "self", ".", "_validate_launched", "(", ")", "if", "isinstance", "(", "self", ".", "strategy", ",", "_Sharded", ")", ":", "return", "self", ".", "strategy", ".", "module_sharded_context", "(", ")", "return", "nullcontext", "(", ")"], "docstring": "r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.\r\n\r\n        .. deprecated:: This context manager is deprecated in favor of :meth:`init_module`, use it instead.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "instantiate", "a", "model", "under", "this", "context", "manager", "to", "prepare", "it", "for", "model", "parallel", "sharding", "deprecated", "this", "context", "manager", "is", "deprecated", "in", "favor", "of", "meth", "init_module", "use", "it", "instead"], "docstring_summary": "r\"\"\"Instantiate a model under this context manager to prepare it for model-parallel sharding.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 787, "end_line": 797, "hash": "ef575449c774f03f5eac1af8686f5079", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "init_tensor", "original_string": "def init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()", "language": "python", "code": "def init_tensor(self) -> AbstractContextManager:\r\n        \"\"\"Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.\"\"\"\r\n        return self._strategy.tensor_init_context()", "code_tokens": ["def", "init_tensor", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Tensors", "that", "you", "instantiate", "under", "this", "context", "manager", "will", "be", "created", "on", "the", "device", "right", "away", "and", "have", "the", "right", "data", "type", "depending", "on", "the", "precision", "setting", "in", "Fabric", ".", "\"", "\"", "\"", "return", "self", ".", "_strategy", ".", "tensor_init_context", "(", ")"], "docstring": "Tensors that you instantiate under this context manager will be created on the device right away and have\r\n        the right data type depending on the precision setting in Fabric.", "docstring_tokens": ["tensors", "that", "you", "instantiate", "under", "this", "context", "manager", "will", "be", "created", "on", "the", "device", "right", "away", "and", "have", "the", "right", "data", "type", "depending", "on", "the", "precision", "setting", "in", "fabric"], "docstring_summary": "Tensors that you instantiate under this context manager will be created on the device right away and have", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 799, "end_line": 802, "hash": "c863a84b3e4517a33dfafb7534da3d52", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "init_module", "original_string": "def init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)", "language": "python", "code": "def init_module(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.\r\n\r\n        \"\"\"\r\n        self._validate_launched()\r\n        return self._strategy.module_init_context(empty_init=empty_init)", "code_tokens": ["def", "init_module", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Instantiate", "the", "model", "and", "its", "parameters", "under", "this", "context", "manager", "to", "reduce", "peak", "memory", "usage", ".", "The", "parameters", "get", "created", "on", "the", "device", "and", "with", "the", "right", "data", "type", "right", "away", "without", "wasting", "memory", "being", "allocated", "unnecessarily", ".", "Args", ":", "empty_init", ":", "Whether", "to", "initialize", "the", "model", "with", "empty", "weights", "(", "uninitialized", "memory", ")", ".", "If", "`", "`", "None", "`", "`", ",", "the", "strategy", "will", "decide", ".", "Some", "strategies", "may", "not", "support", "all", "options", ".", "Set", "this", "to", "`", "`", "True", "`", "`", "if", "you", "are", "loading", "a", "checkpoint", "into", "a", "large", "model", ".", "\"", "\"", "\"", "self", ".", "_validate_launched", "(", ")", "return", "self", ".", "_strategy", ".", "module_init_context", "(", "empty_init", "=", "empty_init", ")"], "docstring": "Instantiate the model and its parameters under this context manager to reduce peak memory usage.\r\n\r\n        The parameters get created on the device and with the right data type right away without wasting memory being\r\n        allocated unnecessarily.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n                Set this to ``True`` if you are loading a checkpoint into a large model.", "docstring_tokens": ["instantiate", "the", "model", "and", "its", "parameters", "under", "this", "context", "manager", "to", "reduce", "peak", "memory", "usage", "the", "parameters", "get", "created", "on", "the", "device", "and", "with", "the", "right", "data", "type", "right", "away", "without", "wasting", "memory", "being", "allocated", "unnecessarily", "args", "empty_init", "whether", "to", "initialize", "the", "model", "with", "empty", "weights", "uninitialized", "memory", "if", "none", "the", "strategy", "will", "decide", "some", "strategies", "may", "not", "support", "all", "options", "set", "this", "to", "true", "if", "you", "are", "loading", "a", "checkpoint", "into", "a", "large", "model"], "docstring_summary": "Instantiate the model and its parameters under this context manager to reduce peak memory usage.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 804, "end_line": 817, "hash": "ec4d0d6166c399820af7efd99e907d16", "complexity": 1, "parameters": ["empty_init"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "save", "original_string": "def save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            # With filter\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()", "language": "python", "code": "def save(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: dict[str, Union[nn.Module, Optimizer, Any]],\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            # With filter\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"\r\n        if filter is not None:\r\n            if not isinstance(filter, dict):\r\n                raise TypeError(f\"Filter should be a dictionary, given {filter!r}\")\r\n            if not set(filter).issubset(state):\r\n                raise ValueError(\r\n                    f\"The filter keys {filter.keys() - state} are not present in the state keys {set(state)}.\"\r\n                )\r\n            for k, v in filter.items():\r\n                if not callable(v):\r\n                    raise TypeError(f\"Expected `fabric.save(filter=...)` for key {k!r} to be a callable, given {v!r}\")\r\n        self._strategy.save_checkpoint(path=path, state=_unwrap_objects(state), filter=filter)\r\n        self.barrier()", "code_tokens": ["def", "save", "(", "self", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "nn", ".", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Save", "checkpoint", "contents", "to", "a", "file", ".", "How", "and", "which", "processes", "save", "gets", "determined", "by", "the", "`", "strategy", "`", ".", "For", "example", ",", "the", "`", "ddp", "`", "strategy", "saves", "checkpoints", "only", "on", "process", "0", ",", "while", "the", "`", "fsdp", "`", "strategy", "saves", "files", "from", "every", "rank", ".", "This", "method", "must", "be", "called", "on", "all", "processes", "!", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "(", "s", ")", "should", "be", "saved", ".", "state", ":", "A", "dictionary", "with", "contents", "to", "be", "saved", ".", "If", "the", "dict", "contains", "modules", "or", "optimizers", ",", "their", "state", "-", "dict", "will", "be", "retrieved", "and", "converted", "automatically", ".", "filter", ":", "An", "optional", "dictionary", "containing", "filter", "callables", "that", "return", "a", "boolean", "indicating", "whether", "the", "given", "item", "should", "be", "saved", "(", "`", "`", "True", "`", "`", ")", "or", "filtered", "out", "(", "`", "`", "False", "`", "`", ")", ".", "Each", "filter", "key", "should", "match", "a", "state", "key", ",", "where", "its", "filter", "will", "be", "applied", "to", "the", "`", "`", "state_dict", "`", "`", "generated", ".", "Raises", ":", "TypeError", ":", "If", "filter", "is", "not", "a", "dictionary", "or", "contains", "non", "-", "callable", "values", ".", "ValueError", ":", "If", "filter", "keys", "don", "'", "t", "match", "state", "keys", ".", "Example", ":", ":", "state", "=", "{", "\"", "model", "\"", ":", "model", ",", "\"", "optimizer", "\"", ":", "optimizer", ",", "\"", "epoch", "\"", ":", "epoch", "}", "fabric", ".", "save", "(", "\"", "checkpoint", ".", "pth", "\"", ",", "state", ")", "def", "param_filter", "(", "name", ",", "param", ")", ":", "return", "\"", "bias", "\"", "not", "in", "name", "fabric", ".", "save", "(", "\"", "checkpoint", ".", "pth", "\"", ",", "state", ",", "filter", "=", "{", "\"", "model", "\"", ":", "param_filter", "}", ")", "\"", "\"", "\"", "if", "filter", "is", "not", "None", ":", "if", "not", "isinstance", "(", "filter", ",", "dict", ")", ":", "raise", "TypeError", "(", "f", "\"", "Filter", "should", "be", "a", "dictionary", ",", "given", "{", "filter", "!", "r", "}", "\"", ")", "if", "not", "set", "(", "filter", ")", ".", "issubset", "(", "state", ")", ":", "raise", "ValueError", "(", "f", "\"", "The", "filter", "keys", "{", "filter", ".", "keys", "(", ")", "-", "state", "}", "are", "not", "present", "in", "the", "state", "keys", "{", "set", "(", "state", ")", "}", ".", "\"", ")", "for", "k", ",", "v", "in", "filter", ".", "items", "(", ")", ":", "if", "not", "callable", "(", "v", ")", ":", "raise", "TypeError", "(", "f", "\"", "Expected", "`", "fabric", ".", "save", "(", "filter", "=", ".", ".", ".", ")", "`", "for", "key", "{", "k", "!", "r", "}", "to", "be", "a", "callable", ",", "given", "{", "v", "!", "r", "}", "\"", ")", "self", ".", "_strategy", ".", "save_checkpoint", "(", "path", "=", "path", ",", "state", "=", "_unwrap_objects", "(", "state", ")", ",", "filter", "=", "filter", ")", "self", ".", "barrier", "(", ")"], "docstring": "r\"\"\"Save checkpoint contents to a file.\r\n\r\n        How and which processes save gets determined by the `strategy`. For example, the `ddp` strategy\r\n        saves checkpoints only on process 0, while the `fsdp` strategy saves files from every rank.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved.\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        Raises:\r\n            TypeError: If filter is not a dictionary or contains non-callable values.\r\n            ValueError: If filter keys don't match state keys.\r\n\r\n        Example::\r\n\r\n            state = {\"model\": model, \"optimizer\": optimizer, \"epoch\": epoch}\r\n            fabric.save(\"checkpoint.pth\", state)\r\n\r\n            # With filter\r\n            def param_filter(name, param):\r\n                return \"bias\" not in name  # Save only non-bias parameters\r\n\r\n            fabric.save(\"checkpoint.pth\", state, filter={\"model\": param_filter})\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "save", "checkpoint", "contents", "to", "a", "file", "how", "and", "which", "processes", "save", "gets", "determined", "by", "the", "strategy", "for", "example", "the", "ddp", "strategy", "saves", "checkpoints", "only", "on", "process", "0", "while", "the", "fsdp", "strategy", "saves", "files", "from", "every", "rank", "this", "method", "must", "be", "called", "on", "all", "processes", "args", "path", "a", "path", "to", "where", "the", "file", "s", "should", "be", "saved", "state", "a", "dictionary", "with", "contents", "to", "be", "saved", "if", "the", "dict", "contains", "modules", "or", "optimizers", "their", "state", "dict", "will", "be", "retrieved", "and", "converted", "automatically", "filter", "an", "optional", "dictionary", "containing", "filter", "callables", "that", "return", "a", "boolean", "indicating", "whether", "the", "given", "item", "should", "be", "saved", "true", "or", "filtered", "out", "false", "each", "filter", "key", "should", "match", "a", "state", "key", "where", "its", "filter", "will", "be", "applied", "to", "the", "state_dict", "generated", "raises", "typeerror", "if", "filter", "is", "not", "a", "dictionary", "or", "contains", "non", "callable", "values", "valueerror", "if", "filter", "keys", "don", "t", "match", "state", "keys", "example", "state", "model", "model", "optimizer", "optimizer", "epoch", "epoch", "fabric", "save", "checkpoint", "pth", "state", "with", "filter", "def", "param_filter", "name", "param", "return", "bias", "not", "in", "name", "save", "only", "non", "bias", "parameters", "fabric", "save", "checkpoint", "pth", "state", "filter", "model", "param_filter"], "docstring_summary": "r\"\"\"Save checkpoint contents to a file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 819, "end_line": 866, "hash": "1cc9bf9e39c8d95f24df817b36216676", "complexity": 6, "parameters": ["path", "Path]", "state", "Union[nn.Module", "Optimizer", "Any]]", "filter", "Callable[[str", "Any]", "bool]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "load", "original_string": "def load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            # Load full checkpoint\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            # Load into existing objects\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            # We need to unwrap objects (see above) but this creates a new dictionary. In-place updates\r\n            # (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder", "language": "python", "code": "def load(\r\n        self,\r\n        path: Union[str, Path],\r\n        state: Optional[dict[str, Union[nn.Module, Optimizer, Any]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            # Load full checkpoint\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            # Load into existing objects\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)\r\n\r\n        \"\"\"\r\n        unwrapped_state = _unwrap_objects(state)\r\n        remainder = self._strategy.load_checkpoint(path=path, state=unwrapped_state, strict=strict)\r\n        self.barrier()\r\n        if state is not None:\r\n            # We need to unwrap objects (see above) but this creates a new dictionary. In-place updates\r\n            # (for user metadata) wouldn't show up in the original dict, so we need to copy the data back.\r\n            for k in list(unwrapped_state.keys()):\r\n                obj, _ = _unwrap_compiled(state[k])\r\n                if isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader)):\r\n                    continue\r\n                state[k] = unwrapped_state[k]\r\n        return remainder", "code_tokens": ["def", "load", "(", "self", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "state", ":", "Optional", "[", "dict", "[", "str", ",", "Union", "[", "nn", ".", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "a", "checkpoint", "from", "a", "file", "and", "restore", "the", "state", "of", "objects", "(", "modules", ",", "optimizers", ",", "etc", ".", ")", "How", "and", "which", "processes", "load", "gets", "determined", "by", "the", "`", "strategy", "`", ".", "This", "method", "must", "be", "called", "on", "all", "processes", "!", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "is", "located", ".", "state", ":", "A", "dictionary", "of", "objects", "whose", "state", "will", "be", "restored", "in", "-", "place", "from", "the", "checkpoint", "path", ".", "If", "no", "state", "is", "given", ",", "then", "the", "checkpoint", "will", "be", "returned", "in", "full", ".", "strict", ":", "Whether", "to", "enforce", "that", "the", "keys", "in", "`", "state", "`", "match", "the", "keys", "in", "the", "checkpoint", ".", "Returns", ":", "The", "remaining", "items", "that", "were", "not", "restored", "into", "the", "given", "state", "dictionary", ".", "If", "no", "state", "dictionary", "is", "given", ",", "the", "full", "checkpoint", "will", "be", "returned", ".", "Example", ":", ":", "checkpoint", "=", "fabric", ".", "load", "(", "\"", "checkpoint", ".", "pth", "\"", ")", "state", "=", "{", "\"", "model", "\"", ":", "model", ",", "\"", "optimizer", "\"", ":", "optimizer", "}", "remainder", "=", "fabric", ".", "load", "(", "\"", "checkpoint", ".", "pth", "\"", ",", "state", ")", "epoch", "=", "remainder", ".", "get", "(", "\"", "epoch", "\"", ",", "0", ")", "\"", "\"", "\"", "unwrapped_state", "=", "_unwrap_objects", "(", "state", ")", "remainder", "=", "self", ".", "_strategy", ".", "load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "unwrapped_state", ",", "strict", "=", "strict", ")", "self", ".", "barrier", "(", ")", "if", "state", "is", "not", "None", ":", "for", "k", "in", "list", "(", "unwrapped_state", ".", "keys", "(", ")", ")", ":", "obj", ",", "_", "=", "_unwrap_compiled", "(", "state", "[", "k", "]", ")", "if", "isinstance", "(", "obj", ",", "(", "_FabricModule", ",", "_FabricOptimizer", ",", "_FabricDataLoader", ")", ")", ":", "continue", "state", "[", "k", "]", "=", "unwrapped_state", "[", "k", "]", "return", "remainder"], "docstring": "Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)\r\n\r\n        How and which processes load gets determined by the `strategy`.\r\n        This method must be called on all processes!\r\n\r\n        Args:\r\n            path: A path to where the file is located.\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                If no state is given, then the checkpoint will be returned in full.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        Example::\r\n\r\n            # Load full checkpoint\r\n            checkpoint = fabric.load(\"checkpoint.pth\")\r\n\r\n            # Load into existing objects\r\n            state = {\"model\": model, \"optimizer\": optimizer}\r\n            remainder = fabric.load(\"checkpoint.pth\", state)\r\n            epoch = remainder.get(\"epoch\", 0)", "docstring_tokens": ["load", "a", "checkpoint", "from", "a", "file", "and", "restore", "the", "state", "of", "objects", "modules", "optimizers", "etc", "how", "and", "which", "processes", "load", "gets", "determined", "by", "the", "strategy", "this", "method", "must", "be", "called", "on", "all", "processes", "args", "path", "a", "path", "to", "where", "the", "file", "is", "located", "state", "a", "dictionary", "of", "objects", "whose", "state", "will", "be", "restored", "in", "place", "from", "the", "checkpoint", "path", "if", "no", "state", "is", "given", "then", "the", "checkpoint", "will", "be", "returned", "in", "full", "strict", "whether", "to", "enforce", "that", "the", "keys", "in", "state", "match", "the", "keys", "in", "the", "checkpoint", "returns", "the", "remaining", "items", "that", "were", "not", "restored", "into", "the", "given", "state", "dictionary", "if", "no", "state", "dictionary", "is", "given", "the", "full", "checkpoint", "will", "be", "returned", "example", "load", "full", "checkpoint", "checkpoint", "fabric", "load", "checkpoint", "pth", "load", "into", "existing", "objects", "state", "model", "model", "optimizer", "optimizer", "remainder", "fabric", "load", "checkpoint", "pth", "state", "epoch", "remainder", "get", "epoch", "0"], "docstring_summary": "Load a checkpoint from a file and restore the state of objects (modules, optimizers, etc.)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 868, "end_line": 911, "hash": "c7fc697708c467949c7fbfc53ebe9b9a", "complexity": 4, "parameters": ["path", "Path]", "state", "Union[nn.Module", "Optimizer", "Any]]]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "load_raw", "original_string": "def load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)", "language": "python", "code": "def load_raw(self, path: Union[str, Path], obj: Union[nn.Module, Optimizer], strict: bool = True) -> None:\r\n        \"\"\"Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.\r\n\r\n        \"\"\"\r\n        obj = _unwrap_objects(obj)\r\n        self._strategy.load_checkpoint(path=path, state=obj, strict=strict)", "code_tokens": ["def", "load_raw", "(", "self", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "obj", ":", "Union", "[", "nn", ".", "Module", ",", "Optimizer", "]", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Load", "the", "state", "of", "a", "module", "or", "optimizer", "from", "a", "single", "state", "-", "dict", "file", ".", "Use", "this", "for", "loading", "a", "raw", "PyTorch", "model", "checkpoint", "created", "without", "Fabric", ".", "This", "is", "conceptually", "equivalent", "to", "`", "`", "obj", ".", "load_state_dict", "(", "torch", ".", "load", "(", "path", ")", ")", "`", "`", ",", "but", "is", "agnostic", "to", "the", "strategy", "being", "used", ".", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "is", "located", "obj", ":", "A", ":", "class", ":", "`", "~", "torch", ".", "nn", ".", "Module", "`", "or", ":", "class", ":", "`", "~", "torch", ".", "optim", ".", "Optimizer", "`", "instance", ".", "strict", ":", "Whether", "to", "enforce", "that", "the", "keys", "in", "the", "module", "'", "s", "state", "-", "dict", "match", "the", "keys", "in", "the", "checkpoint", ".", "Does", "not", "apply", "to", "optimizers", ".", "\"", "\"", "\"", "obj", "=", "_unwrap_objects", "(", "obj", ")", "self", ".", "_strategy", ".", "load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "obj", ",", "strict", "=", "strict", ")"], "docstring": "Load the state of a module or optimizer from a single state-dict file.\r\n\r\n        Use this for loading a raw PyTorch model checkpoint created without Fabric.\r\n        This is conceptually equivalent to ``obj.load_state_dict(torch.load(path))``, but is agnostic to the strategy\r\n        being used.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            obj: A :class:`~torch.nn.Module` or :class:`~torch.optim.Optimizer` instance.\r\n            strict: Whether to enforce that the keys in the module's state-dict match the keys in the checkpoint.\r\n                Does not apply to optimizers.", "docstring_tokens": ["load", "the", "state", "of", "a", "module", "or", "optimizer", "from", "a", "single", "state", "dict", "file", "use", "this", "for", "loading", "a", "raw", "pytorch", "model", "checkpoint", "created", "without", "fabric", "this", "is", "conceptually", "equivalent", "to", "obj", "load_state_dict", "torch", "load", "path", "but", "is", "agnostic", "to", "the", "strategy", "being", "used", "args", "path", "a", "path", "to", "where", "the", "file", "is", "located", "obj", "a", "class", "torch", "nn", "module", "or", "class", "torch", "optim", "optimizer", "instance", "strict", "whether", "to", "enforce", "that", "the", "keys", "in", "the", "module", "s", "state", "dict", "match", "the", "keys", "in", "the", "checkpoint", "does", "not", "apply", "to", "optimizers"], "docstring_summary": "Load the state of a module or optimizer from a single state-dict file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 913, "end_line": 928, "hash": "7807e929c337703a8661d8312c610b13", "complexity": 1, "parameters": ["path", "Path]", "obj", "Optimizer]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "launch", "original_string": "def launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n                # ... training code ...\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)", "language": "python", "code": "def launch(self, function: Callable[[\"Fabric\"], Any] = _do_nothing, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n                # ... training code ...\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)\r\n\r\n        \"\"\"\r\n        if _is_using_cli():\r\n            raise RuntimeError(\r\n                \"This script was launched through the CLI, and processes have already been created. Calling \"\r\n                \" `.launch()` again is not allowed.\"\r\n            )\r\n        if function is not _do_nothing:\r\n            if not callable(function):\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(...)` needs to be a callable, but got {function}.\"\r\n                    \" HINT: do `.launch(your_fn)` instead of `.launch(your_fn())`\"\r\n                )\r\n            if not inspect.signature(function).parameters:\r\n                raise TypeError(\r\n                    f\"`Fabric.launch(function={function})` needs to take at least one argument. The launcher will\"\r\n                    \" pass in the `Fabric` object so you can use it inside the function.\"\r\n                )\r\n        elif isinstance(self.strategy.launcher, (_MultiProcessingLauncher, _XLALauncher)):\r\n            raise TypeError(\r\n                f\"To spawn processes with the `{type(self.strategy).__name__}` strategy, `.launch()` needs to be called\"\r\n                \" with a function that contains the code to launch in processes.\"\r\n            )\r\n        return self._wrap_and_launch(function, self, *args, **kwargs)", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", "[", "[", "\"", "Fabric", "\"", "]", ",", "Any", "]", "=", "_do_nothing", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Launch", "and", "initialize", "all", "the", "processes", "needed", "for", "distributed", "execution", ".", "Args", ":", "function", ":", "Optional", "function", "to", "launch", "when", "using", "a", "spawn", "/", "fork", "-", "based", "strategy", ",", "for", "example", ",", "when", "using", "the", "XLA", "strategy", "(", "`", "`", "accelerator", "=", "\"", "tpu", "\"", "`", "`", ")", ".", "The", "function", "must", "accept", "at", "least", "one", "argument", ",", "to", "which", "the", "Fabric", "object", "itself", "will", "be", "passed", ".", "If", "not", "provided", ",", "only", "process", "initialization", "will", "be", "performed", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "function", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "function", ".", "Returns", ":", "Returns", "the", "output", "of", "the", "function", "that", "ran", "in", "worker", "process", "with", "rank", "0", ".", "Raises", ":", "RuntimeError", ":", "If", "called", "when", "script", "was", "launched", "through", "the", "CLI", ".", "TypeError", ":", "If", "function", "is", "provided", "but", "not", "callable", ",", "or", "if", "function", "doesn", "'", "t", "accept", "required", "arguments", ".", "Note", ":", "The", "`", "`", "launch", "(", ")", "`", "`", "method", "should", "only", "be", "used", "if", "you", "intend", "to", "specify", "accelerator", ",", "devices", ",", "and", "so", "on", "in", "the", "code", "(", "programmatically", ")", ".", "If", "you", "are", "launching", "with", "the", "Lightning", "CLI", ",", "`", "`", "fabric", "run", ".", ".", ".", "`", "`", ",", "remove", "`", "`", "launch", "(", ")", "`", "`", "from", "your", "code", ".", "The", "`", "`", "launch", "(", ")", "`", "`", "is", "a", "no", "-", "op", "when", "called", "multiple", "times", "and", "no", "function", "is", "passed", "in", ".", "Example", ":", ":", "def", "train_function", "(", "fabric", ")", ":", "model", ",", "optimizer", "=", "fabric", ".", "setup", "(", "model", ",", "optimizer", ")", "fabric", "=", "Fabric", "(", "accelerator", "=", "\"", "tpu", "\"", ",", "devices", "=", "8", ")", "fabric", ".", "launch", "(", "train_function", ")", "\"", "\"", "\"", "if", "_is_using_cli", "(", ")", ":", "raise", "RuntimeError", "(", "\"", "This", "script", "was", "launched", "through", "the", "CLI", ",", "and", "processes", "have", "already", "been", "created", ".", "Calling", "\"", "\"", "`", ".", "launch", "(", ")", "`", "again", "is", "not", "allowed", ".", "\"", ")", "if", "function", "is", "not", "_do_nothing", ":", "if", "not", "callable", "(", "function", ")", ":", "raise", "TypeError", "(", "f", "\"", "`", "Fabric", ".", "launch", "(", ".", ".", ".", ")", "`", "needs", "to", "be", "a", "callable", ",", "but", "got", "{", "function", "}", ".", "\"", "\"", "HINT", ":", "do", "`", ".", "launch", "(", "your_fn", ")", "`", "instead", "of", "`", ".", "launch", "(", "your_fn", "(", ")", ")", "`", "\"", ")", "if", "not", "inspect", ".", "signature", "(", "function", ")", ".", "parameters", ":", "raise", "TypeError", "(", "f", "\"", "`", "Fabric", ".", "launch", "(", "function", "=", "{", "function", "}", ")", "`", "needs", "to", "take", "at", "least", "one", "argument", ".", "The", "launcher", "will", "\"", "\"", "pass", "in", "the", "`", "Fabric", "`", "object", "so", "you", "can", "use", "it", "inside", "the", "function", ".", "\"", ")", "elif", "isinstance", "(", "self", ".", "strategy", ".", "launcher", ",", "(", "_MultiProcessingLauncher", ",", "_XLALauncher", ")", ")", ":", "raise", "TypeError", "(", "f", "\"", "To", "spawn", "processes", "with", "the", "`", "{", "type", "(", "self", ".", "strategy", ")", ".", "__name__", "}", "`", "strategy", ",", "`", ".", "launch", "(", ")", "`", "needs", "to", "be", "called", "\"", "\"", "with", "a", "function", "that", "contains", "the", "code", "to", "launch", "in", "processes", ".", "\"", ")", "return", "self", ".", "_wrap_and_launch", "(", "function", ",", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Launch and initialize all the processes needed for distributed execution.\r\n\r\n        Args:\r\n            function: Optional function to launch when using a spawn/fork-based strategy, for example, when using the\r\n                XLA strategy (``accelerator=\"tpu\"``). The function must accept at least one argument, to which\r\n                the Fabric object itself will be passed. If not provided, only process initialization will be performed.\r\n            *args: Optional positional arguments to be passed to the function.\r\n            **kwargs: Optional keyword arguments to be passed to the function.\r\n\r\n        Returns:\r\n            Returns the output of the function that ran in worker process with rank 0.\r\n\r\n        Raises:\r\n            RuntimeError: If called when script was launched through the CLI.\r\n            TypeError: If function is provided but not callable, or if function doesn't accept required arguments.\r\n\r\n        Note:\r\n            The ``launch()`` method should only be used if you intend to specify accelerator, devices, and so on in\r\n            the code (programmatically). If you are launching with the Lightning CLI, ``fabric run ...``, remove\r\n            ``launch()`` from your code.\r\n\r\n            The ``launch()`` is a no-op when called multiple times and no function is passed in.\r\n\r\n        Example::\r\n\r\n            def train_function(fabric):\r\n                model, optimizer = fabric.setup(model, optimizer)\r\n                # ... training code ...\r\n\r\n            fabric = Fabric(accelerator=\"tpu\", devices=8)\r\n            fabric.launch(train_function)", "docstring_tokens": ["launch", "and", "initialize", "all", "the", "processes", "needed", "for", "distributed", "execution", "args", "function", "optional", "function", "to", "launch", "when", "using", "a", "spawn", "fork", "based", "strategy", "for", "example", "when", "using", "the", "xla", "strategy", "accelerator", "tpu", "the", "function", "must", "accept", "at", "least", "one", "argument", "to", "which", "the", "fabric", "object", "itself", "will", "be", "passed", "if", "not", "provided", "only", "process", "initialization", "will", "be", "performed", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "function", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "function", "returns", "returns", "the", "output", "of", "the", "function", "that", "ran", "in", "worker", "process", "with", "rank", "0", "raises", "runtimeerror", "if", "called", "when", "script", "was", "launched", "through", "the", "cli", "typeerror", "if", "function", "is", "provided", "but", "not", "callable", "or", "if", "function", "doesn", "t", "accept", "required", "arguments", "note", "the", "launch", "method", "should", "only", "be", "used", "if", "you", "intend", "to", "specify", "accelerator", "devices", "and", "so", "on", "in", "the", "code", "programmatically", "if", "you", "are", "launching", "with", "the", "lightning", "cli", "fabric", "run", "remove", "launch", "from", "your", "code", "the", "launch", "is", "a", "no", "op", "when", "called", "multiple", "times", "and", "no", "function", "is", "passed", "in", "example", "def", "train_function", "fabric", "model", "optimizer", "fabric", "setup", "model", "optimizer", "training", "code", "fabric", "fabric", "accelerator", "tpu", "devices", "8", "fabric", "launch", "train_function"], "docstring_summary": "Launch and initialize all the processes needed for distributed execution.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 930, "end_line": 985, "hash": "1e362e90efbe6a8b51fc0e33daeaa08c", "complexity": 6, "parameters": ["function", "Any]", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "call", "original_string": "def call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r\n            # TODO(fabric): handle the following signatures\r\n            # method(self, fabric|trainer, x, y=1)\r\n            # method(self, fabric|trainer, *args, x, y=1)\r\n            # method(self, *args, y=1)\r\n            # method(self, *args, **kwargs)\r", "language": "python", "code": "def call(self, hook_name: str, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"\r\n        for callback in self._callbacks:\r\n            method = getattr(callback, hook_name, None)\r\n            if method is None:\r\n                continue\r\n            if not callable(method):\r\n                rank_zero_warn(\r\n                    f\"Skipping the callback `{type(callback).__name__}.{hook_name}` because it is not callable.\"\r\n                )\r\n                continue\r\n\r\n            method(*args, **kwargs)\r\n\r\n            # TODO(fabric): handle the following signatures\r\n            # method(self, fabric|trainer, x, y=1)\r\n            # method(self, fabric|trainer, *args, x, y=1)\r\n            # method(self, *args, y=1)\r\n            # method(self, *args, **kwargs)\r", "code_tokens": ["def", "call", "(", "self", ",", "hook_name", ":", "str", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Trigger", "the", "callback", "methods", "with", "the", "given", "name", "and", "arguments", ".", "Not", "all", "objects", "registered", "via", "`", "`", "Fabric", "(", "callbacks", "=", ".", ".", ".", ")", "`", "`", "must", "implement", "a", "method", "with", "the", "given", "name", ".", "The", "ones", "that", "have", "a", "matching", "method", "name", "will", "get", "called", ".", "Args", ":", "hook_name", ":", "The", "name", "of", "the", "callback", "method", ".", "*", "args", ":", "Optional", "positional", "arguments", "that", "get", "passed", "down", "to", "the", "callback", "method", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "that", "get", "passed", "down", "to", "the", "callback", "method", ".", "Example", ":", ":", "class", "MyCallback", ":", "def", "on_train_epoch_end", "(", "self", ",", "results", ")", ":", ".", ".", ".", "fabric", "=", "Fabric", "(", "callbacks", "=", "[", "MyCallback", "(", ")", "]", ")", "fabric", ".", "call", "(", "\"", "on_train_epoch_end", "\"", ",", "results", "=", "{", ".", ".", ".", "}", ")", "\"", "\"", "\"", "for", "callback", "in", "self", ".", "_callbacks", ":", "method", "=", "getattr", "(", "callback", ",", "hook_name", ",", "None", ")", "if", "method", "is", "None", ":", "continue", "if", "not", "callable", "(", "method", ")", ":", "rank_zero_warn", "(", "f", "\"", "Skipping", "the", "callback", "`", "{", "type", "(", "callback", ")", ".", "__name__", "}", ".", "{", "hook_name", "}", "`", "because", "it", "is", "not", "callable", ".", "\"", ")", "continue", "method", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Trigger the callback methods with the given name and arguments.\r\n\r\n        Not all objects registered via ``Fabric(callbacks=...)`` must implement a method with the given name. The ones\r\n        that have a matching method name will get called.\r\n\r\n        Args:\r\n            hook_name: The name of the callback method.\r\n            *args: Optional positional arguments that get passed down to the callback method.\r\n            **kwargs: Optional keyword arguments that get passed down to the callback method.\r\n\r\n        Example::\r\n\r\n            class MyCallback:\r\n                def on_train_epoch_end(self, results):\r\n                    ...\r\n\r\n            fabric = Fabric(callbacks=[MyCallback()])\r\n            fabric.call(\"on_train_epoch_end\", results={...})\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "trigger", "the", "callback", "methods", "with", "the", "given", "name", "and", "arguments", "not", "all", "objects", "registered", "via", "fabric", "callbacks", "must", "implement", "a", "method", "with", "the", "given", "name", "the", "ones", "that", "have", "a", "matching", "method", "name", "will", "get", "called", "args", "hook_name", "the", "name", "of", "the", "callback", "method", "args", "optional", "positional", "arguments", "that", "get", "passed", "down", "to", "the", "callback", "method", "kwargs", "optional", "keyword", "arguments", "that", "get", "passed", "down", "to", "the", "callback", "method", "example", "class", "mycallback", "def", "on_train_epoch_end", "self", "results", "fabric", "fabric", "callbacks", "mycallback", "fabric", "call", "on_train_epoch_end", "results"], "docstring_summary": "r\"\"\"Trigger the callback methods with the given name and arguments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 987, "end_line": 1024, "hash": "aa67ddb4718647baa94ce85ac3624386", "complexity": 4, "parameters": ["hook_name", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "log", "original_string": "def log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)", "language": "python", "code": "def log(self, name: str, value: Any, step: Optional[int] = None) -> None:\r\n        \"\"\"Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        self.log_dict(metrics={name: value}, step=step)", "code_tokens": ["def", "log", "(", "self", ",", "name", ":", "str", ",", "value", ":", "Any", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "a", "scalar", "to", "all", "loggers", "that", "were", "added", "to", "Fabric", ".", "Args", ":", "name", ":", "The", "name", "of", "the", "metric", "to", "log", ".", "value", ":", "The", "metric", "value", "to", "collect", ".", "If", "the", "value", "is", "a", ":", "class", ":", "`", "torch", ".", "Tensor", "`", ",", "it", "gets", "detached", "from", "the", "graph", "automatically", ".", "step", ":", "Optional", "step", "number", ".", "Most", "Logger", "implementations", "auto", "-", "increment", "the", "step", "value", "by", "one", "with", "every", "log", "call", ".", "You", "can", "specify", "your", "own", "value", "here", ".", "\"", "\"", "\"", "self", ".", "log_dict", "(", "metrics", "=", "{", "name", ":", "value", "}", ",", "step", "=", "step", ")"], "docstring": "Log a scalar to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            name: The name of the metric to log.\r\n            value: The metric value to collect. If the value is a :class:`torch.Tensor`, it gets detached from the\r\n                graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment the step value by one with every\r\n                log call. You can specify your own value here.", "docstring_tokens": ["log", "a", "scalar", "to", "all", "loggers", "that", "were", "added", "to", "fabric", "args", "name", "the", "name", "of", "the", "metric", "to", "log", "value", "the", "metric", "value", "to", "collect", "if", "the", "value", "is", "a", "class", "torch", "tensor", "it", "gets", "detached", "from", "the", "graph", "automatically", "step", "optional", "step", "number", "most", "logger", "implementations", "auto", "increment", "the", "step", "value", "by", "one", "with", "every", "log", "call", "you", "can", "specify", "your", "own", "value", "here"], "docstring_summary": "Log a scalar to all loggers that were added to Fabric.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 1026, "end_line": 1037, "hash": "7d4ead3f7488e9f14107d357b1689be7", "complexity": 1, "parameters": ["name", "value", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "log_dict", "original_string": "def log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)", "language": "python", "code": "def log_dict(self, metrics: Mapping[str, Any], step: Optional[int] = None) -> None:\r\n        \"\"\"Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.\r\n\r\n        \"\"\"\r\n        metrics = convert_tensors_to_scalars(metrics)\r\n        for logger in self._loggers:\r\n            logger.log_metrics(metrics=metrics, step=step)", "code_tokens": ["def", "log_dict", "(", "self", ",", "metrics", ":", "Mapping", "[", "str", ",", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "multiple", "scalars", "at", "once", "to", "all", "loggers", "that", "were", "added", "to", "Fabric", ".", "Args", ":", "metrics", ":", "A", "dictionary", "where", "the", "key", "is", "the", "name", "of", "the", "metric", "and", "the", "value", "the", "scalar", "to", "be", "logged", ".", "Any", ":", "class", ":", "`", "torch", ".", "Tensor", "`", "in", "the", "dictionary", "get", "detached", "from", "the", "graph", "automatically", ".", "step", ":", "Optional", "step", "number", ".", "Most", "Logger", "implementations", "auto", "-", "increment", "this", "value", "by", "one", "with", "every", "log", "call", ".", "You", "can", "specify", "your", "own", "value", "here", ".", "\"", "\"", "\"", "metrics", "=", "convert_tensors_to_scalars", "(", "metrics", ")", "for", "logger", "in", "self", ".", "_loggers", ":", "logger", ".", "log_metrics", "(", "metrics", "=", "metrics", ",", "step", "=", "step", ")"], "docstring": "Log multiple scalars at once to all loggers that were added to Fabric.\r\n\r\n        Args:\r\n            metrics: A dictionary where the key is the name of the metric and the value the scalar to be logged.\r\n                Any :class:`torch.Tensor` in the dictionary get detached from the graph automatically.\r\n            step: Optional step number. Most Logger implementations auto-increment this value by one with every\r\n                log call. You can specify your own value here.", "docstring_tokens": ["log", "multiple", "scalars", "at", "once", "to", "all", "loggers", "that", "were", "added", "to", "fabric", "args", "metrics", "a", "dictionary", "where", "the", "key", "is", "the", "name", "of", "the", "metric", "and", "the", "value", "the", "scalar", "to", "be", "logged", "any", "class", "torch", "tensor", "in", "the", "dictionary", "get", "detached", "from", "the", "graph", "automatically", "step", "optional", "step", "number", "most", "logger", "implementations", "auto", "increment", "this", "value", "by", "one", "with", "every", "log", "call", "you", "can", "specify", "your", "own", "value", "here"], "docstring_summary": "Log multiple scalars at once to all loggers that were added to Fabric.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 1039, "end_line": 1051, "hash": "33be14529fb974d5266e34c7727c54f0", "complexity": 2, "parameters": ["metrics", "Any]", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\fabric.py", "func_name": "seed_everything", "original_string": "def seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            # Lightning sets `workers=False` by default to avoid breaking reproducibility, but since this is a new\r\n            # release, we can afford to do it.\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)", "language": "python", "code": "def seed_everything(seed: Optional[int] = None, workers: Optional[bool] = None, verbose: bool = True) -> int:\r\n        r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"\r\n        if workers is None:\r\n            # Lightning sets `workers=False` by default to avoid breaking reproducibility, but since this is a new\r\n            # release, we can afford to do it.\r\n            workers = True\r\n        return seed_everything(seed=seed, workers=workers, verbose=verbose)", "code_tokens": ["def", "seed_everything", "(", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "workers", ":", "Optional", "[", "bool", "]", "=", "None", ",", "verbose", ":", "bool", "=", "True", ")", "-", ">", "int", ":", "r", "\"", "\"", "\"", "Helper", "function", "to", "seed", "everything", "without", "explicitly", "importing", "Lightning", ".", "See", ":", "func", ":", "`", "~", "lightning", ".", "fabric", ".", "utilities", ".", "seed", ".", "seed_everything", "`", "for", "more", "details", ".", "\"", "\"", "\"", "if", "workers", "is", "None", ":", "workers", "=", "True", "return", "seed_everything", "(", "seed", "=", "seed", ",", "workers", "=", "workers", ",", "verbose", "=", "verbose", ")"], "docstring": "r\"\"\"Helper function to seed everything without explicitly importing Lightning.\r\n\r\n        See :func:`~lightning.fabric.utilities.seed.seed_everything` for more details.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "helper", "function", "to", "seed", "everything", "without", "explicitly", "importing", "lightning", "see", "func", "lightning", "fabric", "utilities", "seed", "seed_everything", "for", "more", "details"], "docstring_summary": "r\"\"\"Helper function to seed everything without explicitly importing Lightning.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\fabric.py", "partition": "train", "function_type": "class_method", "class_name": "Fabric", "start_line": 1054, "end_line": 1064, "hash": "98205140544f602e7d5b651b2b1f6d95", "complexity": 2, "parameters": ["seed", "workers", "verbose"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "__init__", "original_string": "def __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        # imitate the class of the wrapped object to make isinstance checks work\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})", "language": "python", "code": "def __init__(self, optimizer: Optimizer, strategy: Strategy, callbacks: Optional[list[Callable]] = None) -> None:\r\n        \"\"\"FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step\r\n\r\n        \"\"\"\r\n        self._optimizer = optimizer\r\n        self._strategy = strategy\r\n        self._callbacks = callbacks or []\r\n        # imitate the class of the wrapped object to make isinstance checks work\r\n        self.__class__ = type(\"Fabric\" + optimizer.__class__.__name__, (self.__class__, optimizer.__class__), {})", "code_tokens": ["def", "__init__", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "strategy", ":", "Strategy", ",", "callbacks", ":", "Optional", "[", "list", "[", "Callable", "]", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "FabricOptimizer", "is", "a", "thin", "wrapper", "around", "the", ":", "class", ":", "`", "~", "torch", ".", "optim", ".", "Optimizer", "`", "that", "delegates", "the", "optimizer", "step", "calls", "to", "the", "strategy", ".", "The", "underlying", "wrapped", "optimizer", "object", "can", "be", "accessed", "via", "the", "property", ":", "attr", ":", "`", "optimizer", "`", ".", "Args", ":", "optimizer", ":", "The", "optimizer", "to", "wrap", "strategy", ":", "Reference", "to", "the", "strategy", "for", "handling", "the", "optimizer", "step", "\"", "\"", "\"", "self", ".", "_optimizer", "=", "optimizer", "self", ".", "_strategy", "=", "strategy", "self", ".", "_callbacks", "=", "callbacks", "or", "[", "]", "self", ".", "__class__", "=", "type", "(", "\"", "Fabric", "\"", "+", "optimizer", ".", "__class__", ".", "__name__", ",", "(", "self", ".", "__class__", ",", "optimizer", ".", "__class__", ")", ",", "{", "}", ")"], "docstring": "FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer\r\n        step calls to the strategy.\r\n\r\n        The underlying wrapped optimizer object can be accessed via the property :attr:`optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to wrap\r\n            strategy: Reference to the strategy for handling the optimizer step", "docstring_tokens": ["fabricoptimizer", "is", "a", "thin", "wrapper", "around", "the", "class", "torch", "optim", "optimizer", "that", "delegates", "the", "optimizer", "step", "calls", "to", "the", "strategy", "the", "underlying", "wrapped", "optimizer", "object", "can", "be", "accessed", "via", "the", "property", "attr", "optimizer", "args", "optimizer", "the", "optimizer", "to", "wrap", "strategy", "reference", "to", "the", "strategy", "for", "handling", "the", "optimizer", "step"], "docstring_summary": "FabricOptimizer is a thin wrapper around the :class:`~torch.optim.Optimizer` that delegates the optimizer", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "class_method", "class_name": "_FabricOptimizer", "start_line": 52, "end_line": 67, "hash": "8e0a7c9721120640fe73e26ac316ff23", "complexity": 2, "parameters": ["optimizer", "strategy", "callbacks"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True", "language": "python", "code": "def __init__(\r\n        self, forward_module: nn.Module, strategy: Strategy, original_module: Optional[nn.Module] = None\r\n    ) -> None:\r\n        \"\"\"The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self._forward_module = forward_module\r\n        self._original_module = original_module or forward_module\r\n        self._strategy = strategy\r\n        self._forward_methods = set(_LIGHTNING_MODULE_STEP_METHODS)\r\n        self._fabric_module_initialized = True", "code_tokens": ["def", "__init__", "(", "self", ",", "forward_module", ":", "nn", ".", "Module", ",", "strategy", ":", "Strategy", ",", "original_module", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "The", "FabricModule", "is", "a", "thin", "wrapper", "around", "the", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", "and", "handles", "precision", "/", "autocast", "automatically", "for", "the", "forward", "pass", ".", "The", "underlying", "wrapped", "module", "can", "be", "accessed", "via", "the", "property", ":", "attr", ":", "`", "module", "`", ".", "Args", ":", "forward_module", ":", "The", "module", "to", "wrap", "the", "`", "`", "forward", "`", "`", "method", "on", ".", "strategy", ":", "Reference", "to", "the", "strategy", "for", "handling", "precision", "etc", ".", "original_module", ":", "The", "original", ",", "unmodified", "module", "as", "passed", "into", "the", ":", "meth", ":", "`", "lightning", ".", "fabric", ".", "fabric", ".", "Fabric", ".", "setup", "`", "method", ".", "This", "is", "needed", "when", "attribute", "lookup", "on", "this", "wrapper", "should", "pass", "through", "to", "the", "original", "module", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "_forward_module", "=", "forward_module", "self", ".", "_original_module", "=", "original_module", "or", "forward_module", "self", ".", "_strategy", "=", "strategy", "self", ".", "_forward_methods", "=", "set", "(", "_LIGHTNING_MODULE_STEP_METHODS", ")", "self", ".", "_fabric_module_initialized", "=", "True"], "docstring": "The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast\r\n        automatically for the forward pass.\r\n\r\n        The underlying wrapped module can be accessed via the property :attr:`module`.\r\n\r\n        Args:\r\n            forward_module: The module to wrap the ``forward`` method on.\r\n            strategy: Reference to the strategy for handling precision etc.\r\n            original_module: The original, unmodified module as passed into the\r\n                :meth:`lightning.fabric.fabric.Fabric.setup` method. This is needed when attribute lookup\r\n                on this wrapper should pass through to the original module.", "docstring_tokens": ["the", "fabricmodule", "is", "a", "thin", "wrapper", "around", "the", "class", "torch", "nn", "module", "and", "handles", "precision", "autocast", "automatically", "for", "the", "forward", "pass", "the", "underlying", "wrapped", "module", "can", "be", "accessed", "via", "the", "property", "attr", "module", "args", "forward_module", "the", "module", "to", "wrap", "the", "forward", "method", "on", "strategy", "reference", "to", "the", "strategy", "for", "handling", "precision", "etc", "original_module", "the", "original", "unmodified", "module", "as", "passed", "into", "the", "meth", "lightning", "fabric", "fabric", "fabric", "setup", "method", "this", "is", "needed", "when", "attribute", "lookup", "on", "this", "wrapper", "should", "pass", "through", "to", "the", "original", "module"], "docstring_summary": "The FabricModule is a thin wrapper around the :class:`torch.nn.Module` and handles precision / autocast", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "class_method", "class_name": "_FabricModule", "start_line": 101, "end_line": 122, "hash": "2949b2fb2f69aa72452ae97128f415f0", "complexity": 2, "parameters": ["forward_module", "strategy", "original_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "forward", "original_string": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output", "language": "python", "code": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Casts all inputs to the right precision and handles autocast for operations in the module forward method.\"\"\"\r\n        precision = self._strategy.precision\r\n        args, kwargs = precision.convert_input((args, kwargs))\r\n\r\n        with precision.forward_context():\r\n            output = self._forward_module(*args, **kwargs)\r\n\r\n        output = precision.convert_output(output)\r\n\r\n        apply_to_collection(output, dtype=Tensor, function=self._register_backward_hook)\r\n        return output", "code_tokens": ["def", "forward", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Casts", "all", "inputs", "to", "the", "right", "precision", "and", "handles", "autocast", "for", "operations", "in", "the", "module", "forward", "method", ".", "\"", "\"", "\"", "precision", "=", "self", ".", "_strategy", ".", "precision", "args", ",", "kwargs", "=", "precision", ".", "convert_input", "(", "(", "args", ",", "kwargs", ")", ")", "with", "precision", ".", "forward_context", "(", ")", ":", "output", "=", "self", ".", "_forward_module", "(", "*", "args", ",", "*", "*", "kwargs", ")", "output", "=", "precision", ".", "convert_output", "(", "output", ")", "apply_to_collection", "(", "output", ",", "dtype", "=", "Tensor", ",", "function", "=", "self", ".", "_register_backward_hook", ")", "return", "output"], "docstring": "Casts all inputs to the right precision and handles autocast for operations in the module forward method.", "docstring_tokens": ["casts", "all", "inputs", "to", "the", "right", "precision", "and", "handles", "autocast", "for", "operations", "in", "the", "module", "forward", "method"], "docstring_summary": "Casts all inputs to the right precision and handles autocast for operations in the module forward method.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "class_method", "class_name": "_FabricModule", "start_line": 129, "end_line": 140, "hash": "486e6d65f31a823a44bee39782bc43f5", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "mark_forward_method", "original_string": "def mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)", "language": "python", "code": "def mark_forward_method(self, method: Union[MethodType, str]) -> None:\r\n        \"\"\"Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).\"\"\"\r\n        if not isinstance(method, (MethodType, str)):\r\n            raise TypeError(f\"Expected a method or a string, but got: {type(method).__name__}\")\r\n        name = method if isinstance(method, str) else method.__name__\r\n        if name == \"forward\":\r\n            raise ValueError(\"You cannot mark the forward method itself as a forward method.\")\r\n        if not isinstance(getattr(self._original_module, name, None), MethodType):\r\n            raise AttributeError(\r\n                f\"You marked '{name}' as a forward method, but `{type(self._original_module).__name__}.{name}` does not\"\r\n                f\" exist or is not a method.\"\r\n            )\r\n        self._forward_methods.add(name)", "code_tokens": ["def", "mark_forward_method", "(", "self", ",", "method", ":", "Union", "[", "MethodType", ",", "str", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Mark", "a", "method", "as", "a", "'", "forward", "'", "method", "to", "prevent", "it", "bypassing", "the", "strategy", "wrapper", "(", "e", ".", "g", ".", ",", "DDP", ")", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "method", ",", "(", "MethodType", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "f", "\"", "Expected", "a", "method", "or", "a", "string", ",", "but", "got", ":", "{", "type", "(", "method", ")", ".", "__name__", "}", "\"", ")", "name", "=", "method", "if", "isinstance", "(", "method", ",", "str", ")", "else", "method", ".", "__name__", "if", "name", "=", "=", "\"", "forward", "\"", ":", "raise", "ValueError", "(", "\"", "You", "cannot", "mark", "the", "forward", "method", "itself", "as", "a", "forward", "method", ".", "\"", ")", "if", "not", "isinstance", "(", "getattr", "(", "self", ".", "_original_module", ",", "name", ",", "None", ")", ",", "MethodType", ")", ":", "raise", "AttributeError", "(", "f", "\"", "You", "marked", "'", "{", "name", "}", "'", "as", "a", "forward", "method", ",", "but", "`", "{", "type", "(", "self", ".", "_original_module", ")", ".", "__name__", "}", ".", "{", "name", "}", "`", "does", "not", "\"", "f", "\"", "exist", "or", "is", "not", "a", "method", ".", "\"", ")", "self", ".", "_forward_methods", ".", "add", "(", "name", ")"], "docstring": "Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).", "docstring_tokens": ["mark", "a", "method", "as", "a", "forward", "method", "to", "prevent", "it", "bypassing", "the", "strategy", "wrapper", "e", "g", "ddp"], "docstring_summary": "Mark a method as a 'forward' method to prevent it bypassing the strategy wrapper (e.g., DDP).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "class_method", "class_name": "_FabricModule", "start_line": 164, "end_line": 176, "hash": "9659520e541fc5756c7b3160e68b1182", "complexity": 5, "parameters": ["method", "str]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "_wrap_method_with_module_call_tracker", "original_string": "def _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method", "language": "python", "code": "def _wrap_method_with_module_call_tracker(self, method: Callable, name: str) -> Callable:\r\n        \"\"\"Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.\"\"\"\r\n        module_called = False\r\n\r\n        def hook(*_: Any, **__: Any) -> None:\r\n            nonlocal module_called\r\n            module_called = True\r\n\r\n        @wraps(method)\r\n        def _wrapped_method(*args: Any, **kwargs: Any) -> Any:\r\n            handles = []\r\n            for module in self._original_module.modules():\r\n                handles.append(module.register_forward_hook(hook))\r\n\r\n            output = method(*args, **kwargs)\r\n\r\n            if module_called:\r\n                raise RuntimeError(\r\n                    f\"You are calling the method `{type(self._original_module).__name__}.{name}()` from outside the\"\r\n                    \" model. To avoid issues with the currently selected strategy, explicitly mark it as a\"\r\n                    f\" forward method with `fabric_model.mark_forward_method({name!r})` after `fabric.setup()`.\"\r\n                )\r\n            for handle in handles:\r\n                handle.remove()\r\n            return output\r\n\r\n        return _wrapped_method", "code_tokens": ["def", "_wrap_method_with_module_call_tracker", "(", "self", ",", "method", ":", "Callable", ",", "name", ":", "str", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Tracks", "whether", "any", "submodule", "in", "`", "`", "self", ".", "_original_module", "`", "`", "was", "called", "during", "the", "execution", "of", "`", "`", "method", "`", "`", "by", "registering", "forward", "hooks", "on", "all", "submodules", ".", "\"", "\"", "\"", "module_called", "=", "False", "def", "hook", "(", "*", "_", ":", "Any", ",", "*", "*", "__", ":", "Any", ")", "-", ">", "None", ":", "nonlocal", "module_called", "module_called", "=", "True", "@", "wraps", "(", "method", ")", "def", "_wrapped_method", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "handles", "=", "[", "]", "for", "module", "in", "self", ".", "_original_module", ".", "modules", "(", ")", ":", "handles", ".", "append", "(", "module", ".", "register_forward_hook", "(", "hook", ")", ")", "output", "=", "method", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "module_called", ":", "raise", "RuntimeError", "(", "f", "\"", "You", "are", "calling", "the", "method", "`", "{", "type", "(", "self", ".", "_original_module", ")", ".", "__name__", "}", ".", "{", "name", "}", "(", ")", "`", "from", "outside", "the", "\"", "\"", "model", ".", "To", "avoid", "issues", "with", "the", "currently", "selected", "strategy", ",", "explicitly", "mark", "it", "as", "a", "\"", "f", "\"", "forward", "method", "with", "`", "fabric_model", ".", "mark_forward_method", "(", "{", "name", "!", "r", "}", ")", "`", "after", "`", "fabric", ".", "setup", "(", ")", "`", ".", "\"", ")", "for", "handle", "in", "handles", ":", "handle", ".", "remove", "(", ")", "return", "output", "return", "_wrapped_method"], "docstring": "Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by\r\n        registering forward hooks on all submodules.", "docstring_tokens": ["tracks", "whether", "any", "submodule", "in", "self", "_original_module", "was", "called", "during", "the", "execution", "of", "method", "by", "registering", "forward", "hooks", "on", "all", "submodules"], "docstring_summary": "Tracks whether any submodule in ``self._original_module`` was called during the execution of ``method`` by", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "class_method", "class_name": "_FabricModule", "start_line": 200, "end_line": 227, "hash": "cafaac248d4a7852a584c3e560a2a80c", "complexity": 4, "parameters": ["method", "name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "__init__", "original_string": "def __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0", "language": "python", "code": "def __init__(self, dataloader: DataLoader, device: Optional[torch.device] = None) -> None:\r\n        \"\"\"The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).\r\n\r\n        \"\"\"\r\n        self.__dict__.update(dataloader.__dict__)\r\n        self._dataloader = dataloader\r\n        self._device = device\r\n        self._num_iter_calls = 0", "code_tokens": ["def", "__init__", "(", "self", ",", "dataloader", ":", "DataLoader", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "The", "FabricDataLoader", "is", "a", "wrapper", "for", "the", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", ".", "It", "moves", "the", "data", "to", "the", "device", "automatically", "if", "the", "device", "is", "specified", ".", "Args", ":", "dataloader", ":", "The", "dataloader", "to", "wrap", "device", ":", "The", "device", "to", "which", "the", "data", "should", "be", "moved", ".", "By", "default", "the", "device", "is", "`", "None", "`", "and", "no", "data", "transfers", "will", "be", "made", "(", "identical", "behavior", "as", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", ")", ".", "\"", "\"", "\"", "self", ".", "__dict__", ".", "update", "(", "dataloader", ".", "__dict__", ")", "self", ".", "_dataloader", "=", "dataloader", "self", ".", "_device", "=", "device", "self", ".", "_num_iter_calls", "=", "0"], "docstring": "The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the\r\n        device automatically if the device is specified.\r\n\r\n        Args:\r\n            dataloader: The dataloader to wrap\r\n            device: The device to which the data should be moved. By default the device is `None` and no data\r\n                transfers will be made (identical behavior as :class:`~torch.utils.data.DataLoader`).", "docstring_tokens": ["the", "fabricdataloader", "is", "a", "wrapper", "for", "the", "class", "torch", "utils", "data", "dataloader", "it", "moves", "the", "data", "to", "the", "device", "automatically", "if", "the", "device", "is", "specified", "args", "dataloader", "the", "dataloader", "to", "wrap", "device", "the", "device", "to", "which", "the", "data", "should", "be", "moved", "by", "default", "the", "device", "is", "none", "and", "no", "data", "transfers", "will", "be", "made", "identical", "behavior", "as", "class", "torch", "utils", "data", "dataloader"], "docstring_summary": "The FabricDataLoader is a wrapper for the :class:`~torch.utils.data.DataLoader`. It moves the data to the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "class_method", "class_name": "_FabricDataLoader", "start_line": 293, "end_line": 306, "hash": "a76b1b2656cdf4afdb77c72b56aa0e8b", "complexity": 1, "parameters": ["dataloader", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "_unwrap_compiled", "original_string": "def _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None", "language": "python", "code": "def _unwrap_compiled(obj: Union[Any, OptimizedModule]) -> tuple[Union[Any, nn.Module], Optional[dict[str, Any]]]:\r\n    \"\"\"Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.\r\n\r\n    \"\"\"\r\n    if isinstance(obj, OptimizedModule):\r\n        if (compile_kwargs := getattr(obj, \"_compile_kwargs\", None)) is None:\r\n            raise RuntimeError(\r\n                \"Failed to determine the arguments that were used to compile the module. Make sure to import\"\r\n                \" lightning before `torch.compile` is used.\"\r\n            )\r\n        return obj._orig_mod, compile_kwargs\r\n    return obj, None", "code_tokens": ["def", "_unwrap_compiled", "(", "obj", ":", "Union", "[", "Any", ",", "OptimizedModule", "]", ")", "-", ">", "tuple", "[", "Union", "[", "Any", ",", "nn", ".", "Module", "]", ",", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "]", ":", "\"", "\"", "\"", "Removes", "the", ":", "class", ":", "`", "torch", ".", "_dynamo", ".", "OptimizedModule", "`", "around", "the", "object", "if", "it", "is", "wrapped", ".", "Use", "this", "function", "before", "instance", "checks", "against", "e", ".", "g", ".", ":", "class", ":", "`", "_FabricModule", "`", ".", "\"", "\"", "\"", "if", "isinstance", "(", "obj", ",", "OptimizedModule", ")", ":", "if", "(", "compile_kwargs", ":", "=", "getattr", "(", "obj", ",", "\"", "_compile_kwargs", "\"", ",", "None", ")", ")", "is", "None", ":", "raise", "RuntimeError", "(", "\"", "Failed", "to", "determine", "the", "arguments", "that", "were", "used", "to", "compile", "the", "module", ".", "Make", "sure", "to", "import", "\"", "\"", "lightning", "before", "`", "torch", ".", "compile", "`", "is", "used", ".", "\"", ")", "return", "obj", ".", "_orig_mod", ",", "compile_kwargs", "return", "obj", ",", "None"], "docstring": "Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.\r\n\r\n    Use this function before instance checks against e.g. :class:`_FabricModule`.", "docstring_tokens": ["removes", "the", "class", "torch", "_dynamo", "optimizedmodule", "around", "the", "object", "if", "it", "is", "wrapped", "use", "this", "function", "before", "instance", "checks", "against", "e", "g", "class", "_fabricmodule"], "docstring_summary": "Removes the :class:`torch._dynamo.OptimizedModule` around the object if it is wrapped.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "function", "start_line": 347, "end_line": 360, "hash": "c278bc62067dd6e944a85e2345d6f4e4", "complexity": 3, "parameters": ["obj", "OptimizedModule]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "is_wrapped", "original_string": "def is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))", "language": "python", "code": "def is_wrapped(obj: object) -> bool:\r\n    \"\"\"Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.\r\n\r\n    \"\"\"\r\n    obj, _ = _unwrap_compiled(obj)\r\n    return isinstance(obj, (_FabricModule, _FabricOptimizer, _FabricDataLoader))", "code_tokens": ["def", "is_wrapped", "(", "obj", ":", "object", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Checks", "if", "an", "object", "was", "set", "up", "by", "Fabric", ".", "A", ":", "class", ":", "`", "~", "torch", ".", "nn", ".", "Module", "`", "may", "be", "wrapped", "by", "a", ":", "class", ":", "`", "_FabricModule", "`", ",", "a", ":", "class", ":", "`", "~", "torch", ".", "optim", ".", "Optimizer", "`", "may", "be", "wrapped", "by", "a", ":", "class", ":", "`", "_FabricOptimizer", "`", ",", "or", "a", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "may", "be", "wrapped", "by", ":", "class", ":", "`", "_FabricDataLoader", "`", ".", "Args", ":", "obj", ":", "The", "object", "to", "test", ".", "\"", "\"", "\"", "obj", ",", "_", "=", "_unwrap_compiled", "(", "obj", ")", "return", "isinstance", "(", "obj", ",", "(", "_FabricModule", ",", "_FabricOptimizer", ",", "_FabricDataLoader", ")", ")"], "docstring": "Checks if an object was set up by Fabric.\r\n\r\n    A :class:`~torch.nn.Module` may be wrapped by a :class:`_FabricModule`, a :class:`~torch.optim.Optimizer`\r\n    may be wrapped by a :class:`_FabricOptimizer`, or a :class:`~torch.utils.data.DataLoader` may be wrapped by\r\n    :class:`_FabricDataLoader`.\r\n\r\n    Args:\r\n        obj: The object to test.", "docstring_tokens": ["checks", "if", "an", "object", "was", "set", "up", "by", "fabric", "a", "class", "torch", "nn", "module", "may", "be", "wrapped", "by", "a", "class", "_fabricmodule", "a", "class", "torch", "optim", "optimizer", "may", "be", "wrapped", "by", "a", "class", "_fabricoptimizer", "or", "a", "class", "torch", "utils", "data", "dataloader", "may", "be", "wrapped", "by", "class", "_fabricdataloader", "args", "obj", "the", "object", "to", "test"], "docstring_summary": "Checks if an object was set up by Fabric.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "function", "start_line": 375, "end_line": 387, "hash": "5dde143d5f7687b1edb4471fbd48318f", "complexity": 1, "parameters": ["obj"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\wrappers.py", "func_name": "_capture_compile_kwargs", "original_string": "def _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n    # Limitation: Currently, the global compile config does not get captured on a per-model basis.\r\n    # PyTorch will resolve this in the future: https://github.com/pytorch/pytorch/issues/116575\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            # either torch.compile is being applied as a decorator or we're compiling something else\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture", "language": "python", "code": "def _capture_compile_kwargs(compile_fn: Callable) -> Callable:\r\n    \"\"\"Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.\r\n\r\n    \"\"\"\r\n    # Limitation: Currently, the global compile config does not get captured on a per-model basis.\r\n    # PyTorch will resolve this in the future: https://github.com/pytorch/pytorch/issues/116575\r\n\r\n    @wraps(compile_fn)\r\n    def _capture(*args: Any, **kwargs: Any) -> Any:\r\n        if not args or not isinstance(args[0], nn.Module):\r\n            # either torch.compile is being applied as a decorator or we're compiling something else\r\n            return compile_fn(*args, **kwargs)\r\n\r\n        model = args[0]\r\n        compiled_model = compile_fn(model, **kwargs)\r\n        compiled_model._compile_kwargs = deepcopy(kwargs)\r\n        return compiled_model\r\n\r\n    return _capture", "code_tokens": ["def", "_capture_compile_kwargs", "(", "compile_fn", ":", "Callable", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Wraps", "the", "`", "`", "torch", ".", "compile", "`", "`", "function", "and", "captures", "the", "compile", "arguments", ".", "We", "extract", "the", "compile", "arguments", "so", "that", "we", "can", "reapply", "`", "`", "torch", ".", "compile", "`", "`", "in", "`", "`", "Fabric", ".", "setup", "(", ")", "`", "`", "with", "the", "same", "arguments", "as", "the", "user", "passed", "to", "the", "original", "call", ".", "The", "arguments", "get", "stored", "in", "a", "dictionary", "`", "`", "_compile_kwargs", "`", "`", "on", "the", "returned", "compiled", "module", ".", "\"", "\"", "\"", "@", "wraps", "(", "compile_fn", ")", "def", "_capture", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "if", "not", "args", "or", "not", "isinstance", "(", "args", "[", "0", "]", ",", "nn", ".", "Module", ")", ":", "return", "compile_fn", "(", "*", "args", ",", "*", "*", "kwargs", ")", "model", "=", "args", "[", "0", "]", "compiled_model", "=", "compile_fn", "(", "model", ",", "*", "*", "kwargs", ")", "compiled_model", ".", "_compile_kwargs", "=", "deepcopy", "(", "kwargs", ")", "return", "compiled_model", "return", "_capture"], "docstring": "Wraps the ``torch.compile`` function and captures the compile arguments.\r\n\r\n    We extract the compile arguments so that we can reapply ``torch.compile`` in ``Fabric.setup()`` with the\r\n    same arguments as the user passed to the original call. The arguments get stored in a dictionary\r\n    ``_compile_kwargs`` on the returned compiled module.", "docstring_tokens": ["wraps", "the", "torch", "compile", "function", "and", "captures", "the", "compile", "arguments", "we", "extract", "the", "compile", "arguments", "so", "that", "we", "can", "reapply", "torch", "compile", "in", "fabric", "setup", "with", "the", "same", "arguments", "as", "the", "user", "passed", "to", "the", "original", "call", "the", "arguments", "get", "stored", "in", "a", "dictionary", "_compile_kwargs", "on", "the", "returned", "compiled", "module"], "docstring_summary": "Wraps the ``torch.compile`` function and captures the compile arguments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\wrappers.py", "partition": "train", "function_type": "function", "start_line": 390, "end_line": 412, "hash": "d272ac01f8da6f7e991e48df8b723b04", "complexity": 3, "parameters": ["compile_fn"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "setup_device", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise ValueError(f\"Device should be CPU, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "ValueError", ":", "If", "the", "selected", "device", "is", "not", "CPU", ".", "\"", "\"", "\"", "if", "device", ".", "type", "!", "=", "\"", "cpu", "\"", ":", "raise", "ValueError", "(", "f", "\"", "Device", "should", "be", "CPU", ",", "got", "{", "device", "}", "instead", ".", "\"", ")"], "docstring": "Raises:\r\n            ValueError:\r\n                If the selected device is not CPU.", "docstring_tokens": ["raises", "valueerror", "if", "the", "selected", "device", "is", "not", "cpu"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cpu.py", "partition": "train", "function_type": "class_method", "class_name": "CPUAccelerator", "start_line": 26, "end_line": 33, "hash": "86b75b4c8610a7f3f0c510d9954475b2", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "get_parallel_devices", "original_string": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "\"", "\"", "\"", "Gets", "parallel", "devices", "for", "the", "Accelerator", ".", "\"", "\"", "\"", "devices", "=", "_parse_cpu_cores", "(", "devices", ")", "return", "[", "torch", ".", "device", "(", "\"", "cpu", "\"", ")", "]", "*", "devices"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "docstring_summary": "Gets parallel devices for the Accelerator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cpu.py", "partition": "train", "function_type": "class_method", "class_name": "CPUAccelerator", "start_line": 47, "end_line": 50, "hash": "bb916317d3eb3b151f30cf9194f5dd0a", "complexity": 1, "parameters": ["devices", "str]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cpu.py", "func_name": "_parse_cpu_cores", "original_string": "def _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores", "language": "python", "code": "def _parse_cpu_cores(cpu_cores: Union[int, str]) -> int:\r\n    \"\"\"Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0\r\n\r\n    \"\"\"\r\n    if isinstance(cpu_cores, str) and cpu_cores.strip().isdigit():\r\n        cpu_cores = int(cpu_cores)\r\n\r\n    if not isinstance(cpu_cores, int) or cpu_cores <= 0:\r\n        raise TypeError(\"`devices` selected with `CPUAccelerator` should be an int > 0.\")\r\n\r\n    return cpu_cores", "code_tokens": ["def", "_parse_cpu_cores", "(", "cpu_cores", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Parses", "the", "cpu_cores", "given", "in", "the", "format", "as", "accepted", "by", "the", "`", "`", "devices", "`", "`", "argument", "in", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", ".", "Args", ":", "cpu_cores", ":", "An", "int", ">", "0", "or", "a", "string", "that", "can", "be", "converted", "to", "an", "int", ">", "0", ".", "Returns", ":", "An", "int", "representing", "the", "number", "of", "processes", "Raises", ":", "MisconfigurationException", ":", "If", "cpu_cores", "is", "not", "an", "int", ">", "0", "\"", "\"", "\"", "if", "isinstance", "(", "cpu_cores", ",", "str", ")", "and", "cpu_cores", ".", "strip", "(", ")", ".", "isdigit", "(", ")", ":", "cpu_cores", "=", "int", "(", "cpu_cores", ")", "if", "not", "isinstance", "(", "cpu_cores", ",", "int", ")", "or", "cpu_cores", "<", "=", "0", ":", "raise", "TypeError", "(", "\"", "`", "devices", "`", "selected", "with", "`", "CPUAccelerator", "`", "should", "be", "an", "int", ">", "0", ".", "\"", ")", "return", "cpu_cores"], "docstring": "Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        cpu_cores: An int > 0 or a string that can be converted to an int > 0.\r\n\r\n    Returns:\r\n        An int representing the number of processes\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If cpu_cores is not an int > 0", "docstring_tokens": ["parses", "the", "cpu_cores", "given", "in", "the", "format", "as", "accepted", "by", "the", "devices", "argument", "in", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "args", "cpu_cores", "an", "int", "0", "or", "a", "string", "that", "can", "be", "converted", "to", "an", "int", "0", "returns", "an", "int", "representing", "the", "number", "of", "processes", "raises", "misconfigurationexception", "if", "cpu_cores", "is", "not", "an", "int", "0"], "docstring_summary": "Parses the cpu_cores given in the format as accepted by the ``devices`` argument in the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cpu.py", "partition": "train", "function_type": "function", "start_line": 74, "end_line": 95, "hash": "18fa8a755a31eaea1d4ee4519387ec9d", "complexity": 5, "parameters": ["cpu_cores", "str]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "setup_device", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise ValueError(f\"Device should be CUDA, got {device} instead.\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "ValueError", ":", "If", "the", "selected", "device", "is", "not", "of", "type", "CUDA", ".", "\"", "\"", "\"", "if", "device", ".", "type", "!", "=", "\"", "cuda", "\"", ":", "raise", "ValueError", "(", "f", "\"", "Device", "should", "be", "CUDA", ",", "got", "{", "device", "}", "instead", ".", "\"", ")", "_check_cuda_matmul_precision", "(", "device", ")", "torch", ".", "cuda", ".", "set_device", "(", "device", ")"], "docstring": "Raises:\r\n            ValueError:\r\n                If the selected device is not of type CUDA.", "docstring_tokens": ["raises", "valueerror", "if", "the", "selected", "device", "is", "not", "of", "type", "cuda"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py", "partition": "train", "function_type": "class_method", "class_name": "CUDAAccelerator", "start_line": 28, "end_line": 37, "hash": "dbded161944c0f283a08cf39d210181a", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "parse_devices", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_cuda=True)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "\"", "\"", "\"", "Accelerator", "device", "parsing", "logic", ".", "\"", "\"", "\"", "from", "lightning", ".", "fabric", ".", "utilities", ".", "device_parser", "import", "_parse_gpu_ids", "return", "_parse_gpu_ids", "(", "devices", ",", "include_cuda", "=", "True", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "docstring_summary": "Accelerator device parsing logic.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py", "partition": "train", "function_type": "class_method", "class_name": "CUDAAccelerator", "start_line": 45, "end_line": 49, "hash": "ce2d9d8a1efa275e4049525a88de1f85", "complexity": 1, "parameters": ["devices", "str", "list[int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "find_usable_cuda_devices", "original_string": "def find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            # exit early if we found the right number of GPUs\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices", "language": "python", "code": "def find_usable_cuda_devices(num_devices: int = -1) -> list[int]:\r\n    \"\"\"Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.\r\n\r\n    \"\"\"\r\n    if num_devices == 0:\r\n        return []\r\n    visible_devices = _get_all_visible_cuda_devices()\r\n    if not visible_devices:\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but there are no visible CUDA devices on this machine.\"\r\n        )\r\n    if num_devices > len(visible_devices):\r\n        raise ValueError(\r\n            f\"You requested to find {num_devices} devices but this machine only has {len(visible_devices)} GPUs.\"\r\n        )\r\n\r\n    available_devices = []\r\n    unavailable_devices = []\r\n\r\n    for gpu_idx in visible_devices:\r\n        try:\r\n            torch.tensor(0, device=torch.device(\"cuda\", gpu_idx))\r\n        except RuntimeError:\r\n            unavailable_devices.append(gpu_idx)\r\n            continue\r\n\r\n        available_devices.append(gpu_idx)\r\n        if len(available_devices) == num_devices:\r\n            # exit early if we found the right number of GPUs\r\n            break\r\n\r\n    if num_devices != -1 and len(available_devices) != num_devices:\r\n        raise RuntimeError(\r\n            f\"You requested to find {num_devices} devices but only {len(available_devices)} are currently available.\"\r\n            f\" The devices {unavailable_devices} are occupied by other processes and can't be used at the moment.\"\r\n        )\r\n    return available_devices", "code_tokens": ["def", "find_usable_cuda_devices", "(", "num_devices", ":", "int", "=", "-", "1", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "Returns", "a", "list", "of", "all", "available", "and", "usable", "CUDA", "GPU", "devices", ".", "A", "GPU", "is", "considered", "usable", "if", "we", "can", "successfully", "move", "a", "tensor", "to", "the", "device", ",", "and", "this", "is", "what", "this", "function", "tests", "for", "each", "GPU", "on", "the", "system", "until", "the", "target", "number", "of", "usable", "devices", "is", "found", ".", "A", "subset", "of", "GPUs", "on", "the", "system", "might", "be", "used", "by", "other", "processes", ",", "and", "if", "the", "GPU", "is", "configured", "to", "operate", "in", "'", "exclusive", "'", "mode", "(", "configurable", "by", "the", "admin", ")", ",", "then", "only", "one", "process", "is", "allowed", "to", "occupy", "it", ".", "Args", ":", "num_devices", ":", "The", "number", "of", "devices", "you", "want", "to", "request", ".", "By", "default", ",", "this", "function", "will", "return", "as", "many", "as", "there", "are", "usable", "CUDA", "GPU", "devices", "available", ".", "Warning", ":", "If", "multiple", "processes", "call", "this", "function", "at", "the", "same", "time", ",", "there", "can", "be", "race", "conditions", "in", "the", "case", "where", "both", "processes", "determine", "that", "the", "device", "is", "unoccupied", ",", "leading", "into", "one", "of", "them", "crashing", "later", "on", ".", "\"", "\"", "\"", "if", "num_devices", "=", "=", "0", ":", "return", "[", "]", "visible_devices", "=", "_get_all_visible_cuda_devices", "(", ")", "if", "not", "visible_devices", ":", "raise", "ValueError", "(", "f", "\"", "You", "requested", "to", "find", "{", "num_devices", "}", "devices", "but", "there", "are", "no", "visible", "CUDA", "devices", "on", "this", "machine", ".", "\"", ")", "if", "num_devices", ">", "len", "(", "visible_devices", ")", ":", "raise", "ValueError", "(", "f", "\"", "You", "requested", "to", "find", "{", "num_devices", "}", "devices", "but", "this", "machine", "only", "has", "{", "len", "(", "visible_devices", ")", "}", "GPUs", ".", "\"", ")", "available_devices", "=", "[", "]", "unavailable_devices", "=", "[", "]", "for", "gpu_idx", "in", "visible_devices", ":", "try", ":", "torch", ".", "tensor", "(", "0", ",", "device", "=", "torch", ".", "device", "(", "\"", "cuda", "\"", ",", "gpu_idx", ")", ")", "except", "RuntimeError", ":", "unavailable_devices", ".", "append", "(", "gpu_idx", ")", "continue", "available_devices", ".", "append", "(", "gpu_idx", ")", "if", "len", "(", "available_devices", ")", "=", "=", "num_devices", ":", "break", "if", "num_devices", "!", "=", "-", "1", "and", "len", "(", "available_devices", ")", "!", "=", "num_devices", ":", "raise", "RuntimeError", "(", "f", "\"", "You", "requested", "to", "find", "{", "num_devices", "}", "devices", "but", "only", "{", "len", "(", "available_devices", ")", "}", "are", "currently", "available", ".", "\"", "f", "\"", "The", "devices", "{", "unavailable_devices", "}", "are", "occupied", "by", "other", "processes", "and", "can", "'", "t", "be", "used", "at", "the", "moment", ".", "\"", ")", "return", "available_devices"], "docstring": "Returns a list of all available and usable CUDA GPU devices.\r\n\r\n    A GPU is considered usable if we can successfully move a tensor to the device, and this is what this function\r\n    tests for each GPU on the system until the target number of usable devices is found.\r\n\r\n    A subset of GPUs on the system might be used by other processes, and if the GPU is configured to operate in\r\n    'exclusive' mode (configurable by the admin), then only one process is allowed to occupy it.\r\n\r\n    Args:\r\n        num_devices: The number of devices you want to request. By default, this function will return as many as there\r\n            are usable CUDA GPU devices available.\r\n\r\n    Warning:\r\n        If multiple processes call this function at the same time, there can be race conditions in the case where\r\n        both processes determine that the device is unoccupied, leading into one of them crashing later on.", "docstring_tokens": ["returns", "a", "list", "of", "all", "available", "and", "usable", "cuda", "gpu", "devices", "a", "gpu", "is", "considered", "usable", "if", "we", "can", "successfully", "move", "a", "tensor", "to", "the", "device", "and", "this", "is", "what", "this", "function", "tests", "for", "each", "gpu", "on", "the", "system", "until", "the", "target", "number", "of", "usable", "devices", "is", "found", "a", "subset", "of", "gpus", "on", "the", "system", "might", "be", "used", "by", "other", "processes", "and", "if", "the", "gpu", "is", "configured", "to", "operate", "in", "exclusive", "mode", "configurable", "by", "the", "admin", "then", "only", "one", "process", "is", "allowed", "to", "occupy", "it", "args", "num_devices", "the", "number", "of", "devices", "you", "want", "to", "request", "by", "default", "this", "function", "will", "return", "as", "many", "as", "there", "are", "usable", "cuda", "gpu", "devices", "available", "warning", "if", "multiple", "processes", "call", "this", "function", "at", "the", "same", "time", "there", "can", "be", "race", "conditions", "in", "the", "case", "where", "both", "processes", "determine", "that", "the", "device", "is", "unoccupied", "leading", "into", "one", "of", "them", "crashing", "later", "on"], "docstring_summary": "Returns a list of all available and usable CUDA GPU devices.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py", "partition": "train", "function_type": "function", "start_line": 78, "end_line": 128, "hash": "da3d4dfddb2eafb3eb3e058a04732b63", "complexity": 9, "parameters": ["num_devices"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "_get_all_visible_cuda_devices", "original_string": "def _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))", "language": "python", "code": "def _get_all_visible_cuda_devices() -> list[int]:\r\n    \"\"\"Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.\r\n\r\n    \"\"\"\r\n    return list(range(num_cuda_devices()))", "code_tokens": ["def", "_get_all_visible_cuda_devices", "(", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "Returns", "a", "list", "of", "all", "visible", "CUDA", "GPU", "devices", ".", "Devices", "masked", "by", "the", "environment", "variabale", "`", "`", "CUDA_VISIBLE_DEVICES", "`", "`", "won", "'", "t", "be", "returned", "here", ".", "For", "example", ",", "assume", "you", "have", "8", "physical", "GPUs", ".", "If", "`", "`", "CUDA_VISIBLE_DEVICES", "=", "\"", "1", ",", "3", ",", "6", "\"", "`", "`", ",", "then", "this", "function", "will", "return", "the", "list", "`", "`", "[", "0", ",", "1", ",", "2", "]", "`", "`", "because", "these", "are", "the", "three", "visible", "GPUs", "after", "applying", "the", "mask", "`", "`", "CUDA_VISIBLE_DEVICES", "`", "`", ".", "\"", "\"", "\"", "return", "list", "(", "range", "(", "num_cuda_devices", "(", ")", ")", ")"], "docstring": "Returns a list of all visible CUDA GPU devices.\r\n\r\n    Devices masked by the environment variabale ``CUDA_VISIBLE_DEVICES`` won't be returned here. For example, assume you\r\n    have 8 physical GPUs. If ``CUDA_VISIBLE_DEVICES=\"1,3,6\"``, then this function will return the list ``[0, 1, 2]``\r\n    because these are the three visible GPUs after applying the mask ``CUDA_VISIBLE_DEVICES``.", "docstring_tokens": ["returns", "a", "list", "of", "all", "visible", "cuda", "gpu", "devices", "devices", "masked", "by", "the", "environment", "variabale", "cuda_visible_devices", "won", "t", "be", "returned", "here", "for", "example", "assume", "you", "have", "8", "physical", "gpus", "if", "cuda_visible_devices", "1", "3", "6", "then", "this", "function", "will", "return", "the", "list", "0", "1", "2", "because", "these", "are", "the", "three", "visible", "gpus", "after", "applying", "the", "mask", "cuda_visible_devices"], "docstring_summary": "Returns a list of all visible CUDA GPU devices.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py", "partition": "train", "function_type": "function", "start_line": 131, "end_line": 139, "hash": "da85c67183c181150123b05cf6d612fc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\cuda.py", "func_name": "is_cuda_available", "original_string": "def is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    # We set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in lightning.fabric.__init__.py\r\n    return torch.cuda.is_available()", "language": "python", "code": "def is_cuda_available() -> bool:\r\n    \"\"\"Returns a bool indicating if CUDA is currently available.\"\"\"\r\n    # We set `PYTORCH_NVML_BASED_CUDA_CHECK=1` in lightning.fabric.__init__.py\r\n    return torch.cuda.is_available()", "code_tokens": ["def", "is_cuda_available", "(", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "a", "bool", "indicating", "if", "CUDA", "is", "currently", "available", ".", "\"", "\"", "\"", "return", "torch", ".", "cuda", ".", "is_available", "(", ")"], "docstring": "Returns a bool indicating if CUDA is currently available.", "docstring_tokens": ["returns", "a", "bool", "indicating", "if", "cuda", "is", "currently", "available"], "docstring_summary": "Returns a bool indicating if CUDA is currently available.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\cuda.py", "partition": "train", "function_type": "function", "start_line": 147, "end_line": 150, "hash": "4b1e0d5a2682ebd4a2a9bcd160016cca", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "setup_device", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise ValueError(f\"Device should be MPS, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "ValueError", ":", "If", "the", "selected", "device", "is", "not", "MPS", ".", "\"", "\"", "\"", "if", "device", ".", "type", "!", "=", "\"", "mps", "\"", ":", "raise", "ValueError", "(", "f", "\"", "Device", "should", "be", "MPS", ",", "got", "{", "device", "}", "instead", ".", "\"", ")"], "docstring": "Raises:\r\n            ValueError:\r\n                If the selected device is not MPS.", "docstring_tokens": ["raises", "valueerror", "if", "the", "selected", "device", "is", "not", "mps"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\mps.py", "partition": "train", "function_type": "class_method", "class_name": "MPSAccelerator", "start_line": 33, "end_line": 40, "hash": "ceeea79cd4e5fd415fd356f50f447ac5", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "parse_devices", "original_string": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)", "language": "python", "code": "def parse_devices(devices: Union[int, str, list[int]]) -> Optional[list[int]]:\r\n        \"\"\"Accelerator device parsing logic.\"\"\"\r\n        from lightning.fabric.utilities.device_parser import _parse_gpu_ids\r\n\r\n        return _parse_gpu_ids(devices, include_mps=True)", "code_tokens": ["def", "parse_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "\"", "\"", "\"", "Accelerator", "device", "parsing", "logic", ".", "\"", "\"", "\"", "from", "lightning", ".", "fabric", ".", "utilities", ".", "device_parser", "import", "_parse_gpu_ids", "return", "_parse_gpu_ids", "(", "devices", ",", "include_mps", "=", "True", ")"], "docstring": "Accelerator device parsing logic.", "docstring_tokens": ["accelerator", "device", "parsing", "logic"], "docstring_summary": "Accelerator device parsing logic.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\mps.py", "partition": "train", "function_type": "class_method", "class_name": "MPSAccelerator", "start_line": 48, "end_line": 52, "hash": "56107523af214017143c6558d51718a9", "complexity": 1, "parameters": ["devices", "str", "list[int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "get_parallel_devices", "original_string": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "\"", "\"", "\"", "Gets", "parallel", "devices", "for", "the", "Accelerator", ".", "\"", "\"", "\"", "parsed_devices", "=", "MPSAccelerator", ".", "parse_devices", "(", "devices", ")", "assert", "parsed_devices", "is", "not", "None", "return", "[", "torch", ".", "device", "(", "\"", "mps", "\"", ",", "i", ")", "for", "i", "in", "range", "(", "len", "(", "parsed_devices", ")", ")", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "docstring_summary": "Gets parallel devices for the Accelerator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\mps.py", "partition": "train", "function_type": "class_method", "class_name": "MPSAccelerator", "start_line": 56, "end_line": 60, "hash": "5328051a60047e087639d2d52bdbce61", "complexity": 2, "parameters": ["devices", "str", "list[int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "is_available", "original_string": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")", "language": "python", "code": "def is_available() -> bool:\r\n        \"\"\"MPS is only available on a machine with the ARM-based Apple Silicon processors.\"\"\"\r\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\r\n        return not mps_disabled and torch.backends.mps.is_available() and platform.processor() in (\"arm\", \"arm64\")", "code_tokens": ["def", "is_available", "(", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "MPS", "is", "only", "available", "on", "a", "machine", "with", "the", "ARM", "-", "based", "Apple", "Silicon", "processors", ".", "\"", "\"", "\"", "mps_disabled", "=", "os", ".", "getenv", "(", "\"", "DISABLE_MPS", "\"", ",", "\"", "0", "\"", ")", "=", "=", "\"", "1", "\"", "return", "not", "mps_disabled", "and", "torch", ".", "backends", ".", "mps", ".", "is_available", "(", ")", "and", "platform", ".", "processor", "(", ")", "in", "(", "\"", "arm", "\"", ",", "\"", "arm64", "\"", ")"], "docstring": "MPS is only available on a machine with the ARM-based Apple Silicon processors.", "docstring_tokens": ["mps", "is", "only", "available", "on", "a", "machine", "with", "the", "arm", "based", "apple", "silicon", "processors"], "docstring_summary": "MPS is only available on a machine with the ARM-based Apple Silicon processors.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\mps.py", "partition": "train", "function_type": "class_method", "class_name": "MPSAccelerator", "start_line": 71, "end_line": 74, "hash": "8f129f38d91dc786b15669ad1e6cd6c1", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\mps.py", "func_name": "_get_all_available_mps_gpus", "original_string": "def _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []", "language": "python", "code": "def _get_all_available_mps_gpus() -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available MPS GPUs\r\n    \"\"\"\r\n    return [0] if MPSAccelerator.is_available() else []", "code_tokens": ["def", "_get_all_available_mps_gpus", "(", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "Returns", ":", "A", "list", "of", "all", "available", "MPS", "GPUs", "\"", "\"", "\"", "return", "[", "0", "]", "if", "MPSAccelerator", ".", "is_available", "(", ")", "else", "[", "]"], "docstring": "Returns:\r\n        A list of all available MPS GPUs", "docstring_tokens": ["returns", "a", "list", "of", "all", "available", "mps", "gpus"], "docstring_summary": "Returns:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\mps.py", "partition": "train", "function_type": "function", "start_line": 86, "end_line": 91, "hash": "45982adc71762249a85be46109b2ddcb", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "register", "original_string": "def register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register", "language": "python", "code": "def register(\r\n        self,\r\n        name: str,\r\n        accelerator: Optional[Callable] = None,\r\n        description: str = \"\",\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise MisconfigurationException(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n\r\n        data[\"description\"] = description\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(accelerator: Callable) -> Callable:\r\n            data[\"accelerator\"] = accelerator\r\n            data[\"accelerator_name\"] = name\r\n            self[name] = data\r\n            return accelerator\r\n\r\n        if accelerator is not None:\r\n            return do_register(accelerator)\r\n\r\n        return do_register", "code_tokens": ["def", "register", "(", "self", ",", "name", ":", "str", ",", "accelerator", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "description", ":", "str", "=", "\"", "\"", ",", "override", ":", "bool", "=", "False", ",", "*", "*", "init_params", ":", "Any", ",", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Registers", "a", "accelerator", "mapped", "to", "a", "name", "and", "with", "required", "metadata", ".", "Args", ":", "name", ":", "the", "name", "that", "identifies", "a", "accelerator", ",", "e", ".", "g", ".", "\"", "gpu", "\"", "accelerator", ":", "accelerator", "class", "description", ":", "accelerator", "description", "override", ":", "overrides", "the", "registered", "accelerator", ",", "if", "True", "init_params", ":", "parameters", "to", "initialize", "the", "accelerator", "\"", "\"", "\"", "if", "not", "(", "name", "is", "None", "or", "isinstance", "(", "name", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "f", "\"", "`", "name", "`", "must", "be", "a", "str", ",", "found", "{", "name", "}", "\"", ")", "if", "name", "in", "self", "and", "not", "override", ":", "raise", "MisconfigurationException", "(", "f", "\"", "'", "{", "name", "}", "'", "is", "already", "present", "in", "the", "registry", ".", "HINT", ":", "Use", "`", "override", "=", "True", "`", ".", "\"", ")", "data", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "data", "[", "\"", "description", "\"", "]", "=", "description", "data", "[", "\"", "init_params", "\"", "]", "=", "init_params", "def", "do_register", "(", "accelerator", ":", "Callable", ")", "-", ">", "Callable", ":", "data", "[", "\"", "accelerator", "\"", "]", "=", "accelerator", "data", "[", "\"", "accelerator_name", "\"", "]", "=", "name", "self", "[", "name", "]", "=", "data", "return", "accelerator", "if", "accelerator", "is", "not", "None", ":", "return", "do_register", "(", "accelerator", ")", "return", "do_register"], "docstring": "Registers a accelerator mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a accelerator, e.g. \"gpu\"\r\n            accelerator : accelerator class\r\n            description : accelerator description\r\n            override : overrides the registered accelerator, if True\r\n            init_params: parameters to initialize the accelerator", "docstring_tokens": ["registers", "a", "accelerator", "mapped", "to", "a", "name", "and", "with", "required", "metadata", "args", "name", "the", "name", "that", "identifies", "a", "accelerator", "e", "g", "gpu", "accelerator", "accelerator", "class", "description", "accelerator", "description", "override", "overrides", "the", "registered", "accelerator", "if", "true", "init_params", "parameters", "to", "initialize", "the", "accelerator"], "docstring_summary": "Registers a accelerator mapped to a name and with required metadata.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\registry.py", "partition": "train", "function_type": "class_method", "class_name": "_AcceleratorRegistry", "start_line": 46, "end_line": 84, "hash": "f7d37e1366cf1faf9cec3121ae892367", "complexity": 6, "parameters": ["name", "accelerator", "description", "override", "**init_params"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "get", "original_string": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))", "language": "python", "code": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"accelerator\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = self.available_accelerators()\r\n        raise KeyError(err_msg.format(name, available_names))", "code_tokens": ["def", "get", "(", "self", ",", "name", ":", "str", ",", "default", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Calls", "the", "registered", "accelerator", "with", "the", "required", "parameters", "and", "returns", "the", "accelerator", "object", ".", "Args", ":", "name", "(", "str", ")", ":", "the", "name", "that", "identifies", "a", "accelerator", ",", "e", ".", "g", ".", "\"", "gpu", "\"", "\"", "\"", "\"", "if", "name", "in", "self", ":", "data", "=", "self", "[", "name", "]", "return", "data", "[", "\"", "accelerator", "\"", "]", "(", "*", "*", "data", "[", "\"", "init_params", "\"", "]", ")", "if", "default", "is", "not", "None", ":", "return", "default", "err_msg", "=", "\"", "'", "{", "}", "'", "not", "found", "in", "registry", ".", "Available", "names", ":", "{", "}", "\"", "available_names", "=", "self", ".", "available_accelerators", "(", ")", "raise", "KeyError", "(", "err_msg", ".", "format", "(", "name", ",", "available_names", ")", ")"], "docstring": "Calls the registered accelerator with the required parameters and returns the accelerator object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a accelerator, e.g. \"gpu\"", "docstring_tokens": ["calls", "the", "registered", "accelerator", "with", "the", "required", "parameters", "and", "returns", "the", "accelerator", "object", "args", "name", "str", "the", "name", "that", "identifies", "a", "accelerator", "e", "g", "gpu"], "docstring_summary": "Calls the registered accelerator with the required parameters and returns the accelerator object.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\registry.py", "partition": "train", "function_type": "class_method", "class_name": "_AcceleratorRegistry", "start_line": 87, "end_line": 103, "hash": "fc6f96d833e33e28f9eaaab71d8470d5", "complexity": 3, "parameters": ["name", "default"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\registry.py", "func_name": "call_register_accelerators", "original_string": "def call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)", "language": "python", "code": "def call_register_accelerators(registry: _AcceleratorRegistry, base_module: str) -> None:  # pragma: no-cover\r\n    \"\"\"Legacy.\r\n\r\n    Do not use.\r\n\r\n    \"\"\"\r\n    import importlib\r\n\r\n    module = importlib.import_module(base_module)\r\n    from lightning.fabric.accelerators.accelerator import Accelerator\r\n\r\n    _register_classes(registry, \"register_accelerators\", module, Accelerator)", "code_tokens": ["def", "call_register_accelerators", "(", "registry", ":", "_AcceleratorRegistry", ",", "base_module", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Legacy", ".", "Do", "not", "use", ".", "\"", "\"", "\"", "import", "importlib", "module", "=", "importlib", ".", "import_module", "(", "base_module", ")", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "accelerator", "import", "Accelerator", "_register_classes", "(", "registry", ",", "\"", "register_accelerators", "\"", ",", "module", ",", "Accelerator", ")"], "docstring": "Legacy.\r\n\r\n    Do not use.", "docstring_tokens": ["legacy", "do", "not", "use"], "docstring_summary": "Legacy.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\registry.py", "partition": "train", "function_type": "function", "start_line": 117, "end_line": 128, "hash": "064115c9979a77cde3052b9381b47d95", "complexity": 1, "parameters": ["registry", "base_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "get_parallel_devices", "original_string": "def get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        # list of devices is not supported, just a specific index, fine to access [0]\r\n        return [torch.device(\"xla\", devices[0])]\r\n        # we cannot create `xla_device` here because processes have not been spawned yet (this is called in the\r\n        # accelerator connector init). However, there doesn't seem to be a problem with instantiating `torch.device`.\r\n        # it will be replaced with `xla_device` (also a torch.device`, but with extra logic) in the strategy\r", "language": "python", "code": "def get_parallel_devices(devices: Union[int, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_tpu_devices(devices)\r\n        if isinstance(devices, int):\r\n            return [torch.device(\"xla\", i) for i in range(devices)]\r\n        # list of devices is not supported, just a specific index, fine to access [0]\r\n        return [torch.device(\"xla\", devices[0])]\r\n        # we cannot create `xla_device` here because processes have not been spawned yet (this is called in the\r\n        # accelerator connector init). However, there doesn't seem to be a problem with instantiating `torch.device`.\r\n        # it will be replaced with `xla_device` (also a torch.device`, but with extra logic) in the strategy\r", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "list", "[", "int", "]", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "\"", "\"", "\"", "Gets", "parallel", "devices", "for", "the", "Accelerator", ".", "\"", "\"", "\"", "devices", "=", "_parse_tpu_devices", "(", "devices", ")", "if", "isinstance", "(", "devices", ",", "int", ")", ":", "return", "[", "torch", ".", "device", "(", "\"", "xla", "\"", ",", "i", ")", "for", "i", "in", "range", "(", "devices", ")", "]", "return", "[", "torch", ".", "device", "(", "\"", "xla", "\"", ",", "devices", "[", "0", "]", ")", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "docstring_summary": "Gets parallel devices for the Accelerator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAAccelerator", "start_line": 55, "end_line": 64, "hash": "107aa306a1cb8216d9d59329d9b50abf", "complexity": 3, "parameters": ["devices", "list[int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "auto_device_count", "original_string": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)", "language": "python", "code": "def auto_device_count() -> int:\r\n        \"\"\"Get the devices when set to auto.\"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            return 0\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla._internal import tpu\r\n\r\n            return tpu.num_available_devices()\r\n        from torch_xla.experimental import tpu\r\n\r\n        device_count_on_version = {2: 8, 3: 8, 4: 4}\r\n        return device_count_on_version.get(tpu.version(), 8)", "code_tokens": ["def", "auto_device_count", "(", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Get", "the", "devices", "when", "set", "to", "auto", ".", "\"", "\"", "\"", "if", "not", "_XLA_AVAILABLE", ":", "return", "0", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", ".", "_internal", "import", "tpu", "return", "tpu", ".", "num_available_devices", "(", ")", "from", "torch_xla", ".", "experimental", "import", "tpu", "device_count_on_version", "=", "{", "2", ":", "8", ",", "3", ":", "8", ",", "4", ":", "4", "}", "return", "device_count_on_version", ".", "get", "(", "tpu", ".", "version", "(", ")", ",", "8", ")"], "docstring": "Get the devices when set to auto.", "docstring_tokens": ["get", "the", "devices", "when", "set", "to", "auto"], "docstring_summary": "Get the devices when set to auto.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAAccelerator", "start_line": 71, "end_line": 82, "hash": "38d2099e36acec3d10f7b706e4cf84dc", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\accelerators\\xla.py", "func_name": "_parse_tpu_devices", "original_string": "def _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices", "language": "python", "code": "def _parse_tpu_devices(devices: Union[int, str, list[int]]) -> Union[int, list[int]]:\r\n    \"\"\"Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.\r\n\r\n    \"\"\"\r\n    _check_data_type(devices)\r\n    if isinstance(devices, str):\r\n        devices = _parse_tpu_devices_str(devices)\r\n    _check_tpu_devices_valid(devices)\r\n    return devices", "code_tokens": ["def", "_parse_tpu_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "Union", "[", "int", ",", "list", "[", "int", "]", "]", ":", "\"", "\"", "\"", "Parses", "the", "TPU", "devices", "given", "in", "the", "format", "as", "accepted", "by", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "and", ":", "class", ":", "`", "~", "lightning", ".", "fabric", ".", "Fabric", "`", ".", "Args", ":", "devices", ":", "An", "int", "of", "1", "or", "string", "'", "1", "'", "indicates", "that", "1", "core", "with", "multi", "-", "processing", "should", "be", "used", "An", "int", "8", "or", "string", "'", "8", "'", "indicates", "that", "all", "8", "cores", "with", "multi", "-", "processing", "should", "be", "used", "A", "single", "element", "list", "of", "int", "or", "string", "can", "be", "used", "to", "indicate", "the", "specific", "TPU", "core", "to", "use", ".", "Returns", ":", "A", "list", "of", "tpu", "cores", "to", "be", "used", ".", "\"", "\"", "\"", "_check_data_type", "(", "devices", ")", "if", "isinstance", "(", "devices", ",", "str", ")", ":", "devices", "=", "_parse_tpu_devices_str", "(", "devices", ")", "_check_tpu_devices_valid", "(", "devices", ")", "return", "devices"], "docstring": "Parses the TPU devices given in the format as accepted by the\r\n    :class:`~lightning.pytorch.trainer.trainer.Trainer` and :class:`~lightning.fabric.Fabric`.\r\n\r\n    Args:\r\n        devices: An int of 1 or string '1' indicates that 1 core with multi-processing should be used\r\n            An int 8 or string '8' indicates that all 8 cores with multi-processing should be used\r\n            A single element list of int or string can be used to indicate the specific TPU core to use.\r\n\r\n    Returns:\r\n        A list of tpu cores to be used.", "docstring_tokens": ["parses", "the", "tpu", "devices", "given", "in", "the", "format", "as", "accepted", "by", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "and", "class", "lightning", "fabric", "fabric", "args", "devices", "an", "int", "of", "1", "or", "string", "1", "indicates", "that", "1", "core", "with", "multi", "processing", "should", "be", "used", "an", "int", "8", "or", "string", "8", "indicates", "that", "all", "8", "cores", "with", "multi", "processing", "should", "be", "used", "a", "single", "element", "list", "of", "int", "or", "string", "can", "be", "used", "to", "indicate", "the", "specific", "tpu", "core", "to", "use", "returns", "a", "list", "of", "tpu", "cores", "to", "be", "used"], "docstring_summary": "Parses the TPU devices given in the format as accepted by the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\accelerators\\xla.py", "partition": "train", "function_type": "function", "start_line": 124, "end_line": 141, "hash": "4a72ec7f3ddc5e44eafc38c8227866a5", "complexity": 2, "parameters": ["devices", "str", "list[int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "name", "original_string": "def name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "language": "python", "code": "def name(self) -> str:\r\n        \"\"\"Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Gets", "the", "name", "of", "the", "experiment", ".", "Returns", ":", "The", "name", "of", "the", "experiment", ".", "\"", "\"", "\"", "return", "self", ".", "_name"], "docstring": "Gets the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.", "docstring_tokens": ["gets", "the", "name", "of", "the", "experiment", "returns", "the", "name", "of", "the", "experiment"], "docstring_summary": "Gets the name of the experiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 80, "end_line": 87, "hash": "721a09a39233c5e06640d15c4a3c466f", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "version", "original_string": "def version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "language": "python", "code": "def version(self) -> Union[int, str]:\r\n        \"\"\"Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "version", "of", "the", "experiment", ".", "Returns", ":", "The", "version", "of", "the", "experiment", "if", "it", "is", "specified", ",", "else", "the", "next", "version", ".", "\"", "\"", "\"", "if", "self", ".", "_version", "is", "None", ":", "self", ".", "_version", "=", "self", ".", "_get_next_version", "(", ")", "return", "self", ".", "_version"], "docstring": "Gets the version of the experiment.\r\n\r\n        Returns:\r\n            The version of the experiment if it is specified, else the next version.", "docstring_tokens": ["gets", "the", "version", "of", "the", "experiment", "returns", "the", "version", "of", "the", "experiment", "if", "it", "is", "specified", "else", "the", "next", "version"], "docstring_summary": "Gets the version of the experiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 91, "end_line": 100, "hash": "b8bfad8c469330adbeed2745b3af8027", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "log_dir", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self._root_dir, self.name, version)", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "log", "directory", "for", "this", "run", ".", "By", "default", ",", "it", "is", "named", "`", "`", "'", "version_", "$", "{", "self", ".", "version", "}", "'", "`", "`", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "'", "s", "version", "parameter", "instead", "of", "`", "`", "None", "`", "`", "or", "an", "int", ".", "\"", "\"", "\"", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "f", "\"", "version_", "{", "self", ".", "version", "}", "\"", "return", "os", ".", "path", ".", "join", "(", "self", ".", "_root_dir", ",", "self", ".", "name", ",", "version", ")"], "docstring": "The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.", "docstring_tokens": ["the", "log", "directory", "for", "this", "run", "by", "default", "it", "is", "named", "version_", "self", "version", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "s", "version", "parameter", "instead", "of", "none", "or", "an", "int"], "docstring_summary": "The log directory for this run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 110, "end_line": 119, "hash": "c64af1788af8b5324dd611c5e48b2768", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "experiment", "original_string": "def experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> \"_ExperimentWriter\":\r\n        \"\"\"Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        os.makedirs(self._root_dir, exist_ok=True)\r\n        self._experiment = _ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "\"", "_ExperimentWriter", "\"", ":", "\"", "\"", "\"", "Actual", "ExperimentWriter", "object", ".", "To", "use", "ExperimentWriter", "features", "anywhere", "in", "your", "code", ",", "do", "the", "following", ".", "Example", ":", ":", "self", ".", "logger", ".", "experiment", ".", "some_experiment_writer_function", "(", ")", "\"", "\"", "\"", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", "os", ".", "makedirs", "(", "self", ".", "_root_dir", ",", "exist_ok", "=", "True", ")", "self", ".", "_experiment", "=", "_ExperimentWriter", "(", "log_dir", "=", "self", ".", "log_dir", ")", "return", "self", ".", "_experiment"], "docstring": "Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()", "docstring_tokens": ["actual", "experimentwriter", "object", "to", "use", "experimentwriter", "features", "anywhere", "in", "your", "code", "do", "the", "following", "example", "self", "logger", "experiment", "some_experiment_writer_function"], "docstring_summary": "Actual ExperimentWriter object. To use ExperimentWriter features anywhere in your code, do the following.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 123, "end_line": 136, "hash": "833991027599f52465f9ab4147870344", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "save", "original_string": "def save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            # we need to re-write the file if the keys (header) change\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                # only write the header if we're writing a fresh file\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset\r", "language": "python", "code": "def save(self) -> None:\r\n        \"\"\"Save recorded metrics into files.\"\"\"\r\n        if not self.metrics:\r\n            return\r\n\r\n        new_keys = self._record_new_keys()\r\n        file_exists = self._fs.isfile(self.metrics_file_path)\r\n\r\n        if new_keys and file_exists:\r\n            # we need to re-write the file if the keys (header) change\r\n            self._rewrite_with_new_header(self.metrics_keys)\r\n\r\n        with self._fs.open(self.metrics_file_path, mode=(\"a\" if file_exists else \"w\"), newline=\"\") as file:\r\n            writer = csv.DictWriter(file, fieldnames=self.metrics_keys)\r\n            if not file_exists:\r\n                # only write the header if we're writing a fresh file\r\n                writer.writeheader()\r\n            writer.writerows(self.metrics)\r\n\r\n        self.metrics = []  # reset\r", "code_tokens": ["def", "save", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "recorded", "metrics", "into", "files", ".", "\"", "\"", "\"", "if", "not", "self", ".", "metrics", ":", "return", "new_keys", "=", "self", ".", "_record_new_keys", "(", ")", "file_exists", "=", "self", ".", "_fs", ".", "isfile", "(", "self", ".", "metrics_file_path", ")", "if", "new_keys", "and", "file_exists", ":", "self", ".", "_rewrite_with_new_header", "(", "self", ".", "metrics_keys", ")", "with", "self", ".", "_fs", ".", "open", "(", "self", ".", "metrics_file_path", ",", "mode", "=", "(", "\"", "a", "\"", "if", "file_exists", "else", "\"", "w", "\"", ")", ",", "newline", "=", "\"", "\"", ")", "as", "file", ":", "writer", "=", "csv", ".", "DictWriter", "(", "file", ",", "fieldnames", "=", "self", ".", "metrics_keys", ")", "if", "not", "file_exists", ":", "writer", ".", "writeheader", "(", ")", "writer", ".", "writerows", "(", "self", ".", "metrics", ")", "self", ".", "metrics", "=", "[", "]"], "docstring": "Save recorded metrics into files.", "docstring_tokens": ["save", "recorded", "metrics", "into", "files"], "docstring_summary": "Save recorded metrics into files.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "_ExperimentWriter", "start_line": 227, "end_line": 246, "hash": "e6aac18fe884f138ac7ced8c0ee4c89c", "complexity": 7, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\csv_logs.py", "func_name": "_record_new_keys", "original_string": "def _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys", "language": "python", "code": "def _record_new_keys(self) -> set[str]:\r\n        \"\"\"Records new keys that have not been logged before.\"\"\"\r\n        current_keys = set().union(*self.metrics)\r\n        new_keys = current_keys - set(self.metrics_keys)\r\n        self.metrics_keys.extend(new_keys)\r\n        self.metrics_keys.sort()\r\n        return new_keys", "code_tokens": ["def", "_record_new_keys", "(", "self", ")", "-", ">", "set", "[", "str", "]", ":", "\"", "\"", "\"", "Records", "new", "keys", "that", "have", "not", "been", "logged", "before", ".", "\"", "\"", "\"", "current_keys", "=", "set", "(", ")", ".", "union", "(", "*", "self", ".", "metrics", ")", "new_keys", "=", "current_keys", "-", "set", "(", "self", ".", "metrics_keys", ")", "self", ".", "metrics_keys", ".", "extend", "(", "new_keys", ")", "self", ".", "metrics_keys", ".", "sort", "(", ")", "return", "new_keys"], "docstring": "Records new keys that have not been logged before.", "docstring_tokens": ["records", "new", "keys", "that", "have", "not", "been", "logged", "before"], "docstring_summary": "Records new keys that have not been logged before.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "_ExperimentWriter", "start_line": 248, "end_line": 254, "hash": "2b1e53957bd0b82f90bb051104f12e8e", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "root_dir", "original_string": "def root_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where all versions of an experiment get saved, or `None` if the logger does not\r\n        save data locally.\"\"\"\r\n        return None", "language": "python", "code": "def root_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where all versions of an experiment get saved, or `None` if the logger does not\r\n        save data locally.\"\"\"\r\n        return None", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Return", "the", "root", "directory", "where", "all", "versions", "of", "an", "experiment", "get", "saved", ",", "or", "`", "None", "`", "if", "the", "logger", "does", "not", "save", "data", "locally", ".", "\"", "\"", "\"", "return", "None"], "docstring": "Return the root directory where all versions of an experiment get saved, or `None` if the logger does not\r\n        save data locally.", "docstring_tokens": ["return", "the", "root", "directory", "where", "all", "versions", "of", "an", "experiment", "get", "saved", "or", "none", "if", "the", "logger", "does", "not", "save", "data", "locally"], "docstring_summary": "Return the root directory where all versions of an experiment get saved, or `None` if the logger does not", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 40, "end_line": 43, "hash": "00212259448de1ea2f1c4b653cdfd15c", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "log_dir", "original_string": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"Return directory the current version of the experiment gets saved, or `None` if the logger does not save\r\n        data locally.\"\"\"\r\n        return None", "language": "python", "code": "def log_dir(self) -> Optional[str]:\r\n        \"\"\"Return directory the current version of the experiment gets saved, or `None` if the logger does not save\r\n        data locally.\"\"\"\r\n        return None", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Return", "directory", "the", "current", "version", "of", "the", "experiment", "gets", "saved", ",", "or", "`", "None", "`", "if", "the", "logger", "does", "not", "save", "data", "locally", ".", "\"", "\"", "\"", "return", "None"], "docstring": "Return directory the current version of the experiment gets saved, or `None` if the logger does not save\r\n        data locally.", "docstring_tokens": ["return", "directory", "the", "current", "version", "of", "the", "experiment", "gets", "saved", "or", "none", "if", "the", "logger", "does", "not", "save", "data", "locally"], "docstring_summary": "Return directory the current version of the experiment gets saved, or `None` if the logger does not save", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 46, "end_line": 49, "hash": "fdeaf9e46601c19b672d390b3330927b", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "log_metrics", "original_string": "def log_metrics(self, metrics: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Records metrics. This method logs metrics as soon as it received them.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def log_metrics(self, metrics: dict[str, float], step: Optional[int] = None) -> None:\r\n        \"\"\"Records metrics. This method logs metrics as soon as it received them.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics", ":", "dict", "[", "str", ",", "float", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Records", "metrics", ".", "This", "method", "logs", "metrics", "as", "soon", "as", "it", "received", "them", ".", "Args", ":", "metrics", ":", "Dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", ":", "Step", "number", "at", "which", "the", "metrics", "should", "be", "recorded", "\"", "\"", "\"", "pass"], "docstring": "Records metrics. This method logs metrics as soon as it received them.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Step number at which the metrics should be recorded", "docstring_tokens": ["records", "metrics", "this", "method", "logs", "metrics", "as", "soon", "as", "it", "received", "them", "args", "metrics", "dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", "step", "number", "at", "which", "the", "metrics", "should", "be", "recorded"], "docstring_summary": "Records metrics. This method logs metrics as soon as it received them.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 57, "end_line": 65, "hash": "964caa75e055b32ffb91564261c69326", "complexity": 1, "parameters": ["metrics", "float]", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "log_hyperparams", "original_string": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Record hyperparameters.\r\n\r\n        Args:\r\n            params: :class:`~argparse.Namespace` or `Dict` containing the hyperparameters\r\n            args: Optional positional arguments, depends on the specific logger being used\r\n            kwargs: Optional keyword arguments, depends on the specific logger being used\r\n\r\n        \"\"\"", "language": "python", "code": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Record hyperparameters.\r\n\r\n        Args:\r\n            params: :class:`~argparse.Namespace` or `Dict` containing the hyperparameters\r\n            args: Optional positional arguments, depends on the specific logger being used\r\n            kwargs: Optional keyword arguments, depends on the specific logger being used\r\n\r\n        \"\"\"", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Record", "hyperparameters", ".", "Args", ":", "params", ":", ":", "class", ":", "`", "~", "argparse", ".", "Namespace", "`", "or", "`", "Dict", "`", "containing", "the", "hyperparameters", "args", ":", "Optional", "positional", "arguments", ",", "depends", "on", "the", "specific", "logger", "being", "used", "kwargs", ":", "Optional", "keyword", "arguments", ",", "depends", "on", "the", "specific", "logger", "being", "used", "\"", "\"", "\""], "docstring": "Record hyperparameters.\r\n\r\n        Args:\r\n            params: :class:`~argparse.Namespace` or `Dict` containing the hyperparameters\r\n            args: Optional positional arguments, depends on the specific logger being used\r\n            kwargs: Optional keyword arguments, depends on the specific logger being used", "docstring_tokens": ["record", "hyperparameters", "args", "params", "class", "argparse", "namespace", "or", "dict", "containing", "the", "hyperparameters", "args", "optional", "positional", "arguments", "depends", "on", "the", "specific", "logger", "being", "used", "kwargs", "optional", "keyword", "arguments", "depends", "on", "the", "specific", "logger", "being", "used"], "docstring_summary": "Record hyperparameters.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 68, "end_line": 76, "hash": "61c23eb8686d6a316ba18d5e70245c62", "complexity": 1, "parameters": ["params", "Any]", "Namespace]", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "log_graph", "original_string": "def log_graph(self, model: Module, input_array: Optional[Tensor] = None) -> None:\r\n        \"\"\"Record model graph.\r\n\r\n        Args:\r\n            model: the model with an implementation of ``forward``.\r\n            input_array: input passes to `model.forward`\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def log_graph(self, model: Module, input_array: Optional[Tensor] = None) -> None:\r\n        \"\"\"Record model graph.\r\n\r\n        Args:\r\n            model: the model with an implementation of ``forward``.\r\n            input_array: input passes to `model.forward`\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "log_graph", "(", "self", ",", "model", ":", "Module", ",", "input_array", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Record", "model", "graph", ".", "Args", ":", "model", ":", "the", "model", "with", "an", "implementation", "of", "`", "`", "forward", "`", "`", ".", "input_array", ":", "input", "passes", "to", "`", "model", ".", "forward", "`", "\"", "\"", "\"", "pass"], "docstring": "Record model graph.\r\n\r\n        Args:\r\n            model: the model with an implementation of ``forward``.\r\n            input_array: input passes to `model.forward`", "docstring_tokens": ["record", "model", "graph", "args", "model", "the", "model", "with", "an", "implementation", "of", "forward", "input_array", "input", "passes", "to", "model", "forward"], "docstring_summary": "Record model graph.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 78, "end_line": 86, "hash": "a54eef2b103b1aa7fe5b02effcdbdb81", "complexity": 1, "parameters": ["model", "input_array"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "finalize", "original_string": "def finalize(self, status: str) -> None:\r\n        \"\"\"Do any processing that is necessary to finalize an experiment.\r\n\r\n        Args:\r\n            status: Status that the experiment finished with (e.g. success, failed, aborted)\r\n\r\n        \"\"\"\r\n        self.save()", "language": "python", "code": "def finalize(self, status: str) -> None:\r\n        \"\"\"Do any processing that is necessary to finalize an experiment.\r\n\r\n        Args:\r\n            status: Status that the experiment finished with (e.g. success, failed, aborted)\r\n\r\n        \"\"\"\r\n        self.save()", "code_tokens": ["def", "finalize", "(", "self", ",", "status", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Do", "any", "processing", "that", "is", "necessary", "to", "finalize", "an", "experiment", ".", "Args", ":", "status", ":", "Status", "that", "the", "experiment", "finished", "with", "(", "e", ".", "g", ".", "success", ",", "failed", ",", "aborted", ")", "\"", "\"", "\"", "self", ".", "save", "(", ")"], "docstring": "Do any processing that is necessary to finalize an experiment.\r\n\r\n        Args:\r\n            status: Status that the experiment finished with (e.g. success, failed, aborted)", "docstring_tokens": ["do", "any", "processing", "that", "is", "necessary", "to", "finalize", "an", "experiment", "args", "status", "status", "that", "the", "experiment", "finished", "with", "e", "g", "success", "failed", "aborted"], "docstring_summary": "Do any processing that is necessary to finalize an experiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 91, "end_line": 98, "hash": "5f109eefe049db4eefcc9a55a3613b57", "complexity": 1, "parameters": ["status"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\logger.py", "func_name": "rank_zero_experiment", "original_string": "def rank_zero_experiment(fn: Callable) -> Callable:\r\n    \"\"\"Returns the real experiment on rank 0 and otherwise the _DummyExperiment.\"\"\"\r\n\r\n    @wraps(fn)\r\n    def experiment(self: Logger) -> Union[Any, _DummyExperiment]:\r\n        \"\"\"\r\n        Note:\r\n            ``self`` is a custom logger instance. The loggers typically wrap an ``experiment`` method\r\n            with a ``@rank_zero_experiment`` decorator.\r\n\r\n            ``Union[Any, _DummyExperiment]`` is used because the wrapped hooks have several return\r\n            types that are specific to the custom logger. The return type here can be considered as\r\n            ``Union[return type of logger.experiment, _DummyExperiment]``.\r\n        \"\"\"\r\n        if rank_zero_only.rank > 0:\r\n            return _DummyExperiment()\r\n        return fn(self)\r\n\r\n    return experiment", "language": "python", "code": "def rank_zero_experiment(fn: Callable) -> Callable:\r\n    \"\"\"Returns the real experiment on rank 0 and otherwise the _DummyExperiment.\"\"\"\r\n\r\n    @wraps(fn)\r\n    def experiment(self: Logger) -> Union[Any, _DummyExperiment]:\r\n        \"\"\"\r\n        Note:\r\n            ``self`` is a custom logger instance. The loggers typically wrap an ``experiment`` method\r\n            with a ``@rank_zero_experiment`` decorator.\r\n\r\n            ``Union[Any, _DummyExperiment]`` is used because the wrapped hooks have several return\r\n            types that are specific to the custom logger. The return type here can be considered as\r\n            ``Union[return type of logger.experiment, _DummyExperiment]``.\r\n        \"\"\"\r\n        if rank_zero_only.rank > 0:\r\n            return _DummyExperiment()\r\n        return fn(self)\r\n\r\n    return experiment", "code_tokens": ["def", "rank_zero_experiment", "(", "fn", ":", "Callable", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Returns", "the", "real", "experiment", "on", "rank", "0", "and", "otherwise", "the", "_DummyExperiment", ".", "\"", "\"", "\"", "@", "wraps", "(", "fn", ")", "def", "experiment", "(", "self", ":", "Logger", ")", "-", ">", "Union", "[", "Any", ",", "_DummyExperiment", "]", ":", "\"", "\"", "\"", "Note", ":", "`", "`", "self", "`", "`", "is", "a", "custom", "logger", "instance", ".", "The", "loggers", "typically", "wrap", "an", "`", "`", "experiment", "`", "`", "method", "with", "a", "`", "`", "@", "rank_zero_experiment", "`", "`", "decorator", ".", "`", "`", "Union", "[", "Any", ",", "_DummyExperiment", "]", "`", "`", "is", "used", "because", "the", "wrapped", "hooks", "have", "several", "return", "types", "that", "are", "specific", "to", "the", "custom", "logger", ".", "The", "return", "type", "here", "can", "be", "considered", "as", "`", "`", "Union", "[", "return", "type", "of", "logger", ".", "experiment", ",", "_DummyExperiment", "]", "`", "`", ".", "\"", "\"", "\"", "if", "rank_zero_only", ".", "rank", ">", "0", ":", "return", "_DummyExperiment", "(", ")", "return", "fn", "(", "self", ")", "return", "experiment"], "docstring": "Returns the real experiment on rank 0 and otherwise the _DummyExperiment.", "docstring_tokens": ["returns", "the", "real", "experiment", "on", "rank", "0", "and", "otherwise", "the", "_dummyexperiment"], "docstring_summary": "Returns the real experiment on rank 0 and otherwise the _DummyExperiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\logger.py", "partition": "train", "function_type": "function", "start_line": 101, "end_line": 119, "hash": "6cbcf53b12f6fa13bde1ad939457100c", "complexity": 2, "parameters": ["fn"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "name", "original_string": "def name(self) -> str:\r\n        \"\"\"Get the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "language": "python", "code": "def name(self) -> str:\r\n        \"\"\"Get the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.\r\n\r\n        \"\"\"\r\n        return self._name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Get", "the", "name", "of", "the", "experiment", ".", "Returns", ":", "The", "name", "of", "the", "experiment", ".", "\"", "\"", "\"", "return", "self", ".", "_name"], "docstring": "Get the name of the experiment.\r\n\r\n        Returns:\r\n            The name of the experiment.", "docstring_tokens": ["get", "the", "name", "of", "the", "experiment", "returns", "the", "name", "of", "the", "experiment"], "docstring_summary": "Get the name of the experiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 112, "end_line": 119, "hash": "0f4673aaebaf9cc2b5a7b8c2199600e7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "version", "original_string": "def version(self) -> Union[int, str]:\r\n        \"\"\"Get the experiment version.\r\n\r\n        Returns:\r\n            The experiment version if specified else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "language": "python", "code": "def version(self) -> Union[int, str]:\r\n        \"\"\"Get the experiment version.\r\n\r\n        Returns:\r\n            The experiment version if specified else the next version.\r\n\r\n        \"\"\"\r\n        if self._version is None:\r\n            self._version = self._get_next_version()\r\n        return self._version", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "str", "]", ":", "\"", "\"", "\"", "Get", "the", "experiment", "version", ".", "Returns", ":", "The", "experiment", "version", "if", "specified", "else", "the", "next", "version", ".", "\"", "\"", "\"", "if", "self", ".", "_version", "is", "None", ":", "self", ".", "_version", "=", "self", ".", "_get_next_version", "(", ")", "return", "self", ".", "_version"], "docstring": "Get the experiment version.\r\n\r\n        Returns:\r\n            The experiment version if specified else the next version.", "docstring_tokens": ["get", "the", "experiment", "version", "returns", "the", "experiment", "version", "if", "specified", "else", "the", "next", "version"], "docstring_summary": "Get the experiment version.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 123, "end_line": 132, "hash": "967bcb471f7c296cf83f9e3fbe352f01", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "root_dir", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Gets", "the", "save", "directory", "where", "the", "TensorBoard", "experiments", "are", "saved", ".", "Returns", ":", "The", "local", "path", "to", "the", "save", "directory", "where", "the", "TensorBoard", "experiments", "are", "saved", ".", "\"", "\"", "\"", "return", "self", ".", "_root_dir"], "docstring": "Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.", "docstring_tokens": ["gets", "the", "save", "directory", "where", "the", "tensorboard", "experiments", "are", "saved", "returns", "the", "local", "path", "to", "the", "save", "directory", "where", "the", "tensorboard", "experiments", "are", "saved"], "docstring_summary": "Gets the save directory where the TensorBoard experiments are saved.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 136, "end_line": 143, "hash": "cd8710d9ee70eed6d8de196c278faf3e", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "log_dir", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, self.name, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, self.name, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "directory", "for", "this", "run", "'", "s", "tensorboard", "checkpoint", ".", "By", "default", ",", "it", "is", "named", "`", "`", "'", "version_", "$", "{", "self", ".", "version", "}", "'", "`", "`", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "'", "s", "version", "parameter", "instead", "of", "`", "`", "None", "`", "`", "or", "an", "int", ".", "\"", "\"", "\"", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "f", "\"", "version_", "{", "self", ".", "version", "}", "\"", "log_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "self", ".", "name", ",", "version", ")", "if", "isinstance", "(", "self", ".", "sub_dir", ",", "str", ")", ":", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "self", ".", "sub_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expandvars", "(", "log_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "log_dir", ")", "return", "log_dir"], "docstring": "The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.", "docstring_tokens": ["the", "directory", "for", "this", "run", "s", "tensorboard", "checkpoint", "by", "default", "it", "is", "named", "version_", "self", "version", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "s", "version", "parameter", "instead", "of", "none", "or", "an", "int"], "docstring_summary": "The directory for this run's tensorboard checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 147, "end_line": 160, "hash": "c4ebcad0bf876bd8f4bba92027513f0a", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "sub_dir", "original_string": "def sub_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the sub directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the sub directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._sub_dir", "language": "python", "code": "def sub_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the sub directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the sub directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._sub_dir", "code_tokens": ["def", "sub_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "sub", "directory", "where", "the", "TensorBoard", "experiments", "are", "saved", ".", "Returns", ":", "The", "local", "path", "to", "the", "sub", "directory", "where", "the", "TensorBoard", "experiments", "are", "saved", ".", "\"", "\"", "\"", "return", "self", ".", "_sub_dir"], "docstring": "Gets the sub directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the sub directory where the TensorBoard experiments are saved.", "docstring_tokens": ["gets", "the", "sub", "directory", "where", "the", "tensorboard", "experiments", "are", "saved", "returns", "the", "local", "path", "to", "the", "sub", "directory", "where", "the", "tensorboard", "experiments", "are", "saved"], "docstring_summary": "Gets the sub directory where the TensorBoard experiments are saved.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 163, "end_line": 170, "hash": "5a076e963924f9c615786bbdcc19680c", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "experiment", "original_string": "def experiment(self) -> \"SummaryWriter\":\r\n        \"\"\"Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            logger.experiment.some_tensorboard_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        assert rank_zero_only.rank == 0, \"tried to init log dirs in non global_rank=0\"\r\n        if self.root_dir:\r\n            self._fs.makedirs(self.root_dir, exist_ok=True)\r\n\r\n        if _TENSORBOARD_AVAILABLE:\r\n            from torch.utils.tensorboard import SummaryWriter\r\n        else:\r\n            from tensorboardX import SummaryWriter  # type: ignore[no-redef]\r\n\r\n        self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> \"SummaryWriter\":\r\n        \"\"\"Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            logger.experiment.some_tensorboard_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        assert rank_zero_only.rank == 0, \"tried to init log dirs in non global_rank=0\"\r\n        if self.root_dir:\r\n            self._fs.makedirs(self.root_dir, exist_ok=True)\r\n\r\n        if _TENSORBOARD_AVAILABLE:\r\n            from torch.utils.tensorboard import SummaryWriter\r\n        else:\r\n            from tensorboardX import SummaryWriter  # type: ignore[no-redef]\r\n\r\n        self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "\"", "SummaryWriter", "\"", ":", "\"", "\"", "\"", "Actual", "tensorboard", "object", ".", "To", "use", "TensorBoard", "features", "anywhere", "in", "your", "code", ",", "do", "the", "following", ".", "Example", ":", ":", "logger", ".", "experiment", ".", "some_tensorboard_function", "(", ")", "\"", "\"", "\"", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", "assert", "rank_zero_only", ".", "rank", "=", "=", "0", ",", "\"", "tried", "to", "init", "log", "dirs", "in", "non", "global_rank", "=", "0", "\"", "if", "self", ".", "root_dir", ":", "self", ".", "_fs", ".", "makedirs", "(", "self", ".", "root_dir", ",", "exist_ok", "=", "True", ")", "if", "_TENSORBOARD_AVAILABLE", ":", "from", "torch", ".", "utils", ".", "tensorboard", "import", "SummaryWriter", "else", ":", "from", "tensorboardX", "import", "SummaryWriter", "self", ".", "_experiment", "=", "SummaryWriter", "(", "log_dir", "=", "self", ".", "log_dir", ",", "*", "*", "self", ".", "_kwargs", ")", "return", "self", ".", "_experiment"], "docstring": "Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.\r\n\r\n        Example::\r\n\r\n            logger.experiment.some_tensorboard_function()", "docstring_tokens": ["actual", "tensorboard", "object", "to", "use", "tensorboard", "features", "anywhere", "in", "your", "code", "do", "the", "following", "example", "logger", "experiment", "some_tensorboard_function"], "docstring_summary": "Actual tensorboard object. To use TensorBoard features anywhere in your code, do the following.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 174, "end_line": 195, "hash": "b84ac58e6fd7e40adbeac3be2a433637", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\loggers\\tensorboard.py", "func_name": "log_hyperparams", "original_string": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        params = _convert_params(params)\r\n\r\n        # format params into the suitable for tensorboard\r\n        params = _flatten_dict(params)\r\n        params = self._sanitize_params(params)\r\n\r\n        if metrics is None:\r\n            if self._default_hp_metric:\r\n                metrics = {\"hp_metric\": -1}\r\n        elif not isinstance(metrics, dict):\r\n            metrics = {\"hp_metric\": metrics}\r\n\r\n        if metrics:\r\n            self.log_metrics(metrics, step)\r\n\r\n            if _TENSORBOARD_AVAILABLE:\r\n                from torch.utils.tensorboard.summary import hparams\r\n            else:\r\n                from tensorboardX.summary import hparams  # type: ignore[no-redef]\r\n\r\n            exp, ssi, sei = hparams(params, metrics)\r\n            writer = self.experiment._get_file_writer()\r\n            writer.add_summary(exp, step)\r\n            writer.add_summary(ssi, step)\r\n            writer.add_summary(sei, step)", "language": "python", "code": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        params = _convert_params(params)\r\n\r\n        # format params into the suitable for tensorboard\r\n        params = _flatten_dict(params)\r\n        params = self._sanitize_params(params)\r\n\r\n        if metrics is None:\r\n            if self._default_hp_metric:\r\n                metrics = {\"hp_metric\": -1}\r\n        elif not isinstance(metrics, dict):\r\n            metrics = {\"hp_metric\": metrics}\r\n\r\n        if metrics:\r\n            self.log_metrics(metrics, step)\r\n\r\n            if _TENSORBOARD_AVAILABLE:\r\n                from torch.utils.tensorboard.summary import hparams\r\n            else:\r\n                from tensorboardX.summary import hparams  # type: ignore[no-redef]\r\n\r\n            exp, ssi, sei = hparams(params, metrics)\r\n            writer = self.experiment._get_file_writer()\r\n            writer.add_summary(exp, step)\r\n            writer.add_summary(ssi, step)\r\n            writer.add_summary(sei, step)", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ",", "metrics", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Record", "hyperparameters", ".", "TensorBoard", "logs", "with", "and", "without", "saved", "hyperparameters", "are", "incompatible", ",", "the", "hyperparameters", "are", "then", "not", "displayed", "in", "the", "TensorBoard", ".", "Please", "delete", "or", "move", "the", "previously", "saved", "logs", "to", "display", "the", "new", "ones", "with", "hyperparameters", ".", "Args", ":", "params", ":", "A", "dictionary", "-", "like", "container", "with", "the", "hyperparameters", "metrics", ":", "Dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", ":", "Optional", "global", "step", "number", "for", "the", "logged", "metrics", "\"", "\"", "\"", "params", "=", "_convert_params", "(", "params", ")", "params", "=", "_flatten_dict", "(", "params", ")", "params", "=", "self", ".", "_sanitize_params", "(", "params", ")", "if", "metrics", "is", "None", ":", "if", "self", ".", "_default_hp_metric", ":", "metrics", "=", "{", "\"", "hp_metric", "\"", ":", "-", "1", "}", "elif", "not", "isinstance", "(", "metrics", ",", "dict", ")", ":", "metrics", "=", "{", "\"", "hp_metric", "\"", ":", "metrics", "}", "if", "metrics", ":", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")", "if", "_TENSORBOARD_AVAILABLE", ":", "from", "torch", ".", "utils", ".", "tensorboard", ".", "summary", "import", "hparams", "else", ":", "from", "tensorboardX", ".", "summary", "import", "hparams", "exp", ",", "ssi", ",", "sei", "=", "hparams", "(", "params", ",", "metrics", ")", "writer", "=", "self", ".", "experiment", ".", "_get_file_writer", "(", ")", "writer", ".", "add_summary", "(", "exp", ",", "step", ")", "writer", ".", "add_summary", "(", "ssi", ",", "step", ")", "writer", ".", "add_summary", "(", "sei", ",", "step", ")"], "docstring": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics", "docstring_tokens": ["record", "hyperparameters", "tensorboard", "logs", "with", "and", "without", "saved", "hyperparameters", "are", "incompatible", "the", "hyperparameters", "are", "then", "not", "displayed", "in", "the", "tensorboard", "please", "delete", "or", "move", "the", "previously", "saved", "logs", "to", "display", "the", "new", "ones", "with", "hyperparameters", "args", "params", "a", "dictionary", "like", "container", "with", "the", "hyperparameters", "metrics", "dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", "optional", "global", "step", "number", "for", "the", "logged", "metrics"], "docstring_summary": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 221, "end_line": 261, "hash": "74f441ebaadc62a026babf36be44f3e5", "complexity": 6, "parameters": ["params", "Any]", "Namespace]", "metrics", "Any]]", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\collectives\\collective.py", "func_name": "create_group", "original_string": "def create_group(self, **kwargs: Any) -> Self:\r\n        \"\"\"Create a group.\r\n\r\n        This assumes that :meth:`~lightning.fabric.plugins.collectives.Collective.init_group` has been\r\n        called already by the user.\r\n\r\n        \"\"\"\r\n        if self._group is not None:\r\n            raise RuntimeError(f\"`{type(self).__name__}` already owns a group.\")\r\n        self._group = self.new_group(**kwargs)\r\n        return self", "language": "python", "code": "def create_group(self, **kwargs: Any) -> Self:\r\n        \"\"\"Create a group.\r\n\r\n        This assumes that :meth:`~lightning.fabric.plugins.collectives.Collective.init_group` has been\r\n        called already by the user.\r\n\r\n        \"\"\"\r\n        if self._group is not None:\r\n            raise RuntimeError(f\"`{type(self).__name__}` already owns a group.\")\r\n        self._group = self.new_group(**kwargs)\r\n        return self", "code_tokens": ["def", "create_group", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "Create", "a", "group", ".", "This", "assumes", "that", ":", "meth", ":", "`", "~", "lightning", ".", "fabric", ".", "plugins", ".", "collectives", ".", "Collective", ".", "init_group", "`", "has", "been", "called", "already", "by", "the", "user", ".", "\"", "\"", "\"", "if", "self", ".", "_group", "is", "not", "None", ":", "raise", "RuntimeError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", "`", "already", "owns", "a", "group", ".", "\"", ")", "self", ".", "_group", "=", "self", ".", "new_group", "(", "*", "*", "kwargs", ")", "return", "self"], "docstring": "Create a group.\r\n\r\n        This assumes that :meth:`~lightning.fabric.plugins.collectives.Collective.init_group` has been\r\n        called already by the user.", "docstring_tokens": ["create", "a", "group", "this", "assumes", "that", "meth", "lightning", "fabric", "plugins", "collectives", "collective", "init_group", "has", "been", "called", "already", "by", "the", "user"], "docstring_summary": "Create a group.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\collectives\\collective.py", "partition": "train", "function_type": "class_method", "class_name": "Collective", "start_line": 101, "end_line": 111, "hash": "595e17aeff27df6fa308443d8b878634", "complexity": 2, "parameters": ["**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "func_name": "validate_settings", "original_string": "def validate_settings(self, num_devices: int, num_nodes: int) -> None:\r\n        \"\"\"Validates settings configured in the script against the environment, and raises an exception if there is an\r\n        inconsistency.\"\"\"\r\n        pass", "language": "python", "code": "def validate_settings(self, num_devices: int, num_nodes: int) -> None:\r\n        \"\"\"Validates settings configured in the script against the environment, and raises an exception if there is an\r\n        inconsistency.\"\"\"\r\n        pass", "code_tokens": ["def", "validate_settings", "(", "self", ",", "num_devices", ":", "int", ",", "num_nodes", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Validates", "settings", "configured", "in", "the", "script", "against", "the", "environment", ",", "and", "raises", "an", "exception", "if", "there", "is", "an", "inconsistency", ".", "\"", "\"", "\"", "pass"], "docstring": "Validates settings configured in the script against the environment, and raises an exception if there is an\r\n        inconsistency.", "docstring_tokens": ["validates", "settings", "configured", "in", "the", "script", "against", "the", "environment", "and", "raises", "an", "exception", "if", "there", "is", "an", "inconsistency"], "docstring_summary": "Validates settings configured in the script against the environment, and raises an exception if there is an", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\cluster_environment.py", "partition": "train", "function_type": "class_method", "class_name": "ClusterEnvironment", "start_line": 63, "end_line": 66, "hash": "34bf49198b0c07cb5b65b17ec12e6d49", "complexity": 1, "parameters": ["num_devices", "num_nodes"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lightning.py", "func_name": "creates_processes_externally", "original_string": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"Returns whether the cluster creates the processes or not.\r\n\r\n        If at least :code:`LOCAL_RANK` is available as environment variable, Lightning assumes the user acts as the\r\n        process launcher/job scheduler and Lightning will not launch new processes.\r\n\r\n        \"\"\"\r\n        return \"LOCAL_RANK\" in os.environ", "language": "python", "code": "def creates_processes_externally(self) -> bool:\r\n        \"\"\"Returns whether the cluster creates the processes or not.\r\n\r\n        If at least :code:`LOCAL_RANK` is available as environment variable, Lightning assumes the user acts as the\r\n        process launcher/job scheduler and Lightning will not launch new processes.\r\n\r\n        \"\"\"\r\n        return \"LOCAL_RANK\" in os.environ", "code_tokens": ["def", "creates_processes_externally", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "whether", "the", "cluster", "creates", "the", "processes", "or", "not", ".", "If", "at", "least", ":", "code", ":", "`", "LOCAL_RANK", "`", "is", "available", "as", "environment", "variable", ",", "Lightning", "assumes", "the", "user", "acts", "as", "the", "process", "launcher", "/", "job", "scheduler", "and", "Lightning", "will", "not", "launch", "new", "processes", ".", "\"", "\"", "\"", "return", "\"", "LOCAL_RANK", "\"", "in", "os", ".", "environ"], "docstring": "Returns whether the cluster creates the processes or not.\r\n\r\n        If at least :code:`LOCAL_RANK` is available as environment variable, Lightning assumes the user acts as the\r\n        process launcher/job scheduler and Lightning will not launch new processes.", "docstring_tokens": ["returns", "whether", "the", "cluster", "creates", "the", "processes", "or", "not", "if", "at", "least", "code", "local_rank", "is", "available", "as", "environment", "variable", "lightning", "assumes", "the", "user", "acts", "as", "the", "process", "launcher", "job", "scheduler", "and", "lightning", "will", "not", "launch", "new", "processes"], "docstring_summary": "Returns whether the cluster creates the processes or not.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lightning.py", "partition": "train", "function_type": "class_method", "class_name": "LightningEnvironment", "start_line": 47, "end_line": 54, "hash": "2e019a23cf292e753c1f41940ee9aae7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lightning.py", "func_name": "find_free_network_port", "original_string": "def find_free_network_port() -> int:\r\n    \"\"\"Finds a free port on localhost.\r\n\r\n    It is useful in single-node training when we don't want to connect to a real main node but have to set the\r\n    `MASTER_PORT` environment variable.\r\n\r\n    \"\"\"\r\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    s.bind((\"\", 0))\r\n    port = s.getsockname()[1]\r\n    s.close()\r\n    return port", "language": "python", "code": "def find_free_network_port() -> int:\r\n    \"\"\"Finds a free port on localhost.\r\n\r\n    It is useful in single-node training when we don't want to connect to a real main node but have to set the\r\n    `MASTER_PORT` environment variable.\r\n\r\n    \"\"\"\r\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    s.bind((\"\", 0))\r\n    port = s.getsockname()[1]\r\n    s.close()\r\n    return port", "code_tokens": ["def", "find_free_network_port", "(", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Finds", "a", "free", "port", "on", "localhost", ".", "It", "is", "useful", "in", "single", "-", "node", "training", "when", "we", "don", "'", "t", "want", "to", "connect", "to", "a", "real", "main", "node", "but", "have", "to", "set", "the", "`", "MASTER_PORT", "`", "environment", "variable", ".", "\"", "\"", "\"", "s", "=", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_STREAM", ")", "s", ".", "bind", "(", "(", "\"", "\"", ",", "0", ")", ")", "port", "=", "s", ".", "getsockname", "(", ")", "[", "1", "]", "s", ".", "close", "(", ")", "return", "port"], "docstring": "Finds a free port on localhost.\r\n\r\n    It is useful in single-node training when we don't want to connect to a real main node but have to set the\r\n    `MASTER_PORT` environment variable.", "docstring_tokens": ["finds", "a", "free", "port", "on", "localhost", "it", "is", "useful", "in", "single", "node", "training", "when", "we", "don", "t", "want", "to", "connect", "to", "a", "real", "main", "node", "but", "have", "to", "set", "the", "master_port", "environment", "variable"], "docstring_summary": "Finds a free port on localhost.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lightning.py", "partition": "train", "function_type": "function", "start_line": 107, "end_line": 118, "hash": "abfd658288a1982a962dfef7ee31fc16", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "main_address", "original_string": "def main_address(self) -> str:\r\n        \"\"\"The main address is read from an OpenMPI host rank file in the environment variable\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._main_address", "language": "python", "code": "def main_address(self) -> str:\r\n        \"\"\"The main address is read from an OpenMPI host rank file in the environment variable\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._main_address", "code_tokens": ["def", "main_address", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "main", "address", "is", "read", "from", "an", "OpenMPI", "host", "rank", "file", "in", "the", "environment", "variable", "`", "`", "LSB_DJOB_RANKFILE", "`", "`", ".", "\"", "\"", "\"", "return", "self", ".", "_main_address"], "docstring": "The main address is read from an OpenMPI host rank file in the environment variable\r\n        ``LSB_DJOB_RANKFILE``.", "docstring_tokens": ["the", "main", "address", "is", "read", "from", "an", "openmpi", "host", "rank", "file", "in", "the", "environment", "variable", "lsb_djob_rankfile"], "docstring_summary": "The main address is read from an OpenMPI host rank file in the environment variable", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 72, "end_line": 75, "hash": "8a3b4a295df7ad9cecce38d9b19dc226", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "detect", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the ``jsrun`` command.\"\"\"\r\n        required_env_vars = {\"LSB_JOBID\", \"LSB_DJOB_RANKFILE\", \"JSM_NAMESPACE_LOCAL_RANK\", \"JSM_NAMESPACE_SIZE\"}\r\n        return required_env_vars.issubset(os.environ.keys())", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the ``jsrun`` command.\"\"\"\r\n        required_env_vars = {\"LSB_JOBID\", \"LSB_DJOB_RANKFILE\", \"JSM_NAMESPACE_LOCAL_RANK\", \"JSM_NAMESPACE_SIZE\"}\r\n        return required_env_vars.issubset(os.environ.keys())", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "`", "`", "True", "`", "`", "if", "the", "current", "process", "was", "launched", "using", "the", "`", "`", "jsrun", "`", "`", "command", ".", "\"", "\"", "\"", "required_env_vars", "=", "{", "\"", "LSB_JOBID", "\"", ",", "\"", "LSB_DJOB_RANKFILE", "\"", ",", "\"", "JSM_NAMESPACE_LOCAL_RANK", "\"", ",", "\"", "JSM_NAMESPACE_SIZE", "\"", "}", "return", "required_env_vars", ".", "issubset", "(", "os", ".", "environ", ".", "keys", "(", ")", ")"], "docstring": "Returns ``True`` if the current process was launched using the ``jsrun`` command.", "docstring_tokens": ["returns", "true", "if", "the", "current", "process", "was", "launched", "using", "the", "jsrun", "command"], "docstring_summary": "Returns ``True`` if the current process was launched using the ``jsrun`` command.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 85, "end_line": 88, "hash": "e03350f3c3247a66cc314dd5cf45ab05", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "world_size", "original_string": "def world_size(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.\"\"\"\r\n        world_size = os.environ.get(\"JSM_NAMESPACE_SIZE\")\r\n        if world_size is None:\r\n            raise ValueError(\r\n                \"Cannot determine world size. Environment variable `JSM_NAMESPACE_SIZE` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(world_size)", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.\"\"\"\r\n        world_size = os.environ.get(\"JSM_NAMESPACE_SIZE\")\r\n        if world_size is None:\r\n            raise ValueError(\r\n                \"Cannot determine world size. Environment variable `JSM_NAMESPACE_SIZE` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(world_size)", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "world", "size", "is", "read", "from", "the", "environment", "variable", "`", "`", "JSM_NAMESPACE_SIZE", "`", "`", ".", "\"", "\"", "\"", "world_size", "=", "os", ".", "environ", ".", "get", "(", "\"", "JSM_NAMESPACE_SIZE", "\"", ")", "if", "world_size", "is", "None", ":", "raise", "ValueError", "(", "\"", "Cannot", "determine", "world", "size", ".", "Environment", "variable", "`", "JSM_NAMESPACE_SIZE", "`", "not", "found", ".", "\"", "\"", "Make", "sure", "you", "run", "your", "executable", "with", "`", "jsrun", "`", ".", "\"", ")", "return", "int", "(", "world_size", ")"], "docstring": "The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.", "docstring_tokens": ["the", "world", "size", "is", "read", "from", "the", "environment", "variable", "jsm_namespace_size"], "docstring_summary": "The world size is read from the environment variable ``JSM_NAMESPACE_SIZE``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 91, "end_line": 99, "hash": "826ec0bd3c68b8b7245c36039a6cd5fa", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "global_rank", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.\"\"\"\r\n        global_rank = os.environ.get(\"JSM_NAMESPACE_RANK\")\r\n        if global_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine global rank. Environment variable `JSM_NAMESPACE_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(global_rank)", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.\"\"\"\r\n        global_rank = os.environ.get(\"JSM_NAMESPACE_RANK\")\r\n        if global_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine global rank. Environment variable `JSM_NAMESPACE_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(global_rank)", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "world", "size", "is", "read", "from", "the", "environment", "variable", "`", "`", "JSM_NAMESPACE_RANK", "`", "`", ".", "\"", "\"", "\"", "global_rank", "=", "os", ".", "environ", ".", "get", "(", "\"", "JSM_NAMESPACE_RANK", "\"", ")", "if", "global_rank", "is", "None", ":", "raise", "ValueError", "(", "\"", "Cannot", "determine", "global", "rank", ".", "Environment", "variable", "`", "JSM_NAMESPACE_RANK", "`", "not", "found", ".", "\"", "\"", "Make", "sure", "you", "run", "your", "executable", "with", "`", "jsrun", "`", ".", "\"", ")", "return", "int", "(", "global_rank", ")"], "docstring": "The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.", "docstring_tokens": ["the", "world", "size", "is", "read", "from", "the", "environment", "variable", "jsm_namespace_rank"], "docstring_summary": "The world size is read from the environment variable ``JSM_NAMESPACE_RANK``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 106, "end_line": 114, "hash": "6127888efe713c8e1c236d27036b9746", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "local_rank", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.\"\"\"\r\n        local_rank = os.environ.get(\"JSM_NAMESPACE_LOCAL_RANK\")\r\n        if local_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine local rank. Environment variable `JSM_NAMESPACE_LOCAL_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(local_rank)", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.\"\"\"\r\n        local_rank = os.environ.get(\"JSM_NAMESPACE_LOCAL_RANK\")\r\n        if local_rank is None:\r\n            raise ValueError(\r\n                \"Cannot determine local rank. Environment variable `JSM_NAMESPACE_LOCAL_RANK` not found.\"\r\n                \" Make sure you run your executable with `jsrun`.\"\r\n            )\r\n        return int(local_rank)", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "local", "rank", "is", "read", "from", "the", "environment", "variable", "`", "JSM_NAMESPACE_LOCAL_RANK", "`", ".", "\"", "\"", "\"", "local_rank", "=", "os", ".", "environ", ".", "get", "(", "\"", "JSM_NAMESPACE_LOCAL_RANK", "\"", ")", "if", "local_rank", "is", "None", ":", "raise", "ValueError", "(", "\"", "Cannot", "determine", "local", "rank", ".", "Environment", "variable", "`", "JSM_NAMESPACE_LOCAL_RANK", "`", "not", "found", ".", "\"", "\"", "Make", "sure", "you", "run", "your", "executable", "with", "`", "jsrun", "`", ".", "\"", ")", "return", "int", "(", "local_rank", ")"], "docstring": "The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.", "docstring_tokens": ["the", "local", "rank", "is", "read", "from", "the", "environment", "variable", "jsm_namespace_local_rank"], "docstring_summary": "The local rank is read from the environment variable `JSM_NAMESPACE_LOCAL_RANK`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 121, "end_line": 129, "hash": "61c32f071f7ea0c736348608eaf56c29", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "node_rank", "original_string": "def node_rank(self) -> int:\r\n        \"\"\"The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._node_rank", "language": "python", "code": "def node_rank(self) -> int:\r\n        \"\"\"The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in\r\n        ``LSB_DJOB_RANKFILE``.\"\"\"\r\n        return self._node_rank", "code_tokens": ["def", "node_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "node", "rank", "is", "determined", "by", "the", "position", "of", "the", "current", "hostname", "in", "the", "OpenMPI", "host", "rank", "file", "stored", "in", "`", "`", "LSB_DJOB_RANKFILE", "`", "`", ".", "\"", "\"", "\"", "return", "self", ".", "_node_rank"], "docstring": "The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in\r\n        ``LSB_DJOB_RANKFILE``.", "docstring_tokens": ["the", "node", "rank", "is", "determined", "by", "the", "position", "of", "the", "current", "hostname", "in", "the", "openmpi", "host", "rank", "file", "stored", "in", "lsb_djob_rankfile"], "docstring_summary": "The node rank is determined by the position of the current hostname in the OpenMPI host rank file stored in", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 132, "end_line": 135, "hash": "5c2c652de48ba6b22dc1532aa324273b", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "_get_node_rank", "original_string": "def _get_node_rank(self) -> int:\r\n        \"\"\"A helper method for getting the node rank.\r\n\r\n        The node rank is determined by the position of the current node in the list of hosts used in the job. This is\r\n        calculated by reading all hosts from ``LSB_DJOB_RANKFILE`` and finding this node's hostname in the list.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        count: dict[str, int] = {}\r\n        for host in hosts:\r\n            if host not in count:\r\n                count[host] = len(count)\r\n        return count[socket.gethostname()]", "language": "python", "code": "def _get_node_rank(self) -> int:\r\n        \"\"\"A helper method for getting the node rank.\r\n\r\n        The node rank is determined by the position of the current node in the list of hosts used in the job. This is\r\n        calculated by reading all hosts from ``LSB_DJOB_RANKFILE`` and finding this node's hostname in the list.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        count: dict[str, int] = {}\r\n        for host in hosts:\r\n            if host not in count:\r\n                count[host] = len(count)\r\n        return count[socket.gethostname()]", "code_tokens": ["def", "_get_node_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "A", "helper", "method", "for", "getting", "the", "node", "rank", ".", "The", "node", "rank", "is", "determined", "by", "the", "position", "of", "the", "current", "node", "in", "the", "list", "of", "hosts", "used", "in", "the", "job", ".", "This", "is", "calculated", "by", "reading", "all", "hosts", "from", "`", "`", "LSB_DJOB_RANKFILE", "`", "`", "and", "finding", "this", "node", "'", "s", "hostname", "in", "the", "list", ".", "\"", "\"", "\"", "hosts", "=", "self", ".", "_read_hosts", "(", ")", "count", ":", "dict", "[", "str", ",", "int", "]", "=", "{", "}", "for", "host", "in", "hosts", ":", "if", "host", "not", "in", "count", ":", "count", "[", "host", "]", "=", "len", "(", "count", ")", "return", "count", "[", "socket", ".", "gethostname", "(", ")", "]"], "docstring": "A helper method for getting the node rank.\r\n\r\n        The node rank is determined by the position of the current node in the list of hosts used in the job. This is\r\n        calculated by reading all hosts from ``LSB_DJOB_RANKFILE`` and finding this node's hostname in the list.", "docstring_tokens": ["a", "helper", "method", "for", "getting", "the", "node", "rank", "the", "node", "rank", "is", "determined", "by", "the", "position", "of", "the", "current", "node", "in", "the", "list", "of", "hosts", "used", "in", "the", "job", "this", "is", "calculated", "by", "reading", "all", "hosts", "from", "lsb_djob_rankfile", "and", "finding", "this", "node", "s", "hostname", "in", "the", "list"], "docstring_summary": "A helper method for getting the node rank.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 137, "end_line": 149, "hash": "55756dfb0813b79eab4567f2042900bf", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "_read_hosts", "original_string": "def _read_hosts() -> list[str]:\r\n        \"\"\"Read compute hosts that are a part of the compute job.\r\n\r\n        LSF uses the Job Step Manager (JSM) to manage job steps. Job steps are executed by the JSM from \"launch\" nodes.\r\n        Each job is assigned a launch node. This launch node will be the first node in the list contained in\r\n        ``LSB_DJOB_RANKFILE``.\r\n\r\n        \"\"\"\r\n        var = \"LSB_DJOB_RANKFILE\"\r\n        rankfile = os.environ.get(var)\r\n        if rankfile is None:\r\n            raise ValueError(\"Did not find the environment variable `LSB_DJOB_RANKFILE`\")\r\n        if not rankfile:\r\n            raise ValueError(\"The environment variable `LSB_DJOB_RANKFILE` is empty\")\r\n\r\n        fs = get_filesystem(rankfile)\r\n        with fs.open(rankfile, \"r\") as f:\r\n            ret = [line.strip() for line in f]\r\n        # remove the launch node (i.e. the first node in LSB_DJOB_RANKFILE) from the list\r\n        return ret[1:]", "language": "python", "code": "def _read_hosts() -> list[str]:\r\n        \"\"\"Read compute hosts that are a part of the compute job.\r\n\r\n        LSF uses the Job Step Manager (JSM) to manage job steps. Job steps are executed by the JSM from \"launch\" nodes.\r\n        Each job is assigned a launch node. This launch node will be the first node in the list contained in\r\n        ``LSB_DJOB_RANKFILE``.\r\n\r\n        \"\"\"\r\n        var = \"LSB_DJOB_RANKFILE\"\r\n        rankfile = os.environ.get(var)\r\n        if rankfile is None:\r\n            raise ValueError(\"Did not find the environment variable `LSB_DJOB_RANKFILE`\")\r\n        if not rankfile:\r\n            raise ValueError(\"The environment variable `LSB_DJOB_RANKFILE` is empty\")\r\n\r\n        fs = get_filesystem(rankfile)\r\n        with fs.open(rankfile, \"r\") as f:\r\n            ret = [line.strip() for line in f]\r\n        # remove the launch node (i.e. the first node in LSB_DJOB_RANKFILE) from the list\r\n        return ret[1:]", "code_tokens": ["def", "_read_hosts", "(", ")", "-", ">", "list", "[", "str", "]", ":", "\"", "\"", "\"", "Read", "compute", "hosts", "that", "are", "a", "part", "of", "the", "compute", "job", ".", "LSF", "uses", "the", "Job", "Step", "Manager", "(", "JSM", ")", "to", "manage", "job", "steps", ".", "Job", "steps", "are", "executed", "by", "the", "JSM", "from", "\"", "launch", "\"", "nodes", ".", "Each", "job", "is", "assigned", "a", "launch", "node", ".", "This", "launch", "node", "will", "be", "the", "first", "node", "in", "the", "list", "contained", "in", "`", "`", "LSB_DJOB_RANKFILE", "`", "`", ".", "\"", "\"", "\"", "var", "=", "\"", "LSB_DJOB_RANKFILE", "\"", "rankfile", "=", "os", ".", "environ", ".", "get", "(", "var", ")", "if", "rankfile", "is", "None", ":", "raise", "ValueError", "(", "\"", "Did", "not", "find", "the", "environment", "variable", "`", "LSB_DJOB_RANKFILE", "`", "\"", ")", "if", "not", "rankfile", ":", "raise", "ValueError", "(", "\"", "The", "environment", "variable", "`", "LSB_DJOB_RANKFILE", "`", "is", "empty", "\"", ")", "fs", "=", "get_filesystem", "(", "rankfile", ")", "with", "fs", ".", "open", "(", "rankfile", ",", "\"", "r", "\"", ")", "as", "f", ":", "ret", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "f", "]", "return", "ret", "[", "1", ":", "]"], "docstring": "Read compute hosts that are a part of the compute job.\r\n\r\n        LSF uses the Job Step Manager (JSM) to manage job steps. Job steps are executed by the JSM from \"launch\" nodes.\r\n        Each job is assigned a launch node. This launch node will be the first node in the list contained in\r\n        ``LSB_DJOB_RANKFILE``.", "docstring_tokens": ["read", "compute", "hosts", "that", "are", "a", "part", "of", "the", "compute", "job", "lsf", "uses", "the", "job", "step", "manager", "jsm", "to", "manage", "job", "steps", "job", "steps", "are", "executed", "by", "the", "jsm", "from", "launch", "nodes", "each", "job", "is", "assigned", "a", "launch", "node", "this", "launch", "node", "will", "be", "the", "first", "node", "in", "the", "list", "contained", "in", "lsb_djob_rankfile"], "docstring_summary": "Read compute hosts that are a part of the compute job.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 152, "end_line": 171, "hash": "90555e9033c7507101cf43f450709df2", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "_get_main_address", "original_string": "def _get_main_address(self) -> str:\r\n        \"\"\"A helper for getting the main address.\r\n\r\n        The main address is assigned to the first node in the list of nodes used for the job.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        return hosts[0]", "language": "python", "code": "def _get_main_address(self) -> str:\r\n        \"\"\"A helper for getting the main address.\r\n\r\n        The main address is assigned to the first node in the list of nodes used for the job.\r\n\r\n        \"\"\"\r\n        hosts = self._read_hosts()\r\n        return hosts[0]", "code_tokens": ["def", "_get_main_address", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "A", "helper", "for", "getting", "the", "main", "address", ".", "The", "main", "address", "is", "assigned", "to", "the", "first", "node", "in", "the", "list", "of", "nodes", "used", "for", "the", "job", ".", "\"", "\"", "\"", "hosts", "=", "self", ".", "_read_hosts", "(", ")", "return", "hosts", "[", "0", "]"], "docstring": "A helper for getting the main address.\r\n\r\n        The main address is assigned to the first node in the list of nodes used for the job.", "docstring_tokens": ["a", "helper", "for", "getting", "the", "main", "address", "the", "main", "address", "is", "assigned", "to", "the", "first", "node", "in", "the", "list", "of", "nodes", "used", "for", "the", "job"], "docstring_summary": "A helper for getting the main address.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 173, "end_line": 180, "hash": "568abf09af38e9c8767f6370545f76d3", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\lsf.py", "func_name": "_get_main_port", "original_string": "def _get_main_port() -> int:\r\n        \"\"\"A helper function for accessing the main port.\r\n\r\n        Uses the LSF job ID so all ranks can compute the main port.\r\n\r\n        \"\"\"\r\n        # check for user-specified main port\r\n        if \"MASTER_PORT\" in os.environ:\r\n            log.debug(f\"Using externally specified main port: {os.environ['MASTER_PORT']}\")\r\n            return int(os.environ[\"MASTER_PORT\"])\r\n        if \"LSB_JOBID\" in os.environ:\r\n            port = int(os.environ[\"LSB_JOBID\"])\r\n            # all ports should be in the 10k+ range\r\n            port = port % 1000 + 10000\r\n            log.debug(f\"calculated LSF main port: {port}\")\r\n            return port\r\n        raise ValueError(\"Could not find job id in environment variable LSB_JOBID\")", "language": "python", "code": "def _get_main_port() -> int:\r\n        \"\"\"A helper function for accessing the main port.\r\n\r\n        Uses the LSF job ID so all ranks can compute the main port.\r\n\r\n        \"\"\"\r\n        # check for user-specified main port\r\n        if \"MASTER_PORT\" in os.environ:\r\n            log.debug(f\"Using externally specified main port: {os.environ['MASTER_PORT']}\")\r\n            return int(os.environ[\"MASTER_PORT\"])\r\n        if \"LSB_JOBID\" in os.environ:\r\n            port = int(os.environ[\"LSB_JOBID\"])\r\n            # all ports should be in the 10k+ range\r\n            port = port % 1000 + 10000\r\n            log.debug(f\"calculated LSF main port: {port}\")\r\n            return port\r\n        raise ValueError(\"Could not find job id in environment variable LSB_JOBID\")", "code_tokens": ["def", "_get_main_port", "(", ")", "-", ">", "int", ":", "\"", "\"", "\"", "A", "helper", "function", "for", "accessing", "the", "main", "port", ".", "Uses", "the", "LSF", "job", "ID", "so", "all", "ranks", "can", "compute", "the", "main", "port", ".", "\"", "\"", "\"", "if", "\"", "MASTER_PORT", "\"", "in", "os", ".", "environ", ":", "log", ".", "debug", "(", "f", "\"", "Using", "externally", "specified", "main", "port", ":", "{", "os", ".", "environ", "[", "'", "MASTER_PORT", "'", "]", "}", "\"", ")", "return", "int", "(", "os", ".", "environ", "[", "\"", "MASTER_PORT", "\"", "]", ")", "if", "\"", "LSB_JOBID", "\"", "in", "os", ".", "environ", ":", "port", "=", "int", "(", "os", ".", "environ", "[", "\"", "LSB_JOBID", "\"", "]", ")", "port", "=", "port", "%", "1000", "+", "10000", "log", ".", "debug", "(", "f", "\"", "calculated", "LSF", "main", "port", ":", "{", "port", "}", "\"", ")", "return", "port", "raise", "ValueError", "(", "\"", "Could", "not", "find", "job", "id", "in", "environment", "variable", "LSB_JOBID", "\"", ")"], "docstring": "A helper function for accessing the main port.\r\n\r\n        Uses the LSF job ID so all ranks can compute the main port.", "docstring_tokens": ["a", "helper", "function", "for", "accessing", "the", "main", "port", "uses", "the", "lsf", "job", "id", "so", "all", "ranks", "can", "compute", "the", "main", "port"], "docstring_summary": "A helper function for accessing the main port.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\lsf.py", "partition": "train", "function_type": "class_method", "class_name": "LSFEnvironment", "start_line": 183, "end_line": 199, "hash": "8ca2e6f4b5b50d18f2481ea472a700c6", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\mpi.py", "func_name": "detect", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.\"\"\"\r\n        if not _MPI4PY_AVAILABLE:\r\n            return False\r\n\r\n        from mpi4py import MPI\r\n\r\n        return MPI.COMM_WORLD.Get_size() > 1", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.\"\"\"\r\n        if not _MPI4PY_AVAILABLE:\r\n            return False\r\n\r\n        from mpi4py import MPI\r\n\r\n        return MPI.COMM_WORLD.Get_size() > 1", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "`", "`", "True", "`", "`", "if", "the", "`", "mpi4py", "`", "package", "is", "installed", "and", "MPI", "returns", "a", "world", "size", "greater", "than", "1", ".", "\"", "\"", "\"", "if", "not", "_MPI4PY_AVAILABLE", ":", "return", "False", "from", "mpi4py", "import", "MPI", "return", "MPI", ".", "COMM_WORLD", ".", "Get_size", "(", ")", ">", "1"], "docstring": "Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.", "docstring_tokens": ["returns", "true", "if", "the", "mpi4py", "package", "is", "installed", "and", "mpi", "returns", "a", "world", "size", "greater", "than", "1"], "docstring_summary": "Returns ``True`` if the `mpi4py` package is installed and MPI returns a world size greater than 1.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\mpi.py", "partition": "train", "function_type": "class_method", "class_name": "MPIEnvironment", "start_line": 70, "end_line": 77, "hash": "de62fca28089aac58c918f312a50fb2a", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "detect", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched on a SLURM cluster.\r\n\r\n        It is possible to use the SLURM scheduler to request resources and then launch processes manually using a\r\n        different environment. For this, the user can set the job name in SLURM to 'bash' or 'interactive' (srun --job-\r\n        name=interactive). This will then avoid the detection of ``SLURMEnvironment`` and another environment can be\r\n        detected automatically.\r\n\r\n        \"\"\"\r\n        SLURMEnvironment._validate_srun_used()\r\n        return _is_srun_used()", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched on a SLURM cluster.\r\n\r\n        It is possible to use the SLURM scheduler to request resources and then launch processes manually using a\r\n        different environment. For this, the user can set the job name in SLURM to 'bash' or 'interactive' (srun --job-\r\n        name=interactive). This will then avoid the detection of ``SLURMEnvironment`` and another environment can be\r\n        detected automatically.\r\n\r\n        \"\"\"\r\n        SLURMEnvironment._validate_srun_used()\r\n        return _is_srun_used()", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "`", "`", "True", "`", "`", "if", "the", "current", "process", "was", "launched", "on", "a", "SLURM", "cluster", ".", "It", "is", "possible", "to", "use", "the", "SLURM", "scheduler", "to", "request", "resources", "and", "then", "launch", "processes", "manually", "using", "a", "different", "environment", ".", "For", "this", ",", "the", "user", "can", "set", "the", "job", "name", "in", "SLURM", "to", "'", "bash", "'", "or", "'", "interactive", "'", "(", "srun", "-", "-", "job", "-", "name", "=", "interactive", ")", ".", "This", "will", "then", "avoid", "the", "detection", "of", "`", "`", "SLURMEnvironment", "`", "`", "and", "another", "environment", "can", "be", "detected", "automatically", ".", "\"", "\"", "\"", "SLURMEnvironment", ".", "_validate_srun_used", "(", ")", "return", "_is_srun_used", "(", ")"], "docstring": "Returns ``True`` if the current process was launched on a SLURM cluster.\r\n\r\n        It is possible to use the SLURM scheduler to request resources and then launch processes manually using a\r\n        different environment. For this, the user can set the job name in SLURM to 'bash' or 'interactive' (srun --job-\r\n        name=interactive). This will then avoid the detection of ``SLURMEnvironment`` and another environment can be\r\n        detected automatically.", "docstring_tokens": ["returns", "true", "if", "the", "current", "process", "was", "launched", "on", "a", "slurm", "cluster", "it", "is", "possible", "to", "use", "the", "slurm", "scheduler", "to", "request", "resources", "and", "then", "launch", "processes", "manually", "using", "a", "different", "environment", "for", "this", "the", "user", "can", "set", "the", "job", "name", "in", "slurm", "to", "bash", "or", "interactive", "srun", "job", "name", "interactive", "this", "will", "then", "avoid", "the", "detection", "of", "slurmenvironment", "and", "another", "environment", "can", "be", "detected", "automatically"], "docstring_summary": "Returns ``True`` if the current process was launched on a SLURM cluster.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\slurm.py", "partition": "train", "function_type": "class_method", "class_name": "SLURMEnvironment", "start_line": 102, "end_line": 112, "hash": "240b947052697bcef939642b4426ad51", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "resolve_root_node_address", "original_string": "def resolve_root_node_address(nodes: str) -> str:\r\n        \"\"\"The node selection format in SLURM supports several formats.\r\n\r\n        This function selects the first host name from\r\n\r\n        - a space-separated list of host names, e.g., 'host0 host1 host3' yields 'host0' as the root\r\n        - a comma-separated list of host names, e.g., 'host0,host1,host3' yields 'host0' as the root\r\n        - the range notation with brackets, e.g., 'host[5-9]' yields 'host5' as the root\r\n\r\n        \"\"\"\r\n        nodes = re.sub(r\"\\[(.*?)[,-].*\\]\", \"\\\\1\", nodes)  # Take the first node of every node range\r\n        nodes = re.sub(r\"\\[(.*?)\\]\", \"\\\\1\", nodes)  # handle special case where node range is single number\r\n        return nodes.split(\" \")[0].split(\",\")[0]", "language": "python", "code": "def resolve_root_node_address(nodes: str) -> str:\r\n        \"\"\"The node selection format in SLURM supports several formats.\r\n\r\n        This function selects the first host name from\r\n\r\n        - a space-separated list of host names, e.g., 'host0 host1 host3' yields 'host0' as the root\r\n        - a comma-separated list of host names, e.g., 'host0,host1,host3' yields 'host0' as the root\r\n        - the range notation with brackets, e.g., 'host[5-9]' yields 'host5' as the root\r\n\r\n        \"\"\"\r\n        nodes = re.sub(r\"\\[(.*?)[,-].*\\]\", \"\\\\1\", nodes)  # Take the first node of every node range\r\n        nodes = re.sub(r\"\\[(.*?)\\]\", \"\\\\1\", nodes)  # handle special case where node range is single number\r\n        return nodes.split(\" \")[0].split(\",\")[0]", "code_tokens": ["def", "resolve_root_node_address", "(", "nodes", ":", "str", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "node", "selection", "format", "in", "SLURM", "supports", "several", "formats", ".", "This", "function", "selects", "the", "first", "host", "name", "from", "-", "a", "space", "-", "separated", "list", "of", "host", "names", ",", "e", ".", "g", ".", ",", "'", "host0", "host1", "host3", "'", "yields", "'", "host0", "'", "as", "the", "root", "-", "a", "comma", "-", "separated", "list", "of", "host", "names", ",", "e", ".", "g", ".", ",", "'", "host0", ",", "host1", ",", "host3", "'", "yields", "'", "host0", "'", "as", "the", "root", "-", "the", "range", "notation", "with", "brackets", ",", "e", ".", "g", ".", ",", "'", "host", "[", "5", "-", "9", "]", "'", "yields", "'", "host5", "'", "as", "the", "root", "\"", "\"", "\"", "nodes", "=", "re", ".", "sub", "(", "r", "\"", "\\", "[", "(", ".", "*", "?", ")", "[", ",", "-", "]", ".", "*", "\\", "]", "\"", ",", "\"", "\\", "\\", "1", "\"", ",", "nodes", ")", "nodes", "=", "re", ".", "sub", "(", "r", "\"", "\\", "[", "(", ".", "*", "?", ")", "\\", "]", "\"", ",", "\"", "\\", "\\", "1", "\"", ",", "nodes", ")", "return", "nodes", ".", "split", "(", "\"", "\"", ")", "[", "0", "]", ".", "split", "(", "\"", ",", "\"", ")", "[", "0", "]"], "docstring": "The node selection format in SLURM supports several formats.\r\n\r\n        This function selects the first host name from\r\n\r\n        - a space-separated list of host names, e.g., 'host0 host1 host3' yields 'host0' as the root\r\n        - a comma-separated list of host names, e.g., 'host0,host1,host3' yields 'host0' as the root\r\n        - the range notation with brackets, e.g., 'host[5-9]' yields 'host5' as the root", "docstring_tokens": ["the", "node", "selection", "format", "in", "slurm", "supports", "several", "formats", "this", "function", "selects", "the", "first", "host", "name", "from", "a", "space", "separated", "list", "of", "host", "names", "e", "g", "host0", "host1", "host3", "yields", "host0", "as", "the", "root", "a", "comma", "separated", "list", "of", "host", "names", "e", "g", "host0", "host1", "host3", "yields", "host0", "as", "the", "root", "the", "range", "notation", "with", "brackets", "e", "g", "host", "5", "9", "yields", "host5", "as", "the", "root"], "docstring_summary": "The node selection format in SLURM supports several formats.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\slurm.py", "partition": "train", "function_type": "class_method", "class_name": "SLURMEnvironment", "start_line": 174, "end_line": 186, "hash": "14d2924be931fb3c4724bb27fdddbe99", "complexity": 1, "parameters": ["nodes"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "_validate_srun_used", "original_string": "def _validate_srun_used() -> None:\r\n        \"\"\"Checks if the `srun` command is available and used.\r\n\r\n        Parallel jobs (multi-GPU, multi-node) in SLURM are launched by prepending `srun` in front of the Python command.\r\n        Not doing so will result in processes hanging, which is a frequent user error. Lightning will emit a warning if\r\n        `srun` is found but not used.\r\n\r\n        \"\"\"\r\n        if _IS_WINDOWS:\r\n            return\r\n\r\n        srun_exists = shutil.which(\"srun\") is not None\r\n        if srun_exists and not _is_srun_used():\r\n            hint = \" \".join([\"srun\", os.path.basename(sys.executable), *sys.argv])[:64]\r\n            rank_zero_warn(\r\n                \"The `srun` command is available on your system but is not used. HINT: If your intention is to run\"\r\n                f\" Lightning on SLURM, prepend your python command with `srun` like so: {hint} ...\",\r\n                category=PossibleUserWarning,\r\n            )", "language": "python", "code": "def _validate_srun_used() -> None:\r\n        \"\"\"Checks if the `srun` command is available and used.\r\n\r\n        Parallel jobs (multi-GPU, multi-node) in SLURM are launched by prepending `srun` in front of the Python command.\r\n        Not doing so will result in processes hanging, which is a frequent user error. Lightning will emit a warning if\r\n        `srun` is found but not used.\r\n\r\n        \"\"\"\r\n        if _IS_WINDOWS:\r\n            return\r\n\r\n        srun_exists = shutil.which(\"srun\") is not None\r\n        if srun_exists and not _is_srun_used():\r\n            hint = \" \".join([\"srun\", os.path.basename(sys.executable), *sys.argv])[:64]\r\n            rank_zero_warn(\r\n                \"The `srun` command is available on your system but is not used. HINT: If your intention is to run\"\r\n                f\" Lightning on SLURM, prepend your python command with `srun` like so: {hint} ...\",\r\n                category=PossibleUserWarning,\r\n            )", "code_tokens": ["def", "_validate_srun_used", "(", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "if", "the", "`", "srun", "`", "command", "is", "available", "and", "used", ".", "Parallel", "jobs", "(", "multi", "-", "GPU", ",", "multi", "-", "node", ")", "in", "SLURM", "are", "launched", "by", "prepending", "`", "srun", "`", "in", "front", "of", "the", "Python", "command", ".", "Not", "doing", "so", "will", "result", "in", "processes", "hanging", ",", "which", "is", "a", "frequent", "user", "error", ".", "Lightning", "will", "emit", "a", "warning", "if", "`", "srun", "`", "is", "found", "but", "not", "used", ".", "\"", "\"", "\"", "if", "_IS_WINDOWS", ":", "return", "srun_exists", "=", "shutil", ".", "which", "(", "\"", "srun", "\"", ")", "is", "not", "None", "if", "srun_exists", "and", "not", "_is_srun_used", "(", ")", ":", "hint", "=", "\"", "\"", ".", "join", "(", "[", "\"", "srun", "\"", ",", "os", ".", "path", ".", "basename", "(", "sys", ".", "executable", ")", ",", "*", "sys", ".", "argv", "]", ")", "[", ":", "64", "]", "rank_zero_warn", "(", "\"", "The", "`", "srun", "`", "command", "is", "available", "on", "your", "system", "but", "is", "not", "used", ".", "HINT", ":", "If", "your", "intention", "is", "to", "run", "\"", "f", "\"", "Lightning", "on", "SLURM", ",", "prepend", "your", "python", "command", "with", "`", "srun", "`", "like", "so", ":", "{", "hint", "}", ".", ".", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")"], "docstring": "Checks if the `srun` command is available and used.\r\n\r\n        Parallel jobs (multi-GPU, multi-node) in SLURM are launched by prepending `srun` in front of the Python command.\r\n        Not doing so will result in processes hanging, which is a frequent user error. Lightning will emit a warning if\r\n        `srun` is found but not used.", "docstring_tokens": ["checks", "if", "the", "srun", "command", "is", "available", "and", "used", "parallel", "jobs", "multi", "gpu", "multi", "node", "in", "slurm", "are", "launched", "by", "prepending", "srun", "in", "front", "of", "the", "python", "command", "not", "doing", "so", "will", "result", "in", "processes", "hanging", "which", "is", "a", "frequent", "user", "error", "lightning", "will", "emit", "a", "warning", "if", "srun", "is", "found", "but", "not", "used"], "docstring_summary": "Checks if the `srun` command is available and used.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\slurm.py", "partition": "train", "function_type": "class_method", "class_name": "SLURMEnvironment", "start_line": 189, "end_line": 207, "hash": "dc0b021f88bf9b482b3638d5fee0979a", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\slurm.py", "func_name": "_validate_srun_variables", "original_string": "def _validate_srun_variables() -> None:\r\n        \"\"\"Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.\r\n\r\n        Right now, we only check for the most common user errors. See\r\n        `the srun docs <https://slurm.schedmd.com/srun.html>`_\r\n        for a complete list of supported srun variables.\r\n\r\n        \"\"\"\r\n        ntasks = int(os.environ.get(\"SLURM_NTASKS\", \"1\"))\r\n        if ntasks > 1 and \"SLURM_NTASKS_PER_NODE\" not in os.environ:\r\n            raise RuntimeError(\r\n                f\"You set `--ntasks={ntasks}` in your SLURM bash script, but this variable is not supported.\"\r\n                f\" HINT: Use `--ntasks-per-node={ntasks}` instead.\"\r\n            )", "language": "python", "code": "def _validate_srun_variables() -> None:\r\n        \"\"\"Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.\r\n\r\n        Right now, we only check for the most common user errors. See\r\n        `the srun docs <https://slurm.schedmd.com/srun.html>`_\r\n        for a complete list of supported srun variables.\r\n\r\n        \"\"\"\r\n        ntasks = int(os.environ.get(\"SLURM_NTASKS\", \"1\"))\r\n        if ntasks > 1 and \"SLURM_NTASKS_PER_NODE\" not in os.environ:\r\n            raise RuntimeError(\r\n                f\"You set `--ntasks={ntasks}` in your SLURM bash script, but this variable is not supported.\"\r\n                f\" HINT: Use `--ntasks-per-node={ntasks}` instead.\"\r\n            )", "code_tokens": ["def", "_validate_srun_variables", "(", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "for", "conflicting", "or", "incorrectly", "set", "variables", "set", "through", "`", "srun", "`", "and", "raises", "a", "useful", "error", "message", ".", "Right", "now", ",", "we", "only", "check", "for", "the", "most", "common", "user", "errors", ".", "See", "`", "the", "srun", "docs", "<", "https", ":", "/", "/", "slurm", ".", "schedmd", ".", "com", "/", "srun", ".", "html", ">", "`", "_", "for", "a", "complete", "list", "of", "supported", "srun", "variables", ".", "\"", "\"", "\"", "ntasks", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "\"", "SLURM_NTASKS", "\"", ",", "\"", "1", "\"", ")", ")", "if", "ntasks", ">", "1", "and", "\"", "SLURM_NTASKS_PER_NODE", "\"", "not", "in", "os", ".", "environ", ":", "raise", "RuntimeError", "(", "f", "\"", "You", "set", "`", "-", "-", "ntasks", "=", "{", "ntasks", "}", "`", "in", "your", "SLURM", "bash", "script", ",", "but", "this", "variable", "is", "not", "supported", ".", "\"", "f", "\"", "HINT", ":", "Use", "`", "-", "-", "ntasks", "-", "per", "-", "node", "=", "{", "ntasks", "}", "`", "instead", ".", "\"", ")"], "docstring": "Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.\r\n\r\n        Right now, we only check for the most common user errors. See\r\n        `the srun docs <https://slurm.schedmd.com/srun.html>`_\r\n        for a complete list of supported srun variables.", "docstring_tokens": ["checks", "for", "conflicting", "or", "incorrectly", "set", "variables", "set", "through", "srun", "and", "raises", "a", "useful", "error", "message", "right", "now", "we", "only", "check", "for", "the", "most", "common", "user", "errors", "see", "the", "srun", "docs", "https", "slurm", "schedmd", "com", "srun", "html", "_", "for", "a", "complete", "list", "of", "supported", "srun", "variables"], "docstring_summary": "Checks for conflicting or incorrectly set variables set through `srun` and raises a useful error message.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\slurm.py", "partition": "train", "function_type": "class_method", "class_name": "SLURMEnvironment", "start_line": 210, "end_line": 223, "hash": "006530163b417f419074826ec51351a4", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\torchelastic.py", "func_name": "detect", "original_string": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"\r\n        # if not available (for example on MacOS), `is_torchelastic_launched` is not defined\r\n        return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()", "language": "python", "code": "def detect() -> bool:\r\n        \"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"\r\n        # if not available (for example on MacOS), `is_torchelastic_launched` is not defined\r\n        return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()", "code_tokens": ["def", "detect", "(", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "`", "`", "True", "`", "`", "if", "the", "current", "process", "was", "launched", "using", "the", "torchelastic", "command", ".", "\"", "\"", "\"", "return", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_torchelastic_launched", "(", ")"], "docstring": "Returns ``True`` if the current process was launched using the torchelastic command.", "docstring_tokens": ["returns", "true", "if", "the", "current", "process", "was", "launched", "using", "the", "torchelastic", "command"], "docstring_summary": "Returns ``True`` if the current process was launched using the torchelastic command.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\torchelastic.py", "partition": "train", "function_type": "class_method", "class_name": "TorchElasticEnvironment", "start_line": 55, "end_line": 58, "hash": "d8d53c75cb9dee6de029179c597485a6", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "world_size", "original_string": "def world_size(self) -> int:\r\n        \"\"\"The number of processes across all devices and hosts.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.world_size()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.xrt_world_size()", "language": "python", "code": "def world_size(self) -> int:\r\n        \"\"\"The number of processes across all devices and hosts.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.world_size()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.xrt_world_size()", "code_tokens": ["def", "world_size", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "number", "of", "processes", "across", "all", "devices", "and", "hosts", ".", "The", "output", "is", "cached", "for", "performance", ".", "\"", "\"", "\"", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "world_size", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "return", "xm", ".", "xrt_world_size", "(", ")"], "docstring": "The number of processes across all devices and hosts.\r\n\r\n        The output is cached for performance.", "docstring_tokens": ["the", "number", "of", "processes", "across", "all", "devices", "and", "hosts", "the", "output", "is", "cached", "for", "performance"], "docstring_summary": "The number of processes across all devices and hosts.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAEnvironment", "start_line": 62, "end_line": 75, "hash": "69c3a47b9ce7bf91238a43d68be1d0d7", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "global_rank", "original_string": "def global_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process across all host and devices.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.global_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_ordinal()", "language": "python", "code": "def global_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process across all host and devices.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.global_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_ordinal()", "code_tokens": ["def", "global_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "rank", "(", "index", ")", "of", "the", "currently", "running", "process", "across", "all", "host", "and", "devices", ".", "The", "output", "is", "cached", "for", "performance", ".", "\"", "\"", "\"", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "global_ordinal", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "return", "xm", ".", "get_ordinal", "(", ")"], "docstring": "The rank (index) of the currently running process across all host and devices.\r\n\r\n        The output is cached for performance.", "docstring_tokens": ["the", "rank", "index", "of", "the", "currently", "running", "process", "across", "all", "host", "and", "devices", "the", "output", "is", "cached", "for", "performance"], "docstring_summary": "The rank (index) of the currently running process across all host and devices.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAEnvironment", "start_line": 83, "end_line": 96, "hash": "4ae9051195efa3fe347e531b3b42c512", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "local_rank", "original_string": "def local_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process inside of the current host.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.local_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_local_ordinal()", "language": "python", "code": "def local_rank(self) -> int:\r\n        \"\"\"The rank (index) of the currently running process inside of the current host.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.local_ordinal()\r\n\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        return xm.get_local_ordinal()", "code_tokens": ["def", "local_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "rank", "(", "index", ")", "of", "the", "currently", "running", "process", "inside", "of", "the", "current", "host", ".", "The", "output", "is", "cached", "for", "performance", ".", "\"", "\"", "\"", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "local_ordinal", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "return", "xm", ".", "get_local_ordinal", "(", ")"], "docstring": "The rank (index) of the currently running process inside of the current host.\r\n\r\n        The output is cached for performance.", "docstring_tokens": ["the", "rank", "index", "of", "the", "currently", "running", "process", "inside", "of", "the", "current", "host", "the", "output", "is", "cached", "for", "performance"], "docstring_summary": "The rank (index) of the currently running process inside of the current host.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAEnvironment", "start_line": 104, "end_line": 117, "hash": "cf4fa4f96ea2232927fe46734c5511f6", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\environments\\xla.py", "func_name": "node_rank", "original_string": "def node_rank(self) -> int:\r\n        \"\"\"The rank (index) of the host on which the current process runs.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.host_index()\r\n        import torch_xla.core.xla_env_vars as xenv\r\n        from torch_xla.utils.utils import getenv_as\r\n\r\n        return getenv_as(xenv.HOST_ORDINAL, int, 0)", "language": "python", "code": "def node_rank(self) -> int:\r\n        \"\"\"The rank (index) of the host on which the current process runs.\r\n\r\n        The output is cached for performance.\r\n\r\n        \"\"\"\r\n        if _XLA_GREATER_EQUAL_2_1:\r\n            from torch_xla import runtime as xr\r\n\r\n            return xr.host_index()\r\n        import torch_xla.core.xla_env_vars as xenv\r\n        from torch_xla.utils.utils import getenv_as\r\n\r\n        return getenv_as(xenv.HOST_ORDINAL, int, 0)", "code_tokens": ["def", "node_rank", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "The", "rank", "(", "index", ")", "of", "the", "host", "on", "which", "the", "current", "process", "runs", ".", "The", "output", "is", "cached", "for", "performance", ".", "\"", "\"", "\"", "if", "_XLA_GREATER_EQUAL_2_1", ":", "from", "torch_xla", "import", "runtime", "as", "xr", "return", "xr", ".", "host_index", "(", ")", "import", "torch_xla", ".", "core", ".", "xla_env_vars", "as", "xenv", "from", "torch_xla", ".", "utils", ".", "utils", "import", "getenv_as", "return", "getenv_as", "(", "xenv", ".", "HOST_ORDINAL", ",", "int", ",", "0", ")"], "docstring": "The rank (index) of the host on which the current process runs.\r\n\r\n        The output is cached for performance.", "docstring_tokens": ["the", "rank", "index", "of", "the", "host", "on", "which", "the", "current", "process", "runs", "the", "output", "is", "cached", "for", "performance"], "docstring_summary": "The rank (index) of the host on which the current process runs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\environments\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAEnvironment", "start_line": 121, "end_line": 134, "hash": "383c9d41b59cbc487e5749fd34003f74", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: Optional parameters when saving the model/training states.\r\n\r\n        \"\"\"", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: Optional parameters when saving the model/training states.\r\n\r\n        \"\"\"", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "path", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", "/", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "-", "dump", "and", "file", "-", "write", ".", "Args", ":", "checkpoint", ":", "dict", "containing", "model", "and", "trainer", "state", "path", ":", "write", "-", "target", "path", "storage_options", ":", "Optional", "parameters", "when", "saving", "the", "model", "/", "training", "states", ".", "\"", "\"", "\""], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: Optional parameters when saving the model/training states.", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write", "args", "checkpoint", "dict", "containing", "model", "and", "trainer", "state", "path", "write", "target", "path", "storage_options", "optional", "parameters", "when", "saving", "the", "model", "training", "states"], "docstring_summary": "Save model/training states as a checkpoint file through state-dump and file-write.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "partition": "train", "function_type": "class_method", "class_name": "CheckpointIO", "start_line": 38, "end_line": 46, "hash": "847467f949ee62b055279a8c415d2a72", "complexity": 1, "parameters": ["checkpoint", "Any]", "path", "storage_options"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "load_checkpoint", "original_string": "def load_checkpoint(self, path: _PATH, map_location: Optional[Any] = None) -> dict[str, Any]:\r\n        \"\"\"Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        \"\"\"", "language": "python", "code": "def load_checkpoint(self, path: _PATH, map_location: Optional[Any] = None) -> dict[str, Any]:\r\n        \"\"\"Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        \"\"\"", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "map_location", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "checkpoint", "from", "a", "path", "when", "resuming", "or", "loading", "ckpt", "for", "test", "/", "validate", "/", "predict", "stages", ".", "Args", ":", "path", ":", "Path", "to", "checkpoint", "map_location", ":", "a", "function", ",", ":", "class", ":", "`", "torch", ".", "device", "`", ",", "string", "or", "a", "dict", "specifying", "how", "to", "remap", "storage", "locations", ".", "Returns", ":", "The", "loaded", "checkpoint", ".", "\"", "\"", "\""], "docstring": "Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.", "docstring_tokens": ["load", "checkpoint", "from", "a", "path", "when", "resuming", "or", "loading", "ckpt", "for", "test", "validate", "predict", "stages", "args", "path", "path", "to", "checkpoint", "map_location", "a", "function", "class", "torch", "device", "string", "or", "a", "dict", "specifying", "how", "to", "remap", "storage", "locations", "returns", "the", "loaded", "checkpoint"], "docstring_summary": "Load checkpoint from a path when resuming or loading ckpt for test/validate/predict stages.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "partition": "train", "function_type": "class_method", "class_name": "CheckpointIO", "start_line": 49, "end_line": 59, "hash": "c7983a3ae758ab7aaafc18bec5d8c43c", "complexity": 1, "parameters": ["path", "map_location"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "func_name": "remove_checkpoint", "original_string": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"", "language": "python", "code": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "path", ":", "_PATH", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Remove", "checkpoint", "file", "from", "the", "filesystem", ".", "Args", ":", "path", ":", "Path", "to", "checkpoint", "\"", "\"", "\""], "docstring": "Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint", "docstring_tokens": ["remove", "checkpoint", "file", "from", "the", "filesystem", "args", "path", "path", "to", "checkpoint"], "docstring_summary": "Remove checkpoint file from the filesystem.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\checkpoint_io.py", "partition": "train", "function_type": "class_method", "class_name": "CheckpointIO", "start_line": 62, "end_line": 68, "hash": "441a90052ad5b6b88a0a6e2f49335579", "complexity": 1, "parameters": ["path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``TorchCheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        _atomic_save(checkpoint, path)", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``TorchCheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        _atomic_save(checkpoint, path)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "path", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", "/", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "-", "dump", "and", "file", "-", "write", ".", "Args", ":", "checkpoint", ":", "dict", "containing", "model", "and", "trainer", "state", "path", ":", "write", "-", "target", "path", "storage_options", ":", "not", "used", "in", "`", "`", "TorchCheckpointIO", ".", "save_checkpoint", "`", "`", "Raises", ":", "TypeError", ":", "If", "`", "`", "storage_options", "`", "`", "arg", "is", "passed", "in", "\"", "\"", "\"", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "\"", "`", "Trainer", ".", "save_checkpoint", "(", ".", ".", ".", ",", "storage_options", "=", ".", ".", ".", ")", "`", "with", "`", "storage_options", "`", "arg", "\"", "f", "\"", "is", "not", "supported", "for", "`", "{", "self", ".", "__class__", ".", "__name__", "}", "`", ".", "Please", "implement", "your", "custom", "`", "CheckpointIO", "`", "\"", "\"", "to", "define", "how", "you", "'", "d", "like", "to", "use", "`", "storage_options", "`", ".", "\"", ")", "fs", "=", "get_filesystem", "(", "path", ")", "fs", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ",", "exist_ok", "=", "True", ")", "_atomic_save", "(", "checkpoint", ",", "path", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``TorchCheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write", "args", "checkpoint", "dict", "containing", "model", "and", "trainer", "state", "path", "write", "target", "path", "storage_options", "not", "used", "in", "torchcheckpointio", "save_checkpoint", "raises", "typeerror", "if", "storage_options", "arg", "is", "passed", "in"], "docstring_summary": "Save model/training states as a checkpoint file through state-dump and file-write.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\torch_io.py", "partition": "train", "function_type": "class_method", "class_name": "TorchCheckpointIO", "start_line": 36, "end_line": 57, "hash": "3284b97eeaf9eec984fb7a299c4fc42f", "complexity": 2, "parameters": ["checkpoint", "Any]", "path", "storage_options"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "func_name": "load_checkpoint", "original_string": "def load_checkpoint(\r\n        self, path: _PATH, map_location: Optional[Callable] = lambda storage, loc: storage\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        Raises:\r\n            FileNotFoundError: If ``path`` is not found by the ``fsspec`` filesystem\r\n\r\n        \"\"\"\r\n\r\n        # Try to read the checkpoint at `path`. If not exist, do not restore checkpoint.\r\n        fs = get_filesystem(path)\r\n        if not fs.exists(path):\r\n            raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\r\n\r\n        return pl_load(path, map_location=map_location)", "language": "python", "code": "def load_checkpoint(\r\n        self, path: _PATH, map_location: Optional[Callable] = lambda storage, loc: storage\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        Raises:\r\n            FileNotFoundError: If ``path`` is not found by the ``fsspec`` filesystem\r\n\r\n        \"\"\"\r\n\r\n        # Try to read the checkpoint at `path`. If not exist, do not restore checkpoint.\r\n        fs = get_filesystem(path)\r\n        if not fs.exists(path):\r\n            raise FileNotFoundError(f\"Checkpoint file not found: {path}\")\r\n\r\n        return pl_load(path, map_location=map_location)", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "map_location", ":", "Optional", "[", "Callable", "]", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Loads", "checkpoint", "using", ":", "func", ":", "`", "torch", ".", "load", "`", ",", "with", "additional", "handling", "for", "`", "`", "fsspec", "`", "`", "remote", "loading", "of", "files", ".", "Args", ":", "path", ":", "Path", "to", "checkpoint", "map_location", ":", "a", "function", ",", ":", "class", ":", "`", "torch", ".", "device", "`", ",", "string", "or", "a", "dict", "specifying", "how", "to", "remap", "storage", "locations", ".", "Returns", ":", "The", "loaded", "checkpoint", ".", "Raises", ":", "FileNotFoundError", ":", "If", "`", "`", "path", "`", "`", "is", "not", "found", "by", "the", "`", "`", "fsspec", "`", "`", "filesystem", "\"", "\"", "\"", "fs", "=", "get_filesystem", "(", "path", ")", "if", "not", "fs", ".", "exists", "(", "path", ")", ":", "raise", "FileNotFoundError", "(", "f", "\"", "Checkpoint", "file", "not", "found", ":", "{", "path", "}", "\"", ")", "return", "pl_load", "(", "path", ",", "map_location", "=", "map_location", ")"], "docstring": "Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n            map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\r\n                locations.\r\n\r\n        Returns: The loaded checkpoint.\r\n\r\n        Raises:\r\n            FileNotFoundError: If ``path`` is not found by the ``fsspec`` filesystem", "docstring_tokens": ["loads", "checkpoint", "using", "func", "torch", "load", "with", "additional", "handling", "for", "fsspec", "remote", "loading", "of", "files", "args", "path", "path", "to", "checkpoint", "map_location", "a", "function", "class", "torch", "device", "string", "or", "a", "dict", "specifying", "how", "to", "remap", "storage", "locations", "returns", "the", "loaded", "checkpoint", "raises", "filenotfounderror", "if", "path", "is", "not", "found", "by", "the", "fsspec", "filesystem"], "docstring_summary": "Loads checkpoint using :func:`torch.load`, with additional handling for ``fsspec`` remote loading of files.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\torch_io.py", "partition": "train", "function_type": "class_method", "class_name": "TorchCheckpointIO", "start_line": 60, "end_line": 82, "hash": "80ad0d1c5ebf882362af5c9aa94d5230", "complexity": 2, "parameters": ["path", "map_location", "loc"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\torch_io.py", "func_name": "remove_checkpoint", "original_string": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"\r\n        fs = get_filesystem(path)\r\n        if fs.exists(path):\r\n            fs.rm(path, recursive=True)\r\n            log.debug(f\"Removed checkpoint: {path}\")", "language": "python", "code": "def remove_checkpoint(self, path: _PATH) -> None:\r\n        \"\"\"Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint\r\n\r\n        \"\"\"\r\n        fs = get_filesystem(path)\r\n        if fs.exists(path):\r\n            fs.rm(path, recursive=True)\r\n            log.debug(f\"Removed checkpoint: {path}\")", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "path", ":", "_PATH", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Remove", "checkpoint", "file", "from", "the", "filesystem", ".", "Args", ":", "path", ":", "Path", "to", "checkpoint", "\"", "\"", "\"", "fs", "=", "get_filesystem", "(", "path", ")", "if", "fs", ".", "exists", "(", "path", ")", ":", "fs", ".", "rm", "(", "path", ",", "recursive", "=", "True", ")", "log", ".", "debug", "(", "f", "\"", "Removed", "checkpoint", ":", "{", "path", "}", "\"", ")"], "docstring": "Remove checkpoint file from the filesystem.\r\n\r\n        Args:\r\n            path: Path to checkpoint", "docstring_tokens": ["remove", "checkpoint", "file", "from", "the", "filesystem", "args", "path", "path", "to", "checkpoint"], "docstring_summary": "Remove checkpoint file from the filesystem.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\torch_io.py", "partition": "train", "function_type": "class_method", "class_name": "TorchCheckpointIO", "start_line": 85, "end_line": 95, "hash": "bb71b55324a3647aff87bc3d89f85144", "complexity": 2, "parameters": ["path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\io\\xla.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``XLACheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        if RequirementCache(\"omegaconf\"):\r\n            # workaround for https://github.com/pytorch/xla/issues/2773\r\n            from omegaconf import DictConfig, ListConfig, OmegaConf\r\n\r\n            checkpoint = apply_to_collection(checkpoint, (DictConfig, ListConfig), OmegaConf.to_container)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        cpu_data = xm._maybe_convert_to_cpu(checkpoint, convert=True)\r\n        log.debug(f\"Saving checkpoint: {path}\")\r\n        torch.save(cpu_data, path)", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict[str, Any], path: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``XLACheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}`. Please implement your custom `CheckpointIO`\"\r\n                \" to define how you'd like to use `storage_options`.\"\r\n            )\r\n        fs = get_filesystem(path)\r\n        fs.makedirs(os.path.dirname(path), exist_ok=True)\r\n        if RequirementCache(\"omegaconf\"):\r\n            # workaround for https://github.com/pytorch/xla/issues/2773\r\n            from omegaconf import DictConfig, ListConfig, OmegaConf\r\n\r\n            checkpoint = apply_to_collection(checkpoint, (DictConfig, ListConfig), OmegaConf.to_container)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        cpu_data = xm._maybe_convert_to_cpu(checkpoint, convert=True)\r\n        log.debug(f\"Saving checkpoint: {path}\")\r\n        torch.save(cpu_data, path)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "path", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", "/", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "-", "dump", "and", "file", "-", "write", ".", "Args", ":", "checkpoint", ":", "dict", "containing", "model", "and", "trainer", "state", "path", ":", "write", "-", "target", "path", "storage_options", ":", "not", "used", "in", "`", "`", "XLACheckpointIO", ".", "save_checkpoint", "`", "`", "Raises", ":", "TypeError", ":", "If", "`", "`", "storage_options", "`", "`", "arg", "is", "passed", "in", "\"", "\"", "\"", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "\"", "`", "Trainer", ".", "save_checkpoint", "(", ".", ".", ".", ",", "storage_options", "=", ".", ".", ".", ")", "`", "with", "`", "storage_options", "`", "arg", "\"", "f", "\"", "is", "not", "supported", "for", "`", "{", "self", ".", "__class__", ".", "__name__", "}", "`", ".", "Please", "implement", "your", "custom", "`", "CheckpointIO", "`", "\"", "\"", "to", "define", "how", "you", "'", "d", "like", "to", "use", "`", "storage_options", "`", ".", "\"", ")", "fs", "=", "get_filesystem", "(", "path", ")", "fs", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "path", ")", ",", "exist_ok", "=", "True", ")", "if", "RequirementCache", "(", "\"", "omegaconf", "\"", ")", ":", "from", "omegaconf", "import", "DictConfig", ",", "ListConfig", ",", "OmegaConf", "checkpoint", "=", "apply_to_collection", "(", "checkpoint", ",", "(", "DictConfig", ",", "ListConfig", ")", ",", "OmegaConf", ".", "to_container", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "cpu_data", "=", "xm", ".", "_maybe_convert_to_cpu", "(", "checkpoint", ",", "convert", "=", "True", ")", "log", ".", "debug", "(", "f", "\"", "Saving", "checkpoint", ":", "{", "path", "}", "\"", ")", "torch", ".", "save", "(", "cpu_data", ",", "path", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: dict containing model and trainer state\r\n            path: write-target path\r\n            storage_options: not used in ``XLACheckpointIO.save_checkpoint``\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write", "args", "checkpoint", "dict", "containing", "model", "and", "trainer", "state", "path", "write", "target", "path", "storage_options", "not", "used", "in", "xlacheckpointio", "save_checkpoint", "raises", "typeerror", "if", "storage_options", "arg", "is", "passed", "in"], "docstring_summary": "Save model/training states as a checkpoint file through state-dump and file-write.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\io\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLACheckpointIO", "start_line": 43, "end_line": 73, "hash": "694fc44a02543630f8f9d14879db8629", "complexity": 3, "parameters": ["checkpoint", "Any]", "path", "storage_options"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\amp.py", "func_name": "_optimizer_handles_unscaling", "original_string": "def _optimizer_handles_unscaling(optimizer: Any) -> bool:\r\n    \"\"\"Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the\r\n    :class:`torch.cuda.amp.GradScaler`.\r\n\r\n    Since, the current implementation of this function checks a PyTorch internal variable on the optimizer, the return\r\n    value will only be reliable for built-in PyTorch optimizers.\r\n\r\n    \"\"\"\r\n    return getattr(optimizer, \"_step_supports_amp_scaling\", False)", "language": "python", "code": "def _optimizer_handles_unscaling(optimizer: Any) -> bool:\r\n    \"\"\"Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the\r\n    :class:`torch.cuda.amp.GradScaler`.\r\n\r\n    Since, the current implementation of this function checks a PyTorch internal variable on the optimizer, the return\r\n    value will only be reliable for built-in PyTorch optimizers.\r\n\r\n    \"\"\"\r\n    return getattr(optimizer, \"_step_supports_amp_scaling\", False)", "code_tokens": ["def", "_optimizer_handles_unscaling", "(", "optimizer", ":", "Any", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Determines", "whether", "a", "PyTorch", "optimizer", "handles", "unscaling", "gradients", "in", "the", "step", "method", "rather", "than", "through", "the", ":", "class", ":", "`", "torch", ".", "cuda", ".", "amp", ".", "GradScaler", "`", ".", "Since", ",", "the", "current", "implementation", "of", "this", "function", "checks", "a", "PyTorch", "internal", "variable", "on", "the", "optimizer", ",", "the", "return", "value", "will", "only", "be", "reliable", "for", "built", "-", "in", "PyTorch", "optimizers", ".", "\"", "\"", "\"", "return", "getattr", "(", "optimizer", ",", "\"", "_step_supports_amp_scaling", "\"", ",", "False", ")"], "docstring": "Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the\r\n    :class:`torch.cuda.amp.GradScaler`.\r\n\r\n    Since, the current implementation of this function checks a PyTorch internal variable on the optimizer, the return\r\n    value will only be reliable for built-in PyTorch optimizers.", "docstring_tokens": ["determines", "whether", "a", "pytorch", "optimizer", "handles", "unscaling", "gradients", "in", "the", "step", "method", "rather", "than", "through", "the", "class", "torch", "cuda", "amp", "gradscaler", "since", "the", "current", "implementation", "of", "this", "function", "checks", "a", "pytorch", "internal", "variable", "on", "the", "optimizer", "the", "return", "value", "will", "only", "be", "reliable", "for", "built", "in", "pytorch", "optimizers"], "docstring_summary": "Determines whether a PyTorch optimizer handles unscaling gradients in the step method rather than through the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\amp.py", "partition": "train", "function_type": "function", "start_line": 115, "end_line": 123, "hash": "c1775c7c7a8f079d72a36bed9f1b5202", "complexity": 1, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "convert_module", "original_string": "def convert_module(self, module: Module) -> Module:\r\n        \"\"\"Convert the module parameters to the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return module", "language": "python", "code": "def convert_module(self, module: Module) -> Module:\r\n        \"\"\"Convert the module parameters to the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return module", "code_tokens": ["def", "convert_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "Module", ":", "\"", "\"", "\"", "Convert", "the", "module", "parameters", "to", "the", "precision", "type", "this", "plugin", "handles", ".", "This", "is", "optional", "and", "depends", "on", "the", "precision", "limitations", "during", "optimization", ".", "\"", "\"", "\"", "return", "module"], "docstring": "Convert the module parameters to the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.", "docstring_tokens": ["convert", "the", "module", "parameters", "to", "the", "precision", "type", "this", "plugin", "handles", "this", "is", "optional", "and", "depends", "on", "the", "precision", "limitations", "during", "optimization"], "docstring_summary": "Convert the module parameters to the precision type this plugin handles.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 47, "end_line": 53, "hash": "72840b2fb7d3ed3bc6ce90cb34539dfa", "complexity": 1, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "module_init_context", "original_string": "def module_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Instantiate module parameters or tensors in the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return nullcontext()", "language": "python", "code": "def module_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Instantiate module parameters or tensors in the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.\r\n\r\n        \"\"\"\r\n        return nullcontext()", "code_tokens": ["def", "module_init_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Instantiate", "module", "parameters", "or", "tensors", "in", "the", "precision", "type", "this", "plugin", "handles", ".", "This", "is", "optional", "and", "depends", "on", "the", "precision", "limitations", "during", "optimization", ".", "\"", "\"", "\"", "return", "nullcontext", "(", ")"], "docstring": "Instantiate module parameters or tensors in the precision type this plugin handles.\r\n\r\n        This is optional and depends on the precision limitations during optimization.", "docstring_tokens": ["instantiate", "module", "parameters", "or", "tensors", "in", "the", "precision", "type", "this", "plugin", "handles", "this", "is", "optional", "and", "depends", "on", "the", "precision", "limitations", "during", "optimization"], "docstring_summary": "Instantiate module parameters or tensors in the precision type this plugin handles.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 59, "end_line": 65, "hash": "f46d07518424e5c94141ebbfb8248abc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "convert_input", "original_string": "def convert_input(self, data: Any) -> Any:\r\n        \"\"\"Convert model inputs (forward) to the floating point precision type of this plugin.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "language": "python", "code": "def convert_input(self, data: Any) -> Any:\r\n        \"\"\"Convert model inputs (forward) to the floating point precision type of this plugin.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "code_tokens": ["def", "convert_input", "(", "self", ",", "data", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Convert", "model", "inputs", "(", "forward", ")", "to", "the", "floating", "point", "precision", "type", "of", "this", "plugin", ".", "This", "is", "a", "no", "-", "op", "in", "the", "base", "precision", "plugin", ",", "since", "we", "assume", "the", "data", "already", "has", "the", "desired", "type", "(", "default", "is", "torch", ".", "float32", ")", ".", "\"", "\"", "\"", "return", "data"], "docstring": "Convert model inputs (forward) to the floating point precision type of this plugin.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).", "docstring_tokens": ["convert", "model", "inputs", "forward", "to", "the", "floating", "point", "precision", "type", "of", "this", "plugin", "this", "is", "a", "no", "op", "in", "the", "base", "precision", "plugin", "since", "we", "assume", "the", "data", "already", "has", "the", "desired", "type", "default", "is", "torch", "float32"], "docstring_summary": "Convert model inputs (forward) to the floating point precision type of this plugin.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 71, "end_line": 78, "hash": "9b6fabeddc9fca8f9b90a7157dbae9a9", "complexity": 1, "parameters": ["data"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "convert_output", "original_string": "def convert_output(self, data: Any) -> Any:\r\n        \"\"\"Convert outputs to the floating point precision type expected after model's forward.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "language": "python", "code": "def convert_output(self, data: Any) -> Any:\r\n        \"\"\"Convert outputs to the floating point precision type expected after model's forward.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).\r\n\r\n        \"\"\"\r\n        return data", "code_tokens": ["def", "convert_output", "(", "self", ",", "data", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Convert", "outputs", "to", "the", "floating", "point", "precision", "type", "expected", "after", "model", "'", "s", "forward", ".", "This", "is", "a", "no", "-", "op", "in", "the", "base", "precision", "plugin", ",", "since", "we", "assume", "the", "data", "already", "has", "the", "desired", "type", "(", "default", "is", "torch", ".", "float32", ")", ".", "\"", "\"", "\"", "return", "data"], "docstring": "Convert outputs to the floating point precision type expected after model's forward.\r\n\r\n        This is a no-op in the base precision plugin, since we assume the data already has the desired type (default is\r\n        torch.float32).", "docstring_tokens": ["convert", "outputs", "to", "the", "floating", "point", "precision", "type", "expected", "after", "model", "s", "forward", "this", "is", "a", "no", "op", "in", "the", "base", "precision", "plugin", "since", "we", "assume", "the", "data", "already", "has", "the", "desired", "type", "default", "is", "torch", "float32"], "docstring_summary": "Convert outputs to the floating point precision type expected after model's forward.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 80, "end_line": 87, "hash": "2aebcaf8cee49f50bc987db31f273173", "complexity": 1, "parameters": ["data"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "pre_backward", "original_string": "def pre_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs before precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "language": "python", "code": "def pre_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs before precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "code_tokens": ["def", "pre_backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "module", ":", "Optional", "[", "Module", "]", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Runs", "before", "precision", "plugin", "executes", "backward", ".", "Args", ":", "tensor", ":", "The", "tensor", "that", "will", "be", "used", "for", "backpropagation", "module", ":", "The", "module", "that", "was", "involved", "in", "producing", "the", "tensor", "and", "whose", "parameters", "need", "the", "gradients", "\"", "\"", "\""], "docstring": "Runs before precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients", "docstring_tokens": ["runs", "before", "precision", "plugin", "executes", "backward", "args", "tensor", "the", "tensor", "that", "will", "be", "used", "for", "backpropagation", "module", "the", "module", "that", "was", "involved", "in", "producing", "the", "tensor", "and", "whose", "parameters", "need", "the", "gradients"], "docstring_summary": "Runs before precision plugin executes backward.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 89, "end_line": 96, "hash": "03787fb533d6f5bb2751c775e375b954", "complexity": 1, "parameters": ["tensor", "module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "backward", "original_string": "def backward(self, tensor: Tensor, model: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            model: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"\r\n        tensor.backward(*args, **kwargs)", "language": "python", "code": "def backward(self, tensor: Tensor, model: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            model: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"\r\n        tensor.backward(*args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "Optional", "[", "Module", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Performs", "the", "actual", "backpropagation", ".", "Args", ":", "tensor", ":", "The", "tensor", "that", "will", "be", "used", "for", "backpropagation", "model", ":", "The", "module", "that", "was", "involved", "in", "producing", "the", "tensor", "and", "whose", "parameters", "need", "the", "gradients", "\"", "\"", "\"", "tensor", ".", "backward", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            model: The module that was involved in producing the tensor and whose parameters need the gradients", "docstring_tokens": ["performs", "the", "actual", "backpropagation", "args", "tensor", "the", "tensor", "that", "will", "be", "used", "for", "backpropagation", "model", "the", "module", "that", "was", "involved", "in", "producing", "the", "tensor", "and", "whose", "parameters", "need", "the", "gradients"], "docstring_summary": "Performs the actual backpropagation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 98, "end_line": 106, "hash": "4e6203810154368f4c1ecf4b1c0008c9", "complexity": 1, "parameters": ["tensor", "model", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "post_backward", "original_string": "def post_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs after precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "language": "python", "code": "def post_backward(self, tensor: Tensor, module: Optional[Module]) -> Any:\r\n        \"\"\"Runs after precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients\r\n\r\n        \"\"\"", "code_tokens": ["def", "post_backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "module", ":", "Optional", "[", "Module", "]", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Runs", "after", "precision", "plugin", "executes", "backward", ".", "Args", ":", "tensor", ":", "The", "tensor", "that", "will", "be", "used", "for", "backpropagation", "module", ":", "The", "module", "that", "was", "involved", "in", "producing", "the", "tensor", "and", "whose", "parameters", "need", "the", "gradients", "\"", "\"", "\""], "docstring": "Runs after precision plugin executes backward.\r\n\r\n        Args:\r\n            tensor: The tensor that will be used for backpropagation\r\n            module: The module that was involved in producing the tensor and whose parameters need the gradients", "docstring_tokens": ["runs", "after", "precision", "plugin", "executes", "backward", "args", "tensor", "the", "tensor", "that", "will", "be", "used", "for", "backpropagation", "module", "the", "module", "that", "was", "involved", "in", "producing", "the", "tensor", "and", "whose", "parameters", "need", "the", "gradients"], "docstring_summary": "Runs after precision plugin executes backward.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 108, "end_line": 115, "hash": "b9244b19487f7a8a647569eec06193ee", "complexity": 1, "parameters": ["tensor", "module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        return optimizer.step(**kwargs)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        return optimizer.step(**kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizable", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Hook", "to", "run", "the", "optimizer", "step", ".", "\"", "\"", "\"", "return", "optimizer", ".", "step", "(", "*", "*", "kwargs", ")"], "docstring": "Hook to run the optimizer step.", "docstring_tokens": ["hook", "to", "run", "the", "optimizer", "step"], "docstring_summary": "Hook to run the optimizer step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 117, "end_line": 123, "hash": "112109b2adcbda75b32885b396036a65", "complexity": 1, "parameters": ["optimizer", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "main_params", "original_string": "def main_params(self, optimizer: Optimizer) -> _PARAMETERS:\r\n        \"\"\"The main params of the model.\r\n\r\n        Returns the plain model params here. Maybe different in other precision plugins.\r\n\r\n        \"\"\"\r\n        for group in optimizer.param_groups:\r\n            yield from group[\"params\"]", "language": "python", "code": "def main_params(self, optimizer: Optimizer) -> _PARAMETERS:\r\n        \"\"\"The main params of the model.\r\n\r\n        Returns the plain model params here. Maybe different in other precision plugins.\r\n\r\n        \"\"\"\r\n        for group in optimizer.param_groups:\r\n            yield from group[\"params\"]", "code_tokens": ["def", "main_params", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "_PARAMETERS", ":", "\"", "\"", "\"", "The", "main", "params", "of", "the", "model", ".", "Returns", "the", "plain", "model", "params", "here", ".", "Maybe", "different", "in", "other", "precision", "plugins", ".", "\"", "\"", "\"", "for", "group", "in", "optimizer", ".", "param_groups", ":", "yield", "from", "group", "[", "\"", "params", "\"", "]"], "docstring": "The main params of the model.\r\n\r\n        Returns the plain model params here. Maybe different in other precision plugins.", "docstring_tokens": ["the", "main", "params", "of", "the", "model", "returns", "the", "plain", "model", "params", "here", "maybe", "different", "in", "other", "precision", "plugins"], "docstring_summary": "The main params of the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 125, "end_line": 132, "hash": "32b5d339648e9276addb24fe470e9cf3", "complexity": 2, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "state_dict", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate precision plugin state_dict.\r\n\r\n        Returns:\r\n            A dictionary containing precision plugin state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate precision plugin state_dict.\r\n\r\n        Returns:\r\n            A dictionary containing precision plugin state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "checkpoint", ",", "implement", "to", "generate", "precision", "plugin", "state_dict", ".", "Returns", ":", "A", "dictionary", "containing", "precision", "plugin", "state", ".", "\"", "\"", "\"", "return", "{", "}"], "docstring": "Called when saving a checkpoint, implement to generate precision plugin state_dict.\r\n\r\n        Returns:\r\n            A dictionary containing precision plugin state.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "implement", "to", "generate", "precision", "plugin", "state_dict", "returns", "a", "dictionary", "containing", "precision", "plugin", "state"], "docstring_summary": "Called when saving a checkpoint, implement to generate precision plugin state_dict.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 137, "end_line": 144, "hash": "0cdeeab0ba32b1f57154f6d81dc14fcd", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "load_state_dict", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload precision plugin state given precision plugin\r\n        state_dict.\r\n\r\n        Args:\r\n            state_dict: the precision plugin state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload precision plugin state given precision plugin\r\n        state_dict.\r\n\r\n        Args:\r\n            state_dict: the precision plugin state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "loading", "a", "checkpoint", ",", "implement", "to", "reload", "precision", "plugin", "state", "given", "precision", "plugin", "state_dict", ".", "Args", ":", "state_dict", ":", "the", "precision", "plugin", "state", "returned", "by", "`", "`", "state_dict", "`", "`", ".", "\"", "\"", "\"", "pass"], "docstring": "Called when loading a checkpoint, implement to reload precision plugin state given precision plugin\r\n        state_dict.\r\n\r\n        Args:\r\n            state_dict: the precision plugin state returned by ``state_dict``.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "implement", "to", "reload", "precision", "plugin", "state", "given", "precision", "plugin", "state_dict", "args", "state_dict", "the", "precision", "plugin", "state", "returned", "by", "state_dict"], "docstring_summary": "Called when loading a checkpoint, implement to reload precision plugin state given precision plugin", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 146, "end_line": 154, "hash": "de61f1d22764a72763902784be9e5a0f", "complexity": 1, "parameters": ["state_dict", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\plugins\\precision\\precision.py", "func_name": "teardown", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "method", "is", "called", "to", "teardown", "the", "training", "process", ".", "It", "is", "the", "right", "place", "to", "release", "memory", "and", "free", "other", "resources", ".", "\"", "\"", "\""], "docstring": "This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "training", "process", "it", "is", "the", "right", "place", "to", "release", "memory", "and", "free", "other", "resources"], "docstring_summary": "This method is called to teardown the training process.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 156, "end_line": 161, "hash": "09ceb1830b090d0b8924d359ab7de16e", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\ddp.py", "func_name": "setup_module", "original_string": "def setup_module(self, module: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self._determine_ddp_device_ids()\r\n        # https://pytorch.org/docs/stable/notes/cuda.html#id5\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=module, device_ids=device_ids, **self._ddp_kwargs)", "language": "python", "code": "def setup_module(self, module: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self._determine_ddp_device_ids()\r\n        # https://pytorch.org/docs/stable/notes/cuda.html#id5\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=module, device_ids=device_ids, **self._ddp_kwargs)", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "DistributedDataParallel", ":", "\"", "\"", "\"", "Wraps", "the", "model", "into", "a", ":", "class", ":", "`", "~", "torch", ".", "nn", ".", "parallel", ".", "distributed", ".", "DistributedDataParallel", "`", "module", ".", "\"", "\"", "\"", "device_ids", "=", "self", ".", "_determine_ddp_device_ids", "(", ")", "ctx", "=", "torch", ".", "cuda", ".", "stream", "(", "torch", ".", "cuda", ".", "Stream", "(", ")", ")", "if", "device_ids", "is", "not", "None", "else", "nullcontext", "(", ")", "with", "ctx", ":", "return", "DistributedDataParallel", "(", "module", "=", "module", ",", "device_ids", "=", "device_ids", ",", "*", "*", "self", ".", "_ddp_kwargs", ")"], "docstring": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "nn", "parallel", "distributed", "distributeddataparallel", "module"], "docstring_summary": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 123, "end_line": 129, "hash": "b5a4b3a7731f78e71633bfcf76a81899", "complexity": 3, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\ddp.py", "func_name": "all_reduce", "original_string": "def all_reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "language": "python", "code": "def all_reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "code_tokens": ["def", "all_reduce", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "\"", "mean", "\"", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "group", ":", "the", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "reduce_op", ":", "the", "reduction", "operation", ".", "Defaults", "to", "'", "mean", "'", "/", "'", "avg", "'", ".", "Can", "also", "be", "a", "string", "'", "sum", "'", "to", "calculate", "the", "sum", "during", "reduction", ".", "Return", ":", "reduced", "value", ",", "except", "when", "the", "input", "was", "not", "a", "tensor", "the", "output", "remains", "is", "unchanged", "\"", "\"", "\"", "if", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "return", "_sync_ddp_if_available", "(", "tensor", ",", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "reduce_op", "the", "reduction", "operation", "defaults", "to", "mean", "avg", "can", "also", "be", "a", "string", "sum", "to", "calculate", "the", "sum", "during", "reduction", "return", "reduced", "value", "except", "when", "the", "input", "was", "not", "a", "tensor", "the", "output", "remains", "is", "unchanged"], "docstring_summary": "Reduces a tensor from several distributed processes to one aggregated tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 136, "end_line": 153, "hash": "e125250d7c5194a04e3d90a956720078", "complexity": 2, "parameters": ["tensor", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\ddp.py", "func_name": "no_backward_sync", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n\r\n        if not isinstance(module, DistributedDataParallel):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `DistributedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n\r\n        if not isinstance(module, DistributedDataParallel):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `DistributedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Blocks", "gradient", "synchronization", "inside", "the", ":", "class", ":", "`", "~", "torch", ".", "nn", ".", "parallel", ".", "distributed", ".", "DistributedDataParallel", "`", "wrapper", ".", "\"", "\"", "\"", "if", "not", "enabled", ":", "return", "nullcontext", "(", ")", "if", "not", "isinstance", "(", "module", ",", "DistributedDataParallel", ")", ":", "raise", "TypeError", "(", "\"", "Blocking", "backward", "sync", "is", "only", "possible", "if", "the", "module", "passed", "to", "\"", "f", "\"", "`", "{", "self", ".", "__class__", ".", "__name__", "}", ".", "no_backward_sync", "`", "is", "wrapped", "in", "`", "DistributedDataParallel", "`", ".", "\"", "f", "\"", "Got", ":", "{", "module", ".", "__class__", ".", "__name__", "}", ".", "\"", ")", "return", "module", ".", "no_sync", "(", ")"], "docstring": "Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`\r\n        wrapper.", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "class", "torch", "nn", "parallel", "distributed", "distributeddataparallel", "wrapper"], "docstring_summary": "Blocks gradient synchronization inside the :class:`~torch.nn.parallel.distributed.DistributedDataParallel`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "_DDPBackwardSyncControl", "start_line": 247, "end_line": 259, "hash": "9fad3eaf3bcc862ac16e5bfc1e7b9410", "complexity": 3, "parameters": ["module", "enabled"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        accelerator: Optional[Accelerator] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Optional[int] = None,\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. `For more information: https://pytorch-\r\n        lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        `For more information: https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training`.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either ``precision=\"16-mixed\"`` or\r\n                ``precision=\"bf16-mixed\"``.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size.\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise ImportError(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            # Starting with PyTorch 2.6, `torch.load` defaults to `weights_only=True` when loading full checkpoints.\r\n            # DeepSpeed added support for this behavior in version 0.16.0.\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision=precision,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._backward_sync_control = None  # DeepSpeed handles gradient accumulation internally\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            # User has not overridden config, set defaults\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        # default FP16 parameters.\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale\r\n\r\n        self._deepspeed_engine: Optional[DeepSpeedEngine] = None", "language": "python", "code": "def __init__(\r\n        self,\r\n        accelerator: Optional[Accelerator] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Optional[int] = None,\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. `For more information: https://pytorch-\r\n        lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        `For more information: https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training`.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either ``precision=\"16-mixed\"`` or\r\n                ``precision=\"bf16-mixed\"``.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size.\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise ImportError(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            # Starting with PyTorch 2.6, `torch.load` defaults to `weights_only=True` when loading full checkpoints.\r\n            # DeepSpeed added support for this behavior in version 0.16.0.\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision=precision,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._backward_sync_control = None  # DeepSpeed handles gradient accumulation internally\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            # User has not overridden config, set defaults\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        # default FP16 parameters.\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale\r\n\r\n        self._deepspeed_engine: Optional[DeepSpeedEngine] = None", "code_tokens": ["def", "__init__", "(", "self", ",", "accelerator", ":", "Optional", "[", "Accelerator", "]", "=", "None", ",", "zero_optimization", ":", "bool", "=", "True", ",", "stage", ":", "int", "=", "2", ",", "remote_device", ":", "Optional", "[", "str", "]", "=", "None", ",", "offload_optimizer", ":", "bool", "=", "False", ",", "offload_parameters", ":", "bool", "=", "False", ",", "offload_params_device", ":", "str", "=", "\"", "cpu", "\"", ",", "nvme_path", ":", "str", "=", "\"", "/", "local_nvme", "\"", ",", "params_buffer_count", ":", "int", "=", "5", ",", "params_buffer_size", ":", "int", "=", "100_000_000", ",", "max_in_cpu", ":", "int", "=", "1_000_000_000", ",", "offload_optimizer_device", ":", "str", "=", "\"", "cpu", "\"", ",", "optimizer_buffer_count", ":", "int", "=", "4", ",", "block_size", ":", "int", "=", "1048576", ",", "queue_depth", ":", "int", "=", "8", ",", "single_submit", ":", "bool", "=", "False", ",", "overlap_events", ":", "bool", "=", "True", ",", "thread_count", ":", "int", "=", "1", ",", "pin_memory", ":", "bool", "=", "False", ",", "sub_group_size", ":", "int", "=", "1_000_000_000_000", ",", "contiguous_gradients", ":", "bool", "=", "True", ",", "overlap_comm", ":", "bool", "=", "True", ",", "allgather_partitions", ":", "bool", "=", "True", ",", "reduce_scatter", ":", "bool", "=", "True", ",", "allgather_bucket_size", ":", "int", "=", "200_000_000", ",", "reduce_bucket_size", ":", "int", "=", "200_000_000", ",", "zero_allow_untested_optimizer", ":", "bool", "=", "True", ",", "logging_batch_size_per_gpu", ":", "Optional", "[", "int", "]", "=", "None", ",", "config", ":", "Optional", "[", "Union", "[", "_PATH", ",", "dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "logging_level", ":", "int", "=", "logging", ".", "WARN", ",", "parallel_devices", ":", "Optional", "[", "list", "[", "torch", ".", "device", "]", "]", "=", "None", ",", "cluster_environment", ":", "Optional", "[", "ClusterEnvironment", "]", "=", "None", ",", "loss_scale", ":", "float", "=", "0", ",", "initial_scale_power", ":", "int", "=", "16", ",", "loss_scale_window", ":", "int", "=", "1000", ",", "hysteresis", ":", "int", "=", "2", ",", "min_loss_scale", ":", "int", "=", "1", ",", "partition_activations", ":", "bool", "=", "False", ",", "cpu_checkpointing", ":", "bool", "=", "False", ",", "contiguous_memory_optimization", ":", "bool", "=", "False", ",", "synchronize_checkpoint_boundary", ":", "bool", "=", "False", ",", "load_full_weights", ":", "bool", "=", "False", ",", "precision", ":", "Optional", "[", "Precision", "]", "=", "None", ",", "process_group_backend", ":", "Optional", "[", "str", "]", "=", "None", ",", "timeout", ":", "Optional", "[", "timedelta", "]", "=", "default_pg_timeout", ",", "exclude_frozen_parameters", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Provides", "capabilities", "to", "run", "training", "using", "the", "DeepSpeed", "library", ",", "with", "training", "optimizations", "for", "large", "billion", "parameter", "models", ".", "`", "For", "more", "information", ":", "https", ":", "/", "/", "pytorch", "-", "lightning", ".", "readthedocs", ".", "io", "/", "en", "/", "stable", "/", "advanced", "/", "model_parallel", ".", "html", ".", ".", "warning", ":", ":", "This", "is", "an", ":", "ref", ":", "`", "experimental", "<", "versioning", ":", "Experimental", "API", ">", "`", "feature", ".", "Defaults", "have", "been", "set", "to", "enable", "ZeRO", "-", "Offload", "and", "some", "have", "been", "taken", "from", "the", "link", "below", ".", "These", "defaults", "have", "been", "set", "generally", ",", "but", "may", "require", "tuning", "for", "optimum", "performance", "based", "on", "your", "model", "size", ".", "`", "For", "more", "information", ":", "https", ":", "/", "/", "www", ".", "deepspeed", ".", "ai", "/", "docs", "/", "config", "-", "json", "/", "Arguments", ":", "zero_optimization", ":", "Enable", "ZeRO", "optimization", ".", "This", "is", "compatible", "with", "either", "`", "`", "precision", "=", "\"", "16", "-", "mixed", "\"", "`", "`", "or", "`", "`", "precision", "=", "\"", "bf16", "-", "mixed", "\"", "`", "`", ".", "stage", ":", "Different", "stages", "of", "the", "ZeRO", "Optimizer", ".", "0", "is", "disabled", ",", "1", "is", "optimizer", "state", "partitioning", ",", "2", "is", "optimizer", "+", "gradient", "state", "partitioning", ",", "3", "is", "optimizer", "+", "gradient_parameter", "partitioning", "using", "the", "infinity", "engine", ".", "remote_device", ":", "Device", "to", "instantiate", "the", "model", "on", "initially", "(", "`", "`", "cpu", "`", "`", "or", "`", "`", "nvme", "`", "`", ")", ".", "Defaults", "to", "GPU", ".", "offload_optimizer", ":", "Enable", "offloading", "optimizer", "memory", "and", "computation", "to", "CPU", "or", "NVMe", "based", "on", "`", "`", "offload_optimizer_device", "`", "`", ".", "offload_parameters", ":", "When", "using", "ZeRO", "Stage", "3", ",", "Enable", "offloading", "parameter", "memory", "and", "computation", "to", "CPU", "or", "NVMe", "based", "on", "`", "`", "offload_params_device", "`", "`", ".", "offload_params_device", ":", "When", "offloading", "parameters", "choose", "the", "device", "to", "offload", "to", ",", "`", "`", "cpu", "`", "`", "or", "`", "`", "nvme", "`", "`", ".", "offload_optimizer_device", ":", "When", "offloading", "optimizer", "state", "choose", "the", "device", "to", "offload", "to", ",", "`", "`", "cpu", "`", "`", "or", "`", "`", "nvme", "`", "`", ".", "params_buffer_count", ":", "Number", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "`", "`", "offload_params_device", "`", "`", "is", "`", "`", "nvme", "`", "`", ".", "params_buffer_size", ":", "Size", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "`", "`", "offload_params_device", "`", "`", "is", "`", "`", "nvme", "`", "`", ".", "max_in_cpu", ":", "Number", "of", "parameter", "elements", "to", "maintain", "in", "CPU", "memory", "when", "offloading", "to", "NVMe", "is", "enabled", ".", "nvme_path", ":", "Filesystem", "path", "for", "NVMe", "device", "for", "optimizer", "/", "parameter", "state", "offloading", ".", "optimizer_buffer_count", ":", "Number", "of", "buffers", "in", "buffer", "pool", "for", "optimizer", "state", "offloading", "when", "`", "`", "offload_optimizer_device", "`", "`", "is", "set", "to", "`", "`", "nvme", "`", "`", ".", "This", "should", "be", "at", "least", "the", "number", "of", "states", "maintained", "per", "parameter", "by", "the", "optimizer", ".", "For", "example", ",", "Adam", "optimizer", "has", "4", "states", "(", "parameter", ",", "gradient", ",", "momentum", ",", "and", "variance", ")", ".", "block_size", ":", "When", "using", "NVMe", "Offloading", ",", "the", "I", "/", "O", "block", "size", "in", "bytes", ".", "queue_depth", ":", "When", "using", "NVMe", "Offloading", ",", "the", "I", "/", "O", "queue", "depth", ".", "single_submit", ":", "When", "using", "NVMe", "Offloading", ",", "submit", "requests", "to", "storage", "device", "as", "multiple", "individual", "requests", ",", "as", "opposed", "to", "one", "block", "of", "requests", ".", "overlap_events", ":", "When", "using", "NVMe", "Offloading", ",", "submit", "requests", "to", "storage", "device", "in", "an", "overlapped", "fashion", "without", "waiting", "for", "completion", "of", "earlier", "requests", ".", "thread_count", ":", "When", "using", "NVMe", "Offloading", ",", "Intra", "-", "request", "parallelism", "for", "each", "read", "/", "write", "submitted", "by", "a", "user", "thread", ".", "pin_memory", ":", "When", "using", "ZeRO", "stage", "3", ",", "pin", "optimizer", "state", "memory", "on", "CPU", ".", "This", "could", "boost", "throughput", "at", "the", "cost", "of", "extra", "memory", "overhead", ".", "sub_group_size", ":", "When", "using", "ZeRO", "stage", "3", ",", "defines", "the", "number", "of", "parameters", "within", "a", "sub", "group", "to", "offload", "at", "a", "time", ".", "Smaller", "numbers", "require", "more", "communication", ",", "but", "improve", "memory", "efficiency", ".", "contiguous_gradients", ":", "Copies", "gradients", "to", "a", "continuous", "buffer", "as", "they", "are", "produced", ".", "Avoids", "memory", "fragmentation", "during", "backwards", ".", "Useful", "when", "training", "large", "models", ".", "overlap_comm", ":", "Overlap", "the", "reduction", "(", "synchronization", ")", "of", "gradients", "with", "the", "backwards", "computation", ".", "This", "is", "a", "speed", "optimization", "when", "training", "across", "multiple", "GPUs", "/", "machines", ".", "allgather_partitions", ":", "All", "gather", "updated", "parameters", "at", "the", "end", "of", "training", "step", ",", "instead", "of", "using", "a", "series", "of", "broadcast", "collectives", ".", "reduce_scatter", ":", "Use", "reduce", "/", "scatter", "instead", "of", "allreduce", "to", "average", "gradients", ".", "allgather_bucket_size", ":", "Number", "of", "elements", "to", "allgather", "at", "once", ".", "Used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", ",", "with", "a", "tradeoff", "with", "speed", ".", "reduce_bucket_size", ":", "Number", "of", "elements", "to", "reduce", "at", "once", ".", "Used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", ",", "with", "a", "tradeoff", "with", "speed", ".", "zero_allow_untested_optimizer", ":", "Allow", "untested", "optimizers", "to", "be", "used", "with", "ZeRO", ".", "Currently", "only", "Adam", "is", "a", "DeepSpeed", "supported", "optimizer", "when", "using", "ZeRO", ".", "logging_batch_size_per_gpu", ":", "Config", "used", "in", "DeepSpeed", "to", "calculate", "verbose", "timing", "for", "logging", "on", "a", "per", "sample", "per", "second", "basis", "(", "only", "displayed", "if", "logging", "=", "logging", ".", "INFO", ")", ".", "To", "obtain", "accurate", "logs", "when", "using", "datasets", "that", "do", "not", "support", "batch", "samplers", ",", "set", "this", "to", "the", "actual", "per", "gpu", "batch", "size", ".", "config", ":", "Pass", "in", "a", "deepspeed", "formatted", "config", "dict", ",", "or", "path", "to", "a", "deepspeed", "config", ":", "https", ":", "/", "/", "www", ".", "deepspeed", ".", "ai", "/", "docs", "/", "config", "-", "json", ".", "All", "defaults", "will", "be", "ignored", "if", "a", "config", "is", "passed", "in", ".", "logging_level", ":", "Set", "logging", "level", "for", "deepspeed", ".", "loss_scale", ":", "Loss", "scaling", "value", "for", "FP16", "training", ".", "0", ".", "0", "results", "in", "dynamic", "loss", "scaling", ",", "otherwise", "static", ".", "initial_scale_power", ":", "Power", "of", "the", "initial", "dynamic", "loss", "scale", "value", ".", "Loss", "scale", "is", "computed", "by", "`", "`", "2", "^", "initial_scale_power", "`", "`", ".", "loss_scale_window", ":", "Window", "in", "which", "to", "raise", "/", "lower", "the", "dynamic", "FP16", "loss", "scaling", "value", ".", "hysteresis", ":", "FP16", "Delay", "shift", "in", "Dynamic", "Loss", "scaling", ".", "min_loss_scale", ":", "The", "minimum", "FP16", "dynamic", "loss", "scaling", "value", ".", "partition_activations", ":", "Enables", "partition", "activation", "when", "used", "with", "ZeRO", "stage", "3", "and", "model", "parallelism", ".", "Still", "requires", "you", "to", "wrap", "your", "forward", "functions", "in", "deepspeed", ".", "checkpointing", ".", "checkpoint", ".", "See", "`", "deepspeed", "tutorial", "<", "https", ":", "/", "/", "www", ".", "deepspeed", ".", "ai", "/", "tutorials", "/", "megatron", "/", "cpu_checkpointing", ":", "Offloads", "partitioned", "activations", "to", "CPU", "if", "`", "`", "partition_activations", "`", "`", "is", "enabled", ".", "contiguous_memory_optimization", ":", "Copies", "partitioned", "activations", "so", "that", "they", "are", "contiguous", "in", "memory", ".", "Not", "supported", "by", "all", "models", ".", "synchronize_checkpoint_boundary", ":", "Insert", ":", "func", ":", "`", "torch", ".", "cuda", ".", "synchronize", "`", "at", "each", "checkpoint", "boundary", ".", "load_full_weights", ":", "True", "when", "loading", "a", "single", "checkpoint", "file", "containing", "the", "model", "state", "dict", "when", "using", "ZeRO", "Stage", "3", ".", "This", "differs", "from", "the", "DeepSpeed", "checkpoint", "which", "contains", "shards", "per", "worker", ".", "exclude_frozen_parameters", ":", "Exclude", "frozen", "parameters", "when", "saving", "checkpoints", ".", "\"", "\"", "\"", "if", "not", "_DEEPSPEED_AVAILABLE", ":", "raise", "ImportError", "(", "\"", "To", "use", "the", "`", "DeepSpeedStrategy", "`", ",", "you", "must", "have", "DeepSpeed", "installed", ".", "\"", "\"", "Install", "it", "by", "running", "`", "pip", "install", "-", "U", "deepspeed", "`", ".", "\"", ")", "if", "_TORCH_GREATER_EQUAL_2_6", "and", "not", "_DEEPSPEED_GREATER_EQUAL_0_16", ":", "import", "deepspeed", "deepspeed_version", "=", "deepspeed", ".", "__version__", "raise", "ImportError", "(", "f", "\"", "PyTorch", ">", "=", "2", ".", "6", "requires", "DeepSpeed", ">", "=", "0", ".", "16", ".", "0", ".", "\"", "f", "\"", "Detected", "DeepSpeed", "version", ":", "{", "deepspeed_version", "}", ".", "\"", "\"", "Please", "upgrade", "by", "running", "`", "pip", "install", "-", "U", "'", "deepspeed", ">", "=", "0", ".", "16", ".", "0", "'", "`", ".", "\"", ")", "super", "(", ")", ".", "__init__", "(", "accelerator", "=", "accelerator", ",", "parallel_devices", "=", "parallel_devices", ",", "cluster_environment", "=", "cluster_environment", ",", "precision", "=", "precision", ",", "process_group_backend", "=", "process_group_backend", ",", ")", "self", ".", "_backward_sync_control", "=", "None", "self", ".", "_timeout", ":", "Optional", "[", "timedelta", "]", "=", "timeout", "self", ".", "config", "=", "self", ".", "_load_config", "(", "config", ")", "if", "self", ".", "config", "is", "None", ":", "self", ".", "config", "=", "self", ".", "_create_default_config", "(", "zero_optimization", ",", "zero_allow_untested_optimizer", ",", "logging_batch_size_per_gpu", ",", "offload_optimizer", "=", "offload_optimizer", ",", "offload_parameters", "=", "offload_parameters", ",", "nvme_path", "=", "nvme_path", ",", "offload_params_device", "=", "offload_params_device", ",", "params_buffer_count", "=", "params_buffer_count", ",", "params_buffer_size", "=", "params_buffer_size", ",", "max_in_cpu", "=", "max_in_cpu", ",", "pin_memory", "=", "pin_memory", ",", "offload_optimizer_device", "=", "offload_optimizer_device", ",", "optimizer_buffer_count", "=", "optimizer_buffer_count", ",", "block_size", "=", "block_size", ",", "queue_depth", "=", "queue_depth", ",", "single_submit", "=", "single_submit", ",", "overlap_events", "=", "overlap_events", ",", "thread_count", "=", "thread_count", ",", "partition_activations", "=", "partition_activations", ",", "cpu_checkpointing", "=", "cpu_checkpointing", ",", "contiguous_memory_optimization", "=", "contiguous_memory_optimization", ",", "synchronize_checkpoint_boundary", "=", "synchronize_checkpoint_boundary", ",", "stage", "=", "stage", ",", "contiguous_gradients", "=", "contiguous_gradients", ",", "overlap_comm", "=", "overlap_comm", ",", "allgather_partitions", "=", "allgather_partitions", ",", "reduce_scatter", "=", "reduce_scatter", ",", "allgather_bucket_size", "=", "allgather_bucket_size", ",", "reduce_bucket_size", "=", "reduce_bucket_size", ",", "sub_group_size", "=", "sub_group_size", ",", ")", "import", "deepspeed", "self", ".", "_config_initialized", "=", "False", "deepspeed", ".", "utils", ".", "logging", ".", "logger", ".", "setLevel", "(", "logging_level", ")", "self", ".", "remote_device", "=", "remote_device", "self", ".", "load_full_weights", "=", "load_full_weights", "self", ".", "exclude_frozen_parameters", "=", "exclude_frozen_parameters", "self", ".", "loss_scale", "=", "loss_scale", "self", ".", "initial_scale_power", "=", "initial_scale_power", "self", ".", "loss_scale_window", "=", "loss_scale_window", "self", ".", "hysteresis", "=", "hysteresis", "self", ".", "min_loss_scale", "=", "min_loss_scale", "self", ".", "_deepspeed_engine", ":", "Optional", "[", "DeepSpeedEngine", "]", "=", "None"], "docstring": "Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. `For more information: https://pytorch-\r\n        lightning.readthedocs.io/en/stable/advanced/model_parallel.html#deepspeed`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        `For more information: https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training`.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either ``precision=\"16-mixed\"`` or\r\n                ``precision=\"bf16-mixed\"``.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size.\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.", "docstring_tokens": ["provides", "capabilities", "to", "run", "training", "using", "the", "deepspeed", "library", "with", "training", "optimizations", "for", "large", "billion", "parameter", "models", "for", "more", "information", "https", "pytorch", "lightning", "readthedocs", "io", "en", "stable", "advanced", "model_parallel", "html", "deepspeed", "warning", "this", "is", "an", "ref", "experimental", "versioning", "experimental", "api", "feature", "defaults", "have", "been", "set", "to", "enable", "zero", "offload", "and", "some", "have", "been", "taken", "from", "the", "link", "below", "these", "defaults", "have", "been", "set", "generally", "but", "may", "require", "tuning", "for", "optimum", "performance", "based", "on", "your", "model", "size", "for", "more", "information", "https", "www", "deepspeed", "ai", "docs", "config", "json", "zero", "optimizations", "for", "fp16", "training", "arguments", "zero_optimization", "enable", "zero", "optimization", "this", "is", "compatible", "with", "either", "precision", "16", "mixed", "or", "precision", "bf16", "mixed", "stage", "different", "stages", "of", "the", "zero", "optimizer", "0", "is", "disabled", "1", "is", "optimizer", "state", "partitioning", "2", "is", "optimizer", "gradient", "state", "partitioning", "3", "is", "optimizer", "gradient_parameter", "partitioning", "using", "the", "infinity", "engine", "remote_device", "device", "to", "instantiate", "the", "model", "on", "initially", "cpu", "or", "nvme", "defaults", "to", "gpu", "offload_optimizer", "enable", "offloading", "optimizer", "memory", "and", "computation", "to", "cpu", "or", "nvme", "based", "on", "offload_optimizer_device", "offload_parameters", "when", "using", "zero", "stage", "3", "enable", "offloading", "parameter", "memory", "and", "computation", "to", "cpu", "or", "nvme", "based", "on", "offload_params_device", "offload_params_device", "when", "offloading", "parameters", "choose", "the", "device", "to", "offload", "to", "cpu", "or", "nvme", "offload_optimizer_device", "when", "offloading", "optimizer", "state", "choose", "the", "device", "to", "offload", "to", "cpu", "or", "nvme", "params_buffer_count", "number", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "offload_params_device", "is", "nvme", "params_buffer_size", "size", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "offload_params_device", "is", "nvme", "max_in_cpu", "number", "of", "parameter", "elements", "to", "maintain", "in", "cpu", "memory", "when", "offloading", "to", "nvme", "is", "enabled", "nvme_path", "filesystem", "path", "for", "nvme", "device", "for", "optimizer", "parameter", "state", "offloading", "optimizer_buffer_count", "number", "of", "buffers", "in", "buffer", "pool", "for", "optimizer", "state", "offloading", "when", "offload_optimizer_device", "is", "set", "to", "nvme", "this", "should", "be", "at", "least", "the", "number", "of", "states", "maintained", "per", "parameter", "by", "the", "optimizer", "for", "example", "adam", "optimizer", "has", "4", "states", "parameter", "gradient", "momentum", "and", "variance", "block_size", "when", "using", "nvme", "offloading", "the", "i", "o", "block", "size", "in", "bytes", "queue_depth", "when", "using", "nvme", "offloading", "the", "i", "o", "queue", "depth", "single_submit", "when", "using", "nvme", "offloading", "submit", "requests", "to", "storage", "device", "as", "multiple", "individual", "requests", "as", "opposed", "to", "one", "block", "of", "requests", "overlap_events", "when", "using", "nvme", "offloading", "submit", "requests", "to", "storage", "device", "in", "an", "overlapped", "fashion", "without", "waiting", "for", "completion", "of", "earlier", "requests", "thread_count", "when", "using", "nvme", "offloading", "intra", "request", "parallelism", "for", "each", "read", "write", "submitted", "by", "a", "user", "thread", "pin_memory", "when", "using", "zero", "stage", "3", "pin", "optimizer", "state", "memory", "on", "cpu", "this", "could", "boost", "throughput", "at", "the", "cost", "of", "extra", "memory", "overhead", "sub_group_size", "when", "using", "zero", "stage", "3", "defines", "the", "number", "of", "parameters", "within", "a", "sub", "group", "to", "offload", "at", "a", "time", "smaller", "numbers", "require", "more", "communication", "but", "improve", "memory", "efficiency", "contiguous_gradients", "copies", "gradients", "to", "a", "continuous", "buffer", "as", "they", "are", "produced", "avoids", "memory", "fragmentation", "during", "backwards", "useful", "when", "training", "large", "models", "overlap_comm", "overlap", "the", "reduction", "synchronization", "of", "gradients", "with", "the", "backwards", "computation", "this", "is", "a", "speed", "optimization", "when", "training", "across", "multiple", "gpus", "machines", "allgather_partitions", "all", "gather", "updated", "parameters", "at", "the", "end", "of", "training", "step", "instead", "of", "using", "a", "series", "of", "broadcast", "collectives", "reduce_scatter", "use", "reduce", "scatter", "instead", "of", "allreduce", "to", "average", "gradients", "allgather_bucket_size", "number", "of", "elements", "to", "allgather", "at", "once", "used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", "with", "a", "tradeoff", "with", "speed", "reduce_bucket_size", "number", "of", "elements", "to", "reduce", "at", "once", "used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", "with", "a", "tradeoff", "with", "speed", "zero_allow_untested_optimizer", "allow", "untested", "optimizers", "to", "be", "used", "with", "zero", "currently", "only", "adam", "is", "a", "deepspeed", "supported", "optimizer", "when", "using", "zero", "logging_batch_size_per_gpu", "config", "used", "in", "deepspeed", "to", "calculate", "verbose", "timing", "for", "logging", "on", "a", "per", "sample", "per", "second", "basis", "only", "displayed", "if", "logging", "logging", "info", "to", "obtain", "accurate", "logs", "when", "using", "datasets", "that", "do", "not", "support", "batch", "samplers", "set", "this", "to", "the", "actual", "per", "gpu", "batch", "size", "config", "pass", "in", "a", "deepspeed", "formatted", "config", "dict", "or", "path", "to", "a", "deepspeed", "config", "https", "www", "deepspeed", "ai", "docs", "config", "json", "all", "defaults", "will", "be", "ignored", "if", "a", "config", "is", "passed", "in", "logging_level", "set", "logging", "level", "for", "deepspeed", "loss_scale", "loss", "scaling", "value", "for", "fp16", "training", "0", "0", "results", "in", "dynamic", "loss", "scaling", "otherwise", "static", "initial_scale_power", "power", "of", "the", "initial", "dynamic", "loss", "scale", "value", "loss", "scale", "is", "computed", "by", "2", "initial_scale_power", "loss_scale_window", "window", "in", "which", "to", "raise", "lower", "the", "dynamic", "fp16", "loss", "scaling", "value", "hysteresis", "fp16", "delay", "shift", "in", "dynamic", "loss", "scaling", "min_loss_scale", "the", "minimum", "fp16", "dynamic", "loss", "scaling", "value", "partition_activations", "enables", "partition", "activation", "when", "used", "with", "zero", "stage", "3", "and", "model", "parallelism", "still", "requires", "you", "to", "wrap", "your", "forward", "functions", "in", "deepspeed", "checkpointing", "checkpoint", "see", "deepspeed", "tutorial", "https", "www", "deepspeed", "ai", "tutorials", "megatron", "deepspeed", "activation", "checkpoints", "optional", "_", "cpu_checkpointing", "offloads", "partitioned", "activations", "to", "cpu", "if", "partition_activations", "is", "enabled", "contiguous_memory_optimization", "copies", "partitioned", "activations", "so", "that", "they", "are", "contiguous", "in", "memory", "not", "supported", "by", "all", "models", "synchronize_checkpoint_boundary", "insert", "func", "torch", "cuda", "synchronize", "at", "each", "checkpoint", "boundary", "load_full_weights", "true", "when", "loading", "a", "single", "checkpoint", "file", "containing", "the", "model", "state", "dict", "when", "using", "zero", "stage", "3", "this", "differs", "from", "the", "deepspeed", "checkpoint", "which", "contains", "shards", "per", "worker", "exclude_frozen_parameters", "exclude", "frozen", "parameters", "when", "saving", "checkpoints"], "docstring_summary": "Provides capabilities to run training using the DeepSpeed library, with training optimizations for large", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 57, "end_line": 318, "hash": "4564d7c6ca5ab4118ed889add2fe0eda", "complexity": 5, "parameters": ["accelerator", "zero_optimization", "stage", "remote_device", "offload_optimizer", "offload_parameters", "offload_params_device", "nvme_path", "params_buffer_count", "params_buffer_size", "max_in_cpu", "offload_optimizer_device", "optimizer_buffer_count", "block_size", "queue_depth", "single_submit", "overlap_events", "thread_count", "pin_memory", "sub_group_size", "contiguous_gradients", "overlap_comm", "allgather_partitions", "reduce_scatter", "allgather_bucket_size", "reduce_bucket_size", "zero_allow_untested_optimizer", "logging_batch_size_per_gpu", "config", "dict[str", "Any]]]", "logging_level", "parallel_devices", "cluster_environment", "loss_scale", "initial_scale_power", "loss_scale_window", "hysteresis", "min_loss_scale", "partition_activations", "cpu_checkpointing", "contiguous_memory_optimization", "synchronize_checkpoint_boundary", "load_full_weights", "precision", "process_group_backend", "timeout", "exclude_frozen_parameters"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "setup_module_and_optimizers", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", list[Optimizer], Any]:\r\n        \"\"\"Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,\r\n        only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine`, a list with a single\r\n            deepspeed optimizer, and an optional learning rate scheduler.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        self._deepspeed_engine, optimizer, scheduler = self._initialize_engine(module, optimizers[0], scheduler)\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self._deepspeed_engine, [optimizer], scheduler", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", list[Optimizer], Any]:\r\n        \"\"\"Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,\r\n        only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine`, a list with a single\r\n            deepspeed optimizer, and an optional learning rate scheduler.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        self._deepspeed_engine, optimizer, scheduler = self._initialize_engine(module, optimizers[0], scheduler)\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self._deepspeed_engine, [optimizer], scheduler", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "=", "None", ")", "-", ">", "tuple", "[", "\"", "DeepSpeedEngine", "\"", ",", "list", "[", "Optimizer", "]", ",", "Any", "]", ":", "\"", "\"", "\"", "Set", "up", "a", "model", "and", "multiple", "optimizers", "together", ",", "along", "with", "an", "optional", "learning", "rate", "scheduler", ".", "Currently", ",", "only", "a", "single", "optimizer", "is", "supported", ".", "Return", ":", "The", "model", "wrapped", "into", "a", ":", "class", ":", "`", "deepspeed", ".", "DeepSpeedEngine", "`", ",", "a", "list", "with", "a", "single", "deepspeed", "optimizer", ",", "and", "an", "optional", "learning", "rate", "scheduler", ".", "\"", "\"", "\"", "if", "len", "(", "optimizers", ")", "!", "=", "1", ":", "raise", "ValueError", "(", "f", "\"", "Currently", "only", "one", "optimizer", "is", "supported", "with", "DeepSpeed", ".", "Got", "{", "len", "(", "optimizers", ")", "}", "optimizers", "instead", ".", "\"", ")", "self", ".", "_deepspeed_engine", ",", "optimizer", ",", "scheduler", "=", "self", ".", "_initialize_engine", "(", "module", ",", "optimizers", "[", "0", "]", ",", "scheduler", ")", "self", ".", "_set_deepspeed_activation_checkpointing", "(", ")", "return", "self", ".", "_deepspeed_engine", ",", "[", "optimizer", "]", ",", "scheduler"], "docstring": "Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,\r\n        only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine`, a list with a single\r\n            deepspeed optimizer, and an optional learning rate scheduler.", "docstring_tokens": ["set", "up", "a", "model", "and", "multiple", "optimizers", "together", "along", "with", "an", "optional", "learning", "rate", "scheduler", "currently", "only", "a", "single", "optimizer", "is", "supported", "return", "the", "model", "wrapped", "into", "a", "class", "deepspeed", "deepspeedengine", "a", "list", "with", "a", "single", "deepspeed", "optimizer", "and", "an", "optional", "learning", "rate", "scheduler"], "docstring_summary": "Set up a model and multiple optimizers together, along with an optional learning rate scheduler. Currently,", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 336, "end_line": 354, "hash": "b9524ebfb34e2e0c91f7fc8b46d0273f", "complexity": 2, "parameters": ["module", "optimizers", "scheduler"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "setup_module", "original_string": "def setup_module(self, module: Module) -> \"DeepSpeedEngine\":\r\n        \"\"\"Set up a module for inference (no optimizers).\r\n\r\n        For training, see :meth:`setup_module_and_optimizers`.\r\n\r\n        \"\"\"\r\n        self._deepspeed_engine, _, _ = self._initialize_engine(module)\r\n        return self._deepspeed_engine", "language": "python", "code": "def setup_module(self, module: Module) -> \"DeepSpeedEngine\":\r\n        \"\"\"Set up a module for inference (no optimizers).\r\n\r\n        For training, see :meth:`setup_module_and_optimizers`.\r\n\r\n        \"\"\"\r\n        self._deepspeed_engine, _, _ = self._initialize_engine(module)\r\n        return self._deepspeed_engine", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "\"", "DeepSpeedEngine", "\"", ":", "\"", "\"", "\"", "Set", "up", "a", "module", "for", "inference", "(", "no", "optimizers", ")", ".", "For", "training", ",", "see", ":", "meth", ":", "`", "setup_module_and_optimizers", "`", ".", "\"", "\"", "\"", "self", ".", "_deepspeed_engine", ",", "_", ",", "_", "=", "self", ".", "_initialize_engine", "(", "module", ")", "return", "self", ".", "_deepspeed_engine"], "docstring": "Set up a module for inference (no optimizers).\r\n\r\n        For training, see :meth:`setup_module_and_optimizers`.", "docstring_tokens": ["set", "up", "a", "module", "for", "inference", "no", "optimizers", "for", "training", "see", "meth", "setup_module_and_optimizers"], "docstring_summary": "Set up a module for inference (no optimizers).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 357, "end_line": 364, "hash": "b06b1ed9b692d3b136e5f9cccd06a2a5", "complexity": 1, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "setup_optimizer", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Optimizers can only be set up jointly with the model in this strategy.\r\n\r\n        Please use :meth:`setup_module_and_optimizers` to set up both module and optimizer together.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError(self._err_msg_joint_setup_required())", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Optimizers can only be set up jointly with the model in this strategy.\r\n\r\n        Please use :meth:`setup_module_and_optimizers` to set up both module and optimizer together.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError(self._err_msg_joint_setup_required())", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "\"", "\"", "\"", "Optimizers", "can", "only", "be", "set", "up", "jointly", "with", "the", "model", "in", "this", "strategy", ".", "Please", "use", ":", "meth", ":", "`", "setup_module_and_optimizers", "`", "to", "set", "up", "both", "module", "and", "optimizer", "together", ".", "\"", "\"", "\"", "raise", "NotImplementedError", "(", "self", ".", "_err_msg_joint_setup_required", "(", ")", ")"], "docstring": "Optimizers can only be set up jointly with the model in this strategy.\r\n\r\n        Please use :meth:`setup_module_and_optimizers` to set up both module and optimizer together.", "docstring_tokens": ["optimizers", "can", "only", "be", "set", "up", "jointly", "with", "the", "model", "in", "this", "strategy", "please", "use", "meth", "setup_module_and_optimizers", "to", "set", "up", "both", "module", "and", "optimizer", "together"], "docstring_summary": "Optimizers can only be set up jointly with the model in this strategy.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 367, "end_line": 373, "hash": "909f6c48d24ad0a243ec0cbe864644c4", "complexity": 1, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in a checkpoint directory.\r\n\r\n        Args:\r\n            path: A path to where the files should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Unused by this strategy, since it doesn't use a ``CheckpointIO`` plugin.\r\n            filter: Unsupported.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If the unused ``storage_options`` gets passed.\r\n            ValueError:\r\n                When no :class:`deepspeed.DeepSpeedEngine` objects were found in the state, or when multiple\r\n                :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., filter=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` manages the state serialization internally.\"\r\n            )\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving checkpoints with DeepSpeed is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\r\n        path = self.broadcast(path)\r\n\r\n        # split the checkpoint into two parts:\r\n        # 1) the deepspeed engine encapsulating both the model and optionally the optimizer(s)\r\n        # 2) the rest of the user's state, which in deepspeed is called `client state`\r\n        excluded_objects = (engine, engine.optimizer) if engine.optimizer is not None else (engine,)\r\n        state = {k: v for k, v in state.items() if v not in excluded_objects}\r\n        _validate_state_keys(state)\r\n        # there might be other stateful objects unrelated to the deepspeed engine - convert them to a state_dict\r\n        state = self._convert_stateful_objects_in_state(state, filter={})\r\n        # use deepspeed's internal checkpointing function to handle partitioned weights across processes\r\n        engine.save_checkpoint(\r\n            path, client_state=state, tag=\"checkpoint\", exclude_frozen_parameters=self.exclude_frozen_parameters\r\n        )", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in a checkpoint directory.\r\n\r\n        Args:\r\n            path: A path to where the files should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Unused by this strategy, since it doesn't use a ``CheckpointIO`` plugin.\r\n            filter: Unsupported.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If the unused ``storage_options`` gets passed.\r\n            ValueError:\r\n                When no :class:`deepspeed.DeepSpeedEngine` objects were found in the state, or when multiple\r\n                :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None:\r\n            raise TypeError(\r\n                \"`DeepSpeedStrategy.save_checkpoint(..., filter=...)` is not supported because\"\r\n                \" `DeepSpeedStrategy` manages the state serialization internally.\"\r\n            )\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving checkpoints with DeepSpeed is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\r\n        path = self.broadcast(path)\r\n\r\n        # split the checkpoint into two parts:\r\n        # 1) the deepspeed engine encapsulating both the model and optionally the optimizer(s)\r\n        # 2) the rest of the user's state, which in deepspeed is called `client state`\r\n        excluded_objects = (engine, engine.optimizer) if engine.optimizer is not None else (engine,)\r\n        state = {k: v for k, v in state.items() if v not in excluded_objects}\r\n        _validate_state_keys(state)\r\n        # there might be other stateful objects unrelated to the deepspeed engine - convert them to a state_dict\r\n        state = self._convert_stateful_objects_in_state(state, filter={})\r\n        # use deepspeed's internal checkpointing function to handle partitioned weights across processes\r\n        engine.save_checkpoint(\r\n            path, client_state=state, tag=\"checkpoint\", exclude_frozen_parameters=self.exclude_frozen_parameters\r\n        )", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", ",", "optimizer", ",", "and", "other", "state", "in", "a", "checkpoint", "directory", ".", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "files", "should", "be", "saved", "state", ":", "A", "dictionary", "with", "contents", "to", "be", "saved", ".", "If", "the", "dict", "contains", "modules", "or", "optimizers", ",", "their", "state", "-", "dict", "will", "be", "retrieved", "and", "converted", "automatically", ".", "storage_options", ":", "Unused", "by", "this", "strategy", ",", "since", "it", "doesn", "'", "t", "use", "a", "`", "`", "CheckpointIO", "`", "`", "plugin", ".", "filter", ":", "Unsupported", ".", "Raises", ":", "TypeError", ":", "If", "the", "unused", "`", "`", "storage_options", "`", "`", "gets", "passed", ".", "ValueError", ":", "When", "no", ":", "class", ":", "`", "deepspeed", ".", "DeepSpeedEngine", "`", "objects", "were", "found", "in", "the", "state", ",", "or", "when", "multiple", ":", "class", ":", "`", "deepspeed", ".", "DeepSpeedEngine", "`", "objects", "were", "found", ".", "\"", "\"", "\"", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "\"", "`", "DeepSpeedStrategy", ".", "save_checkpoint", "(", ".", ".", ".", ",", "storage_options", "=", ".", ".", ".", ")", "`", "is", "not", "supported", "because", "\"", "\"", "`", "DeepSpeedStrategy", "`", "does", "not", "use", "the", "`", "CheckpointIO", "`", ".", "\"", ")", "if", "filter", "is", "not", "None", ":", "raise", "TypeError", "(", "\"", "`", "DeepSpeedStrategy", ".", "save_checkpoint", "(", ".", ".", ".", ",", "filter", "=", ".", ".", ".", ")", "`", "is", "not", "supported", "because", "\"", "\"", "`", "DeepSpeedStrategy", "`", "manages", "the", "state", "serialization", "internally", ".", "\"", ")", "engines", "=", "_get_deepspeed_engines_from_state", "(", "state", ")", "if", "len", "(", "engines", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "\"", "Could", "not", "find", "a", "DeepSpeed", "model", "in", "the", "provided", "checkpoint", "state", ".", "Please", "provide", "the", "model", "as", "\"", "\"", "part", "of", "the", "state", "like", "so", ":", "`", "save_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "'", "model", "'", ":", "model", ",", ".", ".", ".", "}", ")", "`", ".", "Make", "sure", "\"", "\"", "you", "set", "up", "the", "model", "(", "and", "optimizers", "if", "any", ")", "through", "the", "strategy", "before", "saving", "the", "checkpoint", ".", "\"", ")", "if", "len", "(", "engines", ")", ">", "1", ":", "raise", "ValueError", "(", "\"", "Found", "multiple", "DeepSpeed", "engine", "modules", "in", "the", "given", "state", ".", "Saving", "checkpoints", "with", "DeepSpeed", "is", "\"", "\"", "currently", "limited", "to", "a", "single", "model", "per", "checkpoint", ".", "To", "save", "multiple", "models", ",", "call", "the", "\"", "\"", "save", "method", "for", "each", "model", "separately", "with", "a", "different", "path", ".", "\"", ")", "engine", "=", "engines", "[", "0", "]", "path", "=", "self", ".", "broadcast", "(", "path", ")", "excluded_objects", "=", "(", "engine", ",", "engine", ".", "optimizer", ")", "if", "engine", ".", "optimizer", "is", "not", "None", "else", "(", "engine", ",", ")", "state", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "state", ".", "items", "(", ")", "if", "v", "not", "in", "excluded_objects", "}", "_validate_state_keys", "(", "state", ")", "state", "=", "self", ".", "_convert_stateful_objects_in_state", "(", "state", ",", "filter", "=", "{", "}", ")", "engine", ".", "save_checkpoint", "(", "path", ",", "client_state", "=", "state", ",", "tag", "=", "\"", "checkpoint", "\"", ",", "exclude_frozen_parameters", "=", "self", ".", "exclude_frozen_parameters", ")"], "docstring": "Save model, optimizer, and other state in a checkpoint directory.\r\n\r\n        Args:\r\n            path: A path to where the files should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Unused by this strategy, since it doesn't use a ``CheckpointIO`` plugin.\r\n            filter: Unsupported.\r\n\r\n        Raises:\r\n            TypeError:\r\n                If the unused ``storage_options`` gets passed.\r\n            ValueError:\r\n                When no :class:`deepspeed.DeepSpeedEngine` objects were found in the state, or when multiple\r\n                :class:`deepspeed.DeepSpeedEngine` objects were found.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "in", "a", "checkpoint", "directory", "args", "path", "a", "path", "to", "where", "the", "files", "should", "be", "saved", "state", "a", "dictionary", "with", "contents", "to", "be", "saved", "if", "the", "dict", "contains", "modules", "or", "optimizers", "their", "state", "dict", "will", "be", "retrieved", "and", "converted", "automatically", "storage_options", "unused", "by", "this", "strategy", "since", "it", "doesn", "t", "use", "a", "checkpointio", "plugin", "filter", "unsupported", "raises", "typeerror", "if", "the", "unused", "storage_options", "gets", "passed", "valueerror", "when", "no", "class", "deepspeed", "deepspeedengine", "objects", "were", "found", "in", "the", "state", "or", "when", "multiple", "class", "deepspeed", "deepspeedengine", "objects", "were", "found"], "docstring_summary": "Save model, optimizer, and other state in a checkpoint directory.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 403, "end_line": 467, "hash": "0c7d2ab63aed5f3be07a247d7cc48a60", "complexity": 8, "parameters": ["path", "state", "Union[Module", "Optimizer", "Any]]", "storage_options", "filter", "Callable[[str", "Any]", "bool]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "load_checkpoint", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                This should contain exactly one model, and the model must already be set up by DeepSpeed.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            Dictionary with the state inside DeepSpeed's engine\r\n\r\n        Raises:\r\n            ValueError:\r\n                If no state is provided, when no :class:`deepspeed.DeepSpeedEngine` objects were found in the\r\n                state, or when multiple :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n            RuntimeError:\r\n                If DeepSpeed was unable to load the checkpoint due to missing files or because the checkpoint is\r\n                not in the expected DeepSpeed format.\r\n\r\n        \"\"\"\r\n        if isinstance(state, (Module, Optimizer)) or self.load_full_weights and self.zero_stage_3:\r\n            # This code path to enables loading a checkpoint from a non-deepspeed checkpoint or from\r\n            # a consolidated checkpoint\r\n            path = self.broadcast(path)\r\n            return super().load_checkpoint(path=path, state=state, strict=strict)\r\n\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got DeepSpeedStrategy.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                f\" a model instance to reload is required. Pass it in like so:\"\r\n                \" DeepSpeedStrategy.load_checkpoint(..., state={'model': model, ...})\"\r\n            )\r\n        _validate_checkpoint_directory(path)\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving and loading checkpoints\"\r\n                \" with DeepSpeed is currently limited to a single model per checkpoint. To load multiple model\"\r\n                \" states, call the load method for each model checkpoint separately.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        from deepspeed.runtime.base_optimizer import DeepSpeedOptimizer\r\n\r\n        optimzer_state_requested = any(isinstance(item, (Optimizer, DeepSpeedOptimizer)) for item in state.values())\r\n\r\n        torch.cuda.empty_cache()\r\n        _, client_state = engine.load_checkpoint(\r\n            path,\r\n            tag=\"checkpoint\",\r\n            load_optimizer_states=optimzer_state_requested,\r\n            load_lr_scheduler_states=False,\r\n            load_module_strict=strict,\r\n        )\r\n\r\n        if client_state is None:\r\n            raise RuntimeError(\r\n                \"DeepSpeed was unable to load the checkpoint. Ensure you passed in a DeepSpeed compatible checkpoint\"\r\n                \" or a single checkpoint file by setting `DeepSpeedStrategy(..., load_full_weights=True)`.\"\r\n            )\r\n\r\n        # `Engine.load_checkpoint` adds useless keys 'optimizer' and 'lr_scheduler' to the client state; remove\r\n        # them to avoid name collision with user state\r\n        keys = set(client_state) & set(state) - {\"optimizer\", \"lr_scheduler\"}\r\n        _move_state_into(source=client_state, destination=state, keys=keys)\r\n        return client_state", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                This should contain exactly one model, and the model must already be set up by DeepSpeed.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            Dictionary with the state inside DeepSpeed's engine\r\n\r\n        Raises:\r\n            ValueError:\r\n                If no state is provided, when no :class:`deepspeed.DeepSpeedEngine` objects were found in the\r\n                state, or when multiple :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n            RuntimeError:\r\n                If DeepSpeed was unable to load the checkpoint due to missing files or because the checkpoint is\r\n                not in the expected DeepSpeed format.\r\n\r\n        \"\"\"\r\n        if isinstance(state, (Module, Optimizer)) or self.load_full_weights and self.zero_stage_3:\r\n            # This code path to enables loading a checkpoint from a non-deepspeed checkpoint or from\r\n            # a consolidated checkpoint\r\n            path = self.broadcast(path)\r\n            return super().load_checkpoint(path=path, state=state, strict=strict)\r\n\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got DeepSpeedStrategy.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                f\" a model instance to reload is required. Pass it in like so:\"\r\n                \" DeepSpeedStrategy.load_checkpoint(..., state={'model': model, ...})\"\r\n            )\r\n        _validate_checkpoint_directory(path)\r\n\r\n        engines = _get_deepspeed_engines_from_state(state)\r\n        if len(engines) == 0:\r\n            raise ValueError(\r\n                \"Could not find a DeepSpeed model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `load_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before loading the checkpoint.\"\r\n            )\r\n        if len(engines) > 1:\r\n            raise ValueError(\r\n                \"Found multiple DeepSpeed engine modules in the given state. Saving and loading checkpoints\"\r\n                \" with DeepSpeed is currently limited to a single model per checkpoint. To load multiple model\"\r\n                \" states, call the load method for each model checkpoint separately.\"\r\n            )\r\n        engine = engines[0]\r\n\r\n        from deepspeed.runtime.base_optimizer import DeepSpeedOptimizer\r\n\r\n        optimzer_state_requested = any(isinstance(item, (Optimizer, DeepSpeedOptimizer)) for item in state.values())\r\n\r\n        torch.cuda.empty_cache()\r\n        _, client_state = engine.load_checkpoint(\r\n            path,\r\n            tag=\"checkpoint\",\r\n            load_optimizer_states=optimzer_state_requested,\r\n            load_lr_scheduler_states=False,\r\n            load_module_strict=strict,\r\n        )\r\n\r\n        if client_state is None:\r\n            raise RuntimeError(\r\n                \"DeepSpeed was unable to load the checkpoint. Ensure you passed in a DeepSpeed compatible checkpoint\"\r\n                \" or a single checkpoint file by setting `DeepSpeedStrategy(..., load_full_weights=True)`.\"\r\n            )\r\n\r\n        # `Engine.load_checkpoint` adds useless keys 'optimizer' and 'lr_scheduler' to the client state; remove\r\n        # them to avoid name collision with user state\r\n        keys = set(client_state) & set(state) - {\"optimizer\", \"lr_scheduler\"}\r\n        _move_state_into(source=client_state, destination=state, keys=keys)\r\n        return client_state", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects", ".", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "is", "located", "state", ":", "A", "dictionary", "of", "objects", "whose", "state", "will", "be", "restored", "in", "-", "place", "from", "the", "checkpoint", "path", ".", "This", "should", "contain", "exactly", "one", "model", ",", "and", "the", "model", "must", "already", "be", "set", "up", "by", "DeepSpeed", ".", "strict", ":", "Whether", "to", "enforce", "that", "the", "keys", "in", "`", "state", "`", "match", "the", "keys", "in", "the", "checkpoint", ".", "Returns", ":", "Dictionary", "with", "the", "state", "inside", "DeepSpeed", "'", "s", "engine", "Raises", ":", "ValueError", ":", "If", "no", "state", "is", "provided", ",", "when", "no", ":", "class", ":", "`", "deepspeed", ".", "DeepSpeedEngine", "`", "objects", "were", "found", "in", "the", "state", ",", "or", "when", "multiple", ":", "class", ":", "`", "deepspeed", ".", "DeepSpeedEngine", "`", "objects", "were", "found", ".", "RuntimeError", ":", "If", "DeepSpeed", "was", "unable", "to", "load", "the", "checkpoint", "due", "to", "missing", "files", "or", "because", "the", "checkpoint", "is", "not", "in", "the", "expected", "DeepSpeed", "format", ".", "\"", "\"", "\"", "if", "isinstance", "(", "state", ",", "(", "Module", ",", "Optimizer", ")", ")", "or", "self", ".", "load_full_weights", "and", "self", ".", "zero_stage_3", ":", "path", "=", "self", ".", "broadcast", "(", "path", ")", "return", "super", "(", ")", ".", "load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "state", ",", "strict", "=", "strict", ")", "if", "not", "state", ":", "raise", "ValueError", "(", "f", "\"", "Got", "DeepSpeedStrategy", ".", "load_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "state", "!", "r", "}", ")", "but", "a", "state", "with", "at", "least", "\"", "f", "\"", "a", "model", "instance", "to", "reload", "is", "required", ".", "Pass", "it", "in", "like", "so", ":", "\"", "\"", "DeepSpeedStrategy", ".", "load_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "'", "model", "'", ":", "model", ",", ".", ".", ".", "}", ")", "\"", ")", "_validate_checkpoint_directory", "(", "path", ")", "engines", "=", "_get_deepspeed_engines_from_state", "(", "state", ")", "if", "len", "(", "engines", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "\"", "Could", "not", "find", "a", "DeepSpeed", "model", "in", "the", "provided", "checkpoint", "state", ".", "Please", "provide", "the", "model", "as", "\"", "\"", "part", "of", "the", "state", "like", "so", ":", "`", "load_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "'", "model", "'", ":", "model", ",", ".", ".", ".", "}", ")", "`", ".", "Make", "sure", "\"", "\"", "you", "set", "up", "the", "model", "(", "and", "optimizers", "if", "any", ")", "through", "the", "strategy", "before", "loading", "the", "checkpoint", ".", "\"", ")", "if", "len", "(", "engines", ")", ">", "1", ":", "raise", "ValueError", "(", "\"", "Found", "multiple", "DeepSpeed", "engine", "modules", "in", "the", "given", "state", ".", "Saving", "and", "loading", "checkpoints", "\"", "\"", "with", "DeepSpeed", "is", "currently", "limited", "to", "a", "single", "model", "per", "checkpoint", ".", "To", "load", "multiple", "model", "\"", "\"", "states", ",", "call", "the", "load", "method", "for", "each", "model", "checkpoint", "separately", ".", "\"", ")", "engine", "=", "engines", "[", "0", "]", "from", "deepspeed", ".", "runtime", ".", "base_optimizer", "import", "DeepSpeedOptimizer", "optimzer_state_requested", "=", "any", "(", "isinstance", "(", "item", ",", "(", "Optimizer", ",", "DeepSpeedOptimizer", ")", ")", "for", "item", "in", "state", ".", "values", "(", ")", ")", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "_", ",", "client_state", "=", "engine", ".", "load_checkpoint", "(", "path", ",", "tag", "=", "\"", "checkpoint", "\"", ",", "load_optimizer_states", "=", "optimzer_state_requested", ",", "load_lr_scheduler_states", "=", "False", ",", "load_module_strict", "=", "strict", ",", ")", "if", "client_state", "is", "None", ":", "raise", "RuntimeError", "(", "\"", "DeepSpeed", "was", "unable", "to", "load", "the", "checkpoint", ".", "Ensure", "you", "passed", "in", "a", "DeepSpeed", "compatible", "checkpoint", "\"", "\"", "or", "a", "single", "checkpoint", "file", "by", "setting", "`", "DeepSpeedStrategy", "(", ".", ".", ".", ",", "load_full_weights", "=", "True", ")", "`", ".", "\"", ")", "keys", "=", "set", "(", "client_state", ")", "&", "set", "(", "state", ")", "-", "{", "\"", "optimizer", "\"", ",", "\"", "lr_scheduler", "\"", "}", "_move_state_into", "(", "source", "=", "client_state", ",", "destination", "=", "state", ",", "keys", "=", "keys", ")", "return", "client_state"], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                This should contain exactly one model, and the model must already be set up by DeepSpeed.\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            Dictionary with the state inside DeepSpeed's engine\r\n\r\n        Raises:\r\n            ValueError:\r\n                If no state is provided, when no :class:`deepspeed.DeepSpeedEngine` objects were found in the\r\n                state, or when multiple :class:`deepspeed.DeepSpeedEngine` objects were found.\r\n            RuntimeError:\r\n                If DeepSpeed was unable to load the checkpoint due to missing files or because the checkpoint is\r\n                not in the expected DeepSpeed format.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects", "args", "path", "a", "path", "to", "where", "the", "file", "is", "located", "state", "a", "dictionary", "of", "objects", "whose", "state", "will", "be", "restored", "in", "place", "from", "the", "checkpoint", "path", "this", "should", "contain", "exactly", "one", "model", "and", "the", "model", "must", "already", "be", "set", "up", "by", "deepspeed", "strict", "whether", "to", "enforce", "that", "the", "keys", "in", "state", "match", "the", "keys", "in", "the", "checkpoint", "returns", "dictionary", "with", "the", "state", "inside", "deepspeed", "s", "engine", "raises", "valueerror", "if", "no", "state", "is", "provided", "when", "no", "class", "deepspeed", "deepspeedengine", "objects", "were", "found", "in", "the", "state", "or", "when", "multiple", "class", "deepspeed", "deepspeedengine", "objects", "were", "found", "runtimeerror", "if", "deepspeed", "was", "unable", "to", "load", "the", "checkpoint", "due", "to", "missing", "files", "or", "because", "the", "checkpoint", "is", "not", "in", "the", "expected", "deepspeed", "format"], "docstring_summary": "Load the contents from a checkpoint and restore the state of the given objects.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 470, "end_line": 548, "hash": "bb5527becd333470272cd78182bac432", "complexity": 9, "parameters": ["path", "state", "Optimizer", "dict[str", "Union[Module", "Optimizer", "Any]]]]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "_initialize_engine", "original_string": "def _initialize_engine(\r\n        self, model: Module, optimizer: Optional[Optimizer] = None, scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", Optimizer, Any]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, deepspeed_scheduler = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer, deepspeed_scheduler", "language": "python", "code": "def _initialize_engine(\r\n        self, model: Module, optimizer: Optional[Optimizer] = None, scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[\"DeepSpeedEngine\", Optimizer, Any]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, deepspeed_scheduler = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer, deepspeed_scheduler", "code_tokens": ["def", "_initialize_engine", "(", "self", ",", "model", ":", "Module", ",", "optimizer", ":", "Optional", "[", "Optimizer", "]", "=", "None", ",", "scheduler", ":", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "=", "None", ")", "-", ">", "tuple", "[", "\"", "DeepSpeedEngine", "\"", ",", "Optimizer", ",", "Any", "]", ":", "\"", "\"", "\"", "Initialize", "one", "model", "and", "one", "optimizer", "with", "an", "optional", "learning", "rate", "scheduler", ".", "This", "calls", "`", "`", "deepspeed", ".", "initialize", "`", "`", "internally", ".", "\"", "\"", "\"", "import", "deepspeed", "model_parameters", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", "deepspeed_engine", ",", "deepspeed_optimizer", ",", "_", ",", "deepspeed_scheduler", "=", "deepspeed", ".", "initialize", "(", "args", "=", "argparse", ".", "Namespace", "(", "device_rank", "=", "self", ".", "root_device", ".", "index", ")", ",", "config", "=", "self", ".", "config", ",", "model", "=", "model", ",", "model_parameters", "=", "model_parameters", ",", "optimizer", "=", "optimizer", ",", "lr_scheduler", "=", "scheduler", ",", "dist_init_required", "=", "False", ",", ")", "return", "deepspeed_engine", ",", "deepspeed_optimizer", ",", "deepspeed_scheduler"], "docstring": "Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.", "docstring_tokens": ["initialize", "one", "model", "and", "one", "optimizer", "with", "an", "optional", "learning", "rate", "scheduler", "this", "calls", "deepspeed", "initialize", "internally"], "docstring_summary": "Initialize one model and one optimizer with an optional learning rate scheduler.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 614, "end_line": 634, "hash": "0b1d1d4ccf559c8b8d69a181f197122c", "complexity": 1, "parameters": ["model", "optimizer", "scheduler"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "_restore_zero_state", "original_string": "def _restore_zero_state(self, module: Module, ckpt: Mapping[str, Any]) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            # copy state_dict so _load_from_state_dict can modify it\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            # because zero3 puts placeholders in model params, this context\r\n            # manager gathers (unpartitions) the params of the current layer, then loads from\r\n            # the state dict and then re-partitions them again\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=True,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(module, prefix=\"\")", "language": "python", "code": "def _restore_zero_state(self, module: Module, ckpt: Mapping[str, Any]) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            # copy state_dict so _load_from_state_dict can modify it\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            # because zero3 puts placeholders in model params, this context\r\n            # manager gathers (unpartitions) the params of the current layer, then loads from\r\n            # the state dict and then re-partitions them again\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=True,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(module, prefix=\"\")", "code_tokens": ["def", "_restore_zero_state", "(", "self", ",", "module", ":", "Module", ",", "ckpt", ":", "Mapping", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Overrides", "the", "normal", "load_state_dict", "behaviour", "in", "PyTorch", "to", "ensure", "we", "gather", "parameters", "that", "may", "be", "sharded", "across", "processes", "before", "loading", "the", "state", "dictionary", "when", "using", "ZeRO", "stage", "3", ".", "This", "is", "then", "automatically", "synced", "across", "processes", ".", "Args", ":", "ckpt", ":", "The", "ckpt", "file", ".", "\"", "\"", "\"", "import", "deepspeed", "def", "load", "(", "module", ":", "torch", ".", "nn", ".", "Module", ",", "prefix", ":", "str", "=", "\"", "\"", ")", "-", ">", "None", ":", "missing_keys", ":", "list", "[", "str", "]", "=", "[", "]", "unexpected_keys", ":", "list", "[", "str", "]", "=", "[", "]", "error_msgs", ":", "list", "[", "str", "]", "=", "[", "]", "state_dict", "=", "ckpt", "[", "\"", "state_dict", "\"", "]", "metadata", "=", "getattr", "(", "state_dict", ",", "\"", "_metadata", "\"", ",", "None", ")", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "if", "metadata", "is", "not", "None", ":", "state_dict", ".", "_metadata", "=", "metadata", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "with", "deepspeed", ".", "zero", ".", "GatheredParameters", "(", "list", "(", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ")", ",", "modifier_rank", "=", "0", ")", ":", "if", "self", ".", "is_global_zero", ":", "module", ".", "_load_from_state_dict", "(", "state_dict", "=", "state_dict", ",", "prefix", "=", "prefix", ",", "local_metadata", "=", "local_metadata", ",", "strict", "=", "True", ",", "missing_keys", "=", "missing_keys", ",", "unexpected_keys", "=", "unexpected_keys", ",", "error_msgs", "=", "error_msgs", ",", ")", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "if", "child", "is", "not", "None", ":", "load", "(", "child", ",", "prefix", "+", "name", "+", "\"", ".", "\"", ")", "load", "(", "module", ",", "prefix", "=", "\"", "\"", ")"], "docstring": "Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.", "docstring_tokens": ["overrides", "the", "normal", "load_state_dict", "behaviour", "in", "pytorch", "to", "ensure", "we", "gather", "parameters", "that", "may", "be", "sharded", "across", "processes", "before", "loading", "the", "state", "dictionary", "when", "using", "zero", "stage", "3", "this", "is", "then", "automatically", "synced", "across", "processes", "args", "ckpt", "the", "ckpt", "file"], "docstring_summary": "Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 781, "end_line": 824, "hash": "b6ceccfa3ecaecbaf9a41b56afc41e61", "complexity": 7, "parameters": ["module", "ckpt", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\deepspeed.py", "func_name": "_validate_checkpoint_directory", "original_string": "def _validate_checkpoint_directory(path: _PATH) -> None:\r\n    \"\"\"Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.\"\"\"\r\n    # Example DeepSpeed checkpoint directory:\r\n    #\r\n    # epoch=5-step=10999.ckpt\r\n    # \u251c\u2500\u2500 checkpoint\r\n    # \u2502   \u251c\u2500\u2500 zero_pp_rank_0_mp_rank_00_model_states.pt\r\n    # \u2502   \u251c\u2500\u2500 zero_pp_rank_0_mp_rank_00_optim_states.pt\r\n    # \u2502   \u251c\u2500\u2500 zero_pp_rank_1_mp_rank_00_model_states.pt\r\n    # \u2502   \u2514\u2500\u2500 zero_pp_rank_1_mp_rank_00_optim_states.pt\r\n    # \u251c\u2500\u2500 latest\r\n    # \u2514\u2500\u2500 zero_to_fp32.py\r\n\r\n    path = Path(path)\r\n    path_is_ds_checkpoint = _is_deepspeed_checkpoint(path)\r\n    default_message = f\"The provided path is not a valid DeepSpeed checkpoint: {path}\"\r\n\r\n    if not path_is_ds_checkpoint:\r\n        # Case 1: User may have accidentally passed the subfolder \"checkpoint\"\r\n        parent_is_ds_checkpoint = _is_deepspeed_checkpoint(path.parent)\r\n        if parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a subfolder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent}\"\r\n            )\r\n        # Case 2: User may have accidentally passed the path to a file inside the \"checkpoint\" subfolder\r\n        parent_parent_is_ds_checkpoint = path.is_file() and _is_deepspeed_checkpoint(path.parent.parent)\r\n        if parent_parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a file inside a DeepSpeed checkpoint folder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent.parent}\"\r\n            )\r\n        raise FileNotFoundError(default_message)", "language": "python", "code": "def _validate_checkpoint_directory(path: _PATH) -> None:\r\n    \"\"\"Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.\"\"\"\r\n    # Example DeepSpeed checkpoint directory:\r\n    #\r\n    # epoch=5-step=10999.ckpt\r\n    # \u251c\u2500\u2500 checkpoint\r\n    # \u2502   \u251c\u2500\u2500 zero_pp_rank_0_mp_rank_00_model_states.pt\r\n    # \u2502   \u251c\u2500\u2500 zero_pp_rank_0_mp_rank_00_optim_states.pt\r\n    # \u2502   \u251c\u2500\u2500 zero_pp_rank_1_mp_rank_00_model_states.pt\r\n    # \u2502   \u2514\u2500\u2500 zero_pp_rank_1_mp_rank_00_optim_states.pt\r\n    # \u251c\u2500\u2500 latest\r\n    # \u2514\u2500\u2500 zero_to_fp32.py\r\n\r\n    path = Path(path)\r\n    path_is_ds_checkpoint = _is_deepspeed_checkpoint(path)\r\n    default_message = f\"The provided path is not a valid DeepSpeed checkpoint: {path}\"\r\n\r\n    if not path_is_ds_checkpoint:\r\n        # Case 1: User may have accidentally passed the subfolder \"checkpoint\"\r\n        parent_is_ds_checkpoint = _is_deepspeed_checkpoint(path.parent)\r\n        if parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a subfolder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent}\"\r\n            )\r\n        # Case 2: User may have accidentally passed the path to a file inside the \"checkpoint\" subfolder\r\n        parent_parent_is_ds_checkpoint = path.is_file() and _is_deepspeed_checkpoint(path.parent.parent)\r\n        if parent_parent_is_ds_checkpoint:\r\n            raise FileNotFoundError(\r\n                f\"{default_message}. It looks like you passed the path to a file inside a DeepSpeed checkpoint folder.\"\r\n                f\" Try to load using this parent directory instead: {path.parent.parent}\"\r\n            )\r\n        raise FileNotFoundError(default_message)", "code_tokens": ["def", "_validate_checkpoint_directory", "(", "path", ":", "_PATH", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Validates", "that", "the", "path", "points", "to", "a", "DeepSpeed", "checkpoint", "directory", "and", "suggests", "fixes", "for", "user", "error", ".", "\"", "\"", "\"", "path", "=", "Path", "(", "path", ")", "path_is_ds_checkpoint", "=", "_is_deepspeed_checkpoint", "(", "path", ")", "default_message", "=", "f", "\"", "The", "provided", "path", "is", "not", "a", "valid", "DeepSpeed", "checkpoint", ":", "{", "path", "}", "\"", "if", "not", "path_is_ds_checkpoint", ":", "parent_is_ds_checkpoint", "=", "_is_deepspeed_checkpoint", "(", "path", ".", "parent", ")", "if", "parent_is_ds_checkpoint", ":", "raise", "FileNotFoundError", "(", "f", "\"", "{", "default_message", "}", ".", "It", "looks", "like", "you", "passed", "the", "path", "to", "a", "subfolder", ".", "\"", "f", "\"", "Try", "to", "load", "using", "this", "parent", "directory", "instead", ":", "{", "path", ".", "parent", "}", "\"", ")", "parent_parent_is_ds_checkpoint", "=", "path", ".", "is_file", "(", ")", "and", "_is_deepspeed_checkpoint", "(", "path", ".", "parent", ".", "parent", ")", "if", "parent_parent_is_ds_checkpoint", ":", "raise", "FileNotFoundError", "(", "f", "\"", "{", "default_message", "}", ".", "It", "looks", "like", "you", "passed", "the", "path", "to", "a", "file", "inside", "a", "DeepSpeed", "checkpoint", "folder", ".", "\"", "f", "\"", "Try", "to", "load", "using", "this", "parent", "directory", "instead", ":", "{", "path", ".", "parent", ".", "parent", "}", "\"", ")", "raise", "FileNotFoundError", "(", "default_message", ")"], "docstring": "Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.", "docstring_tokens": ["validates", "that", "the", "path", "points", "to", "a", "deepspeed", "checkpoint", "directory", "and", "suggests", "fixes", "for", "user", "error"], "docstring_summary": "Validates that the path points to a DeepSpeed checkpoint directory and suggests fixes for user error.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\deepspeed.py", "partition": "train", "function_type": "function", "start_line": 891, "end_line": 923, "hash": "83fad275d7799e1178e12c17fb32616d", "complexity": 5, "parameters": ["path"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "setup_module_and_optimizers", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module and sets `use_orig_params=True` to keep the reference to the original parameters in the optimizer.\"\"\"\r\n        use_orig_params = self._fsdp_kwargs.get(\"use_orig_params\")\r\n        if use_orig_params is False:\r\n            raise ValueError(\r\n                f\"You set `{type(self).__name__}(use_orig_params=False)` but this is not supported when\"\r\n                \" setting the model and optimizer up jointly. Either set it to `True` or set the objects\"\r\n                \" up in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n                \" call `setup_optimizer`.\"\r\n            )\r\n        module = self.setup_module(module)\r\n        return module, optimizers, scheduler", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module and sets `use_orig_params=True` to keep the reference to the original parameters in the optimizer.\"\"\"\r\n        use_orig_params = self._fsdp_kwargs.get(\"use_orig_params\")\r\n        if use_orig_params is False:\r\n            raise ValueError(\r\n                f\"You set `{type(self).__name__}(use_orig_params=False)` but this is not supported when\"\r\n                \" setting the model and optimizer up jointly. Either set it to `True` or set the objects\"\r\n                \" up in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n                \" call `setup_optimizer`.\"\r\n            )\r\n        module = self.setup_module(module)\r\n        return module, optimizers, scheduler", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "=", "None", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "]", ":", "\"", "\"", "\"", "Wraps", "the", "model", "into", "a", ":", "class", ":", "`", "~", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", ".", "FullyShardedDataParallel", "`", "module", "and", "sets", "`", "use_orig_params", "=", "True", "`", "to", "keep", "the", "reference", "to", "the", "original", "parameters", "in", "the", "optimizer", ".", "\"", "\"", "\"", "use_orig_params", "=", "self", ".", "_fsdp_kwargs", ".", "get", "(", "\"", "use_orig_params", "\"", ")", "if", "use_orig_params", "is", "False", ":", "raise", "ValueError", "(", "f", "\"", "You", "set", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", "(", "use_orig_params", "=", "False", ")", "`", "but", "this", "is", "not", "supported", "when", "\"", "\"", "setting", "the", "model", "and", "optimizer", "up", "jointly", ".", "Either", "set", "it", "to", "`", "True", "`", "or", "set", "the", "objects", "\"", "\"", "up", "in", "this", "order", ":", "Create", "the", "model", ",", "call", "`", "setup_module", "`", ",", "create", "the", "optimizer", ",", "\"", "\"", "call", "`", "setup_optimizer", "`", ".", "\"", ")", "module", "=", "self", ".", "setup_module", "(", "module", ")", "return", "module", ",", "optimizers", ",", "scheduler"], "docstring": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module and sets `use_orig_params=True` to keep the reference to the original parameters in the optimizer.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "distributed", "fsdp", "fully_sharded_data_parallel", "fullyshardeddataparallel", "module", "and", "sets", "use_orig_params", "true", "to", "keep", "the", "reference", "to", "the", "original", "parameters", "in", "the", "optimizer"], "docstring_summary": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "FSDPStrategy", "start_line": 263, "end_line": 277, "hash": "3dcbae3e1ff83c453a96b92af173cbae", "complexity": 2, "parameters": ["module", "optimizers", "scheduler"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "setup_module", "original_string": "def setup_module(self, module: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in module.modules()):\r\n            # The user has wrapped their submodules manually, don't apply the auto wrap policy.\r\n            if _has_meta_device_parameters_or_buffers(module):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self._fsdp_kwargs:\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self._fsdp_kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            module = FullyShardedDataParallel(\r\n                module=module,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self._fsdp_kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(module, self.root_device)\r\n\r\n        # activation checkpointing needs to be set up after wrapping the model\r\n        _setup_activation_checkpointing(module, self._activation_checkpointing_kwargs)\r\n\r\n        return module", "language": "python", "code": "def setup_module(self, module: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in module.modules()):\r\n            # The user has wrapped their submodules manually, don't apply the auto wrap policy.\r\n            if _has_meta_device_parameters_or_buffers(module):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self._fsdp_kwargs:\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self._fsdp_kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            module = FullyShardedDataParallel(\r\n                module=module,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self._fsdp_kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(module, self.root_device)\r\n\r\n        # activation checkpointing needs to be set up after wrapping the model\r\n        _setup_activation_checkpointing(module, self._activation_checkpointing_kwargs)\r\n\r\n        return module", "code_tokens": ["def", "setup_module", "(", "self", ",", "module", ":", "Module", ")", "-", ">", "Module", ":", "\"", "\"", "\"", "Wraps", "the", "model", "into", "a", ":", "class", ":", "`", "~", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", ".", "FullyShardedDataParallel", "`", "module", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "if", "any", "(", "isinstance", "(", "mod", ",", "FullyShardedDataParallel", ")", "for", "mod", "in", "module", ".", "modules", "(", ")", ")", ":", "if", "_has_meta_device_parameters_or_buffers", "(", "module", ")", ":", "rank_zero_warn", "(", "\"", "The", "model", "is", "already", "wrapped", "in", "`", "FSDP", "`", "but", "there", "are", "still", "parameters", "on", "the", "meta", "device", ".", "\"", ")", "if", "\"", "auto_wrap_policy", "\"", "in", "self", ".", "_fsdp_kwargs", ":", "rank_zero_warn", "(", "\"", "A", "FSDP", "`", "auto_wrap_policy", "`", "is", "set", ",", "but", "the", "model", "is", "already", "wrapped", ".", "The", "policy", "will", "be", "ignored", ".", "\"", ")", "del", "self", ".", "_fsdp_kwargs", "[", "\"", "auto_wrap_policy", "\"", "]", "else", ":", "module", "=", "FullyShardedDataParallel", "(", "module", "=", "module", ",", "cpu_offload", "=", "self", ".", "cpu_offload", ",", "mixed_precision", "=", "self", ".", "mixed_precision_config", ",", "sharding_strategy", "=", "self", ".", "sharding_strategy", ",", "device_id", "=", "self", ".", "root_device", ".", "index", ",", "*", "*", "self", ".", "_fsdp_kwargs", ",", ")", "_move_torchmetrics_to_device", "(", "module", ",", "self", ".", "root_device", ")", "_setup_activation_checkpointing", "(", "module", ",", "self", ".", "_activation_checkpointing_kwargs", ")", "return", "module"], "docstring": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "distributed", "fsdp", "fully_sharded_data_parallel", "fullyshardeddataparallel", "module"], "docstring_summary": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "FSDPStrategy", "start_line": 280, "end_line": 311, "hash": "5e3569f3dae18195eefba219d05afc03", "complexity": 5, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "setup_optimizer", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with FSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if self._fsdp_kwargs.get(\"use_orig_params\"):\r\n            return super().setup_optimizer(optimizer)\r\n        if not _optimizer_has_flat_params(optimizer):\r\n            # We avoid this limitation by setting `use_orig_params=True`\r\n            raise ValueError(\r\n                \"The optimizer does not seem to reference any FSDP parameters. HINT: Make sure to create the optimizer\"\r\n                \" after setting up the model.\"\r\n            )\r\n        return optimizer", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with FSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if self._fsdp_kwargs.get(\"use_orig_params\"):\r\n            return super().setup_optimizer(optimizer)\r\n        if not _optimizer_has_flat_params(optimizer):\r\n            # We avoid this limitation by setting `use_orig_params=True`\r\n            raise ValueError(\r\n                \"The optimizer does not seem to reference any FSDP parameters. HINT: Make sure to create the optimizer\"\r\n                \" after setting up the model.\"\r\n            )\r\n        return optimizer", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "\"", "\"", "\"", "Set", "up", "an", "optimizer", "for", "a", "model", "wrapped", "with", "FSDP", ".", "This", "setup", "method", "doesn", "'", "t", "modify", "the", "optimizer", "or", "wrap", "the", "optimizer", ".", "The", "only", "thing", "it", "currently", "does", "is", "verify", "that", "the", "optimizer", "was", "created", "after", "the", "model", "was", "wrapped", "with", ":", "meth", ":", "`", "setup_module", "`", "with", "a", "reference", "to", "the", "flattened", "parameters", ".", "\"", "\"", "\"", "if", "self", ".", "_fsdp_kwargs", ".", "get", "(", "\"", "use_orig_params", "\"", ")", ":", "return", "super", "(", ")", ".", "setup_optimizer", "(", "optimizer", ")", "if", "not", "_optimizer_has_flat_params", "(", "optimizer", ")", ":", "raise", "ValueError", "(", "\"", "The", "optimizer", "does", "not", "seem", "to", "reference", "any", "FSDP", "parameters", ".", "HINT", ":", "Make", "sure", "to", "create", "the", "optimizer", "\"", "\"", "after", "setting", "up", "the", "model", ".", "\"", ")", "return", "optimizer"], "docstring": "Set up an optimizer for a model wrapped with FSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.", "docstring_tokens": ["set", "up", "an", "optimizer", "for", "a", "model", "wrapped", "with", "fsdp", "this", "setup", "method", "doesn", "t", "modify", "the", "optimizer", "or", "wrap", "the", "optimizer", "the", "only", "thing", "it", "currently", "does", "is", "verify", "that", "the", "optimizer", "was", "created", "after", "the", "model", "was", "wrapped", "with", "meth", "setup_module", "with", "a", "reference", "to", "the", "flattened", "parameters"], "docstring_summary": "Set up an optimizer for a model wrapped with FSDP.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "FSDPStrategy", "start_line": 314, "end_line": 330, "hash": "3819d9698826db506d4d72f1a319617e", "complexity": 3, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "clip_gradients_norm", "original_string": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            # the root must be wrapped\r\n            raise TypeError(\r\n                \"Gradient clipping with FSDP is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.clip_gradients_norm` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        self.precision.unscale_gradients(optimizer)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "language": "python", "code": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            # the root must be wrapped\r\n            raise TypeError(\r\n                \"Gradient clipping with FSDP is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.clip_gradients_norm` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        self.precision.unscale_gradients(optimizer)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "code_tokens": ["def", "clip_gradients_norm", "(", "self", ",", "module", ":", "Module", ",", "optimizer", ":", "Optimizer", ",", "max_norm", ":", "Union", "[", "float", ",", "int", "]", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "norm", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", "import", "FullyShardedDataParallel", "if", "not", "isinstance", "(", "module", ",", "FullyShardedDataParallel", ")", ":", "raise", "TypeError", "(", "\"", "Gradient", "clipping", "with", "FSDP", "is", "only", "possible", "if", "the", "module", "passed", "to", "\"", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "clip_gradients_norm", "`", "is", "wrapped", "in", "`", "FullyShardedDataParallel", "`", ".", "\"", "f", "\"", "Got", ":", "{", "module", ".", "__class__", ".", "__name__", "}", ".", "\"", ")", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "return", "module", ".", "clip_grad_norm_", "(", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "docstring_summary": "Clip gradients by norm.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "FSDPStrategy", "start_line": 391, "end_line": 410, "hash": "850a7f33c60725bdb75bb8fa40aedd32", "complexity": 2, "parameters": ["module", "optimizer", "max_norm", "int]", "norm_type", "int]", "error_if_nonfinite"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\fsdp.py", "func_name": "no_backward_sync", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            # the root must be wrapped\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.no_backward_sync` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel\r\n\r\n        if not isinstance(module, FullyShardedDataParallel):\r\n            # the root must be wrapped\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{type(self).__name__}.no_backward_sync` is wrapped in `FullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Blocks", "gradient", "synchronization", "inside", "the", ":", "class", ":", "`", "~", "torch", ".", "distributed", ".", "fsdp", ".", "FullyShardedDataParallel", "`", "wrapper", ".", "\"", "\"", "\"", "if", "not", "enabled", ":", "return", "nullcontext", "(", ")", "from", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", "import", "FullyShardedDataParallel", "if", "not", "isinstance", "(", "module", ",", "FullyShardedDataParallel", ")", ":", "raise", "TypeError", "(", "\"", "Blocking", "backward", "sync", "is", "only", "possible", "if", "the", "module", "passed", "to", "\"", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "no_backward_sync", "`", "is", "wrapped", "in", "`", "FullyShardedDataParallel", "`", ".", "\"", "f", "\"", "Got", ":", "{", "module", ".", "__class__", ".", "__name__", "}", ".", "\"", ")", "return", "module", ".", "no_sync", "(", ")"], "docstring": "Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`\r\n        wrapper.", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "class", "torch", "distributed", "fsdp", "fullyshardeddataparallel", "wrapper"], "docstring_summary": "Blocks gradient synchronization inside the :class:`~torch.distributed.fsdp.FullyShardedDataParallel`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "_FSDPBackwardSyncControl", "start_line": 745, "end_line": 759, "hash": "1fbec043b7eabe8ec6e24b62fd879717", "complexity": 3, "parameters": ["module", "enabled"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If distributed checkpointing is enabled (default), the checkpoint gets saved as a directory containing one file\r\n        per process, with model- and optimizer shards stored per file. Additionally, it creates a metadata file\r\n        `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n        If distributed checkpointing is disabled (``save_distributed_checkpoint=False``), the checkpoint will be\r\n        written to a single file containing the weights, optimizer state and other metadata.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                f\"`{type(self).__name__}.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                f\" `{type(self).__name__}` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None and self._save_distributed_checkpoint:\r\n            # https://github.com/pytorch/pytorch/issues/105379\r\n            raise NotImplementedError(\r\n                f\"{type(self).__name__} doesn't support loading distributed filtered checkpoints,\"\r\n                \" so saving them is disabled.\"\r\n            )\r\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\r\n        path = Path(self.broadcast(path))\r\n        _save_checkpoint(\r\n            path=path,\r\n            state=state,\r\n            full_state_dict=(not self._save_distributed_checkpoint),\r\n            rank=self.global_rank,\r\n            filter=filter,\r\n        )", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If distributed checkpointing is enabled (default), the checkpoint gets saved as a directory containing one file\r\n        per process, with model- and optimizer shards stored per file. Additionally, it creates a metadata file\r\n        `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n        If distributed checkpointing is disabled (``save_distributed_checkpoint=False``), the checkpoint will be\r\n        written to a single file containing the weights, optimizer state and other metadata.\r\n\r\n        \"\"\"\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                f\"`{type(self).__name__}.save_checkpoint(..., storage_options=...)` is not supported because\"\r\n                f\" `{type(self).__name__}` does not use the `CheckpointIO`.\"\r\n            )\r\n        if filter is not None and self._save_distributed_checkpoint:\r\n            # https://github.com/pytorch/pytorch/issues/105379\r\n            raise NotImplementedError(\r\n                f\"{type(self).__name__} doesn't support loading distributed filtered checkpoints,\"\r\n                \" so saving them is disabled.\"\r\n            )\r\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\r\n        path = Path(self.broadcast(path))\r\n        _save_checkpoint(\r\n            path=path,\r\n            state=state,\r\n            full_state_dict=(not self._save_distributed_checkpoint),\r\n            rank=self.global_rank,\r\n            filter=filter,\r\n        )", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", ",", "optimizer", ",", "and", "other", "state", "to", "a", "checkpoint", "on", "disk", ".", "If", "distributed", "checkpointing", "is", "enabled", "(", "default", ")", ",", "the", "checkpoint", "gets", "saved", "as", "a", "directory", "containing", "one", "file", "per", "process", ",", "with", "model", "-", "and", "optimizer", "shards", "stored", "per", "file", ".", "Additionally", ",", "it", "creates", "a", "metadata", "file", "`", "meta", ".", "pt", "`", "with", "the", "rest", "of", "the", "user", "'", "s", "state", "(", "only", "saved", "from", "rank", "0", ")", ".", "If", "distributed", "checkpointing", "is", "disabled", "(", "`", "`", "save_distributed_checkpoint", "=", "False", "`", "`", ")", ",", "the", "checkpoint", "will", "be", "written", "to", "a", "single", "file", "containing", "the", "weights", ",", "optimizer", "state", "and", "other", "metadata", ".", "\"", "\"", "\"", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "save_checkpoint", "(", ".", ".", ".", ",", "storage_options", "=", ".", ".", ".", ")", "`", "is", "not", "supported", "because", "\"", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", "`", "does", "not", "use", "the", "`", "CheckpointIO", "`", ".", "\"", ")", "if", "filter", "is", "not", "None", "and", "self", ".", "_save_distributed_checkpoint", ":", "raise", "NotImplementedError", "(", "f", "\"", "{", "type", "(", "self", ")", ".", "__name__", "}", "doesn", "'", "t", "support", "loading", "distributed", "filtered", "checkpoints", ",", "\"", "\"", "so", "saving", "them", "is", "disabled", ".", "\"", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "_save_checkpoint", "(", "path", "=", "path", ",", "state", "=", "state", ",", "full_state_dict", "=", "(", "not", "self", ".", "_save_distributed_checkpoint", ")", ",", "rank", "=", "self", ".", "global_rank", ",", "filter", "=", "filter", ",", ")"], "docstring": "Save model, optimizer, and other state to a checkpoint on disk.\r\n\r\n        If distributed checkpointing is enabled (default), the checkpoint gets saved as a directory containing one file\r\n        per process, with model- and optimizer shards stored per file. Additionally, it creates a metadata file\r\n        `meta.pt` with the rest of the user's state (only saved from rank 0).\r\n        If distributed checkpointing is disabled (``save_distributed_checkpoint=False``), the checkpoint will be\r\n        written to a single file containing the weights, optimizer state and other metadata.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "to", "a", "checkpoint", "on", "disk", "if", "distributed", "checkpointing", "is", "enabled", "default", "the", "checkpoint", "gets", "saved", "as", "a", "directory", "containing", "one", "file", "per", "process", "with", "model", "and", "optimizer", "shards", "stored", "per", "file", "additionally", "it", "creates", "a", "metadata", "file", "meta", "pt", "with", "the", "rest", "of", "the", "user", "s", "state", "only", "saved", "from", "rank", "0", "if", "distributed", "checkpointing", "is", "disabled", "save_distributed_checkpoint", "false", "the", "checkpoint", "will", "be", "written", "to", "a", "single", "file", "containing", "the", "weights", "optimizer", "state", "and", "other", "metadata"], "docstring_summary": "Save model, optimizer, and other state to a checkpoint on disk.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ModelParallelStrategy", "start_line": 234, "end_line": 269, "hash": "db509aaa06f039756a7b1774e02eac50", "complexity": 4, "parameters": ["path", "state", "Union[Module", "Optimizer", "Any]]", "storage_options", "filter", "Callable[[str", "Any]", "bool]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "load_checkpoint", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got {type(self).__name__}.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                \" a model instance to reload is required. Pass it in like so:\"\r\n                f\" {type(self).__name__}.load_checkpoint(..., state={{'model': model, ...}})\"\r\n            )\r\n        # broadcast the path from rank 0 to ensure all the states are loaded from a common path\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, Module):\r\n            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            raise NotImplementedError(\r\n                f\"Loading a single optimizer object from a checkpoint is not supported yet with {type(self).__name__}.\"\r\n            )\r\n\r\n        return _load_checkpoint(path=path, state=state, strict=strict)", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\"\"\"\r\n        if not state:\r\n            raise ValueError(\r\n                f\"Got {type(self).__name__}.load_checkpoint(..., state={state!r}) but a state with at least \"\r\n                \" a model instance to reload is required. Pass it in like so:\"\r\n                f\" {type(self).__name__}.load_checkpoint(..., state={{'model': model, ...}})\"\r\n            )\r\n        # broadcast the path from rank 0 to ensure all the states are loaded from a common path\r\n        path = Path(self.broadcast(path))\r\n\r\n        if isinstance(state, Module):\r\n            _load_raw_module_state_from_path(path, module=state, world_size=self.world_size, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            raise NotImplementedError(\r\n                f\"Loading a single optimizer object from a checkpoint is not supported yet with {type(self).__name__}.\"\r\n            )\r\n\r\n        return _load_checkpoint(path=path, state=state, strict=strict)", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects", ".", "\"", "\"", "\"", "if", "not", "state", ":", "raise", "ValueError", "(", "f", "\"", "Got", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "load_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "state", "!", "r", "}", ")", "but", "a", "state", "with", "at", "least", "\"", "\"", "a", "model", "instance", "to", "reload", "is", "required", ".", "Pass", "it", "in", "like", "so", ":", "\"", "f", "\"", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "load_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "{", "'", "model", "'", ":", "model", ",", ".", ".", ".", "}", "}", ")", "\"", ")", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "isinstance", "(", "state", ",", "Module", ")", ":", "_load_raw_module_state_from_path", "(", "path", ",", "module", "=", "state", ",", "world_size", "=", "self", ".", "world_size", ",", "strict", "=", "strict", ")", "return", "{", "}", "if", "isinstance", "(", "state", ",", "Optimizer", ")", ":", "raise", "NotImplementedError", "(", "f", "\"", "Loading", "a", "single", "optimizer", "object", "from", "a", "checkpoint", "is", "not", "supported", "yet", "with", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "\"", ")", "return", "_load_checkpoint", "(", "path", "=", "path", ",", "state", "=", "state", ",", "strict", "=", "strict", ")"], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects"], "docstring_summary": "Load the contents from a checkpoint and restore the state of the given objects.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ModelParallelStrategy", "start_line": 272, "end_line": 297, "hash": "6fd16e36114bef8ef7d03fc599587781", "complexity": 4, "parameters": ["path", "state", "Optimizer", "dict[str", "Union[Module", "Optimizer", "Any]]]]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "_load_raw_module_state_from_path", "original_string": "def _load_raw_module_state_from_path(path: Path, module: Module, world_size: int, strict: bool = True) -> None:\r\n    \"\"\"Loads the state dict from a file path into the FSDP module.\"\"\"\r\n    if not _is_full_checkpoint(path):\r\n        raise ValueError(\r\n            \"Failed to load checkpoint directly into the model. The given path must be a single file containing the\"\r\n            f\" full state dict: {path}\"\r\n        )\r\n    # Use `lazy_load`/`mmap` instead to avoid storing a copy of the full checkpoint per rank\r\n    state_dict = torch.load(path, mmap=True, map_location=\"cpu\") if _TORCH_GREATER_EQUAL_2_3 else _lazy_load(path)\r\n    _load_raw_module_state(state_dict=state_dict, module=module, world_size=world_size, strict=strict)", "language": "python", "code": "def _load_raw_module_state_from_path(path: Path, module: Module, world_size: int, strict: bool = True) -> None:\r\n    \"\"\"Loads the state dict from a file path into the FSDP module.\"\"\"\r\n    if not _is_full_checkpoint(path):\r\n        raise ValueError(\r\n            \"Failed to load checkpoint directly into the model. The given path must be a single file containing the\"\r\n            f\" full state dict: {path}\"\r\n        )\r\n    # Use `lazy_load`/`mmap` instead to avoid storing a copy of the full checkpoint per rank\r\n    state_dict = torch.load(path, mmap=True, map_location=\"cpu\") if _TORCH_GREATER_EQUAL_2_3 else _lazy_load(path)\r\n    _load_raw_module_state(state_dict=state_dict, module=module, world_size=world_size, strict=strict)", "code_tokens": ["def", "_load_raw_module_state_from_path", "(", "path", ":", "Path", ",", "module", ":", "Module", ",", "world_size", ":", "int", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Loads", "the", "state", "dict", "from", "a", "file", "path", "into", "the", "FSDP", "module", ".", "\"", "\"", "\"", "if", "not", "_is_full_checkpoint", "(", "path", ")", ":", "raise", "ValueError", "(", "\"", "Failed", "to", "load", "checkpoint", "directly", "into", "the", "model", ".", "The", "given", "path", "must", "be", "a", "single", "file", "containing", "the", "\"", "f", "\"", "full", "state", "dict", ":", "{", "path", "}", "\"", ")", "state_dict", "=", "torch", ".", "load", "(", "path", ",", "mmap", "=", "True", ",", "map_location", "=", "\"", "cpu", "\"", ")", "if", "_TORCH_GREATER_EQUAL_2_3", "else", "_lazy_load", "(", "path", ")", "_load_raw_module_state", "(", "state_dict", "=", "state_dict", ",", "module", "=", "module", ",", "world_size", "=", "world_size", ",", "strict", "=", "strict", ")"], "docstring": "Loads the state dict from a file path into the FSDP module.", "docstring_tokens": ["loads", "the", "state", "dict", "from", "a", "file", "path", "into", "the", "fsdp", "module"], "docstring_summary": "Loads the state dict from a file path into the FSDP module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py", "partition": "train", "function_type": "function", "start_line": 529, "end_line": 538, "hash": "58ea54efcc2eb13aecee39213c61effd", "complexity": 3, "parameters": ["path", "module", "world_size", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "_load_raw_module_state", "original_string": "def _load_raw_module_state(\r\n    state_dict: dict[str, Any], module: Module, world_size: int = 1, strict: bool = True\r\n) -> None:\r\n    \"\"\"Loads the state dict into the module by gathering all weights first and then and writing back to each shard.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n    if _has_dtensor_modules(module):\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, set_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(\r\n            broadcast_from_rank0=True,\r\n            full_state_dict=True,\r\n            # must be set False to allow loading each param separately below\r\n            strict=False,\r\n        )\r\n\r\n        for submodule_name, submodule in module.named_modules():\r\n            for param_name, _ in _named_parameters_and_buffers_to_load(submodule):\r\n                full_param_name = f\"{submodule_name}{'.' if submodule_name else ''}{param_name}\"\r\n                if full_param_name not in state_dict:\r\n                    if not strict:\r\n                        continue\r\n                    raise KeyError(\r\n                        f\"The model contains a key '{full_param_name}' that does not exist in the loaded checkpoint.\"\r\n                        \" To disable strict loading, set `strict=False`.\"\r\n                    )\r\n                local_state_dict = {param_name: state_dict[full_param_name]}\r\n                set_model_state_dict(submodule, local_state_dict, options=state_dict_options)\r\n\r\n    elif isinstance(module, FSDP):\r\n        with _get_full_state_dict_context(module, world_size=world_size, rank0_only=False):\r\n            module.load_state_dict(state_dict, strict=strict)\r\n    else:\r\n        module.load_state_dict(state_dict, strict=strict)", "language": "python", "code": "def _load_raw_module_state(\r\n    state_dict: dict[str, Any], module: Module, world_size: int = 1, strict: bool = True\r\n) -> None:\r\n    \"\"\"Loads the state dict into the module by gathering all weights first and then and writing back to each shard.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n\r\n    if _has_dtensor_modules(module):\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, set_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(\r\n            broadcast_from_rank0=True,\r\n            full_state_dict=True,\r\n            # must be set False to allow loading each param separately below\r\n            strict=False,\r\n        )\r\n\r\n        for submodule_name, submodule in module.named_modules():\r\n            for param_name, _ in _named_parameters_and_buffers_to_load(submodule):\r\n                full_param_name = f\"{submodule_name}{'.' if submodule_name else ''}{param_name}\"\r\n                if full_param_name not in state_dict:\r\n                    if not strict:\r\n                        continue\r\n                    raise KeyError(\r\n                        f\"The model contains a key '{full_param_name}' that does not exist in the loaded checkpoint.\"\r\n                        \" To disable strict loading, set `strict=False`.\"\r\n                    )\r\n                local_state_dict = {param_name: state_dict[full_param_name]}\r\n                set_model_state_dict(submodule, local_state_dict, options=state_dict_options)\r\n\r\n    elif isinstance(module, FSDP):\r\n        with _get_full_state_dict_context(module, world_size=world_size, rank0_only=False):\r\n            module.load_state_dict(state_dict, strict=strict)\r\n    else:\r\n        module.load_state_dict(state_dict, strict=strict)", "code_tokens": ["def", "_load_raw_module_state", "(", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ",", "module", ":", "Module", ",", "world_size", ":", "int", "=", "1", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Loads", "the", "state", "dict", "into", "the", "module", "by", "gathering", "all", "weights", "first", "and", "then", "and", "writing", "back", "to", "each", "shard", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "if", "_has_dtensor_modules", "(", "module", ")", ":", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict", "import", "StateDictOptions", ",", "set_model_state_dict", "state_dict_options", "=", "StateDictOptions", "(", "broadcast_from_rank0", "=", "True", ",", "full_state_dict", "=", "True", ",", "strict", "=", "False", ",", ")", "for", "submodule_name", ",", "submodule", "in", "module", ".", "named_modules", "(", ")", ":", "for", "param_name", ",", "_", "in", "_named_parameters_and_buffers_to_load", "(", "submodule", ")", ":", "full_param_name", "=", "f", "\"", "{", "submodule_name", "}", "{", "'", ".", "'", "if", "submodule_name", "else", "'", "'", "}", "{", "param_name", "}", "\"", "if", "full_param_name", "not", "in", "state_dict", ":", "if", "not", "strict", ":", "continue", "raise", "KeyError", "(", "f", "\"", "The", "model", "contains", "a", "key", "'", "{", "full_param_name", "}", "'", "that", "does", "not", "exist", "in", "the", "loaded", "checkpoint", ".", "\"", "\"", "To", "disable", "strict", "loading", ",", "set", "`", "strict", "=", "False", "`", ".", "\"", ")", "local_state_dict", "=", "{", "param_name", ":", "state_dict", "[", "full_param_name", "]", "}", "set_model_state_dict", "(", "submodule", ",", "local_state_dict", ",", "options", "=", "state_dict_options", ")", "elif", "isinstance", "(", "module", ",", "FSDP", ")", ":", "with", "_get_full_state_dict_context", "(", "module", ",", "world_size", "=", "world_size", ",", "rank0_only", "=", "False", ")", ":", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "else", ":", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")"], "docstring": "Loads the state dict into the module by gathering all weights first and then and writing back to each shard.", "docstring_tokens": ["loads", "the", "state", "dict", "into", "the", "module", "by", "gathering", "all", "weights", "first", "and", "then", "and", "writing", "back", "to", "each", "shard"], "docstring_summary": "Loads the state dict into the module by gathering all weights first and then and writing back to each shard.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py", "partition": "train", "function_type": "function", "start_line": 541, "end_line": 574, "hash": "7f11b146e75d70206d153a3202344025", "complexity": 9, "parameters": ["state_dict", "Any]", "module", "world_size", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "_named_parameters_and_buffers_to_load", "original_string": "def _named_parameters_and_buffers_to_load(module: Module) -> Generator:\r\n    \"\"\"Returns parameters and buffers, with non-persistent buffers excluded.\"\"\"\r\n    for param_name, param in itertools.chain(\r\n        module.named_buffers(recurse=False),\r\n        module.named_parameters(recurse=False),\r\n    ):\r\n        if param_name in module._non_persistent_buffers_set:\r\n            continue\r\n        yield param_name, param", "language": "python", "code": "def _named_parameters_and_buffers_to_load(module: Module) -> Generator:\r\n    \"\"\"Returns parameters and buffers, with non-persistent buffers excluded.\"\"\"\r\n    for param_name, param in itertools.chain(\r\n        module.named_buffers(recurse=False),\r\n        module.named_parameters(recurse=False),\r\n    ):\r\n        if param_name in module._non_persistent_buffers_set:\r\n            continue\r\n        yield param_name, param", "code_tokens": ["def", "_named_parameters_and_buffers_to_load", "(", "module", ":", "Module", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Returns", "parameters", "and", "buffers", ",", "with", "non", "-", "persistent", "buffers", "excluded", ".", "\"", "\"", "\"", "for", "param_name", ",", "param", "in", "itertools", ".", "chain", "(", "module", ".", "named_buffers", "(", "recurse", "=", "False", ")", ",", "module", ".", "named_parameters", "(", "recurse", "=", "False", ")", ",", ")", ":", "if", "param_name", "in", "module", ".", "_non_persistent_buffers_set", ":", "continue", "yield", "param_name", ",", "param"], "docstring": "Returns parameters and buffers, with non-persistent buffers excluded.", "docstring_tokens": ["returns", "parameters", "and", "buffers", "with", "non", "persistent", "buffers", "excluded"], "docstring_summary": "Returns parameters and buffers, with non-persistent buffers excluded.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py", "partition": "train", "function_type": "function", "start_line": 577, "end_line": 585, "hash": "1a9e3ff74a2642e6261a1af09b8dd35e", "complexity": 3, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\model_parallel.py", "func_name": "_rekey_optimizer_state_if_needed", "original_string": "def _rekey_optimizer_state_if_needed(optimizer_state_dict: dict[str, Any], module: Module) -> dict[str, Any]:\r\n    \"\"\"Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter\r\n    names.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n    from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n    if isinstance(list(optimizer_state_dict[\"state\"].keys())[0], int):\r\n        optimizer_state_dict = FSDP.rekey_optim_state_dict(optimizer_state_dict, OptimStateKeyType.PARAM_NAME, module)\r\n    return optimizer_state_dict", "language": "python", "code": "def _rekey_optimizer_state_if_needed(optimizer_state_dict: dict[str, Any], module: Module) -> dict[str, Any]:\r\n    \"\"\"Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter\r\n    names.\"\"\"\r\n    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n    from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n    if isinstance(list(optimizer_state_dict[\"state\"].keys())[0], int):\r\n        optimizer_state_dict = FSDP.rekey_optim_state_dict(optimizer_state_dict, OptimStateKeyType.PARAM_NAME, module)\r\n    return optimizer_state_dict", "code_tokens": ["def", "_rekey_optimizer_state_if_needed", "(", "optimizer_state_dict", ":", "dict", "[", "str", ",", "Any", "]", ",", "module", ":", "Module", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Handles", "the", "case", "where", "the", "optimizer", "state", "is", "saved", "from", "a", "normal", "optimizer", "and", "converts", "the", "keys", "to", "parameter", "names", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "from", "torch", ".", "distributed", ".", "fsdp", "import", "OptimStateKeyType", "if", "isinstance", "(", "list", "(", "optimizer_state_dict", "[", "\"", "state", "\"", "]", ".", "keys", "(", ")", ")", "[", "0", "]", ",", "int", ")", ":", "optimizer_state_dict", "=", "FSDP", ".", "rekey_optim_state_dict", "(", "optimizer_state_dict", ",", "OptimStateKeyType", ".", "PARAM_NAME", ",", "module", ")", "return", "optimizer_state_dict"], "docstring": "Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter\r\n    names.", "docstring_tokens": ["handles", "the", "case", "where", "the", "optimizer", "state", "is", "saved", "from", "a", "normal", "optimizer", "and", "converts", "the", "keys", "to", "parameter", "names"], "docstring_summary": "Handles the case where the optimizer state is saved from a normal optimizer and converts the keys to parameter", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\model_parallel.py", "partition": "train", "function_type": "function", "start_line": 588, "end_line": 596, "hash": "54102b0d45477bbb966c2f743e41abcf", "complexity": 2, "parameters": ["optimizer_state_dict", "Any]", "module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\parallel.py", "func_name": "distributed_sampler_kwargs", "original_string": "def distributed_sampler_kwargs(self) -> Optional[dict[str, Any]]:\r\n        \"\"\"Arguments for the ``DistributedSampler``.\r\n\r\n        If this method is not defined, or it returns ``None``, then the ``DistributedSampler`` will not be used.\r\n\r\n        \"\"\"\r\n        return {\"num_replicas\": self.world_size, \"rank\": self.global_rank}", "language": "python", "code": "def distributed_sampler_kwargs(self) -> Optional[dict[str, Any]]:\r\n        \"\"\"Arguments for the ``DistributedSampler``.\r\n\r\n        If this method is not defined, or it returns ``None``, then the ``DistributedSampler`` will not be used.\r\n\r\n        \"\"\"\r\n        return {\"num_replicas\": self.world_size, \"rank\": self.global_rank}", "code_tokens": ["def", "distributed_sampler_kwargs", "(", "self", ")", "-", ">", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", ":", "\"", "\"", "\"", "Arguments", "for", "the", "`", "`", "DistributedSampler", "`", "`", ".", "If", "this", "method", "is", "not", "defined", ",", "or", "it", "returns", "`", "`", "None", "`", "`", ",", "then", "the", "`", "`", "DistributedSampler", "`", "`", "will", "not", "be", "used", ".", "\"", "\"", "\"", "return", "{", "\"", "num_replicas", "\"", ":", "self", ".", "world_size", ",", "\"", "rank", "\"", ":", "self", ".", "global_rank", "}"], "docstring": "Arguments for the ``DistributedSampler``.\r\n\r\n        If this method is not defined, or it returns ``None``, then the ``DistributedSampler`` will not be used.", "docstring_tokens": ["arguments", "for", "the", "distributedsampler", "if", "this", "method", "is", "not", "defined", "or", "it", "returns", "none", "then", "the", "distributedsampler", "will", "not", "be", "used"], "docstring_summary": "Arguments for the ``DistributedSampler``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ParallelStrategy", "start_line": 74, "end_line": 80, "hash": "c2bd04a16d1a3630de5fdb7c50f9b2bc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\parallel.py", "func_name": "reduce_boolean_decision", "original_string": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.all_reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "language": "python", "code": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.all_reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "code_tokens": ["def", "reduce_boolean_decision", "(", "self", ",", "decision", ":", "bool", ",", "all", ":", "bool", "=", "True", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Reduces", "a", "boolean", "decision", "over", "distributed", "processes", ".", "By", "default", "is", "analogous", "to", "`", "`", "all", "`", "`", "from", "the", "standard", "library", ",", "returning", "`", "`", "True", "`", "`", "only", "if", "all", "input", "decisions", "evaluate", "to", "`", "`", "True", "`", "`", ".", "If", "`", "`", "all", "`", "`", "is", "set", "to", "`", "`", "False", "`", "`", ",", "it", "behaves", "like", "`", "`", "any", "`", "`", "instead", ".", "Args", ":", "decision", ":", "A", "single", "input", "decision", ".", "all", ":", "Whether", "to", "logically", "emulate", "`", "`", "all", "`", "`", "or", "`", "`", "any", "`", "`", ".", "Defaults", "to", "True", ".", "Returns", ":", "bool", ":", "The", "reduced", "boolean", "decision", ".", "\"", "\"", "\"", "decision", "=", "torch", ".", "tensor", "(", "int", "(", "decision", ")", ",", "device", "=", "self", ".", "root_device", ")", "decision", "=", "self", ".", "all_reduce", "(", "decision", ",", "reduce_op", "=", "ReduceOp", ".", "SUM", ",", ")", "decision", "=", "bool", "(", "decision", "=", "=", "self", ".", "world_size", ")", "if", "all", "else", "bool", "(", "decision", ")", "return", "decision"], "docstring": "Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.", "docstring_tokens": ["reduces", "a", "boolean", "decision", "over", "distributed", "processes", "by", "default", "is", "analogous", "to", "all", "from", "the", "standard", "library", "returning", "true", "only", "if", "all", "input", "decisions", "evaluate", "to", "true", "if", "all", "is", "set", "to", "false", "it", "behaves", "like", "any", "instead", "args", "decision", "a", "single", "input", "decision", "all", "whether", "to", "logically", "emulate", "all", "or", "any", "defaults", "to", "true", "returns", "bool", "the", "reduced", "boolean", "decision"], "docstring_summary": "Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ParallelStrategy", "start_line": 88, "end_line": 107, "hash": "70bd35e679b43425ee06a31c143c827e", "complexity": 2, "parameters": ["decision", "all"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\registry.py", "func_name": "register", "original_string": "def register(\r\n        self,\r\n        name: str,\r\n        strategy: Optional[Callable] = None,\r\n        description: Optional[str] = None,\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a strategy mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n            strategy : strategy class\r\n            description : strategy description\r\n            override : overrides the registered strategy, if True\r\n            init_params: parameters to initialize the strategy\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise ValueError(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n        data[\"description\"] = description if description is not None else \"\"\r\n\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(strategy: Callable) -> Callable:\r\n            data[\"strategy\"] = strategy\r\n            data[\"strategy_name\"] = name\r\n            self[name] = data\r\n            return strategy\r\n\r\n        if strategy is not None:\r\n            return do_register(strategy)\r\n\r\n        return do_register", "language": "python", "code": "def register(\r\n        self,\r\n        name: str,\r\n        strategy: Optional[Callable] = None,\r\n        description: Optional[str] = None,\r\n        override: bool = False,\r\n        **init_params: Any,\r\n    ) -> Callable:\r\n        \"\"\"Registers a strategy mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n            strategy : strategy class\r\n            description : strategy description\r\n            override : overrides the registered strategy, if True\r\n            init_params: parameters to initialize the strategy\r\n\r\n        \"\"\"\r\n        if not (name is None or isinstance(name, str)):\r\n            raise TypeError(f\"`name` must be a str, found {name}\")\r\n\r\n        if name in self and not override:\r\n            raise ValueError(f\"'{name}' is already present in the registry. HINT: Use `override=True`.\")\r\n\r\n        data: dict[str, Any] = {}\r\n        data[\"description\"] = description if description is not None else \"\"\r\n\r\n        data[\"init_params\"] = init_params\r\n\r\n        def do_register(strategy: Callable) -> Callable:\r\n            data[\"strategy\"] = strategy\r\n            data[\"strategy_name\"] = name\r\n            self[name] = data\r\n            return strategy\r\n\r\n        if strategy is not None:\r\n            return do_register(strategy)\r\n\r\n        return do_register", "code_tokens": ["def", "register", "(", "self", ",", "name", ":", "str", ",", "strategy", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "description", ":", "Optional", "[", "str", "]", "=", "None", ",", "override", ":", "bool", "=", "False", ",", "*", "*", "init_params", ":", "Any", ",", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Registers", "a", "strategy", "mapped", "to", "a", "name", "and", "with", "required", "metadata", ".", "Args", ":", "name", ":", "the", "name", "that", "identifies", "a", "strategy", ",", "e", ".", "g", ".", "\"", "deepspeed_stage_3", "\"", "strategy", ":", "strategy", "class", "description", ":", "strategy", "description", "override", ":", "overrides", "the", "registered", "strategy", ",", "if", "True", "init_params", ":", "parameters", "to", "initialize", "the", "strategy", "\"", "\"", "\"", "if", "not", "(", "name", "is", "None", "or", "isinstance", "(", "name", ",", "str", ")", ")", ":", "raise", "TypeError", "(", "f", "\"", "`", "name", "`", "must", "be", "a", "str", ",", "found", "{", "name", "}", "\"", ")", "if", "name", "in", "self", "and", "not", "override", ":", "raise", "ValueError", "(", "f", "\"", "'", "{", "name", "}", "'", "is", "already", "present", "in", "the", "registry", ".", "HINT", ":", "Use", "`", "override", "=", "True", "`", ".", "\"", ")", "data", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "data", "[", "\"", "description", "\"", "]", "=", "description", "if", "description", "is", "not", "None", "else", "\"", "\"", "data", "[", "\"", "init_params", "\"", "]", "=", "init_params", "def", "do_register", "(", "strategy", ":", "Callable", ")", "-", ">", "Callable", ":", "data", "[", "\"", "strategy", "\"", "]", "=", "strategy", "data", "[", "\"", "strategy_name", "\"", "]", "=", "name", "self", "[", "name", "]", "=", "data", "return", "strategy", "if", "strategy", "is", "not", "None", ":", "return", "do_register", "(", "strategy", ")", "return", "do_register"], "docstring": "Registers a strategy mapped to a name and with required metadata.\r\n\r\n        Args:\r\n            name : the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n            strategy : strategy class\r\n            description : strategy description\r\n            override : overrides the registered strategy, if True\r\n            init_params: parameters to initialize the strategy", "docstring_tokens": ["registers", "a", "strategy", "mapped", "to", "a", "name", "and", "with", "required", "metadata", "args", "name", "the", "name", "that", "identifies", "a", "strategy", "e", "g", "deepspeed_stage_3", "strategy", "strategy", "class", "description", "strategy", "description", "override", "overrides", "the", "registered", "strategy", "if", "true", "init_params", "parameters", "to", "initialize", "the", "strategy"], "docstring_summary": "Registers a strategy mapped to a name and with required metadata.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\registry.py", "partition": "train", "function_type": "class_method", "class_name": "_StrategyRegistry", "start_line": 43, "end_line": 81, "hash": "d5c5f36cbc945d9364c08ad7602605fd", "complexity": 7, "parameters": ["name", "strategy", "description", "override", "**init_params"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\registry.py", "func_name": "get", "original_string": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered strategy with the required parameters and returns the strategy object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"strategy\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = \", \".join(sorted(self.keys())) or \"none\"\r\n        raise KeyError(err_msg.format(name, available_names))", "language": "python", "code": "def get(self, name: str, default: Optional[Any] = None) -> Any:\r\n        \"\"\"Calls the registered strategy with the required parameters and returns the strategy object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a strategy, e.g. \"deepspeed_stage_3\"\r\n\r\n        \"\"\"\r\n        if name in self:\r\n            data = self[name]\r\n            return data[\"strategy\"](**data[\"init_params\"])\r\n\r\n        if default is not None:\r\n            return default\r\n\r\n        err_msg = \"'{}' not found in registry. Available names: {}\"\r\n        available_names = \", \".join(sorted(self.keys())) or \"none\"\r\n        raise KeyError(err_msg.format(name, available_names))", "code_tokens": ["def", "get", "(", "self", ",", "name", ":", "str", ",", "default", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Calls", "the", "registered", "strategy", "with", "the", "required", "parameters", "and", "returns", "the", "strategy", "object", ".", "Args", ":", "name", "(", "str", ")", ":", "the", "name", "that", "identifies", "a", "strategy", ",", "e", ".", "g", ".", "\"", "deepspeed_stage_3", "\"", "\"", "\"", "\"", "if", "name", "in", "self", ":", "data", "=", "self", "[", "name", "]", "return", "data", "[", "\"", "strategy", "\"", "]", "(", "*", "*", "data", "[", "\"", "init_params", "\"", "]", ")", "if", "default", "is", "not", "None", ":", "return", "default", "err_msg", "=", "\"", "'", "{", "}", "'", "not", "found", "in", "registry", ".", "Available", "names", ":", "{", "}", "\"", "available_names", "=", "\"", ",", "\"", ".", "join", "(", "sorted", "(", "self", ".", "keys", "(", ")", ")", ")", "or", "\"", "none", "\"", "raise", "KeyError", "(", "err_msg", ".", "format", "(", "name", ",", "available_names", ")", ")"], "docstring": "Calls the registered strategy with the required parameters and returns the strategy object.\r\n\r\n        Args:\r\n            name (str): the name that identifies a strategy, e.g. \"deepspeed_stage_3\"", "docstring_tokens": ["calls", "the", "registered", "strategy", "with", "the", "required", "parameters", "and", "returns", "the", "strategy", "object", "args", "name", "str", "the", "name", "that", "identifies", "a", "strategy", "e", "g", "deepspeed_stage_3"], "docstring_summary": "Calls the registered strategy with the required parameters and returns the strategy object.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\registry.py", "partition": "train", "function_type": "class_method", "class_name": "_StrategyRegistry", "start_line": 84, "end_line": 100, "hash": "9a2695800a867075725f78400480c1ab", "complexity": 4, "parameters": ["name", "default"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\single_device.py", "func_name": "all_reduce", "original_string": "def all_reduce(self, tensor: Any | torch.Tensor, *args: Any, **kwargs: Any) -> Any | torch.Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates\r\n        with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "language": "python", "code": "def all_reduce(self, tensor: Any | torch.Tensor, *args: Any, **kwargs: Any) -> Any | torch.Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates\r\n        with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "code_tokens": ["def", "all_reduce", "(", "self", ",", "tensor", ":", "Any", "|", "torch", ".", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", "|", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", ".", "As", "this", "plugin", "only", "operates", "with", "a", "single", "device", ",", "the", "reduction", "is", "simply", "the", "identity", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "*", "args", ":", "ignored", "*", "*", "kwargs", ":", "ignored", "Return", ":", "the", "unmodified", "input", "as", "reduction", "is", "not", "needed", "for", "single", "process", "operation", "\"", "\"", "\"", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates\r\n        with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "as", "this", "plugin", "only", "operates", "with", "a", "single", "device", "the", "reduction", "is", "simply", "the", "identity", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "args", "ignored", "kwargs", "ignored", "return", "the", "unmodified", "input", "as", "reduction", "is", "not", "needed", "for", "single", "process", "operation"], "docstring_summary": "Reduces a tensor from several distributed processes to one aggregated tensor. As this plugin only operates", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\single_device.py", "partition": "train", "function_type": "class_method", "class_name": "SingleDeviceStrategy", "start_line": 61, "end_line": 74, "hash": "01718cdf456651f69da7eec6af6e1a9b", "complexity": 1, "parameters": ["tensor", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "setup_environment", "original_string": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This must be called by the framework at the beginning of every process, before any distributed communication\r\n        takes place.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "language": "python", "code": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This must be called by the framework at the beginning of every process, before any distributed communication\r\n        takes place.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "code_tokens": ["def", "setup_environment", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Setup", "any", "processes", "or", "distributed", "connections", ".", "This", "must", "be", "called", "by", "the", "framework", "at", "the", "beginning", "of", "every", "process", ",", "before", "any", "distributed", "communication", "takes", "place", ".", "\"", "\"", "\"", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "setup_device", "(", "self", ".", "root_device", ")"], "docstring": "Setup any processes or distributed connections.\r\n\r\n        This must be called by the framework at the beginning of every process, before any distributed communication\r\n        takes place.", "docstring_tokens": ["setup", "any", "processes", "or", "distributed", "connections", "this", "must", "be", "called", "by", "the", "framework", "at", "the", "beginning", "of", "every", "process", "before", "any", "distributed", "communication", "takes", "place"], "docstring_summary": "Setup any processes or distributed connections.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 104, "end_line": 112, "hash": "ea5d841be3e2f7d7dcda72bfd68bbdab", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "process_dataloader", "original_string": "def process_dataloader(self, dataloader: DataLoader) -> DataLoader:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "language": "python", "code": "def process_dataloader(self, dataloader: DataLoader) -> DataLoader:\r\n        \"\"\"Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`\r\n\r\n        \"\"\"\r\n        return dataloader", "code_tokens": ["def", "process_dataloader", "(", "self", ",", "dataloader", ":", "DataLoader", ")", "-", ">", "DataLoader", ":", "\"", "\"", "\"", "Wraps", "the", "dataloader", "if", "necessary", ".", "Args", ":", "dataloader", ":", "iterable", ".", "Ideally", "of", "type", ":", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "\"", "\"", "\"", "return", "dataloader"], "docstring": "Wraps the dataloader if necessary.\r\n\r\n        Args:\r\n            dataloader: iterable. Ideally of type: :class:`torch.utils.data.DataLoader`", "docstring_tokens": ["wraps", "the", "dataloader", "if", "necessary", "args", "dataloader", "iterable", "ideally", "of", "type", "class", "torch", "utils", "data", "dataloader"], "docstring_summary": "Wraps the dataloader if necessary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 114, "end_line": 121, "hash": "ae8ce83fc2a3fa9ef72d508f6e5ab7ea", "complexity": 1, "parameters": ["dataloader"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "tensor_init_context", "original_string": "def tensor_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Controls how tensors get created (device, dtype).\"\"\"\r\n        precision_init_ctx = self.precision.tensor_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(precision_init_ctx)\r\n        return stack", "language": "python", "code": "def tensor_init_context(self) -> AbstractContextManager:\r\n        \"\"\"Controls how tensors get created (device, dtype).\"\"\"\r\n        precision_init_ctx = self.precision.tensor_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(precision_init_ctx)\r\n        return stack", "code_tokens": ["def", "tensor_init_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Controls", "how", "tensors", "get", "created", "(", "device", ",", "dtype", ")", ".", "\"", "\"", "\"", "precision_init_ctx", "=", "self", ".", "precision", ".", "tensor_init_context", "(", ")", "stack", "=", "ExitStack", "(", ")", "stack", ".", "enter_context", "(", "self", ".", "root_device", ")", "stack", ".", "enter_context", "(", "precision_init_ctx", ")", "return", "stack"], "docstring": "Controls how tensors get created (device, dtype).", "docstring_tokens": ["controls", "how", "tensors", "get", "created", "device", "dtype"], "docstring_summary": "Controls how tensors get created (device, dtype).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 123, "end_line": 129, "hash": "efeddd4b178290d4458f5adbcc350858", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "module_init_context", "original_string": "def module_init_context(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"A context manager wrapping the model instantiation.\r\n\r\n        Here, the strategy can control how the parameters of the model get created (device, dtype) and or apply other\r\n        patches to the model.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        precision_module_ctx = self.precision.module_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(_EmptyInit(enabled=bool(empty_init)))\r\n        stack.enter_context(precision_module_ctx)\r\n        return stack", "language": "python", "code": "def module_init_context(self, empty_init: Optional[bool] = None) -> AbstractContextManager:\r\n        \"\"\"A context manager wrapping the model instantiation.\r\n\r\n        Here, the strategy can control how the parameters of the model get created (device, dtype) and or apply other\r\n        patches to the model.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.\r\n\r\n        \"\"\"\r\n        precision_module_ctx = self.precision.module_init_context()\r\n        stack = ExitStack()\r\n        stack.enter_context(self.root_device)\r\n        stack.enter_context(_EmptyInit(enabled=bool(empty_init)))\r\n        stack.enter_context(precision_module_ctx)\r\n        return stack", "code_tokens": ["def", "module_init_context", "(", "self", ",", "empty_init", ":", "Optional", "[", "bool", "]", "=", "None", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "A", "context", "manager", "wrapping", "the", "model", "instantiation", ".", "Here", ",", "the", "strategy", "can", "control", "how", "the", "parameters", "of", "the", "model", "get", "created", "(", "device", ",", "dtype", ")", "and", "or", "apply", "other", "patches", "to", "the", "model", ".", "Args", ":", "empty_init", ":", "Whether", "to", "initialize", "the", "model", "with", "empty", "weights", "(", "uninitialized", "memory", ")", ".", "If", "`", "`", "None", "`", "`", ",", "the", "strategy", "will", "decide", ".", "Some", "strategies", "may", "not", "support", "all", "options", ".", "\"", "\"", "\"", "precision_module_ctx", "=", "self", ".", "precision", ".", "module_init_context", "(", ")", "stack", "=", "ExitStack", "(", ")", "stack", ".", "enter_context", "(", "self", ".", "root_device", ")", "stack", ".", "enter_context", "(", "_EmptyInit", "(", "enabled", "=", "bool", "(", "empty_init", ")", ")", ")", "stack", ".", "enter_context", "(", "precision_module_ctx", ")", "return", "stack"], "docstring": "A context manager wrapping the model instantiation.\r\n\r\n        Here, the strategy can control how the parameters of the model get created (device, dtype) and or apply other\r\n        patches to the model.\r\n\r\n        Args:\r\n            empty_init: Whether to initialize the model with empty weights (uninitialized memory).\r\n                If ``None``, the strategy will decide. Some strategies may not support all options.", "docstring_tokens": ["a", "context", "manager", "wrapping", "the", "model", "instantiation", "here", "the", "strategy", "can", "control", "how", "the", "parameters", "of", "the", "model", "get", "created", "device", "dtype", "and", "or", "apply", "other", "patches", "to", "the", "model", "args", "empty_init", "whether", "to", "initialize", "the", "model", "with", "empty", "weights", "uninitialized", "memory", "if", "none", "the", "strategy", "will", "decide", "some", "strategies", "may", "not", "support", "all", "options"], "docstring_summary": "A context manager wrapping the model instantiation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 131, "end_line": 147, "hash": "39be6fb33789d96e1f3612c1ad2db37c", "complexity": 1, "parameters": ["empty_init"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "setup_module_and_optimizers", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Set up a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`setup_module` and :meth:`setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        module = self.setup_module(module)\r\n        optimizers = [self.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return module, optimizers, scheduler", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Set up a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`setup_module` and :meth:`setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        module = self.setup_module(module)\r\n        optimizers = [self.setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return module, optimizers, scheduler", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "=", "None", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "]", ":", "\"", "\"", "\"", "Set", "up", "a", "model", "and", "multiple", "optimizers", "together", ".", "The", "returned", "objects", "are", "expected", "to", "be", "in", "the", "same", "order", "they", "were", "passed", "in", ".", "The", "default", "implementation", "will", "call", ":", "meth", ":", "`", "setup_module", "`", "and", ":", "meth", ":", "`", "setup_optimizer", "`", "on", "the", "inputs", ".", "\"", "\"", "\"", "module", "=", "self", ".", "setup_module", "(", "module", ")", "optimizers", "=", "[", "self", ".", "setup_optimizer", "(", "optimizer", ")", "for", "optimizer", "in", "optimizers", "]", "return", "module", ",", "optimizers", ",", "scheduler"], "docstring": "Set up a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`setup_module` and :meth:`setup_optimizer` on the inputs.", "docstring_tokens": ["set", "up", "a", "model", "and", "multiple", "optimizers", "together", "the", "returned", "objects", "are", "expected", "to", "be", "in", "the", "same", "order", "they", "were", "passed", "in", "the", "default", "implementation", "will", "call", "meth", "setup_module", "and", "meth", "setup_optimizer", "on", "the", "inputs"], "docstring_summary": "Set up a model and multiple optimizers together.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 149, "end_line": 160, "hash": "9c1d223a889b5130ab83d174ec00ed6b", "complexity": 2, "parameters": ["module", "optimizers", "scheduler"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "batch_to_device", "original_string": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n\r\n        \"\"\"\r\n        device = device or self.root_device\r\n        return move_data_to_device(batch, device)", "language": "python", "code": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n\r\n        \"\"\"\r\n        device = device or self.root_device\r\n        return move_data_to_device(batch, device)", "code_tokens": ["def", "batch_to_device", "(", "self", ",", "batch", ":", "Any", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Moves", "the", "batch", "to", "the", "correct", "device", ".", "The", "returned", "batch", "is", "of", "the", "same", "type", "as", "the", "input", "batch", ",", "just", "having", "all", "tensors", "on", "the", "correct", "device", ".", "Args", ":", "batch", ":", "The", "batch", "of", "samples", "to", "move", "to", "the", "correct", "device", "device", ":", "The", "target", "device", "\"", "\"", "\"", "device", "=", "device", "or", "self", ".", "root_device", "return", "move_data_to_device", "(", "batch", ",", "device", ")"], "docstring": "Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device", "docstring_tokens": ["moves", "the", "batch", "to", "the", "correct", "device", "the", "returned", "batch", "is", "of", "the", "same", "type", "as", "the", "input", "batch", "just", "having", "all", "tensors", "on", "the", "correct", "device", "args", "batch", "the", "batch", "of", "samples", "to", "move", "to", "the", "correct", "device", "device", "the", "target", "device"], "docstring_summary": "Moves the batch to the correct device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 174, "end_line": 186, "hash": "e5470b00041934e830401047b58201a0", "complexity": 2, "parameters": ["batch", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "backward", "original_string": "def backward(self, tensor: Tensor, module: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"\r\n        self.precision.pre_backward(tensor, module)\r\n        self.precision.backward(tensor, module, *args, **kwargs)\r\n        self.precision.post_backward(tensor, module)", "language": "python", "code": "def backward(self, tensor: Tensor, module: Optional[Module], *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"\r\n        self.precision.pre_backward(tensor, module)\r\n        self.precision.backward(tensor, module, *args, **kwargs)\r\n        self.precision.post_backward(tensor, module)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "module", ":", "Optional", "[", "Module", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Forwards", "backward", "-", "calls", "to", "the", "precision", "plugin", ".", "\"", "\"", "\"", "self", ".", "precision", ".", "pre_backward", "(", "tensor", ",", "module", ")", "self", ".", "precision", ".", "backward", "(", "tensor", ",", "module", ",", "*", "args", ",", "*", "*", "kwargs", ")", "self", ".", "precision", ".", "post_backward", "(", "tensor", ",", "module", ")"], "docstring": "r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"", "docstring_tokens": ["r", "forwards", "backward", "calls", "to", "the", "precision", "plugin"], "docstring_summary": "r\"\"\"Forwards backward-calls to the precision plugin.\"\"\"", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 188, "end_line": 192, "hash": "6a5a6c5b601efa320da281ec0695e11c", "complexity": 1, "parameters": ["tensor", "module", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        return self.precision.optimizer_step(optimizer, **kwargs)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizable,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        return self.precision.optimizer_step(optimizer, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizable", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Performs", "the", "actual", "optimizer", "step", ".", "Args", ":", "optimizer", ":", "the", "optimizer", "performing", "the", "step", "*", "*", "kwargs", ":", "Any", "extra", "arguments", "to", "`", "`", "optimizer", ".", "step", "`", "`", "\"", "\"", "\"", "return", "self", ".", "precision", ".", "optimizer_step", "(", "optimizer", ",", "*", "*", "kwargs", ")"], "docstring": "Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``", "docstring_tokens": ["performs", "the", "actual", "optimizer", "step", "args", "optimizer", "the", "optimizer", "performing", "the", "step", "kwargs", "any", "extra", "arguments", "to", "optimizer", "step"], "docstring_summary": "Performs the actual optimizer step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 194, "end_line": 206, "hash": "512bfa4c257cdefab4fcdcf163da06da", "complexity": 1, "parameters": ["optimizer", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "all_gather", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op\r\n\r\n        \"\"\"", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Perform", "an", "all_gather", "on", "all", "processes", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "all_gather", "group", ":", "the", "process", "group", "to", "gather", "results", "from", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "all_gather", "op", "\"", "\"", "\""], "docstring": "Perform an all_gather on all processes.\r\n\r\n        Args:\r\n            tensor: the tensor to all_gather\r\n            group: the process group to gather results from\r\n            sync_grads: flag that allows users to synchronize gradients for all_gather op", "docstring_tokens": ["perform", "an", "all_gather", "on", "all", "processes", "args", "tensor", "the", "tensor", "to", "all_gather", "group", "the", "process", "group", "to", "gather", "results", "from", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "all_gather", "op"], "docstring_summary": "Perform an all_gather on all processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 209, "end_line": 217, "hash": "94b681ef555b6830d18d6434861941c9", "complexity": 1, "parameters": ["tensor", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "all_reduce", "original_string": "def all_reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "language": "python", "code": "def all_reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Union[Tensor, Any]:\r\n        \"\"\"Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.\r\n\r\n        \"\"\"", "code_tokens": ["def", "all_reduce", "(", "self", ",", "tensor", ":", "Union", "[", "Tensor", ",", "Any", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "\"", "mean", "\"", ",", ")", "-", ">", "Union", "[", "Tensor", ",", "Any", "]", ":", "\"", "\"", "\"", "Reduces", "the", "given", "tensor", "(", "e", ".", "g", ".", "across", "GPUs", "/", "processes", ")", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "group", ":", "the", "process", "group", "to", "reduce", "reduce_op", ":", "the", "reduction", "operation", ".", "Defaults", "to", "'", "mean", "'", ".", "Can", "also", "be", "a", "string", "'", "sum", "'", "or", "ReduceOp", ".", "\"", "\"", "\""], "docstring": "Reduces the given tensor (e.g. across GPUs/processes).\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to reduce\r\n            reduce_op: the reduction operation. Defaults to 'mean'.\r\n                Can also be a string 'sum' or ReduceOp.", "docstring_tokens": ["reduces", "the", "given", "tensor", "e", "g", "across", "gpus", "processes", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "group", "the", "process", "group", "to", "reduce", "reduce_op", "the", "reduction", "operation", "defaults", "to", "mean", "can", "also", "be", "a", "string", "sum", "or", "reduceop"], "docstring_summary": "Reduces the given tensor (e.g. across GPUs/processes).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 220, "end_line": 234, "hash": "1e0e5bdcba9d6cd7209ea1282a91b98e", "complexity": 1, "parameters": ["tensor", "Any]", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "barrier", "original_string": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "language": "python", "code": "def barrier(self, name: Optional[str] = None) -> None:\r\n        \"\"\"Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.\r\n\r\n        \"\"\"", "code_tokens": ["def", "barrier", "(", "self", ",", "name", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Synchronizes", "all", "processes", "which", "blocks", "processes", "until", "the", "whole", "group", "enters", "this", "function", ".", "Args", ":", "name", ":", "an", "optional", "name", "to", "pass", "into", "barrier", ".", "\"", "\"", "\""], "docstring": "Synchronizes all processes which blocks processes until the whole group enters this function.\r\n\r\n        Args:\r\n            name: an optional name to pass into barrier.", "docstring_tokens": ["synchronizes", "all", "processes", "which", "blocks", "processes", "until", "the", "whole", "group", "enters", "this", "function", "args", "name", "an", "optional", "name", "to", "pass", "into", "barrier"], "docstring_summary": "Synchronizes all processes which blocks processes until the whole group enters this function.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 237, "end_line": 243, "hash": "ca72c9ec1f7f7614e6b9d2b1c306093c", "complexity": 1, "parameters": ["name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "broadcast", "original_string": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "language": "python", "code": "def broadcast(self, obj: TBroadcast, src: int = 0) -> TBroadcast:\r\n        \"\"\"Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank\r\n\r\n        \"\"\"", "code_tokens": ["def", "broadcast", "(", "self", ",", "obj", ":", "TBroadcast", ",", "src", ":", "int", "=", "0", ")", "-", ">", "TBroadcast", ":", "\"", "\"", "\"", "Broadcasts", "an", "object", "to", "all", "processes", ".", "Args", ":", "obj", ":", "the", "object", "to", "broadcast", "src", ":", "source", "rank", "\"", "\"", "\""], "docstring": "Broadcasts an object to all processes.\r\n\r\n        Args:\r\n            obj: the object to broadcast\r\n            src: source rank", "docstring_tokens": ["broadcasts", "an", "object", "to", "all", "processes", "args", "obj", "the", "object", "to", "broadcast", "src", "source", "rank"], "docstring_summary": "Broadcasts an object to all processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 246, "end_line": 253, "hash": "f157c4881aa188f48798f316687d7fe0", "complexity": 1, "parameters": ["obj", "src"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        \"\"\"\r\n        state = self._convert_stateful_objects_in_state(state, filter=(filter or {}))\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint=state, path=path, storage_options=storage_options)", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.\r\n\r\n        \"\"\"\r\n        state = self._convert_stateful_objects_in_state(state, filter=(filter or {}))\r\n        if self.is_global_zero:\r\n            self.checkpoint_io.save_checkpoint(checkpoint=state, path=path, storage_options=storage_options)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", ",", "optimizer", ",", "and", "other", "state", "as", "a", "checkpoint", "file", ".", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "(", "s", ")", "should", "be", "saved", "state", ":", "A", "dictionary", "with", "contents", "to", "be", "saved", ".", "If", "the", "dict", "contains", "modules", "or", "optimizers", ",", "their", "state", "-", "dict", "will", "be", "retrieved", "and", "converted", "automatically", ".", "storage_options", ":", "Additional", "options", "for", "the", "`", "`", "CheckpointIO", "`", "`", "plugin", "filter", ":", "An", "optional", "dictionary", "containing", "filter", "callables", "that", "return", "a", "boolean", "indicating", "whether", "the", "given", "item", "should", "be", "saved", "(", "`", "`", "True", "`", "`", ")", "or", "filtered", "out", "(", "`", "`", "False", "`", "`", ")", ".", "Each", "filter", "key", "should", "match", "a", "state", "key", ",", "where", "its", "filter", "will", "be", "applied", "to", "the", "`", "`", "state_dict", "`", "`", "generated", ".", "\"", "\"", "\"", "state", "=", "self", ".", "_convert_stateful_objects_in_state", "(", "state", ",", "filter", "=", "(", "filter", "or", "{", "}", ")", ")", "if", "self", ".", "is_global_zero", ":", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "checkpoint", "=", "state", ",", "path", "=", "path", ",", "storage_options", "=", "storage_options", ")"], "docstring": "Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary containing filter callables that return a boolean indicating whether the\r\n                given item should be saved (``True``) or filtered out (``False``). Each filter key should match a\r\n                state key, where its filter will be applied to the ``state_dict`` generated.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "as", "a", "checkpoint", "file", "args", "path", "a", "path", "to", "where", "the", "file", "s", "should", "be", "saved", "state", "a", "dictionary", "with", "contents", "to", "be", "saved", "if", "the", "dict", "contains", "modules", "or", "optimizers", "their", "state", "dict", "will", "be", "retrieved", "and", "converted", "automatically", "storage_options", "additional", "options", "for", "the", "checkpointio", "plugin", "filter", "an", "optional", "dictionary", "containing", "filter", "callables", "that", "return", "a", "boolean", "indicating", "whether", "the", "given", "item", "should", "be", "saved", "true", "or", "filtered", "out", "false", "each", "filter", "key", "should", "match", "a", "state", "key", "where", "its", "filter", "will", "be", "applied", "to", "the", "state_dict", "generated"], "docstring_summary": "Save model, optimizer, and other state as a checkpoint file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 259, "end_line": 280, "hash": "5ebee288868a4d146c4819711d4d8067", "complexity": 3, "parameters": ["path", "state", "Union[Module", "Optimizer", "Any]]", "storage_options", "filter", "Callable[[str", "Any]", "bool]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "load_module_state_dict", "original_string": "def load_module_state_dict(\r\n        self, module: Module, state_dict: dict[str, Union[Any, Tensor]], strict: bool = True\r\n    ) -> None:\r\n        \"\"\"Loads the given state into the model.\"\"\"\r\n        module.load_state_dict(state_dict, strict=strict)", "language": "python", "code": "def load_module_state_dict(\r\n        self, module: Module, state_dict: dict[str, Union[Any, Tensor]], strict: bool = True\r\n    ) -> None:\r\n        \"\"\"Loads the given state into the model.\"\"\"\r\n        module.load_state_dict(state_dict, strict=strict)", "code_tokens": ["def", "load_module_state_dict", "(", "self", ",", "module", ":", "Module", ",", "state_dict", ":", "dict", "[", "str", ",", "Union", "[", "Any", ",", "Tensor", "]", "]", ",", "strict", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Loads", "the", "given", "state", "into", "the", "model", ".", "\"", "\"", "\"", "module", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")"], "docstring": "Loads the given state into the model.", "docstring_tokens": ["loads", "the", "given", "state", "into", "the", "model"], "docstring_summary": "Loads the given state into the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 286, "end_line": 290, "hash": "f427d5edfaa454013167e927850c0c73", "complexity": 1, "parameters": ["module", "state_dict", "Union[Any", "Tensor]]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "get_optimizer_state", "original_string": "def get_optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom plugins.\r\n\r\n        \"\"\"\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            # there are optimizers like PyTorch's ZeroRedundancyOptimizer that shard their\r\n            # states, and to avoid OOM we consolidate the full state on rank 0 only\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        # for optimizers that are not sharded, we return the state dict on all ranks\r\n        return optimizer.state_dict()", "language": "python", "code": "def get_optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom plugins.\r\n\r\n        \"\"\"\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            # there are optimizers like PyTorch's ZeroRedundancyOptimizer that shard their\r\n            # states, and to avoid OOM we consolidate the full state on rank 0 only\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        # for optimizers that are not sharded, we return the state dict on all ranks\r\n        return optimizer.state_dict()", "code_tokens": ["def", "get_optimizer_state", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "dict", "[", "str", ",", "Tensor", "]", ":", "\"", "\"", "\"", "Returns", "state", "of", "an", "optimizer", ".", "Allows", "for", "syncing", "/", "collating", "optimizer", "state", "from", "processes", "in", "custom", "plugins", ".", "\"", "\"", "\"", "if", "hasattr", "(", "optimizer", ",", "\"", "consolidate_state_dict", "\"", ")", ":", "optimizer", ".", "consolidate_state_dict", "(", ")", "return", "optimizer", ".", "state_dict", "(", ")", "if", "self", ".", "is_global_zero", "else", "{", "}", "return", "optimizer", ".", "state_dict", "(", ")"], "docstring": "Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom plugins.", "docstring_tokens": ["returns", "state", "of", "an", "optimizer", "allows", "for", "syncing", "collating", "optimizer", "state", "from", "processes", "in", "custom", "plugins"], "docstring_summary": "Returns state of an optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 292, "end_line": 305, "hash": "d5cb4b171aab647672978bf855caddf7", "complexity": 3, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "load_checkpoint", "original_string": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: Can be one of:\r\n\r\n                - A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                - ``None`` or the empty dict: The loaded checkpoint will be returned in full.\r\n                - A :class:`~torch.nn.Module` instance, if the checkpoint file contains a raw module state dict.\r\n                - A :class:`~torch.optim.Optimizer` instance, if the checkpoint file contains a raw optimizer state.\r\n\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        \"\"\"\r\n        torch.cuda.empty_cache()\r\n        checkpoint = self.checkpoint_io.load_checkpoint(path)\r\n        if not state:\r\n            return checkpoint\r\n\r\n        if isinstance(state, Module):\r\n            self.load_module_state_dict(module=state, state_dict=checkpoint, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            state.load_state_dict(checkpoint)\r\n            return {}\r\n\r\n        _validate_keys_for_strict_loading(state.keys(), checkpoint.keys(), strict=strict)\r\n        for name, obj in state.copy().items():\r\n            if name not in checkpoint:\r\n                continue\r\n            if isinstance(obj, _Stateful):\r\n                if isinstance(obj, Module):\r\n                    self.load_module_state_dict(module=obj, state_dict=checkpoint.pop(name), strict=strict)\r\n                else:\r\n                    obj.load_state_dict(checkpoint.pop(name))\r\n            else:\r\n                state[name] = checkpoint.pop(name)\r\n        return checkpoint", "language": "python", "code": "def load_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: Optional[Union[Module, Optimizer, dict[str, Union[Module, Optimizer, Any]]]] = None,\r\n        strict: bool = True,\r\n    ) -> dict[str, Any]:\r\n        \"\"\"Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: Can be one of:\r\n\r\n                - A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                - ``None`` or the empty dict: The loaded checkpoint will be returned in full.\r\n                - A :class:`~torch.nn.Module` instance, if the checkpoint file contains a raw module state dict.\r\n                - A :class:`~torch.optim.Optimizer` instance, if the checkpoint file contains a raw optimizer state.\r\n\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.\r\n\r\n        \"\"\"\r\n        torch.cuda.empty_cache()\r\n        checkpoint = self.checkpoint_io.load_checkpoint(path)\r\n        if not state:\r\n            return checkpoint\r\n\r\n        if isinstance(state, Module):\r\n            self.load_module_state_dict(module=state, state_dict=checkpoint, strict=strict)\r\n            return {}\r\n\r\n        if isinstance(state, Optimizer):\r\n            state.load_state_dict(checkpoint)\r\n            return {}\r\n\r\n        _validate_keys_for_strict_loading(state.keys(), checkpoint.keys(), strict=strict)\r\n        for name, obj in state.copy().items():\r\n            if name not in checkpoint:\r\n                continue\r\n            if isinstance(obj, _Stateful):\r\n                if isinstance(obj, Module):\r\n                    self.load_module_state_dict(module=obj, state_dict=checkpoint.pop(name), strict=strict)\r\n                else:\r\n                    obj.load_state_dict(checkpoint.pop(name))\r\n            else:\r\n                state[name] = checkpoint.pop(name)\r\n        return checkpoint", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "Optional", "[", "Union", "[", "Module", ",", "Optimizer", ",", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "strict", ":", "bool", "=", "True", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects", ".", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "is", "located", "state", ":", "Can", "be", "one", "of", ":", "-", "A", "dictionary", "of", "objects", "whose", "state", "will", "be", "restored", "in", "-", "place", "from", "the", "checkpoint", "path", ".", "-", "`", "`", "None", "`", "`", "or", "the", "empty", "dict", ":", "The", "loaded", "checkpoint", "will", "be", "returned", "in", "full", ".", "-", "A", ":", "class", ":", "`", "~", "torch", ".", "nn", ".", "Module", "`", "instance", ",", "if", "the", "checkpoint", "file", "contains", "a", "raw", "module", "state", "dict", ".", "-", "A", ":", "class", ":", "`", "~", "torch", ".", "optim", ".", "Optimizer", "`", "instance", ",", "if", "the", "checkpoint", "file", "contains", "a", "raw", "optimizer", "state", ".", "strict", ":", "Whether", "to", "enforce", "that", "the", "keys", "in", "`", "state", "`", "match", "the", "keys", "in", "the", "checkpoint", ".", "Returns", ":", "The", "remaining", "items", "that", "were", "not", "restored", "into", "the", "given", "state", "dictionary", ".", "If", "no", "state", "dictionary", "is", "given", ",", "the", "full", "checkpoint", "will", "be", "returned", ".", "\"", "\"", "\"", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "checkpoint", "=", "self", ".", "checkpoint_io", ".", "load_checkpoint", "(", "path", ")", "if", "not", "state", ":", "return", "checkpoint", "if", "isinstance", "(", "state", ",", "Module", ")", ":", "self", ".", "load_module_state_dict", "(", "module", "=", "state", ",", "state_dict", "=", "checkpoint", ",", "strict", "=", "strict", ")", "return", "{", "}", "if", "isinstance", "(", "state", ",", "Optimizer", ")", ":", "state", ".", "load_state_dict", "(", "checkpoint", ")", "return", "{", "}", "_validate_keys_for_strict_loading", "(", "state", ".", "keys", "(", ")", ",", "checkpoint", ".", "keys", "(", ")", ",", "strict", "=", "strict", ")", "for", "name", ",", "obj", "in", "state", ".", "copy", "(", ")", ".", "items", "(", ")", ":", "if", "name", "not", "in", "checkpoint", ":", "continue", "if", "isinstance", "(", "obj", ",", "_Stateful", ")", ":", "if", "isinstance", "(", "obj", ",", "Module", ")", ":", "self", ".", "load_module_state_dict", "(", "module", "=", "obj", ",", "state_dict", "=", "checkpoint", ".", "pop", "(", "name", ")", ",", "strict", "=", "strict", ")", "else", ":", "obj", ".", "load_state_dict", "(", "checkpoint", ".", "pop", "(", "name", ")", ")", "else", ":", "state", "[", "name", "]", "=", "checkpoint", ".", "pop", "(", "name", ")", "return", "checkpoint"], "docstring": "Load the contents from a checkpoint and restore the state of the given objects.\r\n\r\n        Args:\r\n            path: A path to where the file is located\r\n            state: Can be one of:\r\n\r\n                - A dictionary of objects whose state will be restored in-place from the checkpoint path.\r\n                - ``None`` or the empty dict: The loaded checkpoint will be returned in full.\r\n                - A :class:`~torch.nn.Module` instance, if the checkpoint file contains a raw module state dict.\r\n                - A :class:`~torch.optim.Optimizer` instance, if the checkpoint file contains a raw optimizer state.\r\n\r\n            strict: Whether to enforce that the keys in `state` match the keys in the checkpoint.\r\n\r\n        Returns:\r\n            The remaining items that were not restored into the given state dictionary. If no state dictionary is\r\n            given, the full checkpoint will be returned.", "docstring_tokens": ["load", "the", "contents", "from", "a", "checkpoint", "and", "restore", "the", "state", "of", "the", "given", "objects", "args", "path", "a", "path", "to", "where", "the", "file", "is", "located", "state", "can", "be", "one", "of", "a", "dictionary", "of", "objects", "whose", "state", "will", "be", "restored", "in", "place", "from", "the", "checkpoint", "path", "none", "or", "the", "empty", "dict", "the", "loaded", "checkpoint", "will", "be", "returned", "in", "full", "a", "class", "torch", "nn", "module", "instance", "if", "the", "checkpoint", "file", "contains", "a", "raw", "module", "state", "dict", "a", "class", "torch", "optim", "optimizer", "instance", "if", "the", "checkpoint", "file", "contains", "a", "raw", "optimizer", "state", "strict", "whether", "to", "enforce", "that", "the", "keys", "in", "state", "match", "the", "keys", "in", "the", "checkpoint", "returns", "the", "remaining", "items", "that", "were", "not", "restored", "into", "the", "given", "state", "dictionary", "if", "no", "state", "dictionary", "is", "given", "the", "full", "checkpoint", "will", "be", "returned"], "docstring_summary": "Load the contents from a checkpoint and restore the state of the given objects.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 307, "end_line": 355, "hash": "fb1c34a8be524471c88a345279bfba66", "complexity": 8, "parameters": ["path", "state", "Optimizer", "dict[str", "Union[Module", "Optimizer", "Any]]]]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "teardown", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        self.precision.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.\r\n\r\n        \"\"\"\r\n        self.precision.teardown()\r\n        assert self.accelerator is not None\r\n        self.accelerator.teardown()\r\n        self.checkpoint_io.teardown()", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "method", "is", "called", "to", "teardown", "the", "training", "process", ".", "It", "is", "the", "right", "place", "to", "release", "memory", "and", "free", "other", "resources", ".", "\"", "\"", "\"", "self", ".", "precision", ".", "teardown", "(", ")", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "teardown", "(", ")", "self", ".", "checkpoint_io", ".", "teardown", "(", ")"], "docstring": "This method is called to teardown the training process.\r\n\r\n        It is the right place to release memory and free other resources.", "docstring_tokens": ["this", "method", "is", "called", "to", "teardown", "the", "training", "process", "it", "is", "the", "right", "place", "to", "release", "memory", "and", "free", "other", "resources"], "docstring_summary": "This method is called to teardown the training process.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 357, "end_line": 366, "hash": "019312eea9b1ca1efd56cec8351cfddd", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "clip_gradients_norm", "original_string": "def clip_gradients_norm(\r\n        self,\r\n        module: torch.nn.Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> torch.Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_norm_(\r\n            parameters, max_norm=max_norm, norm_type=norm_type, error_if_nonfinite=error_if_nonfinite\r\n        )", "language": "python", "code": "def clip_gradients_norm(\r\n        self,\r\n        module: torch.nn.Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> torch.Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_norm_(\r\n            parameters, max_norm=max_norm, norm_type=norm_type, error_if_nonfinite=error_if_nonfinite\r\n        )", "code_tokens": ["def", "clip_gradients_norm", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ",", "optimizer", ":", "Optimizer", ",", "max_norm", ":", "Union", "[", "float", ",", "int", "]", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "norm", ".", "\"", "\"", "\"", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "parameters", "=", "self", ".", "precision", ".", "main_params", "(", "optimizer", ")", "return", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ",", "error_if_nonfinite", "=", "error_if_nonfinite", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "docstring_summary": "Clip gradients by norm.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 368, "end_line": 381, "hash": "ecd0adb256d044b631014df5944ca9f2", "complexity": 1, "parameters": ["module", "optimizer", "max_norm", "int]", "norm_type", "int]", "error_if_nonfinite"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "clip_gradients_value", "original_string": "def clip_gradients_value(self, module: torch.nn.Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "language": "python", "code": "def clip_gradients_value(self, module: torch.nn.Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        parameters = self.precision.main_params(optimizer)\r\n        return torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "code_tokens": ["def", "clip_gradients_value", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "float", ",", "int", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "value", ".", "\"", "\"", "\"", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "parameters", "=", "self", ".", "precision", ".", "main_params", "(", "optimizer", ")", "return", "torch", ".", "nn", ".", "utils", ".", "clip_grad_value_", "(", "parameters", ",", "clip_value", "=", "clip_val", ")"], "docstring": "Clip gradients by value.", "docstring_tokens": ["clip", "gradients", "by", "value"], "docstring_summary": "Clip gradients by value.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 383, "end_line": 387, "hash": "b4c129fcf3feaa0f031c17c7b3b09172", "complexity": 1, "parameters": ["module", "optimizer", "clip_val", "int]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "no_backward_sync", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks the synchronization of gradients during the backward pass.\r\n\r\n        This is a context manager. It is only effective if it wraps a call to `.backward()`.\r\n\r\n        \"\"\"", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks the synchronization of gradients during the backward pass.\r\n\r\n        This is a context manager. It is only effective if it wraps a call to `.backward()`.\r\n\r\n        \"\"\"", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Blocks", "the", "synchronization", "of", "gradients", "during", "the", "backward", "pass", ".", "This", "is", "a", "context", "manager", ".", "It", "is", "only", "effective", "if", "it", "wraps", "a", "call", "to", "`", ".", "backward", "(", ")", "`", ".", "\"", "\"", "\""], "docstring": "Blocks the synchronization of gradients during the backward pass.\r\n\r\n        This is a context manager. It is only effective if it wraps a call to `.backward()`.", "docstring_tokens": ["blocks", "the", "synchronization", "of", "gradients", "during", "the", "backward", "pass", "this", "is", "a", "context", "manager", "it", "is", "only", "effective", "if", "it", "wraps", "a", "call", "to", "backward"], "docstring_summary": "Blocks the synchronization of gradients during the backward pass.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "_BackwardSyncControl", "start_line": 427, "end_line": 432, "hash": "2f490d1da72dda07bd86c3869a23999d", "complexity": 1, "parameters": ["module", "enabled"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\strategy.py", "func_name": "module_sharded_context", "original_string": "def module_sharded_context(self) -> AbstractContextManager:\r\n        \"\"\"A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of\r\n        parameters on creation.\r\n\r\n        By sharding layers directly on instantiation, one can reduce peak memory usage and initialization time.\r\n\r\n        \"\"\"", "language": "python", "code": "def module_sharded_context(self) -> AbstractContextManager:\r\n        \"\"\"A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of\r\n        parameters on creation.\r\n\r\n        By sharding layers directly on instantiation, one can reduce peak memory usage and initialization time.\r\n\r\n        \"\"\"", "code_tokens": ["def", "module_sharded_context", "(", "self", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "A", "context", "manager", "that", "goes", "over", "the", "instantiation", "of", "an", ":", "class", ":", "`", "torch", ".", "nn", ".", "Module", "`", "and", "handles", "sharding", "of", "parameters", "on", "creation", ".", "By", "sharding", "layers", "directly", "on", "instantiation", ",", "one", "can", "reduce", "peak", "memory", "usage", "and", "initialization", "time", ".", "\"", "\"", "\""], "docstring": "A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of\r\n        parameters on creation.\r\n\r\n        By sharding layers directly on instantiation, one can reduce peak memory usage and initialization time.", "docstring_tokens": ["a", "context", "manager", "that", "goes", "over", "the", "instantiation", "of", "an", "class", "torch", "nn", "module", "and", "handles", "sharding", "of", "parameters", "on", "creation", "by", "sharding", "layers", "directly", "on", "instantiation", "one", "can", "reduce", "peak", "memory", "usage", "and", "initialization", "time"], "docstring_summary": "A context manager that goes over the instantiation of an :class:`torch.nn.Module` and handles sharding of", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "_Sharded", "start_line": 439, "end_line": 445, "hash": "4916192d68212baf287fb1242eb16dca", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla.py", "func_name": "all_gather", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", ".", "Args", ":", "tensor", ":", "tensor", "to", "all", "-", "gather", ".", "group", ":", "unused", ".", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all", "-", "gather", "operation", ".", "Return", ":", "A", "tensor", "of", "shape", "(", "world_size", ",", ".", ".", ".", ")", "\"", "\"", "\"", "if", "not", "self", ".", "_launched", ":", "return", "tensor", "if", "not", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "raise", "NotImplementedError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "all_gather", "`", "is", "only", "implemented", "for", "tensors", ".", "Given", "{", "tensor", "}", "\"", ")", "if", "tensor", ".", "dim", "(", ")", "=", "=", "0", ":", "tensor", "=", "tensor", ".", "unsqueeze", "(", "0", ")", "original_device", "=", "tensor", ".", "device", "tensor", "=", "tensor", ".", "to", "(", "self", ".", "root_device", ")", "import", "torch_xla", ".", "core", ".", "functions", "as", "xf", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "tensor", "=", "xf", ".", "all_gather", "(", "tensor", ")", "if", "sync_grads", "else", "xm", ".", "all_gather", "(", "tensor", ")", "tensor", "=", "tensor", ".", "to", "(", "original_device", ")", "return", "tensor"], "docstring": "Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", "args", "tensor", "tensor", "to", "all", "gather", "group", "unused", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all", "gather", "operation", "return", "a", "tensor", "of", "shape", "world_size"], "docstring_summary": "Function to gather a tensor from several distributed processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAStrategy", "start_line": 176, "end_line": 203, "hash": "f8b747ddf7e2b93d37aa05415f08aacb", "complexity": 5, "parameters": ["tensor", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary of the same format as ``state`` mapping keys to callables that return a\r\n                boolean indicating whether the given parameter should be saved (``True``) or filtered out (``False``).\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        # sync any pending lazy tensors on all ranks before saving to prevent potential collective hangs\r\n        xm.mark_step()\r\n        # save on global rank zero only\r\n        super().save_checkpoint(path, state, storage_options=storage_options, filter=filter)", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary of the same format as ``state`` mapping keys to callables that return a\r\n                boolean indicating whether the given parameter should be saved (``True``) or filtered out (``False``).\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        # sync any pending lazy tensors on all ranks before saving to prevent potential collective hangs\r\n        xm.mark_step()\r\n        # save on global rank zero only\r\n        super().save_checkpoint(path, state, storage_options=storage_options, filter=filter)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", ",", "optimizer", ",", "and", "other", "state", "as", "a", "checkpoint", "file", ".", "Args", ":", "path", ":", "A", "path", "to", "where", "the", "file", "(", "s", ")", "should", "be", "saved", "state", ":", "A", "dictionary", "with", "contents", "to", "be", "saved", ".", "If", "the", "dict", "contains", "modules", "or", "optimizers", ",", "their", "state", "-", "dict", "will", "be", "retrieved", "and", "converted", "automatically", ".", "storage_options", ":", "Additional", "options", "for", "the", "`", "`", "CheckpointIO", "`", "`", "plugin", "filter", ":", "An", "optional", "dictionary", "of", "the", "same", "format", "as", "`", "`", "state", "`", "`", "mapping", "keys", "to", "callables", "that", "return", "a", "boolean", "indicating", "whether", "the", "given", "parameter", "should", "be", "saved", "(", "`", "`", "True", "`", "`", ")", "or", "filtered", "out", "(", "`", "`", "False", "`", "`", ")", ".", "\"", "\"", "\"", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "xm", ".", "mark_step", "(", ")", "super", "(", ")", ".", "save_checkpoint", "(", "path", ",", "state", ",", "storage_options", "=", "storage_options", ",", "filter", "=", "filter", ")"], "docstring": "Save model, optimizer, and other state as a checkpoint file.\r\n\r\n        Args:\r\n            path: A path to where the file(s) should be saved\r\n            state: A dictionary with contents to be saved. If the dict contains modules or optimizers, their\r\n                state-dict will be retrieved and converted automatically.\r\n            storage_options: Additional options for the ``CheckpointIO`` plugin\r\n            filter: An optional dictionary of the same format as ``state`` mapping keys to callables that return a\r\n                boolean indicating whether the given parameter should be saved (``True``) or filtered out (``False``).", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "as", "a", "checkpoint", "file", "args", "path", "a", "path", "to", "where", "the", "file", "s", "should", "be", "saved", "state", "a", "dictionary", "with", "contents", "to", "be", "saved", "if", "the", "dict", "contains", "modules", "or", "optimizers", "their", "state", "dict", "will", "be", "retrieved", "and", "converted", "automatically", "storage_options", "additional", "options", "for", "the", "checkpointio", "plugin", "filter", "an", "optional", "dictionary", "of", "the", "same", "format", "as", "state", "mapping", "keys", "to", "callables", "that", "return", "a", "boolean", "indicating", "whether", "the", "given", "parameter", "should", "be", "saved", "true", "or", "filtered", "out", "false"], "docstring_summary": "Save model, optimizer, and other state as a checkpoint file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAStrategy", "start_line": 275, "end_line": 298, "hash": "3f7379b411d46ad2dfb4f61d6968b63e", "complexity": 1, "parameters": ["path", "state", "Union[Module", "Optimizer", "Any]]", "storage_options", "filter", "Callable[[str", "Any]", "bool]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "setup_module_and_optimizers", "original_string": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.\"\"\"\r\n        raise NotImplementedError(\r\n            f\"The `{type(self).__name__}` does not support the joint setup of module and optimizer(s).\"\r\n            \" Please do it in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n            \" call `setup_optimizer`.\"\r\n        )", "language": "python", "code": "def setup_module_and_optimizers(\r\n        self, module: Module, optimizers: list[Optimizer], scheduler: Optional[\"_LRScheduler\"] = None\r\n    ) -> tuple[Module, list[Optimizer], Optional[\"_LRScheduler\"]]:\r\n        \"\"\"Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.\"\"\"\r\n        raise NotImplementedError(\r\n            f\"The `{type(self).__name__}` does not support the joint setup of module and optimizer(s).\"\r\n            \" Please do it in this order: Create the model, call `setup_module`, create the optimizer,\"\r\n            \" call `setup_optimizer`.\"\r\n        )", "code_tokens": ["def", "setup_module_and_optimizers", "(", "self", ",", "module", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "scheduler", ":", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "=", "None", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "Optional", "[", "\"", "_LRScheduler", "\"", "]", "]", ":", "\"", "\"", "\"", "Returns", "NotImplementedError", "since", "for", "XLAFSDP", "optimizer", "setup", "must", "happen", "after", "module", "setup", ".", "\"", "\"", "\"", "raise", "NotImplementedError", "(", "f", "\"", "The", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", "`", "does", "not", "support", "the", "joint", "setup", "of", "module", "and", "optimizer", "(", "s", ")", ".", "\"", "\"", "Please", "do", "it", "in", "this", "order", ":", "Create", "the", "model", ",", "call", "`", "setup_module", "`", ",", "create", "the", "optimizer", ",", "\"", "\"", "call", "`", "setup_optimizer", "`", ".", "\"", ")"], "docstring": "Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.", "docstring_tokens": ["returns", "notimplementederror", "since", "for", "xlafsdp", "optimizer", "setup", "must", "happen", "after", "module", "setup"], "docstring_summary": "Returns NotImplementedError since for XLAFSDP optimizer setup must happen after module setup.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 198, "end_line": 206, "hash": "9294572724df084622ba1814a6cfcad0", "complexity": 1, "parameters": ["module", "optimizers", "scheduler"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "setup_optimizer", "original_string": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with XLAFSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if any(getattr(p, \"_is_sharded\", False) for group in optimizer.param_groups for p in group[\"params\"]):\r\n            return optimizer\r\n        raise ValueError(\r\n            \"The optimizer does not seem to reference any XLAFSDP parameters. HINT: Make sure to create the optimizer\"\r\n            \" after setting up the model.\"\r\n        )", "language": "python", "code": "def setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Set up an optimizer for a model wrapped with XLAFSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.\r\n\r\n        \"\"\"\r\n        if any(getattr(p, \"_is_sharded\", False) for group in optimizer.param_groups for p in group[\"params\"]):\r\n            return optimizer\r\n        raise ValueError(\r\n            \"The optimizer does not seem to reference any XLAFSDP parameters. HINT: Make sure to create the optimizer\"\r\n            \" after setting up the model.\"\r\n        )", "code_tokens": ["def", "setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "\"", "\"", "\"", "Set", "up", "an", "optimizer", "for", "a", "model", "wrapped", "with", "XLAFSDP", ".", "This", "setup", "method", "doesn", "'", "t", "modify", "the", "optimizer", "or", "wrap", "the", "optimizer", ".", "The", "only", "thing", "it", "currently", "does", "is", "verify", "that", "the", "optimizer", "was", "created", "after", "the", "model", "was", "wrapped", "with", ":", "meth", ":", "`", "setup_module", "`", "with", "a", "reference", "to", "the", "flattened", "parameters", ".", "\"", "\"", "\"", "if", "any", "(", "getattr", "(", "p", ",", "\"", "_is_sharded", "\"", ",", "False", ")", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "\"", "params", "\"", "]", ")", ":", "return", "optimizer", "raise", "ValueError", "(", "\"", "The", "optimizer", "does", "not", "seem", "to", "reference", "any", "XLAFSDP", "parameters", ".", "HINT", ":", "Make", "sure", "to", "create", "the", "optimizer", "\"", "\"", "after", "setting", "up", "the", "model", ".", "\"", ")"], "docstring": "Set up an optimizer for a model wrapped with XLAFSDP.\r\n\r\n        This setup method doesn't modify the optimizer or wrap the optimizer. The only thing it currently does is verify\r\n        that the optimizer was created after the model was wrapped with :meth:`setup_module` with a reference to the\r\n        flattened parameters.", "docstring_tokens": ["set", "up", "an", "optimizer", "for", "a", "model", "wrapped", "with", "xlafsdp", "this", "setup", "method", "doesn", "t", "modify", "the", "optimizer", "or", "wrap", "the", "optimizer", "the", "only", "thing", "it", "currently", "does", "is", "verify", "that", "the", "optimizer", "was", "created", "after", "the", "model", "was", "wrapped", "with", "meth", "setup_module", "with", "a", "reference", "to", "the", "flattened", "parameters"], "docstring_summary": "Set up an optimizer for a model wrapped with XLAFSDP.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 256, "end_line": 269, "hash": "117f14bef80c62950527ac7f468c7130", "complexity": 4, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(self, optimizer: Optimizable, **kwargs: Any) -> Any:\r\n        \"\"\"Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.\r\n        Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        loss = optimizer.step(**kwargs)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n        return loss", "language": "python", "code": "def optimizer_step(self, optimizer: Optimizable, **kwargs: Any) -> Any:\r\n        \"\"\"Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.\r\n        Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        loss = optimizer.step(**kwargs)\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        xm.mark_step()\r\n        return loss", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizable", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Overrides", "default", "tpu", "optimizer_step", "since", "FSDP", "should", "not", "call", "`", "torch_xla", ".", "core", ".", "xla_model", ".", "optimizer_step", "`", ".", "Performs", "the", "actual", "optimizer", "step", ".", "Args", ":", "optimizer", ":", "the", "optimizer", "performing", "the", "step", "*", "*", "kwargs", ":", "Any", "extra", "arguments", "to", "`", "`", "optimizer", ".", "step", "`", "`", "\"", "\"", "\"", "loss", "=", "optimizer", ".", "step", "(", "*", "*", "kwargs", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "xm", ".", "mark_step", "(", ")", "return", "loss"], "docstring": "Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.\r\n        Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            **kwargs: Any extra arguments to ``optimizer.step``", "docstring_tokens": ["overrides", "default", "tpu", "optimizer_step", "since", "fsdp", "should", "not", "call", "torch_xla", "core", "xla_model", "optimizer_step", "performs", "the", "actual", "optimizer", "step", "args", "optimizer", "the", "optimizer", "performing", "the", "step", "kwargs", "any", "extra", "arguments", "to", "optimizer", "step"], "docstring_summary": "Overrides default tpu optimizer_step since FSDP should not call `torch_xla.core.xla_model.optimizer_step`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 272, "end_line": 285, "hash": "df172163fb3da7542b87492a5a9896da", "complexity": 1, "parameters": ["optimizer", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "clip_gradients_norm", "original_string": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        assert callable(module.clip_grad_norm_)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "language": "python", "code": "def clip_gradients_norm(\r\n        self,\r\n        module: Module,\r\n        optimizer: Optimizer,\r\n        max_norm: Union[float, int],\r\n        norm_type: Union[float, int] = 2.0,\r\n        error_if_nonfinite: bool = True,\r\n    ) -> Tensor:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        self.precision.unscale_gradients(optimizer)\r\n        assert callable(module.clip_grad_norm_)\r\n        return module.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)", "code_tokens": ["def", "clip_gradients_norm", "(", "self", ",", "module", ":", "Module", ",", "optimizer", ":", "Optimizer", ",", "max_norm", ":", "Union", "[", "float", ",", "int", "]", ",", "norm_type", ":", "Union", "[", "float", ",", "int", "]", "=", "2", ".", "0", ",", "error_if_nonfinite", ":", "bool", "=", "True", ",", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "norm", ".", "\"", "\"", "\"", "self", ".", "precision", ".", "unscale_gradients", "(", "optimizer", ")", "assert", "callable", "(", "module", ".", "clip_grad_norm_", ")", "return", "module", ".", "clip_grad_norm_", "(", "max_norm", "=", "max_norm", ",", "norm_type", "=", "norm_type", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "docstring_summary": "Clip gradients by norm.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 288, "end_line": 299, "hash": "ef00d61f8d9be4a2209d5a72694ef0da", "complexity": 1, "parameters": ["module", "optimizer", "max_norm", "int]", "norm_type", "int]", "error_if_nonfinite"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "clip_gradients_value", "original_string": "def clip_gradients_value(self, module: Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        raise NotImplementedError(\r\n            \"XLA's FSDP strategy does not support to clip gradients by value.\"\r\n            \" Consider clipping by norm instead or choose another strategy!\"\r\n        )", "language": "python", "code": "def clip_gradients_value(self, module: Module, optimizer: Optimizer, clip_val: Union[float, int]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        raise NotImplementedError(\r\n            \"XLA's FSDP strategy does not support to clip gradients by value.\"\r\n            \" Consider clipping by norm instead or choose another strategy!\"\r\n        )", "code_tokens": ["def", "clip_gradients_value", "(", "self", ",", "module", ":", "Module", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "float", ",", "int", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "value", ".", "\"", "\"", "\"", "raise", "NotImplementedError", "(", "\"", "XLA", "'", "s", "FSDP", "strategy", "does", "not", "support", "to", "clip", "gradients", "by", "value", ".", "\"", "\"", "Consider", "clipping", "by", "norm", "instead", "or", "choose", "another", "strategy", "!", "\"", ")"], "docstring": "Clip gradients by value.", "docstring_tokens": ["clip", "gradients", "by", "value"], "docstring_summary": "Clip gradients by value.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 302, "end_line": 307, "hash": "c68cab03360216a433d03e0ed0d7b145", "complexity": 1, "parameters": ["module", "optimizer", "clip_val", "int]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "all_gather", "original_string": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "language": "python", "code": "def all_gather(self, tensor: Tensor, group: Optional[Any] = None, sync_grads: bool = False) -> Tensor:\r\n        \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)\r\n\r\n        \"\"\"\r\n        if not self._launched:\r\n            return tensor\r\n        if not isinstance(tensor, Tensor):\r\n            raise NotImplementedError(\r\n                f\"`{type(self).__name__}.all_gather` is only implemented for tensors. Given {tensor}\"\r\n            )\r\n        if tensor.dim() == 0:\r\n            tensor = tensor.unsqueeze(0)\r\n        original_device = tensor.device\r\n        tensor = tensor.to(self.root_device)\r\n\r\n        import torch_xla.core.functions as xf\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        tensor = xf.all_gather(tensor) if sync_grads else xm.all_gather(tensor)\r\n        tensor = tensor.to(original_device)\r\n        return tensor", "code_tokens": ["def", "all_gather", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", ".", "Args", ":", "tensor", ":", "tensor", "to", "all", "-", "gather", ".", "group", ":", "unused", ".", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all", "-", "gather", "operation", ".", "Return", ":", "A", "tensor", "of", "shape", "(", "world_size", ",", ".", ".", ".", ")", "\"", "\"", "\"", "if", "not", "self", ".", "_launched", ":", "return", "tensor", "if", "not", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "raise", "NotImplementedError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "all_gather", "`", "is", "only", "implemented", "for", "tensors", ".", "Given", "{", "tensor", "}", "\"", ")", "if", "tensor", ".", "dim", "(", ")", "=", "=", "0", ":", "tensor", "=", "tensor", ".", "unsqueeze", "(", "0", ")", "original_device", "=", "tensor", ".", "device", "tensor", "=", "tensor", ".", "to", "(", "self", ".", "root_device", ")", "import", "torch_xla", ".", "core", ".", "functions", "as", "xf", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "tensor", "=", "xf", ".", "all_gather", "(", "tensor", ")", "if", "sync_grads", "else", "xm", ".", "all_gather", "(", "tensor", ")", "tensor", "=", "tensor", ".", "to", "(", "original_device", ")", "return", "tensor"], "docstring": "Function to gather a tensor from several distributed processes.\r\n\r\n        Args:\r\n            tensor: tensor to all-gather.\r\n            group: unused.\r\n            sync_grads: flag that allows users to synchronize gradients for the all-gather operation.\r\n        Return:\r\n            A tensor of shape (world_size, ...)", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", "args", "tensor", "tensor", "to", "all", "gather", "group", "unused", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all", "gather", "operation", "return", "a", "tensor", "of", "shape", "world_size"], "docstring_summary": "Function to gather a tensor from several distributed processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 310, "end_line": 337, "hash": "f8b747ddf7e2b93d37aa05415f08aacb", "complexity": 5, "parameters": ["tensor", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in the provided checkpoint directory.\r\n\r\n        If the user specifies sharded checkpointing, the directory will contain one file per process, with model- and\r\n        optimizer shards stored per file. If the user specifies full checkpointing, the directory will contain a\r\n        consolidated checkpoint combining all of the sharded checkpoints.\r\n\r\n        \"\"\"\r\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\r\n        path = Path(self.broadcast(path))\r\n        if path.is_dir() and any(path.iterdir()):\r\n            raise FileExistsError(f\"The checkpoint directory already exists and is not empty: {path}\")\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        modules = [module for module in state.values() if isinstance(module, XLAFSDP)]\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a XLAFSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple XLAFSDP modules in the given state. Saving checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        # ensure model parameters are updated\r\n        xm.mark_step()\r\n\r\n        parallel_devices = self.parallel_devices\r\n        assert parallel_devices is not None\r\n        if self._sequential_save:\r\n            # each host runs this in parallel, but the ranks in the host run it sequentially\r\n            for rank in range(len(parallel_devices)):\r\n                if rank == self.local_rank:\r\n                    self._save_checkpoint_shard(path, state, storage_options, filter)\r\n                self.barrier(f\"wait-for-{rank}-save\")\r\n        else:\r\n            self._save_checkpoint_shard(path, state, storage_options, filter)\r\n\r\n        if self._state_dict_type == \"full\":\r\n            ckpt_prefix = str(path / \"checkpoint\")\r\n            ckpt_suffix = \"_rank-*-of-*.pth\"\r\n            if len(parallel_devices) != self.world_size:  # multihost\r\n                raise OSError(\r\n                    \"Multihost setups do not have a shared filesystem, so the checkpoint shards cannot be consolidated\"\r\n                    \" into a single checkpoint after saving them. Please switch to\"\r\n                    \" `XLAFSDPStrategy(state_dict_type='sharded')`. TIP: You can consolidate them manually by getting\"\r\n                    \" them together into a single directory and running `python -m\"\r\n                    f\" torch_xla.distributed.fsdp.consolidate_sharded_ckpts --ckpt_prefix {ckpt_prefix!r} --ckpt_suffix\"\r\n                    f\" {ckpt_suffix!r} --save_path 'path/to/consolidated.ckpt'`.\"\r\n                )\r\n\r\n            from torch_xla.distributed.fsdp import consolidate_sharded_model_checkpoints\r\n\r\n            self.barrier(\"before_ckpt_consolidation\")\r\n            if self.is_global_zero:\r\n                save_path = path.parent / \"consolidated.ckpt\"\r\n                # save consolidated checkpoint separate to the shards\r\n                consolidate_sharded_model_checkpoints(ckpt_prefix, ckpt_suffix, str(save_path))\r\n                # remove the shards directory\r\n                self.checkpoint_io.remove_checkpoint(path)\r\n                # mv the consolidated checkpoint where the user would expect it\r\n                get_filesystem(save_path).mv(str(save_path), str(path))\r\n            self.barrier(\"after_ckpt_consolidation\")", "language": "python", "code": "def save_checkpoint(\r\n        self,\r\n        path: _PATH,\r\n        state: dict[str, Union[Module, Optimizer, Any]],\r\n        storage_options: Optional[Any] = None,\r\n        filter: Optional[dict[str, Callable[[str, Any], bool]]] = None,\r\n    ) -> None:\r\n        \"\"\"Save model, optimizer, and other state in the provided checkpoint directory.\r\n\r\n        If the user specifies sharded checkpointing, the directory will contain one file per process, with model- and\r\n        optimizer shards stored per file. If the user specifies full checkpointing, the directory will contain a\r\n        consolidated checkpoint combining all of the sharded checkpoints.\r\n\r\n        \"\"\"\r\n        # broadcast the path from rank 0 to ensure all the states are saved in a common path\r\n        path = Path(self.broadcast(path))\r\n        if path.is_dir() and any(path.iterdir()):\r\n            raise FileExistsError(f\"The checkpoint directory already exists and is not empty: {path}\")\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        modules = [module for module in state.values() if isinstance(module, XLAFSDP)]\r\n        if len(modules) == 0:\r\n            raise ValueError(\r\n                \"Could not find a XLAFSDP model in the provided checkpoint state. Please provide the model as\"\r\n                \" part of the state like so: `save_checkpoint(..., state={'model': model, ...})`. Make sure\"\r\n                \" you set up the model (and optimizers if any) through the strategy before saving the checkpoint.\"\r\n            )\r\n        if len(modules) > 1:\r\n            raise ValueError(\r\n                \"Found multiple XLAFSDP modules in the given state. Saving checkpoints with FSDP is\"\r\n                \" currently limited to a single model per checkpoint. To save multiple models, call the\"\r\n                \" save method for each model separately with a different path.\"\r\n            )\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        # ensure model parameters are updated\r\n        xm.mark_step()\r\n\r\n        parallel_devices = self.parallel_devices\r\n        assert parallel_devices is not None\r\n        if self._sequential_save:\r\n            # each host runs this in parallel, but the ranks in the host run it sequentially\r\n            for rank in range(len(parallel_devices)):\r\n                if rank == self.local_rank:\r\n                    self._save_checkpoint_shard(path, state, storage_options, filter)\r\n                self.barrier(f\"wait-for-{rank}-save\")\r\n        else:\r\n            self._save_checkpoint_shard(path, state, storage_options, filter)\r\n\r\n        if self._state_dict_type == \"full\":\r\n            ckpt_prefix = str(path / \"checkpoint\")\r\n            ckpt_suffix = \"_rank-*-of-*.pth\"\r\n            if len(parallel_devices) != self.world_size:  # multihost\r\n                raise OSError(\r\n                    \"Multihost setups do not have a shared filesystem, so the checkpoint shards cannot be consolidated\"\r\n                    \" into a single checkpoint after saving them. Please switch to\"\r\n                    \" `XLAFSDPStrategy(state_dict_type='sharded')`. TIP: You can consolidate them manually by getting\"\r\n                    \" them together into a single directory and running `python -m\"\r\n                    f\" torch_xla.distributed.fsdp.consolidate_sharded_ckpts --ckpt_prefix {ckpt_prefix!r} --ckpt_suffix\"\r\n                    f\" {ckpt_suffix!r} --save_path 'path/to/consolidated.ckpt'`.\"\r\n                )\r\n\r\n            from torch_xla.distributed.fsdp import consolidate_sharded_model_checkpoints\r\n\r\n            self.barrier(\"before_ckpt_consolidation\")\r\n            if self.is_global_zero:\r\n                save_path = path.parent / \"consolidated.ckpt\"\r\n                # save consolidated checkpoint separate to the shards\r\n                consolidate_sharded_model_checkpoints(ckpt_prefix, ckpt_suffix, str(save_path))\r\n                # remove the shards directory\r\n                self.checkpoint_io.remove_checkpoint(path)\r\n                # mv the consolidated checkpoint where the user would expect it\r\n                get_filesystem(save_path).mv(str(save_path), str(path))\r\n            self.barrier(\"after_ckpt_consolidation\")", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "path", ":", "_PATH", ",", "state", ":", "dict", "[", "str", ",", "Union", "[", "Module", ",", "Optimizer", ",", "Any", "]", "]", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ",", "filter", ":", "Optional", "[", "dict", "[", "str", ",", "Callable", "[", "[", "str", ",", "Any", "]", ",", "bool", "]", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", ",", "optimizer", ",", "and", "other", "state", "in", "the", "provided", "checkpoint", "directory", ".", "If", "the", "user", "specifies", "sharded", "checkpointing", ",", "the", "directory", "will", "contain", "one", "file", "per", "process", ",", "with", "model", "-", "and", "optimizer", "shards", "stored", "per", "file", ".", "If", "the", "user", "specifies", "full", "checkpointing", ",", "the", "directory", "will", "contain", "a", "consolidated", "checkpoint", "combining", "all", "of", "the", "sharded", "checkpoints", ".", "\"", "\"", "\"", "path", "=", "Path", "(", "self", ".", "broadcast", "(", "path", ")", ")", "if", "path", ".", "is_dir", "(", ")", "and", "any", "(", "path", ".", "iterdir", "(", ")", ")", ":", "raise", "FileExistsError", "(", "f", "\"", "The", "checkpoint", "directory", "already", "exists", "and", "is", "not", "empty", ":", "{", "path", "}", "\"", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "XlaFullyShardedDataParallel", "as", "XLAFSDP", "modules", "=", "[", "module", "for", "module", "in", "state", ".", "values", "(", ")", "if", "isinstance", "(", "module", ",", "XLAFSDP", ")", "]", "if", "len", "(", "modules", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "\"", "Could", "not", "find", "a", "XLAFSDP", "model", "in", "the", "provided", "checkpoint", "state", ".", "Please", "provide", "the", "model", "as", "\"", "\"", "part", "of", "the", "state", "like", "so", ":", "`", "save_checkpoint", "(", ".", ".", ".", ",", "state", "=", "{", "'", "model", "'", ":", "model", ",", ".", ".", ".", "}", ")", "`", ".", "Make", "sure", "\"", "\"", "you", "set", "up", "the", "model", "(", "and", "optimizers", "if", "any", ")", "through", "the", "strategy", "before", "saving", "the", "checkpoint", ".", "\"", ")", "if", "len", "(", "modules", ")", ">", "1", ":", "raise", "ValueError", "(", "\"", "Found", "multiple", "XLAFSDP", "modules", "in", "the", "given", "state", ".", "Saving", "checkpoints", "with", "FSDP", "is", "\"", "\"", "currently", "limited", "to", "a", "single", "model", "per", "checkpoint", ".", "To", "save", "multiple", "models", ",", "call", "the", "\"", "\"", "save", "method", "for", "each", "model", "separately", "with", "a", "different", "path", ".", "\"", ")", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "xm", ".", "mark_step", "(", ")", "parallel_devices", "=", "self", ".", "parallel_devices", "assert", "parallel_devices", "is", "not", "None", "if", "self", ".", "_sequential_save", ":", "for", "rank", "in", "range", "(", "len", "(", "parallel_devices", ")", ")", ":", "if", "rank", "=", "=", "self", ".", "local_rank", ":", "self", ".", "_save_checkpoint_shard", "(", "path", ",", "state", ",", "storage_options", ",", "filter", ")", "self", ".", "barrier", "(", "f", "\"", "wait", "-", "for", "-", "{", "rank", "}", "-", "save", "\"", ")", "else", ":", "self", ".", "_save_checkpoint_shard", "(", "path", ",", "state", ",", "storage_options", ",", "filter", ")", "if", "self", ".", "_state_dict_type", "=", "=", "\"", "full", "\"", ":", "ckpt_prefix", "=", "str", "(", "path", "/", "\"", "checkpoint", "\"", ")", "ckpt_suffix", "=", "\"", "_rank", "-", "*", "-", "of", "-", "*", ".", "pth", "\"", "if", "len", "(", "parallel_devices", ")", "!", "=", "self", ".", "world_size", ":", "raise", "OSError", "(", "\"", "Multihost", "setups", "do", "not", "have", "a", "shared", "filesystem", ",", "so", "the", "checkpoint", "shards", "cannot", "be", "consolidated", "\"", "\"", "into", "a", "single", "checkpoint", "after", "saving", "them", ".", "Please", "switch", "to", "\"", "\"", "`", "XLAFSDPStrategy", "(", "state_dict_type", "=", "'", "sharded", "'", ")", "`", ".", "TIP", ":", "You", "can", "consolidate", "them", "manually", "by", "getting", "\"", "\"", "them", "together", "into", "a", "single", "directory", "and", "running", "`", "python", "-", "m", "\"", "f", "\"", "torch_xla", ".", "distributed", ".", "fsdp", ".", "consolidate_sharded_ckpts", "-", "-", "ckpt_prefix", "{", "ckpt_prefix", "!", "r", "}", "-", "-", "ckpt_suffix", "\"", "f", "\"", "{", "ckpt_suffix", "!", "r", "}", "-", "-", "save_path", "'", "path", "/", "to", "/", "consolidated", ".", "ckpt", "'", "`", ".", "\"", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "consolidate_sharded_model_checkpoints", "self", ".", "barrier", "(", "\"", "before_ckpt_consolidation", "\"", ")", "if", "self", ".", "is_global_zero", ":", "save_path", "=", "path", ".", "parent", "/", "\"", "consolidated", ".", "ckpt", "\"", "consolidate_sharded_model_checkpoints", "(", "ckpt_prefix", ",", "ckpt_suffix", ",", "str", "(", "save_path", ")", ")", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "path", ")", "get_filesystem", "(", "save_path", ")", ".", "mv", "(", "str", "(", "save_path", ")", ",", "str", "(", "path", ")", ")", "self", ".", "barrier", "(", "\"", "after_ckpt_consolidation", "\"", ")"], "docstring": "Save model, optimizer, and other state in the provided checkpoint directory.\r\n\r\n        If the user specifies sharded checkpointing, the directory will contain one file per process, with model- and\r\n        optimizer shards stored per file. If the user specifies full checkpointing, the directory will contain a\r\n        consolidated checkpoint combining all of the sharded checkpoints.", "docstring_tokens": ["save", "model", "optimizer", "and", "other", "state", "in", "the", "provided", "checkpoint", "directory", "if", "the", "user", "specifies", "sharded", "checkpointing", "the", "directory", "will", "contain", "one", "file", "per", "process", "with", "model", "and", "optimizer", "shards", "stored", "per", "file", "if", "the", "user", "specifies", "full", "checkpointing", "the", "directory", "will", "contain", "a", "consolidated", "checkpoint", "combining", "all", "of", "the", "sharded", "checkpoints"], "docstring_summary": "Save model, optimizer, and other state in the provided checkpoint directory.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "XLAFSDPStrategy", "start_line": 409, "end_line": 482, "hash": "aa4b31f3d91f77cd6bf45ad7c554f7c7", "complexity": 13, "parameters": ["path", "state", "Union[Module", "Optimizer", "Any]]", "storage_options", "filter", "Callable[[str", "Any]", "bool]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\xla_fsdp.py", "func_name": "no_backward_sync", "original_string": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        if not isinstance(module, XLAFSDP):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `XlaFullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "language": "python", "code": "def no_backward_sync(self, module: Module, enabled: bool) -> AbstractContextManager:\r\n        \"\"\"Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`\r\n        wrapper.\"\"\"\r\n        if not enabled:\r\n            return nullcontext()\r\n        from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as XLAFSDP\r\n\r\n        if not isinstance(module, XLAFSDP):\r\n            raise TypeError(\r\n                \"Blocking backward sync is only possible if the module passed to\"\r\n                f\" `{self.__class__.__name__}.no_backward_sync` is wrapped in `XlaFullyShardedDataParallel`.\"\r\n                f\" Got: {module.__class__.__name__}.\"\r\n            )\r\n        return module.no_sync()", "code_tokens": ["def", "no_backward_sync", "(", "self", ",", "module", ":", "Module", ",", "enabled", ":", "bool", ")", "-", ">", "AbstractContextManager", ":", "\"", "\"", "\"", "Blocks", "gradient", "synchronization", "inside", "the", ":", "class", ":", "`", "~", "torch_xla", ".", "distributed", ".", "fsdp", ".", "XlaFullyShardedDataParallel", "`", "wrapper", ".", "\"", "\"", "\"", "if", "not", "enabled", ":", "return", "nullcontext", "(", ")", "from", "torch_xla", ".", "distributed", ".", "fsdp", "import", "XlaFullyShardedDataParallel", "as", "XLAFSDP", "if", "not", "isinstance", "(", "module", ",", "XLAFSDP", ")", ":", "raise", "TypeError", "(", "\"", "Blocking", "backward", "sync", "is", "only", "possible", "if", "the", "module", "passed", "to", "\"", "f", "\"", "`", "{", "self", ".", "__class__", ".", "__name__", "}", ".", "no_backward_sync", "`", "is", "wrapped", "in", "`", "XlaFullyShardedDataParallel", "`", ".", "\"", "f", "\"", "Got", ":", "{", "module", ".", "__class__", ".", "__name__", "}", ".", "\"", ")", "return", "module", ".", "no_sync", "(", ")"], "docstring": "Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`\r\n        wrapper.", "docstring_tokens": ["blocks", "gradient", "synchronization", "inside", "the", "class", "torch_xla", "distributed", "fsdp", "xlafullyshardeddataparallel", "wrapper"], "docstring_summary": "Blocks gradient synchronization inside the :class:`~torch_xla.distributed.fsdp.XlaFullyShardedDataParallel`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\xla_fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "_XLAFSDPBackwardSyncControl", "start_line": 672, "end_line": 685, "hash": "c06e32c742d249f87c4b1cb24affff76", "complexity": 3, "parameters": ["module", "enabled"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "launch", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n\r\n        # The default cluster environment in Lightning chooses a random free port number\r\n        # This needs to be done in the main process here before starting processes to ensure each rank will connect\r\n        # through the same port\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [function, args, kwargs, return_queue]\r\n\r\n        mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n        )\r\n        return return_queue.get()", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        if self._start_method in (\"fork\", \"forkserver\"):\r\n            _check_bad_cuda_fork()\r\n        if self._start_method == \"spawn\":\r\n            _check_missing_main_guard()\r\n\r\n        # The default cluster environment in Lightning chooses a random free port number\r\n        # This needs to be done in the main process here before starting processes to ensure each rank will connect\r\n        # through the same port\r\n        assert self._strategy.cluster_environment is not None\r\n        os.environ[\"MASTER_PORT\"] = str(self._strategy.cluster_environment.main_port)\r\n\r\n        context = mp.get_context(self._start_method)\r\n        return_queue = context.SimpleQueue()\r\n\r\n        if self._start_method == \"spawn\":\r\n            global_states = _GlobalStateSnapshot.capture()\r\n            process_args = [function, args, kwargs, return_queue, global_states]\r\n        else:\r\n            process_args = [function, args, kwargs, return_queue]\r\n\r\n        mp.start_processes(\r\n            self._wrapping_function,\r\n            args=process_args,\r\n            nprocs=self._strategy.num_processes,\r\n            start_method=self._start_method,\r\n        )\r\n        return return_queue.get()", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", ".", "The", "function", "is", "allowed", "to", "have", "a", "return", "value", ".", "However", ",", "when", "all", "processes", "join", ",", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "`", "launch", "`", "method", "in", "the", "main", "process", ".", "Arguments", ":", "function", ":", "The", "entry", "point", "for", "all", "launched", "processes", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "\"", "\"", "\"", "if", "self", ".", "_start_method", "in", "(", "\"", "fork", "\"", ",", "\"", "forkserver", "\"", ")", ":", "_check_bad_cuda_fork", "(", ")", "if", "self", ".", "_start_method", "=", "=", "\"", "spawn", "\"", ":", "_check_missing_main_guard", "(", ")", "assert", "self", ".", "_strategy", ".", "cluster_environment", "is", "not", "None", "os", ".", "environ", "[", "\"", "MASTER_PORT", "\"", "]", "=", "str", "(", "self", ".", "_strategy", ".", "cluster_environment", ".", "main_port", ")", "context", "=", "mp", ".", "get_context", "(", "self", ".", "_start_method", ")", "return_queue", "=", "context", ".", "SimpleQueue", "(", ")", "if", "self", ".", "_start_method", "=", "=", "\"", "spawn", "\"", ":", "global_states", "=", "_GlobalStateSnapshot", ".", "capture", "(", ")", "process_args", "=", "[", "function", ",", "args", ",", "kwargs", ",", "return_queue", ",", "global_states", "]", "else", ":", "process_args", "=", "[", "function", ",", "args", ",", "kwargs", ",", "return_queue", "]", "mp", ".", "start_processes", "(", "self", ".", "_wrapping_function", ",", "args", "=", "process_args", ",", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", ",", "start_method", "=", "self", ".", "_start_method", ",", ")", "return", "return_queue", ".", "get", "(", ")"], "docstring": "Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", "the", "function", "is", "allowed", "to", "have", "a", "return", "value", "however", "when", "all", "processes", "join", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "launch", "method", "in", "the", "main", "process", "arguments", "function", "the", "entry", "point", "for", "all", "launched", "processes", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function"], "docstring_summary": "Launches processes that run the given function in parallel.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "partition": "train", "function_type": "class_method", "class_name": "_MultiProcessingLauncher", "start_line": 84, "end_line": 122, "hash": "cde603892ce410fede628b263d038684", "complexity": 4, "parameters": ["function", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "capture", "original_string": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "language": "python", "code": "def capture(cls) -> \"_GlobalStateSnapshot\":\r\n        \"\"\"Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.\"\"\"\r\n        return cls(\r\n            use_deterministic_algorithms=torch.are_deterministic_algorithms_enabled(),\r\n            use_deterministic_algorithms_warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),\r\n            cudnn_benchmark=torch.backends.cudnn.benchmark,\r\n            rng_states=_collect_rng_states(),\r\n        )", "code_tokens": ["def", "capture", "(", "cls", ")", "-", ">", "\"", "_GlobalStateSnapshot", "\"", ":", "\"", "\"", "\"", "Capture", "a", "few", "global", "states", "from", "torch", ",", "numpy", ",", "etc", ".", ",", "that", "we", "want", "to", "restore", "in", "a", "spawned", "worker", "process", ".", "\"", "\"", "\"", "return", "cls", "(", "use_deterministic_algorithms", "=", "torch", ".", "are_deterministic_algorithms_enabled", "(", ")", ",", "use_deterministic_algorithms_warn_only", "=", "torch", ".", "is_deterministic_algorithms_warn_only_enabled", "(", ")", ",", "cudnn_benchmark", "=", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", ",", "rng_states", "=", "_collect_rng_states", "(", ")", ",", ")"], "docstring": "Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.", "docstring_tokens": ["capture", "a", "few", "global", "states", "from", "torch", "numpy", "etc", "that", "we", "want", "to", "restore", "in", "a", "spawned", "worker", "process"], "docstring_summary": "Capture a few global states from torch, numpy, etc., that we want to restore in a spawned worker process.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "partition": "train", "function_type": "class_method", "class_name": "_GlobalStateSnapshot", "start_line": 172, "end_line": 179, "hash": "110125338e797add4550ce58c61fcfdf", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "restore", "original_string": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "language": "python", "code": "def restore(self) -> None:\r\n        \"\"\"Restores all globals to the values captured in the :meth:`capture` method.\"\"\"\r\n        torch.use_deterministic_algorithms(\r\n            self.use_deterministic_algorithms, warn_only=self.use_deterministic_algorithms_warn_only\r\n        )\r\n        torch.backends.cudnn.benchmark = self.cudnn_benchmark\r\n        _set_rng_states(self.rng_states)", "code_tokens": ["def", "restore", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Restores", "all", "globals", "to", "the", "values", "captured", "in", "the", ":", "meth", ":", "`", "capture", "`", "method", ".", "\"", "\"", "\"", "torch", ".", "use_deterministic_algorithms", "(", "self", ".", "use_deterministic_algorithms", ",", "warn_only", "=", "self", ".", "use_deterministic_algorithms_warn_only", ")", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "self", ".", "cudnn_benchmark", "_set_rng_states", "(", "self", ".", "rng_states", ")"], "docstring": "Restores all globals to the values captured in the :meth:`capture` method.", "docstring_tokens": ["restores", "all", "globals", "to", "the", "values", "captured", "in", "the", "meth", "capture", "method"], "docstring_summary": "Restores all globals to the values captured in the :meth:`capture` method.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "partition": "train", "function_type": "class_method", "class_name": "_GlobalStateSnapshot", "start_line": 181, "end_line": 187, "hash": "e0ee77b196b67a7498babe45229e9b73", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "_check_bad_cuda_fork", "original_string": "def _check_bad_cuda_fork() -> None:\r\n    \"\"\"Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.\r\n\r\n    The error message replaces PyTorch's 'Cannot re-initialize CUDA in forked subprocess' with helpful advice for\r\n    Lightning users.\r\n\r\n    \"\"\"\r\n    if not torch.cuda.is_initialized():\r\n        return\r\n\r\n    message = (\r\n        \"Lightning can't create new processes if CUDA is already initialized. Did you manually call\"\r\n        \" `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any\"\r\n        \" other way? Please remove any such calls, or change the selected strategy.\"\r\n    )\r\n    if _IS_INTERACTIVE:\r\n        message += \" You will have to restart the Python kernel.\"\r\n    raise RuntimeError(message)", "language": "python", "code": "def _check_bad_cuda_fork() -> None:\r\n    \"\"\"Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.\r\n\r\n    The error message replaces PyTorch's 'Cannot re-initialize CUDA in forked subprocess' with helpful advice for\r\n    Lightning users.\r\n\r\n    \"\"\"\r\n    if not torch.cuda.is_initialized():\r\n        return\r\n\r\n    message = (\r\n        \"Lightning can't create new processes if CUDA is already initialized. Did you manually call\"\r\n        \" `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any\"\r\n        \" other way? Please remove any such calls, or change the selected strategy.\"\r\n    )\r\n    if _IS_INTERACTIVE:\r\n        message += \" You will have to restart the Python kernel.\"\r\n    raise RuntimeError(message)", "code_tokens": ["def", "_check_bad_cuda_fork", "(", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "whether", "it", "is", "safe", "to", "fork", "and", "initialize", "CUDA", "in", "the", "new", "processes", ",", "and", "raises", "an", "exception", "if", "not", ".", "The", "error", "message", "replaces", "PyTorch", "'", "s", "'", "Cannot", "re", "-", "initialize", "CUDA", "in", "forked", "subprocess", "'", "with", "helpful", "advice", "for", "Lightning", "users", ".", "\"", "\"", "\"", "if", "not", "torch", ".", "cuda", ".", "is_initialized", "(", ")", ":", "return", "message", "=", "(", "\"", "Lightning", "can", "'", "t", "create", "new", "processes", "if", "CUDA", "is", "already", "initialized", ".", "Did", "you", "manually", "call", "\"", "\"", "`", "torch", ".", "cuda", ".", "*", "`", "functions", ",", "have", "moved", "the", "model", "to", "the", "device", ",", "or", "allocated", "memory", "on", "the", "GPU", "any", "\"", "\"", "other", "way", "?", "Please", "remove", "any", "such", "calls", ",", "or", "change", "the", "selected", "strategy", ".", "\"", ")", "if", "_IS_INTERACTIVE", ":", "message", "+", "=", "\"", "You", "will", "have", "to", "restart", "the", "Python", "kernel", ".", "\"", "raise", "RuntimeError", "(", "message", ")"], "docstring": "Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.\r\n\r\n    The error message replaces PyTorch's 'Cannot re-initialize CUDA in forked subprocess' with helpful advice for\r\n    Lightning users.", "docstring_tokens": ["checks", "whether", "it", "is", "safe", "to", "fork", "and", "initialize", "cuda", "in", "the", "new", "processes", "and", "raises", "an", "exception", "if", "not", "the", "error", "message", "replaces", "pytorch", "s", "cannot", "re", "initialize", "cuda", "in", "forked", "subprocess", "with", "helpful", "advice", "for", "lightning", "users"], "docstring_summary": "Checks whether it is safe to fork and initialize CUDA in the new processes, and raises an exception if not.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "partition": "train", "function_type": "function", "start_line": 190, "end_line": 207, "hash": "1e294f89fbbbd59291d3540dfeb1e0a9", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "_disable_module_memory_sharing", "original_string": "def _disable_module_memory_sharing(data: Any) -> Any:\r\n    \"\"\"Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.\r\n\r\n    Note: This is only required when running on CPU.\r\n\r\n    \"\"\"\r\n    # PyTorch enables memory sharing automatically on all tensors that are passed through `mp.spawn`.\r\n    # For model weights and buffers, this is undesired and can lead to race conditions between processes.\r\n    # Hence, we copy the tensors in the entire module to ensure it doesn't share memory with other processes.\r\n\r\n    @torch.no_grad()\r\n    def unshare(module: Module) -> Module:\r\n        for tensor in itertools.chain(module.parameters(), module.buffers()):\r\n            tensor.data = tensor.data.clone()\r\n        return module\r\n\r\n    return apply_to_collection(data, function=unshare, dtype=Module)", "language": "python", "code": "def _disable_module_memory_sharing(data: Any) -> Any:\r\n    \"\"\"Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.\r\n\r\n    Note: This is only required when running on CPU.\r\n\r\n    \"\"\"\r\n    # PyTorch enables memory sharing automatically on all tensors that are passed through `mp.spawn`.\r\n    # For model weights and buffers, this is undesired and can lead to race conditions between processes.\r\n    # Hence, we copy the tensors in the entire module to ensure it doesn't share memory with other processes.\r\n\r\n    @torch.no_grad()\r\n    def unshare(module: Module) -> Module:\r\n        for tensor in itertools.chain(module.parameters(), module.buffers()):\r\n            tensor.data = tensor.data.clone()\r\n        return module\r\n\r\n    return apply_to_collection(data, function=unshare, dtype=Module)", "code_tokens": ["def", "_disable_module_memory_sharing", "(", "data", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Disables", "memory", "sharing", "on", "parameters", "and", "buffers", "of", "`", "nn", ".", "Module", "`", "s", "contained", "in", "the", "given", "collection", ".", "Note", ":", "This", "is", "only", "required", "when", "running", "on", "CPU", ".", "\"", "\"", "\"", "@", "torch", ".", "no_grad", "(", ")", "def", "unshare", "(", "module", ":", "Module", ")", "-", ">", "Module", ":", "for", "tensor", "in", "itertools", ".", "chain", "(", "module", ".", "parameters", "(", ")", ",", "module", ".", "buffers", "(", ")", ")", ":", "tensor", ".", "data", "=", "tensor", ".", "data", ".", "clone", "(", ")", "return", "module", "return", "apply_to_collection", "(", "data", ",", "function", "=", "unshare", ",", "dtype", "=", "Module", ")"], "docstring": "Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.\r\n\r\n    Note: This is only required when running on CPU.", "docstring_tokens": ["disables", "memory", "sharing", "on", "parameters", "and", "buffers", "of", "nn", "module", "s", "contained", "in", "the", "given", "collection", "note", "this", "is", "only", "required", "when", "running", "on", "cpu"], "docstring_summary": "Disables memory sharing on parameters and buffers of `nn.Module`s contained in the given collection.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "partition": "train", "function_type": "function", "start_line": 210, "end_line": 226, "hash": "0ee48c849fa86341297e07d984007f2b", "complexity": 2, "parameters": ["data"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "func_name": "_check_missing_main_guard", "original_string": "def _check_missing_main_guard() -> None:\r\n    \"\"\"Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.\"\"\"\r\n    if not getattr(mp.current_process(), \"_inheriting\", False):\r\n        return\r\n    message = dedent(\r\n        \"\"\"\r\n        Launching multiple processes with the 'spawn' start method requires that your script guards the main\r\n        function with an `if __name__ == \\\"__main__\\\"` clause. For example:\r\n\r\n        def main():\r\n            # Put your code here\r\n            ...\r\n\r\n        if __name__ == \"__main__\":\r\n            main()\r\n\r\n        Alternatively, you can run with `strategy=\"ddp\"` to avoid this error.\r\n        \"\"\"\r\n    )\r\n    raise RuntimeError(message)", "language": "python", "code": "def _check_missing_main_guard() -> None:\r\n    \"\"\"Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.\"\"\"\r\n    if not getattr(mp.current_process(), \"_inheriting\", False):\r\n        return\r\n    message = dedent(\r\n        \"\"\"\r\n        Launching multiple processes with the 'spawn' start method requires that your script guards the main\r\n        function with an `if __name__ == \\\"__main__\\\"` clause. For example:\r\n\r\n        def main():\r\n            # Put your code here\r\n            ...\r\n\r\n        if __name__ == \"__main__\":\r\n            main()\r\n\r\n        Alternatively, you can run with `strategy=\"ddp\"` to avoid this error.\r\n        \"\"\"\r\n    )\r\n    raise RuntimeError(message)", "code_tokens": ["def", "_check_missing_main_guard", "(", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", "an", "exception", "if", "the", "`", "`", "__name__", "=", "=", "\"", "__main__", "\"", "`", "`", "guard", "is", "missing", ".", "\"", "\"", "\"", "if", "not", "getattr", "(", "mp", ".", "current_process", "(", ")", ",", "\"", "_inheriting", "\"", ",", "False", ")", ":", "return", "message", "=", "dedent", "(", "\"", "\"", "\"", "Launching", "multiple", "processes", "with", "the", "'", "spawn", "'", "start", "method", "requires", "that", "your", "script", "guards", "the", "main", "function", "with", "an", "`", "if", "__name__", "=", "=", "\\", "\"", "__main__", "\\", "\"", "`", "clause", ".", "For", "example", ":", "def", "main", "(", ")", ":", ".", ".", ".", "if", "__name__", "=", "=", "\"", "__main__", "\"", ":", "main", "(", ")", "Alternatively", ",", "you", "can", "run", "with", "`", "strategy", "=", "\"", "ddp", "\"", "`", "to", "avoid", "this", "error", ".", "\"", "\"", "\"", ")", "raise", "RuntimeError", "(", "message", ")"], "docstring": "Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.", "docstring_tokens": ["raises", "an", "exception", "if", "the", "__name__", "__main__", "guard", "is", "missing"], "docstring_summary": "Raises an exception if the ``__name__ == \"__main__\"`` guard is missing.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\multiprocessing.py", "partition": "train", "function_type": "function", "start_line": 229, "end_line": 248, "hash": "ddb46646bce0f99f135795e33ef824b2", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "launch", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        self.cluster_environment.validate_settings(num_devices=self.num_processes, num_nodes=self.num_nodes)\r\n        if not self.cluster_environment.creates_processes_externally:\r\n            self._call_children_scripts()\r\n            _launch_process_observer(self.procs)\r\n\r\n        _set_num_threads_if_needed(num_processes=self.num_processes)\r\n        return function(*args, **kwargs)", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Creates", "new", "processes", ",", "then", "calls", "the", "given", "function", ".", "Arguments", ":", "function", ":", "A", "callback", "function", "to", "execute", "after", "all", "processes", "have", "been", "created", ".", "It", "is", "up", "to", "the", "implementation", "of", "this", "function", "to", "synchronize", "the", "processes", ",", "e", ".", "g", ".", ",", "with", "barriers", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "\"", "\"", "\"", "self", ".", "cluster_environment", ".", "validate_settings", "(", "num_devices", "=", "self", ".", "num_processes", ",", "num_nodes", "=", "self", ".", "num_nodes", ")", "if", "not", "self", ".", "cluster_environment", ".", "creates_processes_externally", ":", "self", ".", "_call_children_scripts", "(", ")", "_launch_process_observer", "(", "self", ".", "procs", ")", "_set_num_threads_if_needed", "(", "num_processes", "=", "self", ".", "num_processes", ")", "return", "function", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Creates new processes, then calls the given function.\r\n\r\n        Arguments:\r\n            function: A callback function to execute after all processes have been created.\r\n                It is up to the implementation of this function to synchronize the processes, e.g., with barriers.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.", "docstring_tokens": ["creates", "new", "processes", "then", "calls", "the", "given", "function", "arguments", "function", "a", "callback", "function", "to", "execute", "after", "all", "processes", "have", "been", "created", "it", "is", "up", "to", "the", "implementation", "of", "this", "function", "to", "synchronize", "the", "processes", "e", "g", "with", "barriers", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function"], "docstring_summary": "Creates new processes, then calls the given function.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "partition": "train", "function_type": "class_method", "class_name": "_SubprocessScriptLauncher", "start_line": 91, "end_line": 107, "hash": "3e54214003d6ee4eb30a388ccd0aa1ae", "complexity": 2, "parameters": ["function", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "_run", "original_string": "def _run(self) -> bool:\r\n        \"\"\"Runs once over all child processes to check whether they are still running.\"\"\"\r\n        for proc in self._child_processes:\r\n            proc.poll()\r\n\r\n        return_codes = [proc.returncode for proc in self._child_processes]\r\n        if all(return_code == 0 for return_code in return_codes):\r\n            return True\r\n\r\n        for i, proc in enumerate(self._child_processes):\r\n            if proc.returncode:\r\n                message = rank_prefixed_message(\r\n                    f\"Child process with PID {proc.pid} terminated with code {proc.returncode}.\"\r\n                    f\" Forcefully terminating all other processes to avoid zombies \ud83e\udddf\",\r\n                    rank=(i + 1),\r\n                )\r\n                _logger.info(message)\r\n                self._terminate_all()\r\n                return True\r\n\r\n        return False", "language": "python", "code": "def _run(self) -> bool:\r\n        \"\"\"Runs once over all child processes to check whether they are still running.\"\"\"\r\n        for proc in self._child_processes:\r\n            proc.poll()\r\n\r\n        return_codes = [proc.returncode for proc in self._child_processes]\r\n        if all(return_code == 0 for return_code in return_codes):\r\n            return True\r\n\r\n        for i, proc in enumerate(self._child_processes):\r\n            if proc.returncode:\r\n                message = rank_prefixed_message(\r\n                    f\"Child process with PID {proc.pid} terminated with code {proc.returncode}.\"\r\n                    f\" Forcefully terminating all other processes to avoid zombies \ud83e\udddf\",\r\n                    rank=(i + 1),\r\n                )\r\n                _logger.info(message)\r\n                self._terminate_all()\r\n                return True\r\n\r\n        return False", "code_tokens": ["def", "_run", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Runs", "once", "over", "all", "child", "processes", "to", "check", "whether", "they", "are", "still", "running", ".", "\"", "\"", "\"", "for", "proc", "in", "self", ".", "_child_processes", ":", "proc", ".", "poll", "(", ")", "return_codes", "=", "[", "proc", ".", "returncode", "for", "proc", "in", "self", ".", "_child_processes", "]", "if", "all", "(", "return_code", "=", "=", "0", "for", "return_code", "in", "return_codes", ")", ":", "return", "True", "for", "i", ",", "proc", "in", "enumerate", "(", "self", ".", "_child_processes", ")", ":", "if", "proc", ".", "returncode", ":", "message", "=", "rank_prefixed_message", "(", "f", "\"", "Child", "process", "with", "PID", "{", "proc", ".", "pid", "}", "terminated", "with", "code", "{", "proc", ".", "returncode", "}", ".", "\"", "f", "\"", "Forcefully", "terminating", "all", "other", "processes", "to", "avoid", "zombies", "\ud83e\udddf", "\"", ",", "rank", "=", "(", "i", "+", "1", ")", ",", ")", "_logger", ".", "info", "(", "message", ")", "self", ".", "_terminate_all", "(", ")", "return", "True", "return", "False"], "docstring": "Runs once over all child processes to check whether they are still running.", "docstring_tokens": ["runs", "once", "over", "all", "child", "processes", "to", "check", "whether", "they", "are", "still", "running"], "docstring_summary": "Runs once over all child processes to check whether they are still running.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "partition": "train", "function_type": "class_method", "class_name": "_ChildProcessObserver", "start_line": 207, "end_line": 227, "hash": "529ef1e970fb5bd6912293ddfa2bf624", "complexity": 7, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "func_name": "_terminate_all", "original_string": "def _terminate_all(self) -> None:\r\n        \"\"\"Terminates the main process and all its children.\"\"\"\r\n        for p in self._child_processes:\r\n            p.send_signal(self._termination_signal)\r\n        os.kill(self._main_pid, self._termination_signal)", "language": "python", "code": "def _terminate_all(self) -> None:\r\n        \"\"\"Terminates the main process and all its children.\"\"\"\r\n        for p in self._child_processes:\r\n            p.send_signal(self._termination_signal)\r\n        os.kill(self._main_pid, self._termination_signal)", "code_tokens": ["def", "_terminate_all", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Terminates", "the", "main", "process", "and", "all", "its", "children", ".", "\"", "\"", "\"", "for", "p", "in", "self", ".", "_child_processes", ":", "p", ".", "send_signal", "(", "self", ".", "_termination_signal", ")", "os", ".", "kill", "(", "self", ".", "_main_pid", ",", "self", ".", "_termination_signal", ")"], "docstring": "Terminates the main process and all its children.", "docstring_tokens": ["terminates", "the", "main", "process", "and", "all", "its", "children"], "docstring_summary": "Terminates the main process and all its children.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\subprocess_script.py", "partition": "train", "function_type": "class_method", "class_name": "_ChildProcessObserver", "start_line": 229, "end_line": 233, "hash": "080b6f7b67b08bbea554c568145d954e", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\strategies\\launchers\\xla.py", "func_name": "launch", "original_string": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        return_queue: Union[queue.Queue, mp.SimpleQueue]\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            # avoid warning: \"Unsupported nprocs\". If it's 1, it will call the launched function directly.\r\n            # otherwise it will use all devices\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            **spawn_kwargs,\r\n        )\r\n        return return_queue.get()", "language": "python", "code": "def launch(self, function: Callable, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.\r\n\r\n        \"\"\"\r\n        return_queue: Union[queue.Queue, mp.SimpleQueue]\r\n        return_queue = mp.Manager().Queue()\r\n\r\n        import torch_xla.distributed.xla_multiprocessing as xmp\r\n\r\n        spawn_kwargs = {}\r\n        nprocs = self._strategy.num_processes\r\n        if nprocs == 1:\r\n            # avoid warning: \"Unsupported nprocs\". If it's 1, it will call the launched function directly.\r\n            # otherwise it will use all devices\r\n            spawn_kwargs[\"nprocs\"] = nprocs\r\n\r\n        xmp.spawn(\r\n            self._wrapping_function,\r\n            args=(function, args, kwargs, return_queue),\r\n            start_method=self._start_method,\r\n            **spawn_kwargs,\r\n        )\r\n        return return_queue.get()", "code_tokens": ["def", "launch", "(", "self", ",", "function", ":", "Callable", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", ".", "The", "function", "is", "allowed", "to", "have", "a", "return", "value", ".", "However", ",", "when", "all", "processes", "join", ",", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "`", "launch", "`", "method", "in", "the", "main", "process", ".", "Arguments", ":", "function", ":", "The", "entry", "point", "for", "all", "launched", "processes", ".", "*", "args", ":", "Optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "*", "*", "kwargs", ":", "Optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function", ".", "\"", "\"", "\"", "return_queue", ":", "Union", "[", "queue", ".", "Queue", ",", "mp", ".", "SimpleQueue", "]", "return_queue", "=", "mp", ".", "Manager", "(", ")", ".", "Queue", "(", ")", "import", "torch_xla", ".", "distributed", ".", "xla_multiprocessing", "as", "xmp", "spawn_kwargs", "=", "{", "}", "nprocs", "=", "self", ".", "_strategy", ".", "num_processes", "if", "nprocs", "=", "=", "1", ":", "spawn_kwargs", "[", "\"", "nprocs", "\"", "]", "=", "nprocs", "xmp", ".", "spawn", "(", "self", ".", "_wrapping_function", ",", "args", "=", "(", "function", ",", "args", ",", "kwargs", ",", "return_queue", ")", ",", "start_method", "=", "self", ".", "_start_method", ",", "*", "*", "spawn_kwargs", ",", ")", "return", "return_queue", ".", "get", "(", ")"], "docstring": "Launches processes that run the given function in parallel.\r\n\r\n        The function is allowed to have a return value. However, when all processes join, only the return value\r\n        of worker process 0 gets returned from this `launch` method in the main process.\r\n\r\n        Arguments:\r\n            function: The entry point for all launched processes.\r\n            *args: Optional positional arguments to be passed to the given function.\r\n            **kwargs: Optional keyword arguments to be passed to the given function.", "docstring_tokens": ["launches", "processes", "that", "run", "the", "given", "function", "in", "parallel", "the", "function", "is", "allowed", "to", "have", "a", "return", "value", "however", "when", "all", "processes", "join", "only", "the", "return", "value", "of", "worker", "process", "0", "gets", "returned", "from", "this", "launch", "method", "in", "the", "main", "process", "arguments", "function", "the", "entry", "point", "for", "all", "launched", "processes", "args", "optional", "positional", "arguments", "to", "be", "passed", "to", "the", "given", "function", "kwargs", "optional", "keyword", "arguments", "to", "be", "passed", "to", "the", "given", "function"], "docstring_summary": "Launches processes that run the given function in parallel.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\strategies\\launchers\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "_XLALauncher", "start_line": 58, "end_line": 88, "hash": "7008b985060d27b675e2602634572c68", "complexity": 2, "parameters": ["function", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\apply_func.py", "func_name": "move_data_to_device", "original_string": "def move_data_to_device(batch: Any, device: _DEVICE) -> Any:\r\n    \"\"\"Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be\r\n    moved and all other objects in the collection will be left untouched.\r\n\r\n    Args:\r\n        batch: A tensor or collection of tensors or anything that has a method ``.to(...)``.\r\n            See :func:`apply_to_collection` for a list of supported collection types.\r\n        device: The device to which the data should be moved\r\n\r\n    Return:\r\n        the same collection but with all contained tensors residing on the new device.\r\n\r\n    See Also:\r\n        - :meth:`torch.Tensor.to`\r\n        - :class:`torch.device`\r\n\r\n    \"\"\"\r\n    if isinstance(device, str):\r\n        device = torch.device(device)\r\n\r\n    def batch_to(data: Any) -> Any:\r\n        kwargs = {}\r\n        # Don't issue non-blocking transfers to CPU\r\n        # Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015\r\n        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:\r\n            kwargs[\"non_blocking\"] = True\r\n        data_output = data.to(device, **kwargs)\r\n        if data_output is not None:\r\n            return data_output\r\n        # user wrongly implemented the `_TransferableDataType` and forgot to return `self`.\r\n        return data\r\n\r\n    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)", "language": "python", "code": "def move_data_to_device(batch: Any, device: _DEVICE) -> Any:\r\n    \"\"\"Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be\r\n    moved and all other objects in the collection will be left untouched.\r\n\r\n    Args:\r\n        batch: A tensor or collection of tensors or anything that has a method ``.to(...)``.\r\n            See :func:`apply_to_collection` for a list of supported collection types.\r\n        device: The device to which the data should be moved\r\n\r\n    Return:\r\n        the same collection but with all contained tensors residing on the new device.\r\n\r\n    See Also:\r\n        - :meth:`torch.Tensor.to`\r\n        - :class:`torch.device`\r\n\r\n    \"\"\"\r\n    if isinstance(device, str):\r\n        device = torch.device(device)\r\n\r\n    def batch_to(data: Any) -> Any:\r\n        kwargs = {}\r\n        # Don't issue non-blocking transfers to CPU\r\n        # Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015\r\n        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:\r\n            kwargs[\"non_blocking\"] = True\r\n        data_output = data.to(device, **kwargs)\r\n        if data_output is not None:\r\n            return data_output\r\n        # user wrongly implemented the `_TransferableDataType` and forgot to return `self`.\r\n        return data\r\n\r\n    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)", "code_tokens": ["def", "move_data_to_device", "(", "batch", ":", "Any", ",", "device", ":", "_DEVICE", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Transfers", "a", "collection", "of", "data", "to", "the", "given", "device", ".", "Any", "object", "that", "defines", "a", "method", "`", "`", "to", "(", "device", ")", "`", "`", "will", "be", "moved", "and", "all", "other", "objects", "in", "the", "collection", "will", "be", "left", "untouched", ".", "Args", ":", "batch", ":", "A", "tensor", "or", "collection", "of", "tensors", "or", "anything", "that", "has", "a", "method", "`", "`", ".", "to", "(", ".", ".", ".", ")", "`", "`", ".", "See", ":", "func", ":", "`", "apply_to_collection", "`", "for", "a", "list", "of", "supported", "collection", "types", ".", "device", ":", "The", "device", "to", "which", "the", "data", "should", "be", "moved", "Return", ":", "the", "same", "collection", "but", "with", "all", "contained", "tensors", "residing", "on", "the", "new", "device", ".", "See", "Also", ":", "-", ":", "meth", ":", "`", "torch", ".", "Tensor", ".", "to", "`", "-", ":", "class", ":", "`", "torch", ".", "device", "`", "\"", "\"", "\"", "if", "isinstance", "(", "device", ",", "str", ")", ":", "device", "=", "torch", ".", "device", "(", "device", ")", "def", "batch_to", "(", "data", ":", "Any", ")", "-", ">", "Any", ":", "kwargs", "=", "{", "}", "if", "isinstance", "(", "data", ",", "Tensor", ")", "and", "isinstance", "(", "device", ",", "torch", ".", "device", ")", "and", "device", ".", "type", "not", "in", "_BLOCKING_DEVICE_TYPES", ":", "kwargs", "[", "\"", "non_blocking", "\"", "]", "=", "True", "data_output", "=", "data", ".", "to", "(", "device", ",", "*", "*", "kwargs", ")", "if", "data_output", "is", "not", "None", ":", "return", "data_output", "return", "data", "return", "apply_to_collection", "(", "batch", ",", "dtype", "=", "_TransferableDataType", ",", "function", "=", "batch_to", ")"], "docstring": "Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be\r\n    moved and all other objects in the collection will be left untouched.\r\n\r\n    Args:\r\n        batch: A tensor or collection of tensors or anything that has a method ``.to(...)``.\r\n            See :func:`apply_to_collection` for a list of supported collection types.\r\n        device: The device to which the data should be moved\r\n\r\n    Return:\r\n        the same collection but with all contained tensors residing on the new device.\r\n\r\n    See Also:\r\n        - :meth:`torch.Tensor.to`\r\n        - :class:`torch.device`", "docstring_tokens": ["transfers", "a", "collection", "of", "data", "to", "the", "given", "device", "any", "object", "that", "defines", "a", "method", "to", "device", "will", "be", "moved", "and", "all", "other", "objects", "in", "the", "collection", "will", "be", "left", "untouched", "args", "batch", "a", "tensor", "or", "collection", "of", "tensors", "or", "anything", "that", "has", "a", "method", "to", "see", "func", "apply_to_collection", "for", "a", "list", "of", "supported", "collection", "types", "device", "the", "device", "to", "which", "the", "data", "should", "be", "moved", "return", "the", "same", "collection", "but", "with", "all", "contained", "tensors", "residing", "on", "the", "new", "device", "see", "also", "meth", "torch", "tensor", "to", "class", "torch", "device"], "docstring_summary": "Transfers a collection of data to the given device. Any object that defines a method ``to(device)`` will be", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\apply_func.py", "partition": "train", "function_type": "function", "start_line": 77, "end_line": 109, "hash": "5dec9097d18bf47813a920453be57c0e", "complexity": 6, "parameters": ["batch", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\apply_func.py", "func_name": "convert_tensors_to_scalars", "original_string": "def convert_tensors_to_scalars(data: Any) -> Any:\r\n    \"\"\"Recursively walk through a collection and convert single-item tensors to scalar values.\r\n\r\n    Raises:\r\n        ValueError:\r\n            If tensors inside ``metrics`` contains multiple elements, hence preventing conversion to a scalar.\r\n\r\n    \"\"\"\r\n\r\n    def to_item(value: Tensor) -> Union[int, float, bool]:\r\n        if value.numel() != 1:\r\n            raise ValueError(\r\n                f\"The metric `{value}` does not contain a single element, thus it cannot be converted to a scalar.\"\r\n            )\r\n        return value.item()\r\n\r\n    return apply_to_collection(data, Tensor, to_item)", "language": "python", "code": "def convert_tensors_to_scalars(data: Any) -> Any:\r\n    \"\"\"Recursively walk through a collection and convert single-item tensors to scalar values.\r\n\r\n    Raises:\r\n        ValueError:\r\n            If tensors inside ``metrics`` contains multiple elements, hence preventing conversion to a scalar.\r\n\r\n    \"\"\"\r\n\r\n    def to_item(value: Tensor) -> Union[int, float, bool]:\r\n        if value.numel() != 1:\r\n            raise ValueError(\r\n                f\"The metric `{value}` does not contain a single element, thus it cannot be converted to a scalar.\"\r\n            )\r\n        return value.item()\r\n\r\n    return apply_to_collection(data, Tensor, to_item)", "code_tokens": ["def", "convert_tensors_to_scalars", "(", "data", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Recursively", "walk", "through", "a", "collection", "and", "convert", "single", "-", "item", "tensors", "to", "scalar", "values", ".", "Raises", ":", "ValueError", ":", "If", "tensors", "inside", "`", "`", "metrics", "`", "`", "contains", "multiple", "elements", ",", "hence", "preventing", "conversion", "to", "a", "scalar", ".", "\"", "\"", "\"", "def", "to_item", "(", "value", ":", "Tensor", ")", "-", ">", "Union", "[", "int", ",", "float", ",", "bool", "]", ":", "if", "value", ".", "numel", "(", ")", "!", "=", "1", ":", "raise", "ValueError", "(", "f", "\"", "The", "metric", "`", "{", "value", "}", "`", "does", "not", "contain", "a", "single", "element", ",", "thus", "it", "cannot", "be", "converted", "to", "a", "scalar", ".", "\"", ")", "return", "value", ".", "item", "(", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "to_item", ")"], "docstring": "Recursively walk through a collection and convert single-item tensors to scalar values.\r\n\r\n    Raises:\r\n        ValueError:\r\n            If tensors inside ``metrics`` contains multiple elements, hence preventing conversion to a scalar.", "docstring_tokens": ["recursively", "walk", "through", "a", "collection", "and", "convert", "single", "item", "tensors", "to", "scalar", "values", "raises", "valueerror", "if", "tensors", "inside", "metrics", "contains", "multiple", "elements", "hence", "preventing", "conversion", "to", "a", "scalar"], "docstring_summary": "Recursively walk through a collection and convert single-item tensors to scalar values.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\apply_func.py", "partition": "train", "function_type": "function", "start_line": 119, "end_line": 135, "hash": "1a959e461c80e9f3fd0a2d6fbe25d7b4", "complexity": 2, "parameters": ["data"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "func_name": "_load", "original_string": "def _load(\r\n    path_or_url: Union[IO, _PATH],\r\n    map_location: _MAP_LOCATION_TYPE = None,\r\n    weights_only: bool = False,\r\n) -> Any:\r\n    \"\"\"Loads a checkpoint.\r\n\r\n    Args:\r\n        path_or_url: Path or URL of the checkpoint.\r\n        map_location: a function, ``torch.device``, string or a dict specifying how to remap storage locations.\r\n\r\n    \"\"\"\r\n    if not isinstance(path_or_url, (str, Path)):\r\n        # any sort of BytesIO or similar\r\n        return torch.load(\r\n            path_or_url,\r\n            map_location=map_location,  # type: ignore[arg-type] # upstream annotation is not correct\r\n            weights_only=weights_only,\r\n        )\r\n    if str(path_or_url).startswith(\"http\"):\r\n        return torch.hub.load_state_dict_from_url(\r\n            str(path_or_url),\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )\r\n    fs = get_filesystem(path_or_url)\r\n    with fs.open(path_or_url, \"rb\") as f:\r\n        return torch.load(\r\n            f,\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )", "language": "python", "code": "def _load(\r\n    path_or_url: Union[IO, _PATH],\r\n    map_location: _MAP_LOCATION_TYPE = None,\r\n    weights_only: bool = False,\r\n) -> Any:\r\n    \"\"\"Loads a checkpoint.\r\n\r\n    Args:\r\n        path_or_url: Path or URL of the checkpoint.\r\n        map_location: a function, ``torch.device``, string or a dict specifying how to remap storage locations.\r\n\r\n    \"\"\"\r\n    if not isinstance(path_or_url, (str, Path)):\r\n        # any sort of BytesIO or similar\r\n        return torch.load(\r\n            path_or_url,\r\n            map_location=map_location,  # type: ignore[arg-type] # upstream annotation is not correct\r\n            weights_only=weights_only,\r\n        )\r\n    if str(path_or_url).startswith(\"http\"):\r\n        return torch.hub.load_state_dict_from_url(\r\n            str(path_or_url),\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )\r\n    fs = get_filesystem(path_or_url)\r\n    with fs.open(path_or_url, \"rb\") as f:\r\n        return torch.load(\r\n            f,\r\n            map_location=map_location,  # type: ignore[arg-type]\r\n            weights_only=weights_only,\r\n        )", "code_tokens": ["def", "_load", "(", "path_or_url", ":", "Union", "[", "IO", ",", "_PATH", "]", ",", "map_location", ":", "_MAP_LOCATION_TYPE", "=", "None", ",", "weights_only", ":", "bool", "=", "False", ",", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Loads", "a", "checkpoint", ".", "Args", ":", "path_or_url", ":", "Path", "or", "URL", "of", "the", "checkpoint", ".", "map_location", ":", "a", "function", ",", "`", "`", "torch", ".", "device", "`", "`", ",", "string", "or", "a", "dict", "specifying", "how", "to", "remap", "storage", "locations", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "path_or_url", ",", "(", "str", ",", "Path", ")", ")", ":", "return", "torch", ".", "load", "(", "path_or_url", ",", "map_location", "=", "map_location", ",", "weights_only", "=", "weights_only", ",", ")", "if", "str", "(", "path_or_url", ")", ".", "startswith", "(", "\"", "http", "\"", ")", ":", "return", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "str", "(", "path_or_url", ")", ",", "map_location", "=", "map_location", ",", "weights_only", "=", "weights_only", ",", ")", "fs", "=", "get_filesystem", "(", "path_or_url", ")", "with", "fs", ".", "open", "(", "path_or_url", ",", "\"", "rb", "\"", ")", "as", "f", ":", "return", "torch", ".", "load", "(", "f", ",", "map_location", "=", "map_location", ",", "weights_only", "=", "weights_only", ",", ")"], "docstring": "Loads a checkpoint.\r\n\r\n    Args:\r\n        path_or_url: Path or URL of the checkpoint.\r\n        map_location: a function, ``torch.device``, string or a dict specifying how to remap storage locations.", "docstring_tokens": ["loads", "a", "checkpoint", "args", "path_or_url", "path", "or", "url", "of", "the", "checkpoint", "map_location", "a", "function", "torch", "device", "string", "or", "a", "dict", "specifying", "how", "to", "remap", "storage", "locations"], "docstring_summary": "Loads a checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\cloud_io.py", "partition": "train", "function_type": "function", "start_line": 33, "end_line": 64, "hash": "1b2e1584f4bec63d4e1191254443e57e", "complexity": 4, "parameters": ["path_or_url", "_PATH]", "map_location", "weights_only"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "func_name": "_atomic_save", "original_string": "def _atomic_save(checkpoint: dict[str, Any], filepath: Union[str, Path]) -> None:\r\n    \"\"\"Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\r\n\r\n    Args:\r\n        checkpoint: The object to save.\r\n            Built to be used with the ``dump_checkpoint`` method, but can deal with anything which ``torch.save``\r\n            accepts.\r\n        filepath: The path to which the checkpoint will be saved.\r\n            This points to the file that the checkpoint will be stored in.\r\n\r\n    \"\"\"\r\n    bytesbuffer = io.BytesIO()\r\n    log.debug(f\"Saving checkpoint: {filepath}\")\r\n    torch.save(checkpoint, bytesbuffer)\r\n\r\n    try:\r\n        # We use a transaction here to avoid file corruption if the save gets interrupted\r\n        fs, urlpath = fsspec.core.url_to_fs(str(filepath))\r\n        with fs.transaction, fs.open(urlpath, \"wb\") as f:\r\n            f.write(bytesbuffer.getvalue())\r\n    except PermissionError as e:\r\n        if isinstance(e.__context__, OSError) and getattr(e.__context__, \"errno\", None) == errno.EXDEV:\r\n            raise RuntimeError(\r\n                'Upgrade fsspec to enable cross-device local checkpoints: pip install \"fsspec[http]>=2025.5.0\"',\r\n            ) from e", "language": "python", "code": "def _atomic_save(checkpoint: dict[str, Any], filepath: Union[str, Path]) -> None:\r\n    \"\"\"Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\r\n\r\n    Args:\r\n        checkpoint: The object to save.\r\n            Built to be used with the ``dump_checkpoint`` method, but can deal with anything which ``torch.save``\r\n            accepts.\r\n        filepath: The path to which the checkpoint will be saved.\r\n            This points to the file that the checkpoint will be stored in.\r\n\r\n    \"\"\"\r\n    bytesbuffer = io.BytesIO()\r\n    log.debug(f\"Saving checkpoint: {filepath}\")\r\n    torch.save(checkpoint, bytesbuffer)\r\n\r\n    try:\r\n        # We use a transaction here to avoid file corruption if the save gets interrupted\r\n        fs, urlpath = fsspec.core.url_to_fs(str(filepath))\r\n        with fs.transaction, fs.open(urlpath, \"wb\") as f:\r\n            f.write(bytesbuffer.getvalue())\r\n    except PermissionError as e:\r\n        if isinstance(e.__context__, OSError) and getattr(e.__context__, \"errno\", None) == errno.EXDEV:\r\n            raise RuntimeError(\r\n                'Upgrade fsspec to enable cross-device local checkpoints: pip install \"fsspec[http]>=2025.5.0\"',\r\n            ) from e", "code_tokens": ["def", "_atomic_save", "(", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ",", "filepath", ":", "Union", "[", "str", ",", "Path", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Saves", "a", "checkpoint", "atomically", ",", "avoiding", "the", "creation", "of", "incomplete", "checkpoints", ".", "Args", ":", "checkpoint", ":", "The", "object", "to", "save", ".", "Built", "to", "be", "used", "with", "the", "`", "`", "dump_checkpoint", "`", "`", "method", ",", "but", "can", "deal", "with", "anything", "which", "`", "`", "torch", ".", "save", "`", "`", "accepts", ".", "filepath", ":", "The", "path", "to", "which", "the", "checkpoint", "will", "be", "saved", ".", "This", "points", "to", "the", "file", "that", "the", "checkpoint", "will", "be", "stored", "in", ".", "\"", "\"", "\"", "bytesbuffer", "=", "io", ".", "BytesIO", "(", ")", "log", ".", "debug", "(", "f", "\"", "Saving", "checkpoint", ":", "{", "filepath", "}", "\"", ")", "torch", ".", "save", "(", "checkpoint", ",", "bytesbuffer", ")", "try", ":", "fs", ",", "urlpath", "=", "fsspec", ".", "core", ".", "url_to_fs", "(", "str", "(", "filepath", ")", ")", "with", "fs", ".", "transaction", ",", "fs", ".", "open", "(", "urlpath", ",", "\"", "wb", "\"", ")", "as", "f", ":", "f", ".", "write", "(", "bytesbuffer", ".", "getvalue", "(", ")", ")", "except", "PermissionError", "as", "e", ":", "if", "isinstance", "(", "e", ".", "__context__", ",", "OSError", ")", "and", "getattr", "(", "e", ".", "__context__", ",", "\"", "errno", "\"", ",", "None", ")", "=", "=", "errno", ".", "EXDEV", ":", "raise", "RuntimeError", "(", "'", "Upgrade", "fsspec", "to", "enable", "cross", "-", "device", "local", "checkpoints", ":", "pip", "install", "\"", "fsspec", "[", "http", "]", ">", "=", "2025", ".", "5", ".", "0", "\"", "'", ",", ")", "from", "e"], "docstring": "Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\r\n\r\n    Args:\r\n        checkpoint: The object to save.\r\n            Built to be used with the ``dump_checkpoint`` method, but can deal with anything which ``torch.save``\r\n            accepts.\r\n        filepath: The path to which the checkpoint will be saved.\r\n            This points to the file that the checkpoint will be stored in.", "docstring_tokens": ["saves", "a", "checkpoint", "atomically", "avoiding", "the", "creation", "of", "incomplete", "checkpoints", "args", "checkpoint", "the", "object", "to", "save", "built", "to", "be", "used", "with", "the", "dump_checkpoint", "method", "but", "can", "deal", "with", "anything", "which", "torch", "save", "accepts", "filepath", "the", "path", "to", "which", "the", "checkpoint", "will", "be", "saved", "this", "points", "to", "the", "file", "that", "the", "checkpoint", "will", "be", "stored", "in"], "docstring_summary": "Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\cloud_io.py", "partition": "train", "function_type": "function", "start_line": 72, "end_line": 96, "hash": "f07409dba8bbf6283f41554cce6f2ed0", "complexity": 5, "parameters": ["checkpoint", "Any]", "filepath", "Path]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\cloud_io.py", "func_name": "_is_dir", "original_string": "def _is_dir(fs: AbstractFileSystem, path: Union[str, Path], strict: bool = False) -> bool:\r\n    \"\"\"Check if a path is directory-like.\r\n\r\n    This function determines if a given path is considered directory-like, taking into account the behavior\r\n    specific to object storage platforms. For other filesystems, it behaves similarly to the standard `fs.isdir`\r\n    method.\r\n\r\n    Args:\r\n        fs: The filesystem to check the path against.\r\n        path: The path or URL to be checked.\r\n        strict: A flag specific to Object Storage platforms. If set to ``False``, any non-existing path is considered\r\n            as a valid directory-like path. In such cases, the directory (and any non-existing parent directories)\r\n            will be created on the fly. Defaults to False.\r\n\r\n    \"\"\"\r\n    # Object storage fsspec's are inconsistent with other file systems because they do not have real directories,\r\n    # see for instance https://gcsfs.readthedocs.io/en/latest/api.html?highlight=makedirs#gcsfs.core.GCSFileSystem.mkdir\r\n    # In particular, `fs.makedirs` is a no-op so we use `strict=False` to consider any path as valid, except if the\r\n    # path already exists but is a file\r\n    if _is_object_storage(fs):\r\n        if strict:\r\n            return fs.isdir(path)\r\n\r\n        # Check if the path is not already taken by a file. If not, it is considered a valid directory-like path\r\n        # because the directory (and all non-existing parent directories) will be created on the fly.\r\n        return not fs.isfile(path)\r\n\r\n    return fs.isdir(path)", "language": "python", "code": "def _is_dir(fs: AbstractFileSystem, path: Union[str, Path], strict: bool = False) -> bool:\r\n    \"\"\"Check if a path is directory-like.\r\n\r\n    This function determines if a given path is considered directory-like, taking into account the behavior\r\n    specific to object storage platforms. For other filesystems, it behaves similarly to the standard `fs.isdir`\r\n    method.\r\n\r\n    Args:\r\n        fs: The filesystem to check the path against.\r\n        path: The path or URL to be checked.\r\n        strict: A flag specific to Object Storage platforms. If set to ``False``, any non-existing path is considered\r\n            as a valid directory-like path. In such cases, the directory (and any non-existing parent directories)\r\n            will be created on the fly. Defaults to False.\r\n\r\n    \"\"\"\r\n    # Object storage fsspec's are inconsistent with other file systems because they do not have real directories,\r\n    # see for instance https://gcsfs.readthedocs.io/en/latest/api.html?highlight=makedirs#gcsfs.core.GCSFileSystem.mkdir\r\n    # In particular, `fs.makedirs` is a no-op so we use `strict=False` to consider any path as valid, except if the\r\n    # path already exists but is a file\r\n    if _is_object_storage(fs):\r\n        if strict:\r\n            return fs.isdir(path)\r\n\r\n        # Check if the path is not already taken by a file. If not, it is considered a valid directory-like path\r\n        # because the directory (and all non-existing parent directories) will be created on the fly.\r\n        return not fs.isfile(path)\r\n\r\n    return fs.isdir(path)", "code_tokens": ["def", "_is_dir", "(", "fs", ":", "AbstractFileSystem", ",", "path", ":", "Union", "[", "str", ",", "Path", "]", ",", "strict", ":", "bool", "=", "False", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Check", "if", "a", "path", "is", "directory", "-", "like", ".", "This", "function", "determines", "if", "a", "given", "path", "is", "considered", "directory", "-", "like", ",", "taking", "into", "account", "the", "behavior", "specific", "to", "object", "storage", "platforms", ".", "For", "other", "filesystems", ",", "it", "behaves", "similarly", "to", "the", "standard", "`", "fs", ".", "isdir", "`", "method", ".", "Args", ":", "fs", ":", "The", "filesystem", "to", "check", "the", "path", "against", ".", "path", ":", "The", "path", "or", "URL", "to", "be", "checked", ".", "strict", ":", "A", "flag", "specific", "to", "Object", "Storage", "platforms", ".", "If", "set", "to", "`", "`", "False", "`", "`", ",", "any", "non", "-", "existing", "path", "is", "considered", "as", "a", "valid", "directory", "-", "like", "path", ".", "In", "such", "cases", ",", "the", "directory", "(", "and", "any", "non", "-", "existing", "parent", "directories", ")", "will", "be", "created", "on", "the", "fly", ".", "Defaults", "to", "False", ".", "\"", "\"", "\"", "if", "_is_object_storage", "(", "fs", ")", ":", "if", "strict", ":", "return", "fs", ".", "isdir", "(", "path", ")", "return", "not", "fs", ".", "isfile", "(", "path", ")", "return", "fs", ".", "isdir", "(", "path", ")"], "docstring": "Check if a path is directory-like.\r\n\r\n    This function determines if a given path is considered directory-like, taking into account the behavior\r\n    specific to object storage platforms. For other filesystems, it behaves similarly to the standard `fs.isdir`\r\n    method.\r\n\r\n    Args:\r\n        fs: The filesystem to check the path against.\r\n        path: The path or URL to be checked.\r\n        strict: A flag specific to Object Storage platforms. If set to ``False``, any non-existing path is considered\r\n            as a valid directory-like path. In such cases, the directory (and any non-existing parent directories)\r\n            will be created on the fly. Defaults to False.", "docstring_tokens": ["check", "if", "a", "path", "is", "directory", "like", "this", "function", "determines", "if", "a", "given", "path", "is", "considered", "directory", "like", "taking", "into", "account", "the", "behavior", "specific", "to", "object", "storage", "platforms", "for", "other", "filesystems", "it", "behaves", "similarly", "to", "the", "standard", "fs", "isdir", "method", "args", "fs", "the", "filesystem", "to", "check", "the", "path", "against", "path", "the", "path", "or", "url", "to", "be", "checked", "strict", "a", "flag", "specific", "to", "object", "storage", "platforms", "if", "set", "to", "false", "any", "non", "existing", "path", "is", "considered", "as", "a", "valid", "directory", "like", "path", "in", "such", "cases", "the", "directory", "and", "any", "non", "existing", "parent", "directories", "will", "be", "created", "on", "the", "fly", "defaults", "to", "false"], "docstring_summary": "Check if a path is directory-like.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\cloud_io.py", "partition": "train", "function_type": "function", "start_line": 121, "end_line": 148, "hash": "ad381ebd1b631d9e00fe0d0c046f1ea4", "complexity": 3, "parameters": ["fs", "path", "Path]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "sized_len", "original_string": "def sized_len(dataloader: object) -> Optional[int]:\r\n    \"\"\"Try to get the length of an object, return ``None`` otherwise.\"\"\"\r\n    try:\r\n        # try getting the length\r\n        length = len(dataloader)  # type: ignore [arg-type]\r\n    except (TypeError, NotImplementedError):\r\n        length = None\r\n    return length", "language": "python", "code": "def sized_len(dataloader: object) -> Optional[int]:\r\n    \"\"\"Try to get the length of an object, return ``None`` otherwise.\"\"\"\r\n    try:\r\n        # try getting the length\r\n        length = len(dataloader)  # type: ignore [arg-type]\r\n    except (TypeError, NotImplementedError):\r\n        length = None\r\n    return length", "code_tokens": ["def", "sized_len", "(", "dataloader", ":", "object", ")", "-", ">", "Optional", "[", "int", "]", ":", "\"", "\"", "\"", "Try", "to", "get", "the", "length", "of", "an", "object", ",", "return", "`", "`", "None", "`", "`", "otherwise", ".", "\"", "\"", "\"", "try", ":", "length", "=", "len", "(", "dataloader", ")", "except", "(", "TypeError", ",", "NotImplementedError", ")", ":", "length", "=", "None", "return", "length"], "docstring": "Try to get the length of an object, return ``None`` otherwise.", "docstring_tokens": ["try", "to", "get", "the", "length", "of", "an", "object", "return", "none", "otherwise"], "docstring_summary": "Try to get the length of an object, return ``None`` otherwise.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 47, "end_line": 54, "hash": "fab97537b4491490f21687e175ad2d7b", "complexity": 2, "parameters": ["dataloader"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "has_len", "original_string": "def has_len(dataloader: object) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented.\"\"\"\r\n    length = sized_len(dataloader)\r\n    if length == 0:\r\n        rank_zero_warn(\r\n            f\"`{dataloader.__class__.__name__}` returned 0 length. Please make sure this was your intention.\"\r\n        )\r\n    if length is not None and has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return length is not None", "language": "python", "code": "def has_len(dataloader: object) -> TypeGuard[Sized]:\r\n    \"\"\"Checks if a given object has ``__len__`` method implemented.\"\"\"\r\n    length = sized_len(dataloader)\r\n    if length == 0:\r\n        rank_zero_warn(\r\n            f\"`{dataloader.__class__.__name__}` returned 0 length. Please make sure this was your intention.\"\r\n        )\r\n    if length is not None and has_iterable_dataset(dataloader):\r\n        rank_zero_warn(\r\n            \"Your `IterableDataset` has `__len__` defined.\"\r\n            \" In combination with multi-process data loading (when num_workers > 1),\"\r\n            \" `__len__` could be inaccurate if each worker is not configured independently\"\r\n            \" to avoid having duplicate data.\"\r\n        )\r\n    return length is not None", "code_tokens": ["def", "has_len", "(", "dataloader", ":", "object", ")", "-", ">", "TypeGuard", "[", "Sized", "]", ":", "\"", "\"", "\"", "Checks", "if", "a", "given", "object", "has", "`", "`", "__len__", "`", "`", "method", "implemented", ".", "\"", "\"", "\"", "length", "=", "sized_len", "(", "dataloader", ")", "if", "length", "=", "=", "0", ":", "rank_zero_warn", "(", "f", "\"", "`", "{", "dataloader", ".", "__class__", ".", "__name__", "}", "`", "returned", "0", "length", ".", "Please", "make", "sure", "this", "was", "your", "intention", ".", "\"", ")", "if", "length", "is", "not", "None", "and", "has_iterable_dataset", "(", "dataloader", ")", ":", "rank_zero_warn", "(", "\"", "Your", "`", "IterableDataset", "`", "has", "`", "__len__", "`", "defined", ".", "\"", "\"", "In", "combination", "with", "multi", "-", "process", "data", "loading", "(", "when", "num_workers", ">", "1", ")", ",", "\"", "\"", "`", "__len__", "`", "could", "be", "inaccurate", "if", "each", "worker", "is", "not", "configured", "independently", "\"", "\"", "to", "avoid", "having", "duplicate", "data", ".", "\"", ")", "return", "length", "is", "not", "None"], "docstring": "Checks if a given object has ``__len__`` method implemented.", "docstring_tokens": ["checks", "if", "a", "given", "object", "has", "__len__", "method", "implemented"], "docstring_summary": "Checks if a given object has ``__len__`` method implemented.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 57, "end_line": 71, "hash": "874297bfa21d74f3caab692195a0ddb0", "complexity": 4, "parameters": ["dataloader"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "_dataloader_init_kwargs_resolve_sampler", "original_string": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\"\"\"\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n\r\n    if batch_sampler is not None and type(batch_sampler) is not BatchSampler:\r\n        batch_sampler_cls = type(batch_sampler)\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            # This is a PyTorch `BatchSampler` subclass for which we captured the init args\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            # This is a sampler for which we could not capture the init args, but it kinda looks like a batch sampler\r\n            # even if it does not inherit from PyTorch's interface.\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=batch_sampler.drop_last,\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    # an unexpected `TypeError`, continue failure\r\n                    raise\r\n\r\n                # There could either be too few or too many arguments. Customizing the message based on this doesn't\r\n                # make much sense since our MisconfigurationException is going to be raised from the original one.\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                    \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                    \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        else:\r\n            # The sampler is not a PyTorch `BatchSampler`, we don't know how to inject a custom sampler\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "language": "python", "code": "def _dataloader_init_kwargs_resolve_sampler(\r\n    dataloader: DataLoader,\r\n    sampler: Union[Sampler, Iterable],\r\n) -> dict[str, Any]:\r\n    \"\"\"This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.\"\"\"\r\n    batch_sampler = getattr(dataloader, \"batch_sampler\")\r\n\r\n    if batch_sampler is not None and type(batch_sampler) is not BatchSampler:\r\n        batch_sampler_cls = type(batch_sampler)\r\n        if hasattr(batch_sampler, \"__pl_saved_args\"):\r\n            # This is a PyTorch `BatchSampler` subclass for which we captured the init args\r\n            args = batch_sampler.__pl_saved_args\r\n            kwargs = batch_sampler.__pl_saved_kwargs\r\n            default_kwargs = batch_sampler.__pl_saved_default_kwargs\r\n            arg_names = batch_sampler.__pl_saved_arg_names\r\n\r\n            success, args, kwargs = _replace_value_in_saved_args(\r\n                \"sampler\", sampler, args, kwargs, default_kwargs, arg_names\r\n            )\r\n            if not success:\r\n                raise TypeError(\r\n                    \"Trying to inject a modified sampler into the batch sampler; however, it seems the class \"\r\n                    f\"`{batch_sampler_cls.__qualname__}` does not have an argument called `sampler.` To mitigate \"\r\n                    \"this, expose an argument `sampler` in the `__init__` method of your custom class.\"\r\n                )\r\n\r\n            batch_sampler = _reinstantiate_wrapped_cls(batch_sampler, *args, **kwargs)\r\n        elif hasattr(batch_sampler, \"batch_size\") and hasattr(batch_sampler, \"drop_last\"):\r\n            # This is a sampler for which we could not capture the init args, but it kinda looks like a batch sampler\r\n            # even if it does not inherit from PyTorch's interface.\r\n            try:\r\n                batch_sampler = batch_sampler_cls(\r\n                    sampler,\r\n                    batch_size=batch_sampler.batch_size,\r\n                    drop_last=batch_sampler.drop_last,\r\n                )\r\n            except TypeError as ex:\r\n                import re\r\n\r\n                match = re.match(r\".*__init__\\(\\) (got multiple values)|(missing \\d required)\", str(ex))\r\n                if not match:\r\n                    # an unexpected `TypeError`, continue failure\r\n                    raise\r\n\r\n                # There could either be too few or too many arguments. Customizing the message based on this doesn't\r\n                # make much sense since our MisconfigurationException is going to be raised from the original one.\r\n                raise TypeError(\r\n                    \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                    \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                    \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                    \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n                ) from ex\r\n        else:\r\n            # The sampler is not a PyTorch `BatchSampler`, we don't know how to inject a custom sampler\r\n            raise TypeError(\r\n                \" Lightning can't inject a (distributed) sampler into your batch sampler, because it doesn't\"\r\n                \" subclass PyTorch's `BatchSampler`. To mitigate this, either follow the API of `BatchSampler`\"\r\n                \" or set`.setup_dataloaders(..., use_distributed_sampler=False)`. If you choose the latter, you\"\r\n                \" will be responsible for handling the distributed sampling within your batch sampler.\"\r\n            )\r\n\r\n        return {\r\n            \"sampler\": None,\r\n            \"shuffle\": False,\r\n            \"batch_sampler\": batch_sampler,\r\n            \"batch_size\": 1,\r\n            \"drop_last\": False,\r\n        }\r\n\r\n    return {\"sampler\": sampler, \"shuffle\": False, \"batch_sampler\": None}", "code_tokens": ["def", "_dataloader_init_kwargs_resolve_sampler", "(", "dataloader", ":", "DataLoader", ",", "sampler", ":", "Union", "[", "Sampler", ",", "Iterable", "]", ",", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "This", "function", "is", "used", "to", "handle", "the", "sampler", ",", "batch_sampler", "arguments", "associated", "within", "a", "DataLoader", "for", "its", "re", "-", "instantiation", ".", "\"", "\"", "\"", "batch_sampler", "=", "getattr", "(", "dataloader", ",", "\"", "batch_sampler", "\"", ")", "if", "batch_sampler", "is", "not", "None", "and", "type", "(", "batch_sampler", ")", "is", "not", "BatchSampler", ":", "batch_sampler_cls", "=", "type", "(", "batch_sampler", ")", "if", "hasattr", "(", "batch_sampler", ",", "\"", "__pl_saved_args", "\"", ")", ":", "args", "=", "batch_sampler", ".", "__pl_saved_args", "kwargs", "=", "batch_sampler", ".", "__pl_saved_kwargs", "default_kwargs", "=", "batch_sampler", ".", "__pl_saved_default_kwargs", "arg_names", "=", "batch_sampler", ".", "__pl_saved_arg_names", "success", ",", "args", ",", "kwargs", "=", "_replace_value_in_saved_args", "(", "\"", "sampler", "\"", ",", "sampler", ",", "args", ",", "kwargs", ",", "default_kwargs", ",", "arg_names", ")", "if", "not", "success", ":", "raise", "TypeError", "(", "\"", "Trying", "to", "inject", "a", "modified", "sampler", "into", "the", "batch", "sampler", ";", "however", ",", "it", "seems", "the", "class", "\"", "f", "\"", "`", "{", "batch_sampler_cls", ".", "__qualname__", "}", "`", "does", "not", "have", "an", "argument", "called", "`", "sampler", ".", "`", "To", "mitigate", "\"", "\"", "this", ",", "expose", "an", "argument", "`", "sampler", "`", "in", "the", "`", "__init__", "`", "method", "of", "your", "custom", "class", ".", "\"", ")", "batch_sampler", "=", "_reinstantiate_wrapped_cls", "(", "batch_sampler", ",", "*", "args", ",", "*", "*", "kwargs", ")", "elif", "hasattr", "(", "batch_sampler", ",", "\"", "batch_size", "\"", ")", "and", "hasattr", "(", "batch_sampler", ",", "\"", "drop_last", "\"", ")", ":", "try", ":", "batch_sampler", "=", "batch_sampler_cls", "(", "sampler", ",", "batch_size", "=", "batch_sampler", ".", "batch_size", ",", "drop_last", "=", "batch_sampler", ".", "drop_last", ",", ")", "except", "TypeError", "as", "ex", ":", "import", "re", "match", "=", "re", ".", "match", "(", "r", "\"", ".", "*", "__init__", "\\", "(", "\\", ")", "(", "got", "multiple", "values", ")", "|", "(", "missing", "\\", "d", "required", ")", "\"", ",", "str", "(", "ex", ")", ")", "if", "not", "match", ":", "raise", "raise", "TypeError", "(", "\"", "Lightning", "can", "'", "t", "inject", "a", "(", "distributed", ")", "sampler", "into", "your", "batch", "sampler", ",", "because", "it", "doesn", "'", "t", "\"", "\"", "subclass", "PyTorch", "'", "s", "`", "BatchSampler", "`", ".", "To", "mitigate", "this", ",", "either", "follow", "the", "API", "of", "`", "BatchSampler", "`", "\"", "\"", "or", "set", "`", ".", "setup_dataloaders", "(", ".", ".", ".", ",", "use_distributed_sampler", "=", "False", ")", "`", ".", "If", "you", "choose", "the", "latter", ",", "you", "\"", "\"", "will", "be", "responsible", "for", "handling", "the", "distributed", "sampling", "within", "your", "batch", "sampler", ".", "\"", ")", "from", "ex", "else", ":", "raise", "TypeError", "(", "\"", "Lightning", "can", "'", "t", "inject", "a", "(", "distributed", ")", "sampler", "into", "your", "batch", "sampler", ",", "because", "it", "doesn", "'", "t", "\"", "\"", "subclass", "PyTorch", "'", "s", "`", "BatchSampler", "`", ".", "To", "mitigate", "this", ",", "either", "follow", "the", "API", "of", "`", "BatchSampler", "`", "\"", "\"", "or", "set", "`", ".", "setup_dataloaders", "(", ".", ".", ".", ",", "use_distributed_sampler", "=", "False", ")", "`", ".", "If", "you", "choose", "the", "latter", ",", "you", "\"", "\"", "will", "be", "responsible", "for", "handling", "the", "distributed", "sampling", "within", "your", "batch", "sampler", ".", "\"", ")", "return", "{", "\"", "sampler", "\"", ":", "None", ",", "\"", "shuffle", "\"", ":", "False", ",", "\"", "batch_sampler", "\"", ":", "batch_sampler", ",", "\"", "batch_size", "\"", ":", "1", ",", "\"", "drop_last", "\"", ":", "False", ",", "}", "return", "{", "\"", "sampler", "\"", ":", "sampler", ",", "\"", "shuffle", "\"", ":", "False", ",", "\"", "batch_sampler", "\"", ":", "None", "}"], "docstring": "This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-\r\n    instantiation.", "docstring_tokens": ["this", "function", "is", "used", "to", "handle", "the", "sampler", "batch_sampler", "arguments", "associated", "within", "a", "dataloader", "for", "its", "re", "instantiation"], "docstring_summary": "This function is used to handle the sampler, batch_sampler arguments associated within a DataLoader for its re-", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 172, "end_line": 242, "hash": "6eb94d9970b9d70da6f61480e88864ab", "complexity": 9, "parameters": ["dataloader", "sampler", "Iterable]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "_wrap_init_method", "original_string": "def _wrap_init_method(init: Callable, store_explicit_arg: Optional[str] = None) -> Callable:\r\n    \"\"\"Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and\r\n    :class:`~torch.utils.data.BatchSampler`) in order to enable re-instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(init)\r\n    def wrapper(obj: Any, *args: Any, **kwargs: Any) -> None:\r\n        # We need to inspect `init`, as inspecting `obj.__init__`\r\n        # can lead to inspecting the wrong function with multiple inheritance\r\n        old_inside_init = getattr(obj, \"__pl_inside_init\", False)\r\n        object.__setattr__(obj, \"__pl_inside_init\", True)\r\n        params = inspect.signature(init).parameters\r\n\r\n        parameters_defaults = OrderedDict(\r\n            (param.name, param.default)\r\n            for param in params.values()\r\n            if param.name != \"self\" and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)\r\n        )\r\n\r\n        param_names = tuple(parameters_defaults)[: len(args)]\r\n\r\n        default_kwargs = {\r\n            name: value\r\n            for name, value in parameters_defaults.items()\r\n            if name not in kwargs and name not in param_names and value != inspect.Parameter.empty\r\n        }\r\n\r\n        if not hasattr(obj, \"__pl_saved_args\"):\r\n            object.__setattr__(obj, \"__pl_saved_args\", args)\r\n            object.__setattr__(obj, \"__pl_saved_kwargs\", kwargs)\r\n            object.__setattr__(obj, \"__pl_saved_arg_names\", param_names)\r\n            object.__setattr__(obj, \"__pl_saved_default_kwargs\", default_kwargs)\r\n\r\n        # We want to use the latest possible value for explicit argument (i.e. ideally what gets passed to base class)\r\n        # so that we can be sure, that it will not get changed anymore.\r\n        # That is why we are setting this in every `__init__`\r\n        if store_explicit_arg is not None:\r\n            if store_explicit_arg in param_names:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", args[param_names.index(store_explicit_arg)])\r\n            elif store_explicit_arg in kwargs:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", kwargs[store_explicit_arg])\r\n\r\n        init(obj, *args, **kwargs)\r\n        object.__setattr__(obj, \"__pl_inside_init\", old_inside_init)\r\n\r\n    return wrapper", "language": "python", "code": "def _wrap_init_method(init: Callable, store_explicit_arg: Optional[str] = None) -> Callable:\r\n    \"\"\"Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and\r\n    :class:`~torch.utils.data.BatchSampler`) in order to enable re-instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(init)\r\n    def wrapper(obj: Any, *args: Any, **kwargs: Any) -> None:\r\n        # We need to inspect `init`, as inspecting `obj.__init__`\r\n        # can lead to inspecting the wrong function with multiple inheritance\r\n        old_inside_init = getattr(obj, \"__pl_inside_init\", False)\r\n        object.__setattr__(obj, \"__pl_inside_init\", True)\r\n        params = inspect.signature(init).parameters\r\n\r\n        parameters_defaults = OrderedDict(\r\n            (param.name, param.default)\r\n            for param in params.values()\r\n            if param.name != \"self\" and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)\r\n        )\r\n\r\n        param_names = tuple(parameters_defaults)[: len(args)]\r\n\r\n        default_kwargs = {\r\n            name: value\r\n            for name, value in parameters_defaults.items()\r\n            if name not in kwargs and name not in param_names and value != inspect.Parameter.empty\r\n        }\r\n\r\n        if not hasattr(obj, \"__pl_saved_args\"):\r\n            object.__setattr__(obj, \"__pl_saved_args\", args)\r\n            object.__setattr__(obj, \"__pl_saved_kwargs\", kwargs)\r\n            object.__setattr__(obj, \"__pl_saved_arg_names\", param_names)\r\n            object.__setattr__(obj, \"__pl_saved_default_kwargs\", default_kwargs)\r\n\r\n        # We want to use the latest possible value for explicit argument (i.e. ideally what gets passed to base class)\r\n        # so that we can be sure, that it will not get changed anymore.\r\n        # That is why we are setting this in every `__init__`\r\n        if store_explicit_arg is not None:\r\n            if store_explicit_arg in param_names:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", args[param_names.index(store_explicit_arg)])\r\n            elif store_explicit_arg in kwargs:\r\n                object.__setattr__(obj, f\"__{store_explicit_arg}\", kwargs[store_explicit_arg])\r\n\r\n        init(obj, *args, **kwargs)\r\n        object.__setattr__(obj, \"__pl_inside_init\", old_inside_init)\r\n\r\n    return wrapper", "code_tokens": ["def", "_wrap_init_method", "(", "init", ":", "Callable", ",", "store_explicit_arg", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Wraps", "the", "`", "`", "__init__", "`", "`", "method", "of", "classes", "(", "currently", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "and", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "BatchSampler", "`", ")", "in", "order", "to", "enable", "re", "-", "instantiation", "of", "custom", "subclasses", ".", "\"", "\"", "\"", "@", "functools", ".", "wraps", "(", "init", ")", "def", "wrapper", "(", "obj", ":", "Any", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "old_inside_init", "=", "getattr", "(", "obj", ",", "\"", "__pl_inside_init", "\"", ",", "False", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_inside_init", "\"", ",", "True", ")", "params", "=", "inspect", ".", "signature", "(", "init", ")", ".", "parameters", "parameters_defaults", "=", "OrderedDict", "(", "(", "param", ".", "name", ",", "param", ".", "default", ")", "for", "param", "in", "params", ".", "values", "(", ")", "if", "param", ".", "name", "!", "=", "\"", "self", "\"", "and", "param", ".", "kind", "not", "in", "(", "param", ".", "VAR_POSITIONAL", ",", "param", ".", "VAR_KEYWORD", ")", ")", "param_names", "=", "tuple", "(", "parameters_defaults", ")", "[", ":", "len", "(", "args", ")", "]", "default_kwargs", "=", "{", "name", ":", "value", "for", "name", ",", "value", "in", "parameters_defaults", ".", "items", "(", ")", "if", "name", "not", "in", "kwargs", "and", "name", "not", "in", "param_names", "and", "value", "!", "=", "inspect", ".", "Parameter", ".", "empty", "}", "if", "not", "hasattr", "(", "obj", ",", "\"", "__pl_saved_args", "\"", ")", ":", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_saved_args", "\"", ",", "args", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_saved_kwargs", "\"", ",", "kwargs", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_saved_arg_names", "\"", ",", "param_names", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_saved_default_kwargs", "\"", ",", "default_kwargs", ")", "if", "store_explicit_arg", "is", "not", "None", ":", "if", "store_explicit_arg", "in", "param_names", ":", "object", ".", "__setattr__", "(", "obj", ",", "f", "\"", "__", "{", "store_explicit_arg", "}", "\"", ",", "args", "[", "param_names", ".", "index", "(", "store_explicit_arg", ")", "]", ")", "elif", "store_explicit_arg", "in", "kwargs", ":", "object", ".", "__setattr__", "(", "obj", ",", "f", "\"", "__", "{", "store_explicit_arg", "}", "\"", ",", "kwargs", "[", "store_explicit_arg", "]", ")", "init", "(", "obj", ",", "*", "args", ",", "*", "*", "kwargs", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_inside_init", "\"", ",", "old_inside_init", ")", "return", "wrapper"], "docstring": "Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and\r\n    :class:`~torch.utils.data.BatchSampler`) in order to enable re-instantiation of custom subclasses.", "docstring_tokens": ["wraps", "the", "__init__", "method", "of", "classes", "currently", "class", "torch", "utils", "data", "dataloader", "and", "class", "torch", "utils", "data", "batchsampler", "in", "order", "to", "enable", "re", "instantiation", "of", "custom", "subclasses"], "docstring_summary": "Wraps the ``__init__`` method of classes (currently :class:`~torch.utils.data.DataLoader` and", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 283, "end_line": 327, "hash": "6f831d9bdd85ccbcaed9ba2f418b562e", "complexity": 12, "parameters": ["init", "store_explicit_arg"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "_wrap_attr_method", "original_string": "def _wrap_attr_method(method: Callable, tag: _WrapAttrTag) -> Callable:\r\n    \"\"\"Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`\r\n    and :class:`~torch.utils.data.BatchSampler`) in order to enable re- instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(method)\r\n    def wrapper(obj: Any, *args: Any) -> None:\r\n        # First, let's find out if we're the first in inheritance chain calling the patched method.\r\n        name, *_ = args\r\n        prev_call_name, prev_call_method = getattr(obj, \"__pl_current_call\", (None, \"method\"))\r\n        first_call = not (prev_call_name == name and prev_call_method == tag)\r\n\r\n        # Then mark the current called method\r\n        object.__setattr__(obj, \"__pl_current_call\", (name, tag))\r\n\r\n        # call original method\r\n        method(obj, *args)\r\n        if first_call and not getattr(obj, \"__pl_inside_init\", True):\r\n            # and save the value it was called with to the internal list,\r\n            # if we're outside of __init__ and the original call did not fail and we're the first call\r\n            attrs_record = getattr(obj, \"__pl_attrs_record\", [])\r\n            attrs_record.append((args, tag))\r\n            object.__setattr__(obj, \"__pl_attrs_record\", attrs_record)\r\n        object.__setattr__(obj, \"__pl_current_call\", (prev_call_name, prev_call_method))\r\n\r\n    return wrapper", "language": "python", "code": "def _wrap_attr_method(method: Callable, tag: _WrapAttrTag) -> Callable:\r\n    \"\"\"Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`\r\n    and :class:`~torch.utils.data.BatchSampler`) in order to enable re- instantiation of custom subclasses.\"\"\"\r\n\r\n    @functools.wraps(method)\r\n    def wrapper(obj: Any, *args: Any) -> None:\r\n        # First, let's find out if we're the first in inheritance chain calling the patched method.\r\n        name, *_ = args\r\n        prev_call_name, prev_call_method = getattr(obj, \"__pl_current_call\", (None, \"method\"))\r\n        first_call = not (prev_call_name == name and prev_call_method == tag)\r\n\r\n        # Then mark the current called method\r\n        object.__setattr__(obj, \"__pl_current_call\", (name, tag))\r\n\r\n        # call original method\r\n        method(obj, *args)\r\n        if first_call and not getattr(obj, \"__pl_inside_init\", True):\r\n            # and save the value it was called with to the internal list,\r\n            # if we're outside of __init__ and the original call did not fail and we're the first call\r\n            attrs_record = getattr(obj, \"__pl_attrs_record\", [])\r\n            attrs_record.append((args, tag))\r\n            object.__setattr__(obj, \"__pl_attrs_record\", attrs_record)\r\n        object.__setattr__(obj, \"__pl_current_call\", (prev_call_name, prev_call_method))\r\n\r\n    return wrapper", "code_tokens": ["def", "_wrap_attr_method", "(", "method", ":", "Callable", ",", "tag", ":", "_WrapAttrTag", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Wraps", "the", "`", "`", "__setattr__", "`", "`", "or", "`", "`", "__delattr__", "`", "`", "method", "of", "classes", "(", "currently", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "and", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "BatchSampler", "`", ")", "in", "order", "to", "enable", "re", "-", "instantiation", "of", "custom", "subclasses", ".", "\"", "\"", "\"", "@", "functools", ".", "wraps", "(", "method", ")", "def", "wrapper", "(", "obj", ":", "Any", ",", "*", "args", ":", "Any", ")", "-", ">", "None", ":", "name", ",", "*", "_", "=", "args", "prev_call_name", ",", "prev_call_method", "=", "getattr", "(", "obj", ",", "\"", "__pl_current_call", "\"", ",", "(", "None", ",", "\"", "method", "\"", ")", ")", "first_call", "=", "not", "(", "prev_call_name", "=", "=", "name", "and", "prev_call_method", "=", "=", "tag", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_current_call", "\"", ",", "(", "name", ",", "tag", ")", ")", "method", "(", "obj", ",", "*", "args", ")", "if", "first_call", "and", "not", "getattr", "(", "obj", ",", "\"", "__pl_inside_init", "\"", ",", "True", ")", ":", "attrs_record", "=", "getattr", "(", "obj", ",", "\"", "__pl_attrs_record", "\"", ",", "[", "]", ")", "attrs_record", ".", "append", "(", "(", "args", ",", "tag", ")", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_attrs_record", "\"", ",", "attrs_record", ")", "object", ".", "__setattr__", "(", "obj", ",", "\"", "__pl_current_call", "\"", ",", "(", "prev_call_name", ",", "prev_call_method", ")", ")", "return", "wrapper"], "docstring": "Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`\r\n    and :class:`~torch.utils.data.BatchSampler`) in order to enable re- instantiation of custom subclasses.", "docstring_tokens": ["wraps", "the", "__setattr__", "or", "__delattr__", "method", "of", "classes", "currently", "class", "torch", "utils", "data", "dataloader", "and", "class", "torch", "utils", "data", "batchsampler", "in", "order", "to", "enable", "re", "instantiation", "of", "custom", "subclasses"], "docstring_summary": "Wraps the ``__setattr__`` or ``__delattr__`` method of classes (currently :class:`~torch.utils.data.DataLoader`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 330, "end_line": 354, "hash": "a94ef7d55bec0b6ccd098629b8c874ba", "complexity": 4, "parameters": ["method", "tag"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "_replace_dunder_methods", "original_string": "def _replace_dunder_methods(base_cls: type, store_explicit_arg: Optional[str] = None) -> Generator[None, None, None]:\r\n    \"\"\"This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.\r\n\r\n    It patches the ``__init__``, ``__setattr__`` and ``__delattr__`` methods.\r\n\r\n    \"\"\"\r\n    classes = get_all_subclasses(base_cls) | {base_cls}\r\n    for cls in classes:\r\n        # Check that __init__ belongs to the class\r\n        # https://stackoverflow.com/a/5253424\r\n        if \"__init__\" in cls.__dict__:\r\n            cls.__old__init__ = cls.__init__  # type: ignore[misc]\r\n            cls.__init__ = _wrap_init_method(cls.__init__, store_explicit_arg)  # type: ignore[misc]\r\n\r\n        # we want at least one setattr/delattr in the chain to be patched and it can happen, that none of the subclasses\r\n        # implement `__setattr__`/`__delattr__`. Therefore, we are always patching the `base_cls`\r\n        for patch_fn_name, tag in ((\"__setattr__\", _WrapAttrTag.SET), (\"__delattr__\", _WrapAttrTag.DEL)):\r\n            if patch_fn_name in cls.__dict__ or cls is base_cls:\r\n                saved_name = f\"__old{patch_fn_name}\"\r\n                setattr(cls, saved_name, getattr(cls, patch_fn_name))\r\n                setattr(cls, patch_fn_name, _wrap_attr_method(getattr(cls, patch_fn_name), tag))\r\n    yield\r\n    for cls in classes:\r\n        for patched_name in (\"__setattr__\", \"__delattr__\", \"__init__\"):\r\n            # Check that __old__{init,setattr,delattr} belongs to the class\r\n            # https://stackoverflow.com/a/5253424\r\n            if f\"__old{patched_name}\" in cls.__dict__:\r\n                setattr(cls, patched_name, getattr(cls, f\"__old{patched_name}\"))\r\n                delattr(cls, f\"__old{patched_name}\")", "language": "python", "code": "def _replace_dunder_methods(base_cls: type, store_explicit_arg: Optional[str] = None) -> Generator[None, None, None]:\r\n    \"\"\"This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.\r\n\r\n    It patches the ``__init__``, ``__setattr__`` and ``__delattr__`` methods.\r\n\r\n    \"\"\"\r\n    classes = get_all_subclasses(base_cls) | {base_cls}\r\n    for cls in classes:\r\n        # Check that __init__ belongs to the class\r\n        # https://stackoverflow.com/a/5253424\r\n        if \"__init__\" in cls.__dict__:\r\n            cls.__old__init__ = cls.__init__  # type: ignore[misc]\r\n            cls.__init__ = _wrap_init_method(cls.__init__, store_explicit_arg)  # type: ignore[misc]\r\n\r\n        # we want at least one setattr/delattr in the chain to be patched and it can happen, that none of the subclasses\r\n        # implement `__setattr__`/`__delattr__`. Therefore, we are always patching the `base_cls`\r\n        for patch_fn_name, tag in ((\"__setattr__\", _WrapAttrTag.SET), (\"__delattr__\", _WrapAttrTag.DEL)):\r\n            if patch_fn_name in cls.__dict__ or cls is base_cls:\r\n                saved_name = f\"__old{patch_fn_name}\"\r\n                setattr(cls, saved_name, getattr(cls, patch_fn_name))\r\n                setattr(cls, patch_fn_name, _wrap_attr_method(getattr(cls, patch_fn_name), tag))\r\n    yield\r\n    for cls in classes:\r\n        for patched_name in (\"__setattr__\", \"__delattr__\", \"__init__\"):\r\n            # Check that __old__{init,setattr,delattr} belongs to the class\r\n            # https://stackoverflow.com/a/5253424\r\n            if f\"__old{patched_name}\" in cls.__dict__:\r\n                setattr(cls, patched_name, getattr(cls, f\"__old{patched_name}\"))\r\n                delattr(cls, f\"__old{patched_name}\")", "code_tokens": ["def", "_replace_dunder_methods", "(", "base_cls", ":", "type", ",", "store_explicit_arg", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "This", "context", "manager", "is", "used", "to", "add", "support", "for", "re", "-", "instantiation", "of", "custom", "(", "subclasses", ")", "of", "`", "base_cls", "`", ".", "It", "patches", "the", "`", "`", "__init__", "`", "`", ",", "`", "`", "__setattr__", "`", "`", "and", "`", "`", "__delattr__", "`", "`", "methods", ".", "\"", "\"", "\"", "classes", "=", "get_all_subclasses", "(", "base_cls", ")", "|", "{", "base_cls", "}", "for", "cls", "in", "classes", ":", "if", "\"", "__init__", "\"", "in", "cls", ".", "__dict__", ":", "cls", ".", "__old__init__", "=", "cls", ".", "__init__", "cls", ".", "__init__", "=", "_wrap_init_method", "(", "cls", ".", "__init__", ",", "store_explicit_arg", ")", "for", "patch_fn_name", ",", "tag", "in", "(", "(", "\"", "__setattr__", "\"", ",", "_WrapAttrTag", ".", "SET", ")", ",", "(", "\"", "__delattr__", "\"", ",", "_WrapAttrTag", ".", "DEL", ")", ")", ":", "if", "patch_fn_name", "in", "cls", ".", "__dict__", "or", "cls", "is", "base_cls", ":", "saved_name", "=", "f", "\"", "__old", "{", "patch_fn_name", "}", "\"", "setattr", "(", "cls", ",", "saved_name", ",", "getattr", "(", "cls", ",", "patch_fn_name", ")", ")", "setattr", "(", "cls", ",", "patch_fn_name", ",", "_wrap_attr_method", "(", "getattr", "(", "cls", ",", "patch_fn_name", ")", ",", "tag", ")", ")", "yield", "for", "cls", "in", "classes", ":", "for", "patched_name", "in", "(", "\"", "__setattr__", "\"", ",", "\"", "__delattr__", "\"", ",", "\"", "__init__", "\"", ")", ":", "if", "f", "\"", "__old", "{", "patched_name", "}", "\"", "in", "cls", ".", "__dict__", ":", "setattr", "(", "cls", ",", "patched_name", ",", "getattr", "(", "cls", ",", "f", "\"", "__old", "{", "patched_name", "}", "\"", ")", ")", "delattr", "(", "cls", ",", "f", "\"", "__old", "{", "patched_name", "}", "\"", ")"], "docstring": "This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.\r\n\r\n    It patches the ``__init__``, ``__setattr__`` and ``__delattr__`` methods.", "docstring_tokens": ["this", "context", "manager", "is", "used", "to", "add", "support", "for", "re", "instantiation", "of", "custom", "subclasses", "of", "base_cls", "it", "patches", "the", "__init__", "__setattr__", "and", "__delattr__", "methods"], "docstring_summary": "This context manager is used to add support for re-instantiation of custom (subclasses) of `base_cls`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 358, "end_line": 386, "hash": "e249d85fada7658260aa8da6f73bd205", "complexity": 9, "parameters": ["base_cls", "store_explicit_arg"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "_replace_value_in_saved_args", "original_string": "def _replace_value_in_saved_args(\r\n    replace_key: str,\r\n    replace_value: Any,\r\n    args: tuple[Any, ...],\r\n    kwargs: dict[str, Any],\r\n    default_kwargs: dict[str, Any],\r\n    arg_names: tuple[str, ...],\r\n) -> tuple[bool, tuple[Any, ...], dict[str, Any]]:\r\n    \"\"\"Tries to replace an argument value in a saved list of args and kwargs.\r\n\r\n    Returns a tuple indicating success of the operation and modified saved args and kwargs\r\n\r\n    \"\"\"\r\n\r\n    if replace_key in arg_names:\r\n        replace_index = arg_names.index(replace_key)\r\n        args = args[:replace_index] + (replace_value,) + args[replace_index + 1 :]\r\n        return True, args, kwargs\r\n    if replace_key in kwargs or replace_key in default_kwargs:\r\n        kwargs[replace_key] = replace_value\r\n        return True, args, kwargs\r\n\r\n    return False, args, kwargs", "language": "python", "code": "def _replace_value_in_saved_args(\r\n    replace_key: str,\r\n    replace_value: Any,\r\n    args: tuple[Any, ...],\r\n    kwargs: dict[str, Any],\r\n    default_kwargs: dict[str, Any],\r\n    arg_names: tuple[str, ...],\r\n) -> tuple[bool, tuple[Any, ...], dict[str, Any]]:\r\n    \"\"\"Tries to replace an argument value in a saved list of args and kwargs.\r\n\r\n    Returns a tuple indicating success of the operation and modified saved args and kwargs\r\n\r\n    \"\"\"\r\n\r\n    if replace_key in arg_names:\r\n        replace_index = arg_names.index(replace_key)\r\n        args = args[:replace_index] + (replace_value,) + args[replace_index + 1 :]\r\n        return True, args, kwargs\r\n    if replace_key in kwargs or replace_key in default_kwargs:\r\n        kwargs[replace_key] = replace_value\r\n        return True, args, kwargs\r\n\r\n    return False, args, kwargs", "code_tokens": ["def", "_replace_value_in_saved_args", "(", "replace_key", ":", "str", ",", "replace_value", ":", "Any", ",", "args", ":", "tuple", "[", "Any", ",", ".", ".", ".", "]", ",", "kwargs", ":", "dict", "[", "str", ",", "Any", "]", ",", "default_kwargs", ":", "dict", "[", "str", ",", "Any", "]", ",", "arg_names", ":", "tuple", "[", "str", ",", ".", ".", ".", "]", ",", ")", "-", ">", "tuple", "[", "bool", ",", "tuple", "[", "Any", ",", ".", ".", ".", "]", ",", "dict", "[", "str", ",", "Any", "]", "]", ":", "\"", "\"", "\"", "Tries", "to", "replace", "an", "argument", "value", "in", "a", "saved", "list", "of", "args", "and", "kwargs", ".", "Returns", "a", "tuple", "indicating", "success", "of", "the", "operation", "and", "modified", "saved", "args", "and", "kwargs", "\"", "\"", "\"", "if", "replace_key", "in", "arg_names", ":", "replace_index", "=", "arg_names", ".", "index", "(", "replace_key", ")", "args", "=", "args", "[", ":", "replace_index", "]", "+", "(", "replace_value", ",", ")", "+", "args", "[", "replace_index", "+", "1", ":", "]", "return", "True", ",", "args", ",", "kwargs", "if", "replace_key", "in", "kwargs", "or", "replace_key", "in", "default_kwargs", ":", "kwargs", "[", "replace_key", "]", "=", "replace_value", "return", "True", ",", "args", ",", "kwargs", "return", "False", ",", "args", ",", "kwargs"], "docstring": "Tries to replace an argument value in a saved list of args and kwargs.\r\n\r\n    Returns a tuple indicating success of the operation and modified saved args and kwargs", "docstring_tokens": ["tries", "to", "replace", "an", "argument", "value", "in", "a", "saved", "list", "of", "args", "and", "kwargs", "returns", "a", "tuple", "indicating", "success", "of", "the", "operation", "and", "modified", "saved", "args", "and", "kwargs"], "docstring_summary": "Tries to replace an argument value in a saved list of args and kwargs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 389, "end_line": 411, "hash": "a9a67727e1fa236b652917927078ec2a", "complexity": 4, "parameters": ["replace_key", "replace_value", "args", "...]", "kwargs", "Any]", "default_kwargs", "Any]", "arg_names", "...]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "_set_sampler_epoch", "original_string": "def _set_sampler_epoch(dataloader: object, epoch: int) -> None:\r\n    \"\"\"Calls the ``set_epoch`` method on either the sampler of the given dataloader.\r\n\r\n    Every PyTorch dataloader has either a sampler or a batch sampler. If the sampler is wrapped by a\r\n    :class:`~torch.utils.data.distributed.DistributedSampler`, ``set_epoch`` must be called at the beginning\r\n    of every epoch to ensure shuffling applies a new ordering. This has no effect if shuffling is off.\r\n\r\n    \"\"\"\r\n    # cannot use a set because samplers might be unhashable: use a dict based on the id to drop duplicates\r\n    objects: dict[int, Any] = {}\r\n    # check dataloader.sampler\r\n    if (sampler := getattr(dataloader, \"sampler\", None)) is not None:\r\n        objects[id(sampler)] = sampler\r\n    # check dataloader.batch_sampler.sampler\r\n    if (batch_sampler := getattr(dataloader, \"batch_sampler\", None)) is not None and (\r\n        sampler := getattr(batch_sampler, \"sampler\", None)\r\n    ) is not None:\r\n        objects[id(sampler)] = sampler\r\n    for obj in objects.values():\r\n        set_epoch = getattr(obj, \"set_epoch\", None)\r\n        if callable(set_epoch):\r\n            set_epoch(epoch)", "language": "python", "code": "def _set_sampler_epoch(dataloader: object, epoch: int) -> None:\r\n    \"\"\"Calls the ``set_epoch`` method on either the sampler of the given dataloader.\r\n\r\n    Every PyTorch dataloader has either a sampler or a batch sampler. If the sampler is wrapped by a\r\n    :class:`~torch.utils.data.distributed.DistributedSampler`, ``set_epoch`` must be called at the beginning\r\n    of every epoch to ensure shuffling applies a new ordering. This has no effect if shuffling is off.\r\n\r\n    \"\"\"\r\n    # cannot use a set because samplers might be unhashable: use a dict based on the id to drop duplicates\r\n    objects: dict[int, Any] = {}\r\n    # check dataloader.sampler\r\n    if (sampler := getattr(dataloader, \"sampler\", None)) is not None:\r\n        objects[id(sampler)] = sampler\r\n    # check dataloader.batch_sampler.sampler\r\n    if (batch_sampler := getattr(dataloader, \"batch_sampler\", None)) is not None and (\r\n        sampler := getattr(batch_sampler, \"sampler\", None)\r\n    ) is not None:\r\n        objects[id(sampler)] = sampler\r\n    for obj in objects.values():\r\n        set_epoch = getattr(obj, \"set_epoch\", None)\r\n        if callable(set_epoch):\r\n            set_epoch(epoch)", "code_tokens": ["def", "_set_sampler_epoch", "(", "dataloader", ":", "object", ",", "epoch", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "the", "`", "`", "set_epoch", "`", "`", "method", "on", "either", "the", "sampler", "of", "the", "given", "dataloader", ".", "Every", "PyTorch", "dataloader", "has", "either", "a", "sampler", "or", "a", "batch", "sampler", ".", "If", "the", "sampler", "is", "wrapped", "by", "a", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "`", ",", "`", "`", "set_epoch", "`", "`", "must", "be", "called", "at", "the", "beginning", "of", "every", "epoch", "to", "ensure", "shuffling", "applies", "a", "new", "ordering", ".", "This", "has", "no", "effect", "if", "shuffling", "is", "off", ".", "\"", "\"", "\"", "objects", ":", "dict", "[", "int", ",", "Any", "]", "=", "{", "}", "if", "(", "sampler", ":", "=", "getattr", "(", "dataloader", ",", "\"", "sampler", "\"", ",", "None", ")", ")", "is", "not", "None", ":", "objects", "[", "id", "(", "sampler", ")", "]", "=", "sampler", "if", "(", "batch_sampler", ":", "=", "getattr", "(", "dataloader", ",", "\"", "batch_sampler", "\"", ",", "None", ")", ")", "is", "not", "None", "and", "(", "sampler", ":", "=", "getattr", "(", "batch_sampler", ",", "\"", "sampler", "\"", ",", "None", ")", ")", "is", "not", "None", ":", "objects", "[", "id", "(", "sampler", ")", "]", "=", "sampler", "for", "obj", "in", "objects", ".", "values", "(", ")", ":", "set_epoch", "=", "getattr", "(", "obj", ",", "\"", "set_epoch", "\"", ",", "None", ")", "if", "callable", "(", "set_epoch", ")", ":", "set_epoch", "(", "epoch", ")"], "docstring": "Calls the ``set_epoch`` method on either the sampler of the given dataloader.\r\n\r\n    Every PyTorch dataloader has either a sampler or a batch sampler. If the sampler is wrapped by a\r\n    :class:`~torch.utils.data.distributed.DistributedSampler`, ``set_epoch`` must be called at the beginning\r\n    of every epoch to ensure shuffling applies a new ordering. This has no effect if shuffling is off.", "docstring_tokens": ["calls", "the", "set_epoch", "method", "on", "either", "the", "sampler", "of", "the", "given", "dataloader", "every", "pytorch", "dataloader", "has", "either", "a", "sampler", "or", "a", "batch", "sampler", "if", "the", "sampler", "is", "wrapped", "by", "a", "class", "torch", "utils", "data", "distributed", "distributedsampler", "set_epoch", "must", "be", "called", "at", "the", "beginning", "of", "every", "epoch", "to", "ensure", "shuffling", "applies", "a", "new", "ordering", "this", "has", "no", "effect", "if", "shuffling", "is", "off"], "docstring_summary": "Calls the ``set_epoch`` method on either the sampler of the given dataloader.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 414, "end_line": 435, "hash": "b22e508b75bec97fb3710835b2a44fd2", "complexity": 6, "parameters": ["dataloader", "epoch"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\data.py", "func_name": "suggested_max_num_workers", "original_string": "def suggested_max_num_workers(local_world_size: int) -> int:\r\n    \"\"\"Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on\r\n    the number of CPU cores available on the system and the number of distributed processes in the current machine.\r\n\r\n    Args:\r\n        local_world_size: The number of distributed processes running on the current machine. Set this to the number\r\n            of devices configured in Fabric/Trainer.\r\n\r\n    \"\"\"\r\n    if local_world_size < 1:\r\n        raise ValueError(f\"`local_world_size` should be >= 1, got {local_world_size}.\")\r\n    cpu_count = _num_cpus_available()\r\n    return max(1, cpu_count // local_world_size - 1)  # -1 to leave some resources for main process\r", "language": "python", "code": "def suggested_max_num_workers(local_world_size: int) -> int:\r\n    \"\"\"Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on\r\n    the number of CPU cores available on the system and the number of distributed processes in the current machine.\r\n\r\n    Args:\r\n        local_world_size: The number of distributed processes running on the current machine. Set this to the number\r\n            of devices configured in Fabric/Trainer.\r\n\r\n    \"\"\"\r\n    if local_world_size < 1:\r\n        raise ValueError(f\"`local_world_size` should be >= 1, got {local_world_size}.\")\r\n    cpu_count = _num_cpus_available()\r\n    return max(1, cpu_count // local_world_size - 1)  # -1 to leave some resources for main process\r", "code_tokens": ["def", "suggested_max_num_workers", "(", "local_world_size", ":", "int", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Suggests", "an", "upper", "bound", "of", "`", "`", "num_workers", "`", "`", "to", "use", "in", "a", "PyTorch", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "based", "on", "the", "number", "of", "CPU", "cores", "available", "on", "the", "system", "and", "the", "number", "of", "distributed", "processes", "in", "the", "current", "machine", ".", "Args", ":", "local_world_size", ":", "The", "number", "of", "distributed", "processes", "running", "on", "the", "current", "machine", ".", "Set", "this", "to", "the", "number", "of", "devices", "configured", "in", "Fabric", "/", "Trainer", ".", "\"", "\"", "\"", "if", "local_world_size", "<", "1", ":", "raise", "ValueError", "(", "f", "\"", "`", "local_world_size", "`", "should", "be", ">", "=", "1", ",", "got", "{", "local_world_size", "}", ".", "\"", ")", "cpu_count", "=", "_num_cpus_available", "(", ")", "return", "max", "(", "1", ",", "cpu_count", "/", "/", "local_world_size", "-", "1", ")"], "docstring": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on\r\n    the number of CPU cores available on the system and the number of distributed processes in the current machine.\r\n\r\n    Args:\r\n        local_world_size: The number of distributed processes running on the current machine. Set this to the number\r\n            of devices configured in Fabric/Trainer.", "docstring_tokens": ["suggests", "an", "upper", "bound", "of", "num_workers", "to", "use", "in", "a", "pytorch", "class", "torch", "utils", "data", "dataloader", "based", "on", "the", "number", "of", "cpu", "cores", "available", "on", "the", "system", "and", "the", "number", "of", "distributed", "processes", "in", "the", "current", "machine", "args", "local_world_size", "the", "number", "of", "distributed", "processes", "running", "on", "the", "current", "machine", "set", "this", "to", "the", "number", "of", "devices", "configured", "in", "fabric", "trainer"], "docstring_summary": "Suggests an upper bound of ``num_workers`` to use in a PyTorch :class:`~torch.utils.data.DataLoader` based on", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\data.py", "partition": "train", "function_type": "function", "start_line": 438, "end_line": 450, "hash": "2ded2d42073af9feed7c7f513436a441", "complexity": 2, "parameters": ["local_world_size"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "to", "original_string": "def to(self, *args: Any, **kwargs: Any) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.to`.\"\"\"\r\n        # this converts `str` device to `torch.device`\r\n        device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]\r\n        _update_properties(self, device=device, dtype=dtype)\r\n        return super().to(*args, **kwargs)", "language": "python", "code": "def to(self, *args: Any, **kwargs: Any) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.to`.\"\"\"\r\n        # this converts `str` device to `torch.device`\r\n        device, dtype = torch._C._nn._parse_to(*args, **kwargs)[:2]\r\n        _update_properties(self, device=device, dtype=dtype)\r\n        return super().to(*args, **kwargs)", "code_tokens": ["def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "to", "`", ".", "\"", "\"", "\"", "device", ",", "dtype", "=", "torch", ".", "_C", ".", "_nn", ".", "_parse_to", "(", "*", "args", ",", "*", "*", "kwargs", ")", "[", ":", "2", "]", "_update_properties", "(", "self", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "return", "super", "(", ")", ".", "to", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "See :meth:`torch.nn.Module.to`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "to"], "docstring_summary": "See :meth:`torch.nn.Module.to`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 53, "end_line": 58, "hash": "2b3245a5efdc868f9637948b076d6b5c", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "cuda", "original_string": "def cuda(self, device: Optional[Union[torch.device, int]] = None) -> Self:\r\n        \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\r\n        different objects. So it should be called before constructing optimizer if the module will live on GPU while\r\n        being optimized.\r\n\r\n        Arguments:\r\n            device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device\r\n                index will be used.\r\n\r\n        Returns:\r\n            Module: self\r\n\r\n        \"\"\"\r\n        if device is None:\r\n            device = torch.device(\"cuda\", torch.cuda.current_device())\r\n        elif isinstance(device, int):\r\n            device = torch.device(\"cuda\", index=device)\r\n        _update_properties(self, device=device)\r\n        return super().cuda(device=device)", "language": "python", "code": "def cuda(self, device: Optional[Union[torch.device, int]] = None) -> Self:\r\n        \"\"\"Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\r\n        different objects. So it should be called before constructing optimizer if the module will live on GPU while\r\n        being optimized.\r\n\r\n        Arguments:\r\n            device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device\r\n                index will be used.\r\n\r\n        Returns:\r\n            Module: self\r\n\r\n        \"\"\"\r\n        if device is None:\r\n            device = torch.device(\"cuda\", torch.cuda.current_device())\r\n        elif isinstance(device, int):\r\n            device = torch.device(\"cuda\", index=device)\r\n        _update_properties(self, device=device)\r\n        return super().cuda(device=device)", "code_tokens": ["def", "cuda", "(", "self", ",", "device", ":", "Optional", "[", "Union", "[", "torch", ".", "device", ",", "int", "]", "]", "=", "None", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "Moves", "all", "model", "parameters", "and", "buffers", "to", "the", "GPU", ".", "This", "also", "makes", "associated", "parameters", "and", "buffers", "different", "objects", ".", "So", "it", "should", "be", "called", "before", "constructing", "optimizer", "if", "the", "module", "will", "live", "on", "GPU", "while", "being", "optimized", ".", "Arguments", ":", "device", ":", "If", "specified", ",", "all", "parameters", "will", "be", "copied", "to", "that", "device", ".", "If", "`", "None", "`", ",", "the", "current", "CUDA", "device", "index", "will", "be", "used", ".", "Returns", ":", "Module", ":", "self", "\"", "\"", "\"", "if", "device", "is", "None", ":", "device", "=", "torch", ".", "device", "(", "\"", "cuda", "\"", ",", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", "elif", "isinstance", "(", "device", ",", "int", ")", ":", "device", "=", "torch", ".", "device", "(", "\"", "cuda", "\"", ",", "index", "=", "device", ")", "_update_properties", "(", "self", ",", "device", "=", "device", ")", "return", "super", "(", ")", ".", "cuda", "(", "device", "=", "device", ")"], "docstring": "Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\r\n        different objects. So it should be called before constructing optimizer if the module will live on GPU while\r\n        being optimized.\r\n\r\n        Arguments:\r\n            device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device\r\n                index will be used.\r\n\r\n        Returns:\r\n            Module: self", "docstring_tokens": ["moves", "all", "model", "parameters", "and", "buffers", "to", "the", "gpu", "this", "also", "makes", "associated", "parameters", "and", "buffers", "different", "objects", "so", "it", "should", "be", "called", "before", "constructing", "optimizer", "if", "the", "module", "will", "live", "on", "gpu", "while", "being", "optimized", "arguments", "device", "if", "specified", "all", "parameters", "will", "be", "copied", "to", "that", "device", "if", "none", "the", "current", "cuda", "device", "index", "will", "be", "used", "returns", "module", "self"], "docstring_summary": "Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 61, "end_line": 79, "hash": "fa373f6bc8efb899f76b40cfd8acaacd", "complexity": 3, "parameters": ["device", "int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "cpu", "original_string": "def cpu(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\r\n        _update_properties(self, device=torch.device(\"cpu\"))\r\n        return super().cpu()", "language": "python", "code": "def cpu(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\r\n        _update_properties(self, device=torch.device(\"cpu\"))\r\n        return super().cpu()", "code_tokens": ["def", "cpu", "(", "self", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "cpu", "`", ".", "\"", "\"", "\"", "_update_properties", "(", "self", ",", "device", "=", "torch", ".", "device", "(", "\"", "cpu", "\"", ")", ")", "return", "super", "(", ")", ".", "cpu", "(", ")"], "docstring": "See :meth:`torch.nn.Module.cpu`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "cpu"], "docstring_summary": "See :meth:`torch.nn.Module.cpu`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 82, "end_line": 85, "hash": "1243415db0c4c7a293a7cfb4577e33ff", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "type", "original_string": "def type(self, dst_type: Union[str, torch.dtype]) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.type`.\"\"\"\r\n        _update_properties(self, dtype=dst_type)\r\n        return super().type(dst_type=dst_type)", "language": "python", "code": "def type(self, dst_type: Union[str, torch.dtype]) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.type`.\"\"\"\r\n        _update_properties(self, dtype=dst_type)\r\n        return super().type(dst_type=dst_type)", "code_tokens": ["def", "type", "(", "self", ",", "dst_type", ":", "Union", "[", "str", ",", "torch", ".", "dtype", "]", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "type", "`", ".", "\"", "\"", "\"", "_update_properties", "(", "self", ",", "dtype", "=", "dst_type", ")", "return", "super", "(", ")", ".", "type", "(", "dst_type", "=", "dst_type", ")"], "docstring": "See :meth:`torch.nn.Module.type`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "type"], "docstring_summary": "See :meth:`torch.nn.Module.type`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 88, "end_line": 91, "hash": "792993d40e6ee89da6f3603bd5dbb7c5", "complexity": 1, "parameters": ["dst_type", "torch.dtype]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "float", "original_string": "def float(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.float`.\"\"\"\r\n        _update_properties(self, dtype=torch.float)\r\n        return super().float()", "language": "python", "code": "def float(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.float`.\"\"\"\r\n        _update_properties(self, dtype=torch.float)\r\n        return super().float()", "code_tokens": ["def", "float", "(", "self", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "float", "`", ".", "\"", "\"", "\"", "_update_properties", "(", "self", ",", "dtype", "=", "torch", ".", "float", ")", "return", "super", "(", ")", ".", "float", "(", ")"], "docstring": "See :meth:`torch.nn.Module.float`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "float"], "docstring_summary": "See :meth:`torch.nn.Module.float`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 94, "end_line": 97, "hash": "76e1f64dbbc4feda67fa3cdc7f0d7c85", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "double", "original_string": "def double(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.double`.\"\"\"\r\n        _update_properties(self, dtype=torch.double)\r\n        return super().double()", "language": "python", "code": "def double(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.double`.\"\"\"\r\n        _update_properties(self, dtype=torch.double)\r\n        return super().double()", "code_tokens": ["def", "double", "(", "self", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "double", "`", ".", "\"", "\"", "\"", "_update_properties", "(", "self", ",", "dtype", "=", "torch", ".", "double", ")", "return", "super", "(", ")", ".", "double", "(", ")"], "docstring": "See :meth:`torch.nn.Module.double`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "double"], "docstring_summary": "See :meth:`torch.nn.Module.double`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 100, "end_line": 103, "hash": "fab75062401bd5a783f7a1d70de2b4b7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "func_name": "half", "original_string": "def half(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.half`.\"\"\"\r\n        _update_properties(self, dtype=torch.half)\r\n        return super().half()", "language": "python", "code": "def half(self) -> Self:\r\n        \"\"\"See :meth:`torch.nn.Module.half`.\"\"\"\r\n        _update_properties(self, dtype=torch.half)\r\n        return super().half()", "code_tokens": ["def", "half", "(", "self", ")", "-", ">", "Self", ":", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "half", "`", ".", "\"", "\"", "\"", "_update_properties", "(", "self", ",", "dtype", "=", "torch", ".", "half", ")", "return", "super", "(", ")", ".", "half", "(", ")"], "docstring": "See :meth:`torch.nn.Module.half`.", "docstring_tokens": ["see", "meth", "torch", "nn", "module", "half"], "docstring_summary": "See :meth:`torch.nn.Module.half`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_dtype_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "_DeviceDtypeModuleMixin", "start_line": 106, "end_line": 109, "hash": "7a1173ca3ee7fa48447ef215d0d0e9c8", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_determine_root_gpu_device", "original_string": "def _determine_root_gpu_device(gpus: list[_DEVICE]) -> Optional[_DEVICE]:\r\n    \"\"\"\r\n    Args:\r\n        gpus: Non-empty list of ints representing which GPUs to use\r\n\r\n    Returns:\r\n        Designated root GPU device id\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``gpus`` is not a list\r\n        AssertionError:\r\n            If GPU list is empty\r\n    \"\"\"\r\n    if gpus is None:\r\n        return None\r\n\r\n    if not isinstance(gpus, list):\r\n        raise TypeError(\"GPUs should be a list\")\r\n\r\n    assert len(gpus) > 0, \"GPUs should be a non-empty list\"\r\n\r\n    # set root gpu\r\n    return gpus[0]", "language": "python", "code": "def _determine_root_gpu_device(gpus: list[_DEVICE]) -> Optional[_DEVICE]:\r\n    \"\"\"\r\n    Args:\r\n        gpus: Non-empty list of ints representing which GPUs to use\r\n\r\n    Returns:\r\n        Designated root GPU device id\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``gpus`` is not a list\r\n        AssertionError:\r\n            If GPU list is empty\r\n    \"\"\"\r\n    if gpus is None:\r\n        return None\r\n\r\n    if not isinstance(gpus, list):\r\n        raise TypeError(\"GPUs should be a list\")\r\n\r\n    assert len(gpus) > 0, \"GPUs should be a non-empty list\"\r\n\r\n    # set root gpu\r\n    return gpus[0]", "code_tokens": ["def", "_determine_root_gpu_device", "(", "gpus", ":", "list", "[", "_DEVICE", "]", ")", "-", ">", "Optional", "[", "_DEVICE", "]", ":", "\"", "\"", "\"", "Args", ":", "gpus", ":", "Non", "-", "empty", "list", "of", "ints", "representing", "which", "GPUs", "to", "use", "Returns", ":", "Designated", "root", "GPU", "device", "id", "Raises", ":", "TypeError", ":", "If", "`", "`", "gpus", "`", "`", "is", "not", "a", "list", "AssertionError", ":", "If", "GPU", "list", "is", "empty", "\"", "\"", "\"", "if", "gpus", "is", "None", ":", "return", "None", "if", "not", "isinstance", "(", "gpus", ",", "list", ")", ":", "raise", "TypeError", "(", "\"", "GPUs", "should", "be", "a", "list", "\"", ")", "assert", "len", "(", "gpus", ")", ">", "0", ",", "\"", "GPUs", "should", "be", "a", "non", "-", "empty", "list", "\"", "return", "gpus", "[", "0", "]"], "docstring": "Args:\r\n        gpus: Non-empty list of ints representing which GPUs to use\r\n\r\n    Returns:\r\n        Designated root GPU device id\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``gpus`` is not a list\r\n        AssertionError:\r\n            If GPU list is empty", "docstring_tokens": ["args", "gpus", "non", "empty", "list", "of", "ints", "representing", "which", "gpus", "to", "use", "returns", "designated", "root", "gpu", "device", "id", "raises", "typeerror", "if", "gpus", "is", "not", "a", "list", "assertionerror", "if", "gpu", "list", "is", "empty"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 22, "end_line": 45, "hash": "3f627de191bd3542ce7a9b6937aa071e", "complexity": 3, "parameters": ["gpus"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_parse_gpu_ids", "original_string": "def _parse_gpu_ids(\r\n    gpus: Optional[Union[int, str, list[int]]],\r\n    include_cuda: bool = False,\r\n    include_mps: bool = False,\r\n) -> Optional[list[int]]:\r\n    \"\"\"Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        gpus: An int -1 or string '-1' indicate that all available GPUs should be used.\r\n            A list of unique ints or a string containing a list of comma separated unique integers\r\n            indicates specific GPUs to use.\r\n            An int of 0 means that no GPUs should be used.\r\n            Any int N > 0 indicates that GPUs [0..N) should be used.\r\n        include_cuda: A boolean value indicating whether to include CUDA devices for GPU parsing.\r\n        include_mps: A boolean value indicating whether to include MPS devices for GPU parsing.\r\n\r\n    Returns:\r\n        A list of GPUs to be used or ``None`` if no GPUs were requested\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If no GPUs are available but the value of gpus variable indicates request for GPUs\r\n\r\n    .. note::\r\n        ``include_cuda`` and ``include_mps`` default to ``False`` so that you only\r\n        have to specify which device type to use and all other devices are not disabled.\r\n\r\n    \"\"\"\r\n    # Check that gpus param is None, Int, String or Sequence of Ints\r\n    _check_data_type(gpus)\r\n\r\n    # Handle the case when no GPUs are requested\r\n    if gpus is None or (isinstance(gpus, int) and gpus == 0) or str(gpus).strip() in (\"0\", \"[]\"):\r\n        return None\r\n\r\n    # We know the user requested GPUs therefore if some of the\r\n    # requested GPUs are not available an exception is thrown.\r\n    gpus = _normalize_parse_gpu_string_input(gpus)\r\n    gpus = _normalize_parse_gpu_input_to_list(gpus, include_cuda=include_cuda, include_mps=include_mps)\r\n    if not gpus:\r\n        raise MisconfigurationException(\"GPUs requested but none are available.\")\r\n\r\n    if (\r\n        torch.distributed.is_available()\r\n        and torch.distributed.is_torchelastic_launched()\r\n        and len(gpus) != 1\r\n        and len(_get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)) == 1\r\n    ):\r\n        # Omit sanity check on torchelastic because by default it shows one visible GPU per process\r\n        return gpus\r\n\r\n    # Check that GPUs are unique. Duplicate GPUs are not supported by the backend.\r\n    _check_unique(gpus)\r\n\r\n    return _sanitize_gpu_ids(gpus, include_cuda=include_cuda, include_mps=include_mps)", "language": "python", "code": "def _parse_gpu_ids(\r\n    gpus: Optional[Union[int, str, list[int]]],\r\n    include_cuda: bool = False,\r\n    include_mps: bool = False,\r\n) -> Optional[list[int]]:\r\n    \"\"\"Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        gpus: An int -1 or string '-1' indicate that all available GPUs should be used.\r\n            A list of unique ints or a string containing a list of comma separated unique integers\r\n            indicates specific GPUs to use.\r\n            An int of 0 means that no GPUs should be used.\r\n            Any int N > 0 indicates that GPUs [0..N) should be used.\r\n        include_cuda: A boolean value indicating whether to include CUDA devices for GPU parsing.\r\n        include_mps: A boolean value indicating whether to include MPS devices for GPU parsing.\r\n\r\n    Returns:\r\n        A list of GPUs to be used or ``None`` if no GPUs were requested\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If no GPUs are available but the value of gpus variable indicates request for GPUs\r\n\r\n    .. note::\r\n        ``include_cuda`` and ``include_mps`` default to ``False`` so that you only\r\n        have to specify which device type to use and all other devices are not disabled.\r\n\r\n    \"\"\"\r\n    # Check that gpus param is None, Int, String or Sequence of Ints\r\n    _check_data_type(gpus)\r\n\r\n    # Handle the case when no GPUs are requested\r\n    if gpus is None or (isinstance(gpus, int) and gpus == 0) or str(gpus).strip() in (\"0\", \"[]\"):\r\n        return None\r\n\r\n    # We know the user requested GPUs therefore if some of the\r\n    # requested GPUs are not available an exception is thrown.\r\n    gpus = _normalize_parse_gpu_string_input(gpus)\r\n    gpus = _normalize_parse_gpu_input_to_list(gpus, include_cuda=include_cuda, include_mps=include_mps)\r\n    if not gpus:\r\n        raise MisconfigurationException(\"GPUs requested but none are available.\")\r\n\r\n    if (\r\n        torch.distributed.is_available()\r\n        and torch.distributed.is_torchelastic_launched()\r\n        and len(gpus) != 1\r\n        and len(_get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)) == 1\r\n    ):\r\n        # Omit sanity check on torchelastic because by default it shows one visible GPU per process\r\n        return gpus\r\n\r\n    # Check that GPUs are unique. Duplicate GPUs are not supported by the backend.\r\n    _check_unique(gpus)\r\n\r\n    return _sanitize_gpu_ids(gpus, include_cuda=include_cuda, include_mps=include_mps)", "code_tokens": ["def", "_parse_gpu_ids", "(", "gpus", ":", "Optional", "[", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", "]", ",", "include_cuda", ":", "bool", "=", "False", ",", "include_mps", ":", "bool", "=", "False", ",", ")", "-", ">", "Optional", "[", "list", "[", "int", "]", "]", ":", "\"", "\"", "\"", "Parses", "the", "GPU", "IDs", "given", "in", "the", "format", "as", "accepted", "by", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", ".", "Args", ":", "gpus", ":", "An", "int", "-", "1", "or", "string", "'", "-", "1", "'", "indicate", "that", "all", "available", "GPUs", "should", "be", "used", ".", "A", "list", "of", "unique", "ints", "or", "a", "string", "containing", "a", "list", "of", "comma", "separated", "unique", "integers", "indicates", "specific", "GPUs", "to", "use", ".", "An", "int", "of", "0", "means", "that", "no", "GPUs", "should", "be", "used", ".", "Any", "int", "N", ">", "0", "indicates", "that", "GPUs", "[", "0", ".", ".", "N", ")", "should", "be", "used", ".", "include_cuda", ":", "A", "boolean", "value", "indicating", "whether", "to", "include", "CUDA", "devices", "for", "GPU", "parsing", ".", "include_mps", ":", "A", "boolean", "value", "indicating", "whether", "to", "include", "MPS", "devices", "for", "GPU", "parsing", ".", "Returns", ":", "A", "list", "of", "GPUs", "to", "be", "used", "or", "`", "`", "None", "`", "`", "if", "no", "GPUs", "were", "requested", "Raises", ":", "MisconfigurationException", ":", "If", "no", "GPUs", "are", "available", "but", "the", "value", "of", "gpus", "variable", "indicates", "request", "for", "GPUs", ".", ".", "note", ":", ":", "`", "`", "include_cuda", "`", "`", "and", "`", "`", "include_mps", "`", "`", "default", "to", "`", "`", "False", "`", "`", "so", "that", "you", "only", "have", "to", "specify", "which", "device", "type", "to", "use", "and", "all", "other", "devices", "are", "not", "disabled", ".", "\"", "\"", "\"", "_check_data_type", "(", "gpus", ")", "if", "gpus", "is", "None", "or", "(", "isinstance", "(", "gpus", ",", "int", ")", "and", "gpus", "=", "=", "0", ")", "or", "str", "(", "gpus", ")", ".", "strip", "(", ")", "in", "(", "\"", "0", "\"", ",", "\"", "[", "]", "\"", ")", ":", "return", "None", "gpus", "=", "_normalize_parse_gpu_string_input", "(", "gpus", ")", "gpus", "=", "_normalize_parse_gpu_input_to_list", "(", "gpus", ",", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")", "if", "not", "gpus", ":", "raise", "MisconfigurationException", "(", "\"", "GPUs", "requested", "but", "none", "are", "available", ".", "\"", ")", "if", "(", "torch", ".", "distributed", ".", "is_available", "(", ")", "and", "torch", ".", "distributed", ".", "is_torchelastic_launched", "(", ")", "and", "len", "(", "gpus", ")", "!", "=", "1", "and", "len", "(", "_get_all_available_gpus", "(", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")", ")", "=", "=", "1", ")", ":", "return", "gpus", "_check_unique", "(", "gpus", ")", "return", "_sanitize_gpu_ids", "(", "gpus", ",", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")"], "docstring": "Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.\r\n\r\n    Args:\r\n        gpus: An int -1 or string '-1' indicate that all available GPUs should be used.\r\n            A list of unique ints or a string containing a list of comma separated unique integers\r\n            indicates specific GPUs to use.\r\n            An int of 0 means that no GPUs should be used.\r\n            Any int N > 0 indicates that GPUs [0..N) should be used.\r\n        include_cuda: A boolean value indicating whether to include CUDA devices for GPU parsing.\r\n        include_mps: A boolean value indicating whether to include MPS devices for GPU parsing.\r\n\r\n    Returns:\r\n        A list of GPUs to be used or ``None`` if no GPUs were requested\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If no GPUs are available but the value of gpus variable indicates request for GPUs\r\n\r\n    .. note::\r\n        ``include_cuda`` and ``include_mps`` default to ``False`` so that you only\r\n        have to specify which device type to use and all other devices are not disabled.", "docstring_tokens": ["parses", "the", "gpu", "ids", "given", "in", "the", "format", "as", "accepted", "by", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "args", "gpus", "an", "int", "1", "or", "string", "1", "indicate", "that", "all", "available", "gpus", "should", "be", "used", "a", "list", "of", "unique", "ints", "or", "a", "string", "containing", "a", "list", "of", "comma", "separated", "unique", "integers", "indicates", "specific", "gpus", "to", "use", "an", "int", "of", "0", "means", "that", "no", "gpus", "should", "be", "used", "any", "int", "n", "0", "indicates", "that", "gpus", "0", "n", "should", "be", "used", "include_cuda", "a", "boolean", "value", "indicating", "whether", "to", "include", "cuda", "devices", "for", "gpu", "parsing", "include_mps", "a", "boolean", "value", "indicating", "whether", "to", "include", "mps", "devices", "for", "gpu", "parsing", "returns", "a", "list", "of", "gpus", "to", "be", "used", "or", "none", "if", "no", "gpus", "were", "requested", "raises", "misconfigurationexception", "if", "no", "gpus", "are", "available", "but", "the", "value", "of", "gpus", "variable", "indicates", "request", "for", "gpus", "note", "include_cuda", "and", "include_mps", "default", "to", "false", "so", "that", "you", "only", "have", "to", "specify", "which", "device", "type", "to", "use", "and", "all", "other", "devices", "are", "not", "disabled"], "docstring_summary": "Parses the GPU IDs given in the format as accepted by the :class:`~lightning.pytorch.trainer.trainer.Trainer`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 48, "end_line": 102, "hash": "1b93464591701921f495b3be55f25112", "complexity": 10, "parameters": ["gpus", "str", "list[int]]]", "include_cuda", "include_mps"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_sanitize_gpu_ids", "original_string": "def _sanitize_gpu_ids(gpus: list[int], include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the\r\n    GPUs is not available.\r\n\r\n    Args:\r\n        gpus: List of ints corresponding to GPU indices\r\n\r\n    Returns:\r\n        Unmodified gpus variable\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If machine has fewer available GPUs than requested.\r\n\r\n    \"\"\"\r\n    if sum((include_cuda, include_mps)) == 0:\r\n        raise ValueError(\"At least one gpu type should be specified!\")\r\n    all_available_gpus = _get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)\r\n    for gpu in gpus:\r\n        if gpu not in all_available_gpus:\r\n            raise MisconfigurationException(\r\n                f\"You requested gpu: {gpus}\\n But your machine only has: {all_available_gpus}\"\r\n            )\r\n    return gpus", "language": "python", "code": "def _sanitize_gpu_ids(gpus: list[int], include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the\r\n    GPUs is not available.\r\n\r\n    Args:\r\n        gpus: List of ints corresponding to GPU indices\r\n\r\n    Returns:\r\n        Unmodified gpus variable\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If machine has fewer available GPUs than requested.\r\n\r\n    \"\"\"\r\n    if sum((include_cuda, include_mps)) == 0:\r\n        raise ValueError(\"At least one gpu type should be specified!\")\r\n    all_available_gpus = _get_all_available_gpus(include_cuda=include_cuda, include_mps=include_mps)\r\n    for gpu in gpus:\r\n        if gpu not in all_available_gpus:\r\n            raise MisconfigurationException(\r\n                f\"You requested gpu: {gpus}\\n But your machine only has: {all_available_gpus}\"\r\n            )\r\n    return gpus", "code_tokens": ["def", "_sanitize_gpu_ids", "(", "gpus", ":", "list", "[", "int", "]", ",", "include_cuda", ":", "bool", "=", "False", ",", "include_mps", ":", "bool", "=", "False", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "Checks", "that", "each", "of", "the", "GPUs", "in", "the", "list", "is", "actually", "available", ".", "Raises", "a", "MisconfigurationException", "if", "any", "of", "the", "GPUs", "is", "not", "available", ".", "Args", ":", "gpus", ":", "List", "of", "ints", "corresponding", "to", "GPU", "indices", "Returns", ":", "Unmodified", "gpus", "variable", "Raises", ":", "MisconfigurationException", ":", "If", "machine", "has", "fewer", "available", "GPUs", "than", "requested", ".", "\"", "\"", "\"", "if", "sum", "(", "(", "include_cuda", ",", "include_mps", ")", ")", "=", "=", "0", ":", "raise", "ValueError", "(", "\"", "At", "least", "one", "gpu", "type", "should", "be", "specified", "!", "\"", ")", "all_available_gpus", "=", "_get_all_available_gpus", "(", "include_cuda", "=", "include_cuda", ",", "include_mps", "=", "include_mps", ")", "for", "gpu", "in", "gpus", ":", "if", "gpu", "not", "in", "all_available_gpus", ":", "raise", "MisconfigurationException", "(", "f", "\"", "You", "requested", "gpu", ":", "{", "gpus", "}", "\\", "n", "But", "your", "machine", "only", "has", ":", "{", "all_available_gpus", "}", "\"", ")", "return", "gpus"], "docstring": "Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the\r\n    GPUs is not available.\r\n\r\n    Args:\r\n        gpus: List of ints corresponding to GPU indices\r\n\r\n    Returns:\r\n        Unmodified gpus variable\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If machine has fewer available GPUs than requested.", "docstring_tokens": ["checks", "that", "each", "of", "the", "gpus", "in", "the", "list", "is", "actually", "available", "raises", "a", "misconfigurationexception", "if", "any", "of", "the", "gpus", "is", "not", "available", "args", "gpus", "list", "of", "ints", "corresponding", "to", "gpu", "indices", "returns", "unmodified", "gpus", "variable", "raises", "misconfigurationexception", "if", "machine", "has", "fewer", "available", "gpus", "than", "requested"], "docstring_summary": "Checks that each of the GPUs in the list is actually available. Raises a MisconfigurationException if any of the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 115, "end_line": 138, "hash": "3674e096c7912042329292faa0f37fa9", "complexity": 4, "parameters": ["gpus", "include_cuda", "include_mps"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_get_all_available_gpus", "original_string": "def _get_all_available_gpus(include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available GPUs\r\n    \"\"\"\r\n    from lightning.fabric.accelerators.cuda import _get_all_visible_cuda_devices\r\n    from lightning.fabric.accelerators.mps import _get_all_available_mps_gpus\r\n\r\n    cuda_gpus = _get_all_visible_cuda_devices() if include_cuda else []\r\n    mps_gpus = _get_all_available_mps_gpus() if include_mps else []\r\n    return cuda_gpus + mps_gpus", "language": "python", "code": "def _get_all_available_gpus(include_cuda: bool = False, include_mps: bool = False) -> list[int]:\r\n    \"\"\"\r\n    Returns:\r\n        A list of all available GPUs\r\n    \"\"\"\r\n    from lightning.fabric.accelerators.cuda import _get_all_visible_cuda_devices\r\n    from lightning.fabric.accelerators.mps import _get_all_available_mps_gpus\r\n\r\n    cuda_gpus = _get_all_visible_cuda_devices() if include_cuda else []\r\n    mps_gpus = _get_all_available_mps_gpus() if include_mps else []\r\n    return cuda_gpus + mps_gpus", "code_tokens": ["def", "_get_all_available_gpus", "(", "include_cuda", ":", "bool", "=", "False", ",", "include_mps", ":", "bool", "=", "False", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "Returns", ":", "A", "list", "of", "all", "available", "GPUs", "\"", "\"", "\"", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "cuda", "import", "_get_all_visible_cuda_devices", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "mps", "import", "_get_all_available_mps_gpus", "cuda_gpus", "=", "_get_all_visible_cuda_devices", "(", ")", "if", "include_cuda", "else", "[", "]", "mps_gpus", "=", "_get_all_available_mps_gpus", "(", ")", "if", "include_mps", "else", "[", "]", "return", "cuda_gpus", "+", "mps_gpus"], "docstring": "Returns:\r\n        A list of all available GPUs", "docstring_tokens": ["returns", "a", "list", "of", "all", "available", "gpus"], "docstring_summary": "Returns:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 157, "end_line": 167, "hash": "3b192278dd00b34c65bc2a16346e9632", "complexity": 3, "parameters": ["include_cuda", "include_mps"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_check_unique", "original_string": "def _check_unique(device_ids: list[int]) -> None:\r\n    \"\"\"Checks that the device_ids are unique.\r\n\r\n    Args:\r\n        device_ids: List of ints corresponding to GPUs indices\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If ``device_ids`` of GPUs aren't unique\r\n\r\n    \"\"\"\r\n    if len(device_ids) != len(set(device_ids)):\r\n        raise MisconfigurationException(\"Device ID's (GPU) must be unique.\")", "language": "python", "code": "def _check_unique(device_ids: list[int]) -> None:\r\n    \"\"\"Checks that the device_ids are unique.\r\n\r\n    Args:\r\n        device_ids: List of ints corresponding to GPUs indices\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If ``device_ids`` of GPUs aren't unique\r\n\r\n    \"\"\"\r\n    if len(device_ids) != len(set(device_ids)):\r\n        raise MisconfigurationException(\"Device ID's (GPU) must be unique.\")", "code_tokens": ["def", "_check_unique", "(", "device_ids", ":", "list", "[", "int", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "that", "the", "device_ids", "are", "unique", ".", "Args", ":", "device_ids", ":", "List", "of", "ints", "corresponding", "to", "GPUs", "indices", "Raises", ":", "MisconfigurationException", ":", "If", "`", "`", "device_ids", "`", "`", "of", "GPUs", "aren", "'", "t", "unique", "\"", "\"", "\"", "if", "len", "(", "device_ids", ")", "!", "=", "len", "(", "set", "(", "device_ids", ")", ")", ":", "raise", "MisconfigurationException", "(", "\"", "Device", "ID", "'", "s", "(", "GPU", ")", "must", "be", "unique", ".", "\"", ")"], "docstring": "Checks that the device_ids are unique.\r\n\r\n    Args:\r\n        device_ids: List of ints corresponding to GPUs indices\r\n\r\n    Raises:\r\n        MisconfigurationException:\r\n            If ``device_ids`` of GPUs aren't unique", "docstring_tokens": ["checks", "that", "the", "device_ids", "are", "unique", "args", "device_ids", "list", "of", "ints", "corresponding", "to", "gpus", "indices", "raises", "misconfigurationexception", "if", "device_ids", "of", "gpus", "aren", "t", "unique"], "docstring_summary": "Checks that the device_ids are unique.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 170, "end_line": 182, "hash": "f56585efad36840f5ac980171fe2b9cf", "complexity": 2, "parameters": ["device_ids"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_check_data_type", "original_string": "def _check_data_type(device_ids: object) -> None:\r\n    \"\"\"Checks that the device_ids argument is one of the following: int, string, or sequence of integers.\r\n\r\n    Args:\r\n        device_ids: gpus/tpu_cores parameter as passed to the Trainer\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``device_ids`` of GPU/TPUs aren't ``int``, ``str`` or sequence of ``int```\r\n\r\n    \"\"\"\r\n    msg = \"Device IDs (GPU/TPU) must be an int, a string, a sequence of ints, but you passed\"\r\n    if device_ids is None:\r\n        raise TypeError(f\"{msg} None\")\r\n    if isinstance(device_ids, (MutableSequence, tuple)):\r\n        for id_ in device_ids:\r\n            id_type = type(id_)  # because `isinstance(False, int)` -> True\r\n            if id_type is not int:\r\n                raise TypeError(f\"{msg} a sequence of {type(id_).__name__}.\")\r\n    elif type(device_ids) not in (int, str):\r\n        raise TypeError(f\"{msg} {device_ids!r}.\")", "language": "python", "code": "def _check_data_type(device_ids: object) -> None:\r\n    \"\"\"Checks that the device_ids argument is one of the following: int, string, or sequence of integers.\r\n\r\n    Args:\r\n        device_ids: gpus/tpu_cores parameter as passed to the Trainer\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``device_ids`` of GPU/TPUs aren't ``int``, ``str`` or sequence of ``int```\r\n\r\n    \"\"\"\r\n    msg = \"Device IDs (GPU/TPU) must be an int, a string, a sequence of ints, but you passed\"\r\n    if device_ids is None:\r\n        raise TypeError(f\"{msg} None\")\r\n    if isinstance(device_ids, (MutableSequence, tuple)):\r\n        for id_ in device_ids:\r\n            id_type = type(id_)  # because `isinstance(False, int)` -> True\r\n            if id_type is not int:\r\n                raise TypeError(f\"{msg} a sequence of {type(id_).__name__}.\")\r\n    elif type(device_ids) not in (int, str):\r\n        raise TypeError(f\"{msg} {device_ids!r}.\")", "code_tokens": ["def", "_check_data_type", "(", "device_ids", ":", "object", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "that", "the", "device_ids", "argument", "is", "one", "of", "the", "following", ":", "int", ",", "string", ",", "or", "sequence", "of", "integers", ".", "Args", ":", "device_ids", ":", "gpus", "/", "tpu_cores", "parameter", "as", "passed", "to", "the", "Trainer", "Raises", ":", "TypeError", ":", "If", "`", "`", "device_ids", "`", "`", "of", "GPU", "/", "TPUs", "aren", "'", "t", "`", "`", "int", "`", "`", ",", "`", "`", "str", "`", "`", "or", "sequence", "of", "`", "`", "int", "`", "`", "`", "\"", "\"", "\"", "msg", "=", "\"", "Device", "IDs", "(", "GPU", "/", "TPU", ")", "must", "be", "an", "int", ",", "a", "string", ",", "a", "sequence", "of", "ints", ",", "but", "you", "passed", "\"", "if", "device_ids", "is", "None", ":", "raise", "TypeError", "(", "f", "\"", "{", "msg", "}", "None", "\"", ")", "if", "isinstance", "(", "device_ids", ",", "(", "MutableSequence", ",", "tuple", ")", ")", ":", "for", "id_", "in", "device_ids", ":", "id_type", "=", "type", "(", "id_", ")", "if", "id_type", "is", "not", "int", ":", "raise", "TypeError", "(", "f", "\"", "{", "msg", "}", "a", "sequence", "of", "{", "type", "(", "id_", ")", ".", "__name__", "}", ".", "\"", ")", "elif", "type", "(", "device_ids", ")", "not", "in", "(", "int", ",", "str", ")", ":", "raise", "TypeError", "(", "f", "\"", "{", "msg", "}", "{", "device_ids", "!", "r", "}", ".", "\"", ")"], "docstring": "Checks that the device_ids argument is one of the following: int, string, or sequence of integers.\r\n\r\n    Args:\r\n        device_ids: gpus/tpu_cores parameter as passed to the Trainer\r\n\r\n    Raises:\r\n        TypeError:\r\n            If ``device_ids`` of GPU/TPUs aren't ``int``, ``str`` or sequence of ``int```", "docstring_tokens": ["checks", "that", "the", "device_ids", "argument", "is", "one", "of", "the", "following", "int", "string", "or", "sequence", "of", "integers", "args", "device_ids", "gpus", "tpu_cores", "parameter", "as", "passed", "to", "the", "trainer", "raises", "typeerror", "if", "device_ids", "of", "gpu", "tpus", "aren", "t", "int", "str", "or", "sequence", "of", "int"], "docstring_summary": "Checks that the device_ids argument is one of the following: int, string, or sequence of integers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 185, "end_line": 205, "hash": "91a676db7bb3cebbde4a1af235456566", "complexity": 6, "parameters": ["device_ids"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\device_parser.py", "func_name": "_select_auto_accelerator", "original_string": "def _select_auto_accelerator() -> str:\r\n    \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n    from lightning.fabric.accelerators.cuda import CUDAAccelerator\r\n    from lightning.fabric.accelerators.mps import MPSAccelerator\r\n    from lightning.fabric.accelerators.xla import XLAAccelerator\r\n\r\n    if XLAAccelerator.is_available():\r\n        return \"tpu\"\r\n    if MPSAccelerator.is_available():\r\n        return \"mps\"\r\n    if CUDAAccelerator.is_available():\r\n        return \"cuda\"\r\n    return \"cpu\"", "language": "python", "code": "def _select_auto_accelerator() -> str:\r\n    \"\"\"Choose the accelerator type (str) based on availability.\"\"\"\r\n    from lightning.fabric.accelerators.cuda import CUDAAccelerator\r\n    from lightning.fabric.accelerators.mps import MPSAccelerator\r\n    from lightning.fabric.accelerators.xla import XLAAccelerator\r\n\r\n    if XLAAccelerator.is_available():\r\n        return \"tpu\"\r\n    if MPSAccelerator.is_available():\r\n        return \"mps\"\r\n    if CUDAAccelerator.is_available():\r\n        return \"cuda\"\r\n    return \"cpu\"", "code_tokens": ["def", "_select_auto_accelerator", "(", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Choose", "the", "accelerator", "type", "(", "str", ")", "based", "on", "availability", ".", "\"", "\"", "\"", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "cuda", "import", "CUDAAccelerator", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "mps", "import", "MPSAccelerator", "from", "lightning", ".", "fabric", ".", "accelerators", ".", "xla", "import", "XLAAccelerator", "if", "XLAAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "tpu", "\"", "if", "MPSAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "mps", "\"", "if", "CUDAAccelerator", ".", "is_available", "(", ")", ":", "return", "\"", "cuda", "\"", "return", "\"", "cpu", "\""], "docstring": "Choose the accelerator type (str) based on availability.", "docstring_tokens": ["choose", "the", "accelerator", "type", "str", "based", "on", "availability"], "docstring_summary": "Choose the accelerator type (str) based on availability.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\device_parser.py", "partition": "train", "function_type": "function", "start_line": 208, "end_line": 220, "hash": "dc7346b757eccca7c0339372e26ac907", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "is_shared_filesystem", "original_string": "def is_shared_filesystem(strategy: \"Strategy\", path: Optional[_PATH] = None, timeout: int = 3) -> bool:\r\n    \"\"\"Checks whether the filesystem under the given path is shared across all processes.\r\n\r\n    This function should only be used in a context where distributed is initialized.\r\n\r\n    Args:\r\n        strategy: The strategy being used, either from Fabric (``fabric.strategy``) or from Trainer\r\n            (``trainer.strategy``).\r\n        path: The path to check. Defaults to the current working directory. The user must have permissions to write\r\n            to this path or the parent folder, and the filesystem must be writable.\r\n        timeout: If any of the processes can't list the file created by rank 0 within this many seconds, the\r\n            filesystem is determined to be not shared.\r\n\r\n    \"\"\"\r\n    # Fast path: Any non-local filesystem is considered shared (e.g., S3)\r\n    if path is not None and not _is_local_file_protocol(path):\r\n        return True\r\n\r\n    path = Path(Path.cwd() if path is None else path).resolve()\r\n\r\n    # Fast path: Only distributed strategies can detect shared filesystems\r\n    if not hasattr(strategy, \"world_size\") or strategy.world_size == 1:\r\n        return True\r\n\r\n    # Fast path: If the path is not the same on all ranks we know it's not a shared filesystem\r\n    rank_zero_path = strategy.broadcast(path)\r\n    if not strategy.reduce_boolean_decision(rank_zero_path == path, all=True):\r\n        return False\r\n\r\n    if not strategy.reduce_boolean_decision(path.exists(), all=True):\r\n        raise FileNotFoundError(\r\n            f\"Unable to determine if the path belongs to a shared filesystem. The path does not exist: {path}\"\r\n        )\r\n\r\n    path = path.parent if path.is_file() else path\r\n    check_file = path / \".lightning_shared_fs_check\"\r\n    check_file.unlink(missing_ok=True)\r\n\r\n    strategy.barrier()\r\n    if strategy.is_global_zero:\r\n        # Rank 0 creates the file\r\n        check_file.touch()\r\n        found = True\r\n    else:\r\n        # All other ranks will wait until they find the file or timeout\r\n        start = time.perf_counter()\r\n        found = False\r\n        while not found and (time.perf_counter() - start) < timeout:\r\n            found = check_file.exists()\r\n    strategy.barrier()\r\n\r\n    all_found = strategy.reduce_boolean_decision(found, all=True)\r\n\r\n    with contextlib.suppress(OSError):  # handle race condition on deletion\r\n        check_file.unlink()\r\n\r\n    return all_found", "language": "python", "code": "def is_shared_filesystem(strategy: \"Strategy\", path: Optional[_PATH] = None, timeout: int = 3) -> bool:\r\n    \"\"\"Checks whether the filesystem under the given path is shared across all processes.\r\n\r\n    This function should only be used in a context where distributed is initialized.\r\n\r\n    Args:\r\n        strategy: The strategy being used, either from Fabric (``fabric.strategy``) or from Trainer\r\n            (``trainer.strategy``).\r\n        path: The path to check. Defaults to the current working directory. The user must have permissions to write\r\n            to this path or the parent folder, and the filesystem must be writable.\r\n        timeout: If any of the processes can't list the file created by rank 0 within this many seconds, the\r\n            filesystem is determined to be not shared.\r\n\r\n    \"\"\"\r\n    # Fast path: Any non-local filesystem is considered shared (e.g., S3)\r\n    if path is not None and not _is_local_file_protocol(path):\r\n        return True\r\n\r\n    path = Path(Path.cwd() if path is None else path).resolve()\r\n\r\n    # Fast path: Only distributed strategies can detect shared filesystems\r\n    if not hasattr(strategy, \"world_size\") or strategy.world_size == 1:\r\n        return True\r\n\r\n    # Fast path: If the path is not the same on all ranks we know it's not a shared filesystem\r\n    rank_zero_path = strategy.broadcast(path)\r\n    if not strategy.reduce_boolean_decision(rank_zero_path == path, all=True):\r\n        return False\r\n\r\n    if not strategy.reduce_boolean_decision(path.exists(), all=True):\r\n        raise FileNotFoundError(\r\n            f\"Unable to determine if the path belongs to a shared filesystem. The path does not exist: {path}\"\r\n        )\r\n\r\n    path = path.parent if path.is_file() else path\r\n    check_file = path / \".lightning_shared_fs_check\"\r\n    check_file.unlink(missing_ok=True)\r\n\r\n    strategy.barrier()\r\n    if strategy.is_global_zero:\r\n        # Rank 0 creates the file\r\n        check_file.touch()\r\n        found = True\r\n    else:\r\n        # All other ranks will wait until they find the file or timeout\r\n        start = time.perf_counter()\r\n        found = False\r\n        while not found and (time.perf_counter() - start) < timeout:\r\n            found = check_file.exists()\r\n    strategy.barrier()\r\n\r\n    all_found = strategy.reduce_boolean_decision(found, all=True)\r\n\r\n    with contextlib.suppress(OSError):  # handle race condition on deletion\r\n        check_file.unlink()\r\n\r\n    return all_found", "code_tokens": ["def", "is_shared_filesystem", "(", "strategy", ":", "\"", "Strategy", "\"", ",", "path", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "timeout", ":", "int", "=", "3", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Checks", "whether", "the", "filesystem", "under", "the", "given", "path", "is", "shared", "across", "all", "processes", ".", "This", "function", "should", "only", "be", "used", "in", "a", "context", "where", "distributed", "is", "initialized", ".", "Args", ":", "strategy", ":", "The", "strategy", "being", "used", ",", "either", "from", "Fabric", "(", "`", "`", "fabric", ".", "strategy", "`", "`", ")", "or", "from", "Trainer", "(", "`", "`", "trainer", ".", "strategy", "`", "`", ")", ".", "path", ":", "The", "path", "to", "check", ".", "Defaults", "to", "the", "current", "working", "directory", ".", "The", "user", "must", "have", "permissions", "to", "write", "to", "this", "path", "or", "the", "parent", "folder", ",", "and", "the", "filesystem", "must", "be", "writable", ".", "timeout", ":", "If", "any", "of", "the", "processes", "can", "'", "t", "list", "the", "file", "created", "by", "rank", "0", "within", "this", "many", "seconds", ",", "the", "filesystem", "is", "determined", "to", "be", "not", "shared", ".", "\"", "\"", "\"", "if", "path", "is", "not", "None", "and", "not", "_is_local_file_protocol", "(", "path", ")", ":", "return", "True", "path", "=", "Path", "(", "Path", ".", "cwd", "(", ")", "if", "path", "is", "None", "else", "path", ")", ".", "resolve", "(", ")", "if", "not", "hasattr", "(", "strategy", ",", "\"", "world_size", "\"", ")", "or", "strategy", ".", "world_size", "=", "=", "1", ":", "return", "True", "rank_zero_path", "=", "strategy", ".", "broadcast", "(", "path", ")", "if", "not", "strategy", ".", "reduce_boolean_decision", "(", "rank_zero_path", "=", "=", "path", ",", "all", "=", "True", ")", ":", "return", "False", "if", "not", "strategy", ".", "reduce_boolean_decision", "(", "path", ".", "exists", "(", ")", ",", "all", "=", "True", ")", ":", "raise", "FileNotFoundError", "(", "f", "\"", "Unable", "to", "determine", "if", "the", "path", "belongs", "to", "a", "shared", "filesystem", ".", "The", "path", "does", "not", "exist", ":", "{", "path", "}", "\"", ")", "path", "=", "path", ".", "parent", "if", "path", ".", "is_file", "(", ")", "else", "path", "check_file", "=", "path", "/", "\"", ".", "lightning_shared_fs_check", "\"", "check_file", ".", "unlink", "(", "missing_ok", "=", "True", ")", "strategy", ".", "barrier", "(", ")", "if", "strategy", ".", "is_global_zero", ":", "check_file", ".", "touch", "(", ")", "found", "=", "True", "else", ":", "start", "=", "time", ".", "perf_counter", "(", ")", "found", "=", "False", "while", "not", "found", "and", "(", "time", ".", "perf_counter", "(", ")", "-", "start", ")", "<", "timeout", ":", "found", "=", "check_file", ".", "exists", "(", ")", "strategy", ".", "barrier", "(", ")", "all_found", "=", "strategy", ".", "reduce_boolean_decision", "(", "found", ",", "all", "=", "True", ")", "with", "contextlib", ".", "suppress", "(", "OSError", ")", ":", "check_file", ".", "unlink", "(", ")", "return", "all_found"], "docstring": "Checks whether the filesystem under the given path is shared across all processes.\r\n\r\n    This function should only be used in a context where distributed is initialized.\r\n\r\n    Args:\r\n        strategy: The strategy being used, either from Fabric (``fabric.strategy``) or from Trainer\r\n            (``trainer.strategy``).\r\n        path: The path to check. Defaults to the current working directory. The user must have permissions to write\r\n            to this path or the parent folder, and the filesystem must be writable.\r\n        timeout: If any of the processes can't list the file created by rank 0 within this many seconds, the\r\n            filesystem is determined to be not shared.", "docstring_tokens": ["checks", "whether", "the", "filesystem", "under", "the", "given", "path", "is", "shared", "across", "all", "processes", "this", "function", "should", "only", "be", "used", "in", "a", "context", "where", "distributed", "is", "initialized", "args", "strategy", "the", "strategy", "being", "used", "either", "from", "fabric", "fabric", "strategy", "or", "from", "trainer", "trainer", "strategy", "path", "the", "path", "to", "check", "defaults", "to", "the", "current", "working", "directory", "the", "user", "must", "have", "permissions", "to", "write", "to", "this", "path", "or", "the", "parent", "folder", "and", "the", "filesystem", "must", "be", "writable", "timeout", "if", "any", "of", "the", "processes", "can", "t", "list", "the", "file", "created", "by", "rank", "0", "within", "this", "many", "seconds", "the", "filesystem", "is", "determined", "to", "be", "not", "shared"], "docstring_summary": "Checks whether the filesystem under the given path is shared across all processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 43, "end_line": 99, "hash": "13d35a8b068fea10cf1f3afee3c70ce7", "complexity": 13, "parameters": ["strategy", "path", "timeout"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "_gather_all_tensors", "original_string": "def _gather_all_tensors(result: Tensor, group: Optional[Any] = None) -> list[Tensor]:\r\n    \"\"\"Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.\r\n\r\n    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case\r\n    tensors are padded, gathered and then trimmed to secure equal workload for all processes.\r\n\r\n    Args:\r\n        result: The value to sync\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n\r\n    Return:\r\n        gathered_result: List with size equal to the process group where\r\n            gathered_result[i] corresponds to result tensor from process i\r\n\r\n    \"\"\"\r\n    if group is None:\r\n        group = torch.distributed.group.WORLD\r\n\r\n    # Convert tensors to contiguous format\r\n    result = result.contiguous()\r\n\r\n    world_size = torch.distributed.get_world_size(group)\r\n    torch.distributed.barrier(group=group)\r\n\r\n    # If the tensor is scalar, things are easy\r\n    if result.ndim == 0:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    # 1. Gather sizes of all tensors\r\n    local_size = torch.tensor(result.shape, device=result.device)\r\n    local_sizes = [torch.zeros_like(local_size) for _ in range(world_size)]\r\n    torch.distributed.all_gather(local_sizes, local_size, group=group)\r\n    max_size = torch.stack(local_sizes).max(dim=0).values\r\n    all_sizes_equal = all(all(ls == max_size) for ls in local_sizes)\r\n\r\n    # 2. If shapes are all the same, then do a simple gather:\r\n    if all_sizes_equal:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    # 3. If not, we need to pad each local tensor to maximum size, gather and then truncate\r\n    pad_dims = []\r\n    pad_by = (max_size - local_size).detach().cpu()\r\n    for val in reversed(pad_by):\r\n        pad_dims.append(0)\r\n        pad_dims.append(val.item())\r\n    result_padded = F.pad(result, pad_dims)\r\n    gathered_result = [torch.zeros_like(result_padded) for _ in range(world_size)]\r\n    torch.distributed.all_gather(gathered_result, result_padded, group)\r\n    for idx, item_size in enumerate(local_sizes):\r\n        slice_param = [slice(dim_size) for dim_size in item_size]\r\n        gathered_result[idx] = gathered_result[idx][slice_param]\r\n    return gathered_result", "language": "python", "code": "def _gather_all_tensors(result: Tensor, group: Optional[Any] = None) -> list[Tensor]:\r\n    \"\"\"Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.\r\n\r\n    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case\r\n    tensors are padded, gathered and then trimmed to secure equal workload for all processes.\r\n\r\n    Args:\r\n        result: The value to sync\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n\r\n    Return:\r\n        gathered_result: List with size equal to the process group where\r\n            gathered_result[i] corresponds to result tensor from process i\r\n\r\n    \"\"\"\r\n    if group is None:\r\n        group = torch.distributed.group.WORLD\r\n\r\n    # Convert tensors to contiguous format\r\n    result = result.contiguous()\r\n\r\n    world_size = torch.distributed.get_world_size(group)\r\n    torch.distributed.barrier(group=group)\r\n\r\n    # If the tensor is scalar, things are easy\r\n    if result.ndim == 0:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    # 1. Gather sizes of all tensors\r\n    local_size = torch.tensor(result.shape, device=result.device)\r\n    local_sizes = [torch.zeros_like(local_size) for _ in range(world_size)]\r\n    torch.distributed.all_gather(local_sizes, local_size, group=group)\r\n    max_size = torch.stack(local_sizes).max(dim=0).values\r\n    all_sizes_equal = all(all(ls == max_size) for ls in local_sizes)\r\n\r\n    # 2. If shapes are all the same, then do a simple gather:\r\n    if all_sizes_equal:\r\n        return _simple_gather_all_tensors(result, group, world_size)\r\n\r\n    # 3. If not, we need to pad each local tensor to maximum size, gather and then truncate\r\n    pad_dims = []\r\n    pad_by = (max_size - local_size).detach().cpu()\r\n    for val in reversed(pad_by):\r\n        pad_dims.append(0)\r\n        pad_dims.append(val.item())\r\n    result_padded = F.pad(result, pad_dims)\r\n    gathered_result = [torch.zeros_like(result_padded) for _ in range(world_size)]\r\n    torch.distributed.all_gather(gathered_result, result_padded, group)\r\n    for idx, item_size in enumerate(local_sizes):\r\n        slice_param = [slice(dim_size) for dim_size in item_size]\r\n        gathered_result[idx] = gathered_result[idx][slice_param]\r\n    return gathered_result", "code_tokens": ["def", "_gather_all_tensors", "(", "result", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "list", "[", "Tensor", "]", ":", "\"", "\"", "\"", "Function", "to", "gather", "all", "tensors", "from", "several", "DDP", "processes", "onto", "a", "list", "that", "is", "broadcasted", "to", "all", "processes", ".", "Works", "on", "tensors", "that", "have", "the", "same", "number", "of", "dimensions", ",", "but", "where", "each", "dimension", "may", "differ", ".", "In", "this", "case", "tensors", "are", "padded", ",", "gathered", "and", "then", "trimmed", "to", "secure", "equal", "workload", "for", "all", "processes", ".", "Args", ":", "result", ":", "The", "value", "to", "sync", "group", ":", "The", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "Return", ":", "gathered_result", ":", "List", "with", "size", "equal", "to", "the", "process", "group", "where", "gathered_result", "[", "i", "]", "corresponds", "to", "result", "tensor", "from", "process", "i", "\"", "\"", "\"", "if", "group", "is", "None", ":", "group", "=", "torch", ".", "distributed", ".", "group", ".", "WORLD", "result", "=", "result", ".", "contiguous", "(", ")", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "group", ")", "torch", ".", "distributed", ".", "barrier", "(", "group", "=", "group", ")", "if", "result", ".", "ndim", "=", "=", "0", ":", "return", "_simple_gather_all_tensors", "(", "result", ",", "group", ",", "world_size", ")", "local_size", "=", "torch", ".", "tensor", "(", "result", ".", "shape", ",", "device", "=", "result", ".", "device", ")", "local_sizes", "=", "[", "torch", ".", "zeros_like", "(", "local_size", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "torch", ".", "distributed", ".", "all_gather", "(", "local_sizes", ",", "local_size", ",", "group", "=", "group", ")", "max_size", "=", "torch", ".", "stack", "(", "local_sizes", ")", ".", "max", "(", "dim", "=", "0", ")", ".", "values", "all_sizes_equal", "=", "all", "(", "all", "(", "ls", "=", "=", "max_size", ")", "for", "ls", "in", "local_sizes", ")", "if", "all_sizes_equal", ":", "return", "_simple_gather_all_tensors", "(", "result", ",", "group", ",", "world_size", ")", "pad_dims", "=", "[", "]", "pad_by", "=", "(", "max_size", "-", "local_size", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", "for", "val", "in", "reversed", "(", "pad_by", ")", ":", "pad_dims", ".", "append", "(", "0", ")", "pad_dims", ".", "append", "(", "val", ".", "item", "(", ")", ")", "result_padded", "=", "F", ".", "pad", "(", "result", ",", "pad_dims", ")", "gathered_result", "=", "[", "torch", ".", "zeros_like", "(", "result_padded", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "torch", ".", "distributed", ".", "all_gather", "(", "gathered_result", ",", "result_padded", ",", "group", ")", "for", "idx", ",", "item_size", "in", "enumerate", "(", "local_sizes", ")", ":", "slice_param", "=", "[", "slice", "(", "dim_size", ")", "for", "dim_size", "in", "item_size", "]", "gathered_result", "[", "idx", "]", "=", "gathered_result", "[", "idx", "]", "[", "slice_param", "]", "return", "gathered_result"], "docstring": "Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.\r\n\r\n    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case\r\n    tensors are padded, gathered and then trimmed to secure equal workload for all processes.\r\n\r\n    Args:\r\n        result: The value to sync\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n\r\n    Return:\r\n        gathered_result: List with size equal to the process group where\r\n            gathered_result[i] corresponds to result tensor from process i", "docstring_tokens": ["function", "to", "gather", "all", "tensors", "from", "several", "ddp", "processes", "onto", "a", "list", "that", "is", "broadcasted", "to", "all", "processes", "works", "on", "tensors", "that", "have", "the", "same", "number", "of", "dimensions", "but", "where", "each", "dimension", "may", "differ", "in", "this", "case", "tensors", "are", "padded", "gathered", "and", "then", "trimmed", "to", "secure", "equal", "workload", "for", "all", "processes", "args", "result", "the", "value", "to", "sync", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "return", "gathered_result", "list", "with", "size", "equal", "to", "the", "process", "group", "where", "gathered_result", "i", "corresponds", "to", "result", "tensor", "from", "process", "i"], "docstring_summary": "Function to gather all tensors from several DDP processes onto a list that is broadcasted to all processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 102, "end_line": 153, "hash": "48abeac1478795f1bb1dbfd92c5ca370", "complexity": 10, "parameters": ["result", "group"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "_sync_ddp_if_available", "original_string": "def _sync_ddp_if_available(\r\n    result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None\r\n) -> Tensor:\r\n    \"\"\"Function to reduce a tensor across worker processes during distributed training.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        reduced value\r\n\r\n    \"\"\"\r\n    if _distributed_is_initialized():\r\n        return _sync_ddp(result, group=group, reduce_op=reduce_op)\r\n    return result", "language": "python", "code": "def _sync_ddp_if_available(\r\n    result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None\r\n) -> Tensor:\r\n    \"\"\"Function to reduce a tensor across worker processes during distributed training.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        reduced value\r\n\r\n    \"\"\"\r\n    if _distributed_is_initialized():\r\n        return _sync_ddp(result, group=group, reduce_op=reduce_op)\r\n    return result", "code_tokens": ["def", "_sync_ddp_if_available", "(", "result", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "None", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Function", "to", "reduce", "a", "tensor", "across", "worker", "processes", "during", "distributed", "training", ".", "Args", ":", "result", ":", "The", "value", "to", "sync", "and", "reduce", "(", "typically", "tensor", "or", "number", ")", "group", ":", "The", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "reduce_op", ":", "The", "reduction", "operation", ".", "Defaults", "to", "sum", ".", "Can", "also", "be", "a", "string", "of", "'", "avg", "'", ",", "'", "mean", "'", "to", "calculate", "the", "mean", "during", "reduction", ".", "Return", ":", "reduced", "value", "\"", "\"", "\"", "if", "_distributed_is_initialized", "(", ")", ":", "return", "_sync_ddp", "(", "result", ",", "group", "=", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "result"], "docstring": "Function to reduce a tensor across worker processes during distributed training.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        reduced value", "docstring_tokens": ["function", "to", "reduce", "a", "tensor", "across", "worker", "processes", "during", "distributed", "training", "args", "result", "the", "value", "to", "sync", "and", "reduce", "typically", "tensor", "or", "number", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "reduce_op", "the", "reduction", "operation", "defaults", "to", "sum", "can", "also", "be", "a", "string", "of", "avg", "mean", "to", "calculate", "the", "mean", "during", "reduction", "return", "reduced", "value"], "docstring_summary": "Function to reduce a tensor across worker processes during distributed training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 162, "end_line": 179, "hash": "4dc70ceb87422361980316d025bcd7b2", "complexity": 2, "parameters": ["result", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "_sync_ddp", "original_string": "def _sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None) -> Tensor:\r\n    \"\"\"Reduces a tensor across several distributed processes.\r\n\r\n    This operation is performed in-place, meaning the result will be placed back into the input tensor on all processes.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        The reduced value.\r\n\r\n    \"\"\"\r\n    divide_by_world_size = False\r\n    group = torch.distributed.group.WORLD if group is None else group\r\n\r\n    op: Optional[ReduceOp]\r\n    if isinstance(reduce_op, str):\r\n        reduce_op = \"avg\" if reduce_op == \"mean\" else reduce_op\r\n        if reduce_op.lower() == \"avg\" and torch.distributed.get_backend(group) == \"gloo\":\r\n            # The GLOO backend does not support the `ReduceOp.AVG` operation\r\n            op = ReduceOp.SUM  # type: ignore[assignment]\r\n            divide_by_world_size = True\r\n        else:\r\n            op = getattr(ReduceOp, reduce_op.upper())\r\n    else:\r\n        op = reduce_op\r\n\r\n    # HPU doesn't support Long types, forcefully set it to float\r\n    # TODO: move this to the `lightning_habana` package\r\n    if (\r\n        package_available(\"habana_frameworks\")\r\n        and os.environ.get(\"HCCL_DISTRIBUTED_BACKEND\") == \"1\"\r\n        and result.type()\r\n        in (\r\n            \"torch.LongTensor\",\r\n            \"torch.hpu.LongTensor\",\r\n        )\r\n    ):\r\n        rank_zero_info(\"Long tensor unsupported on HPU, casting to float\")\r\n        result = result.float()\r\n\r\n    # Sync all processes before reduction\r\n    torch.distributed.barrier(group=group)\r\n    torch.distributed.all_reduce(result, op=op, group=group, async_op=False)\r\n    world_size = torch.distributed.get_world_size(group)\r\n\r\n    if not divide_by_world_size:\r\n        return result\r\n    # `torch.distributed.all_reduce` is in-place, so we should do the division in-place to leave the modified tensors\r\n    # with the expected value\r\n    if not torch.is_floating_point(result):\r\n        return result.copy_(result / world_size)\r\n    return result.div_(world_size)", "language": "python", "code": "def _sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = None) -> Tensor:\r\n    \"\"\"Reduces a tensor across several distributed processes.\r\n\r\n    This operation is performed in-place, meaning the result will be placed back into the input tensor on all processes.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        The reduced value.\r\n\r\n    \"\"\"\r\n    divide_by_world_size = False\r\n    group = torch.distributed.group.WORLD if group is None else group\r\n\r\n    op: Optional[ReduceOp]\r\n    if isinstance(reduce_op, str):\r\n        reduce_op = \"avg\" if reduce_op == \"mean\" else reduce_op\r\n        if reduce_op.lower() == \"avg\" and torch.distributed.get_backend(group) == \"gloo\":\r\n            # The GLOO backend does not support the `ReduceOp.AVG` operation\r\n            op = ReduceOp.SUM  # type: ignore[assignment]\r\n            divide_by_world_size = True\r\n        else:\r\n            op = getattr(ReduceOp, reduce_op.upper())\r\n    else:\r\n        op = reduce_op\r\n\r\n    # HPU doesn't support Long types, forcefully set it to float\r\n    # TODO: move this to the `lightning_habana` package\r\n    if (\r\n        package_available(\"habana_frameworks\")\r\n        and os.environ.get(\"HCCL_DISTRIBUTED_BACKEND\") == \"1\"\r\n        and result.type()\r\n        in (\r\n            \"torch.LongTensor\",\r\n            \"torch.hpu.LongTensor\",\r\n        )\r\n    ):\r\n        rank_zero_info(\"Long tensor unsupported on HPU, casting to float\")\r\n        result = result.float()\r\n\r\n    # Sync all processes before reduction\r\n    torch.distributed.barrier(group=group)\r\n    torch.distributed.all_reduce(result, op=op, group=group, async_op=False)\r\n    world_size = torch.distributed.get_world_size(group)\r\n\r\n    if not divide_by_world_size:\r\n        return result\r\n    # `torch.distributed.all_reduce` is in-place, so we should do the division in-place to leave the modified tensors\r\n    # with the expected value\r\n    if not torch.is_floating_point(result):\r\n        return result.copy_(result / world_size)\r\n    return result.div_(world_size)", "code_tokens": ["def", "_sync_ddp", "(", "result", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "None", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Reduces", "a", "tensor", "across", "several", "distributed", "processes", ".", "This", "operation", "is", "performed", "in", "-", "place", ",", "meaning", "the", "result", "will", "be", "placed", "back", "into", "the", "input", "tensor", "on", "all", "processes", ".", "Args", ":", "result", ":", "The", "value", "to", "sync", "and", "reduce", "(", "typically", "tensor", "or", "number", ")", "group", ":", "The", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "reduce_op", ":", "The", "reduction", "operation", ".", "Defaults", "to", "sum", ".", "Can", "also", "be", "a", "string", "of", "'", "avg", "'", ",", "'", "mean", "'", "to", "calculate", "the", "mean", "during", "reduction", ".", "Return", ":", "The", "reduced", "value", ".", "\"", "\"", "\"", "divide_by_world_size", "=", "False", "group", "=", "torch", ".", "distributed", ".", "group", ".", "WORLD", "if", "group", "is", "None", "else", "group", "op", ":", "Optional", "[", "ReduceOp", "]", "if", "isinstance", "(", "reduce_op", ",", "str", ")", ":", "reduce_op", "=", "\"", "avg", "\"", "if", "reduce_op", "=", "=", "\"", "mean", "\"", "else", "reduce_op", "if", "reduce_op", ".", "lower", "(", ")", "=", "=", "\"", "avg", "\"", "and", "torch", ".", "distributed", ".", "get_backend", "(", "group", ")", "=", "=", "\"", "gloo", "\"", ":", "op", "=", "ReduceOp", ".", "SUM", "divide_by_world_size", "=", "True", "else", ":", "op", "=", "getattr", "(", "ReduceOp", ",", "reduce_op", ".", "upper", "(", ")", ")", "else", ":", "op", "=", "reduce_op", "if", "(", "package_available", "(", "\"", "habana_frameworks", "\"", ")", "and", "os", ".", "environ", ".", "get", "(", "\"", "HCCL_DISTRIBUTED_BACKEND", "\"", ")", "=", "=", "\"", "1", "\"", "and", "result", ".", "type", "(", ")", "in", "(", "\"", "torch", ".", "LongTensor", "\"", ",", "\"", "torch", ".", "hpu", ".", "LongTensor", "\"", ",", ")", ")", ":", "rank_zero_info", "(", "\"", "Long", "tensor", "unsupported", "on", "HPU", ",", "casting", "to", "float", "\"", ")", "result", "=", "result", ".", "float", "(", ")", "torch", ".", "distributed", ".", "barrier", "(", "group", "=", "group", ")", "torch", ".", "distributed", ".", "all_reduce", "(", "result", ",", "op", "=", "op", ",", "group", "=", "group", ",", "async_op", "=", "False", ")", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", "group", ")", "if", "not", "divide_by_world_size", ":", "return", "result", "if", "not", "torch", ".", "is_floating_point", "(", "result", ")", ":", "return", "result", ".", "copy_", "(", "result", "/", "world_size", ")", "return", "result", ".", "div_", "(", "world_size", ")"], "docstring": "Reduces a tensor across several distributed processes.\r\n\r\n    This operation is performed in-place, meaning the result will be placed back into the input tensor on all processes.\r\n\r\n    Args:\r\n        result: The value to sync and reduce (typically tensor or number)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        reduce_op: The reduction operation. Defaults to sum.\r\n            Can also be a string of 'avg', 'mean' to calculate the mean during reduction.\r\n\r\n    Return:\r\n        The reduced value.", "docstring_tokens": ["reduces", "a", "tensor", "across", "several", "distributed", "processes", "this", "operation", "is", "performed", "in", "place", "meaning", "the", "result", "will", "be", "placed", "back", "into", "the", "input", "tensor", "on", "all", "processes", "args", "result", "the", "value", "to", "sync", "and", "reduce", "typically", "tensor", "or", "number", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "reduce_op", "the", "reduction", "operation", "defaults", "to", "sum", "can", "also", "be", "a", "string", "of", "avg", "mean", "to", "calculate", "the", "mean", "during", "reduction", "return", "the", "reduced", "value"], "docstring_summary": "Reduces a tensor across several distributed processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 182, "end_line": 237, "hash": "0889133279d6108880aded190973240f", "complexity": 11, "parameters": ["result", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "_all_gather_ddp_if_available", "original_string": "def _all_gather_ddp_if_available(\r\n    tensor: Tensor, group: Optional[\"torch.distributed.ProcessGroup\"] = None, sync_grads: bool = False\r\n) -> Tensor:\r\n    \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n    Args:\r\n        tensor: Tensor of shape (batch, ...)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        sync_grads: Flag that allows users to synchronize gradients for all_gather op\r\n\r\n    Return:\r\n        A tensor of shape (world_size, batch, ...)\r\n\r\n    \"\"\"\r\n    if not _distributed_is_initialized():\r\n        return tensor\r\n\r\n    from torch.distributed.nn.functional import all_gather\r\n\r\n    tensor = tensor.contiguous()  # https://github.com/pytorch/pytorch/issues/73515\r\n    with nullcontext() if sync_grads else torch.no_grad():\r\n        gathered_tensors = all_gather(tensor, group)\r\n    return torch.stack(gathered_tensors)", "language": "python", "code": "def _all_gather_ddp_if_available(\r\n    tensor: Tensor, group: Optional[\"torch.distributed.ProcessGroup\"] = None, sync_grads: bool = False\r\n) -> Tensor:\r\n    \"\"\"Function to gather a tensor from several distributed processes.\r\n\r\n    Args:\r\n        tensor: Tensor of shape (batch, ...)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        sync_grads: Flag that allows users to synchronize gradients for all_gather op\r\n\r\n    Return:\r\n        A tensor of shape (world_size, batch, ...)\r\n\r\n    \"\"\"\r\n    if not _distributed_is_initialized():\r\n        return tensor\r\n\r\n    from torch.distributed.nn.functional import all_gather\r\n\r\n    tensor = tensor.contiguous()  # https://github.com/pytorch/pytorch/issues/73515\r\n    with nullcontext() if sync_grads else torch.no_grad():\r\n        gathered_tensors = all_gather(tensor, group)\r\n    return torch.stack(gathered_tensors)", "code_tokens": ["def", "_all_gather_ddp_if_available", "(", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "\"", "torch", ".", "distributed", ".", "ProcessGroup", "\"", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", ".", "Args", ":", "tensor", ":", "Tensor", "of", "shape", "(", "batch", ",", ".", ".", ".", ")", "group", ":", "The", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "sync_grads", ":", "Flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "all_gather", "op", "Return", ":", "A", "tensor", "of", "shape", "(", "world_size", ",", "batch", ",", ".", ".", ".", ")", "\"", "\"", "\"", "if", "not", "_distributed_is_initialized", "(", ")", ":", "return", "tensor", "from", "torch", ".", "distributed", ".", "nn", ".", "functional", "import", "all_gather", "tensor", "=", "tensor", ".", "contiguous", "(", ")", "with", "nullcontext", "(", ")", "if", "sync_grads", "else", "torch", ".", "no_grad", "(", ")", ":", "gathered_tensors", "=", "all_gather", "(", "tensor", ",", "group", ")", "return", "torch", ".", "stack", "(", "gathered_tensors", ")"], "docstring": "Function to gather a tensor from several distributed processes.\r\n\r\n    Args:\r\n        tensor: Tensor of shape (batch, ...)\r\n        group: The process group to gather results from. Defaults to all processes (world)\r\n        sync_grads: Flag that allows users to synchronize gradients for all_gather op\r\n\r\n    Return:\r\n        A tensor of shape (world_size, batch, ...)", "docstring_tokens": ["function", "to", "gather", "a", "tensor", "from", "several", "distributed", "processes", "args", "tensor", "tensor", "of", "shape", "batch", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "all_gather", "op", "return", "a", "tensor", "of", "shape", "world_size", "batch"], "docstring_summary": "Function to gather a tensor from several distributed processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 240, "end_line": 262, "hash": "93b065c593e6f14565e56023522a784a", "complexity": 4, "parameters": ["tensor", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "_init_dist_connection", "original_string": "def _init_dist_connection(\r\n    cluster_environment: \"ClusterEnvironment\",\r\n    torch_distributed_backend: str,\r\n    global_rank: Optional[int] = None,\r\n    world_size: Optional[int] = None,\r\n    **kwargs: Any,\r\n) -> None:\r\n    \"\"\"Utility function to initialize distributed connection by setting env variables and initializing the distributed\r\n    process group.\r\n\r\n    Args:\r\n        cluster_environment: ``ClusterEnvironment`` instance\r\n        torch_distributed_backend: Backend to use (includes `nccl` and `gloo`)\r\n        global_rank: Rank of the current process\r\n        world_size: Number of processes in the group\r\n        kwargs: Kwargs for ``init_process_group``\r\n\r\n    Raises:\r\n        RuntimeError:\r\n            If ``torch.distributed`` is not available\r\n\r\n    \"\"\"\r\n    if not torch.distributed.is_available():\r\n        raise RuntimeError(\"torch.distributed is not available. Cannot initialize distributed process group\")\r\n    if torch.distributed.is_initialized():\r\n        log.debug(\"torch.distributed is already initialized. Exiting early\")\r\n        return\r\n    global_rank = global_rank if global_rank is not None else cluster_environment.global_rank()\r\n    world_size = world_size if world_size is not None else cluster_environment.world_size()\r\n    os.environ[\"MASTER_ADDR\"] = cluster_environment.main_address\r\n    os.environ[\"MASTER_PORT\"] = str(cluster_environment.main_port)\r\n    log.info(f\"Initializing distributed: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank + 1}/{world_size}\")\r\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\n    if torch_distributed_backend == \"nccl\":\r\n        # PyTorch >= 2.4 warns about undestroyed NCCL process group, so we need to do it at program exit\r\n        atexit.register(_destroy_dist_connection)\r\n\r\n    # On rank=0 let everyone know training is starting\r\n    rank_zero_info(\r\n        f\"{'-' * 100}\\n\"\r\n        f\"distributed_backend={torch_distributed_backend}\\n\"\r\n        f\"All distributed processes registered. Starting with {world_size} processes\\n\"\r\n        f\"{'-' * 100}\\n\"\r\n    )", "language": "python", "code": "def _init_dist_connection(\r\n    cluster_environment: \"ClusterEnvironment\",\r\n    torch_distributed_backend: str,\r\n    global_rank: Optional[int] = None,\r\n    world_size: Optional[int] = None,\r\n    **kwargs: Any,\r\n) -> None:\r\n    \"\"\"Utility function to initialize distributed connection by setting env variables and initializing the distributed\r\n    process group.\r\n\r\n    Args:\r\n        cluster_environment: ``ClusterEnvironment`` instance\r\n        torch_distributed_backend: Backend to use (includes `nccl` and `gloo`)\r\n        global_rank: Rank of the current process\r\n        world_size: Number of processes in the group\r\n        kwargs: Kwargs for ``init_process_group``\r\n\r\n    Raises:\r\n        RuntimeError:\r\n            If ``torch.distributed`` is not available\r\n\r\n    \"\"\"\r\n    if not torch.distributed.is_available():\r\n        raise RuntimeError(\"torch.distributed is not available. Cannot initialize distributed process group\")\r\n    if torch.distributed.is_initialized():\r\n        log.debug(\"torch.distributed is already initialized. Exiting early\")\r\n        return\r\n    global_rank = global_rank if global_rank is not None else cluster_environment.global_rank()\r\n    world_size = world_size if world_size is not None else cluster_environment.world_size()\r\n    os.environ[\"MASTER_ADDR\"] = cluster_environment.main_address\r\n    os.environ[\"MASTER_PORT\"] = str(cluster_environment.main_port)\r\n    log.info(f\"Initializing distributed: GLOBAL_RANK: {global_rank}, MEMBER: {global_rank + 1}/{world_size}\")\r\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\n    if torch_distributed_backend == \"nccl\":\r\n        # PyTorch >= 2.4 warns about undestroyed NCCL process group, so we need to do it at program exit\r\n        atexit.register(_destroy_dist_connection)\r\n\r\n    # On rank=0 let everyone know training is starting\r\n    rank_zero_info(\r\n        f\"{'-' * 100}\\n\"\r\n        f\"distributed_backend={torch_distributed_backend}\\n\"\r\n        f\"All distributed processes registered. Starting with {world_size} processes\\n\"\r\n        f\"{'-' * 100}\\n\"\r\n    )", "code_tokens": ["def", "_init_dist_connection", "(", "cluster_environment", ":", "\"", "ClusterEnvironment", "\"", ",", "torch_distributed_backend", ":", "str", ",", "global_rank", ":", "Optional", "[", "int", "]", "=", "None", ",", "world_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Utility", "function", "to", "initialize", "distributed", "connection", "by", "setting", "env", "variables", "and", "initializing", "the", "distributed", "process", "group", ".", "Args", ":", "cluster_environment", ":", "`", "`", "ClusterEnvironment", "`", "`", "instance", "torch_distributed_backend", ":", "Backend", "to", "use", "(", "includes", "`", "nccl", "`", "and", "`", "gloo", "`", ")", "global_rank", ":", "Rank", "of", "the", "current", "process", "world_size", ":", "Number", "of", "processes", "in", "the", "group", "kwargs", ":", "Kwargs", "for", "`", "`", "init_process_group", "`", "`", "Raises", ":", "RuntimeError", ":", "If", "`", "`", "torch", ".", "distributed", "`", "`", "is", "not", "available", "\"", "\"", "\"", "if", "not", "torch", ".", "distributed", ".", "is_available", "(", ")", ":", "raise", "RuntimeError", "(", "\"", "torch", ".", "distributed", "is", "not", "available", ".", "Cannot", "initialize", "distributed", "process", "group", "\"", ")", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "log", ".", "debug", "(", "\"", "torch", ".", "distributed", "is", "already", "initialized", ".", "Exiting", "early", "\"", ")", "return", "global_rank", "=", "global_rank", "if", "global_rank", "is", "not", "None", "else", "cluster_environment", ".", "global_rank", "(", ")", "world_size", "=", "world_size", "if", "world_size", "is", "not", "None", "else", "cluster_environment", ".", "world_size", "(", ")", "os", ".", "environ", "[", "\"", "MASTER_ADDR", "\"", "]", "=", "cluster_environment", ".", "main_address", "os", ".", "environ", "[", "\"", "MASTER_PORT", "\"", "]", "=", "str", "(", "cluster_environment", ".", "main_port", ")", "log", ".", "info", "(", "f", "\"", "Initializing", "distributed", ":", "GLOBAL_RANK", ":", "{", "global_rank", "}", ",", "MEMBER", ":", "{", "global_rank", "+", "1", "}", "/", "{", "world_size", "}", "\"", ")", "torch", ".", "distributed", ".", "init_process_group", "(", "torch_distributed_backend", ",", "rank", "=", "global_rank", ",", "world_size", "=", "world_size", ",", "*", "*", "kwargs", ")", "if", "torch_distributed_backend", "=", "=", "\"", "nccl", "\"", ":", "atexit", ".", "register", "(", "_destroy_dist_connection", ")", "rank_zero_info", "(", "f", "\"", "{", "'", "-", "'", "*", "100", "}", "\\", "n", "\"", "f", "\"", "distributed_backend", "=", "{", "torch_distributed_backend", "}", "\\", "n", "\"", "f", "\"", "All", "distributed", "processes", "registered", ".", "Starting", "with", "{", "world_size", "}", "processes", "\\", "n", "\"", "f", "\"", "{", "'", "-", "'", "*", "100", "}", "\\", "n", "\"", ")"], "docstring": "Utility function to initialize distributed connection by setting env variables and initializing the distributed\r\n    process group.\r\n\r\n    Args:\r\n        cluster_environment: ``ClusterEnvironment`` instance\r\n        torch_distributed_backend: Backend to use (includes `nccl` and `gloo`)\r\n        global_rank: Rank of the current process\r\n        world_size: Number of processes in the group\r\n        kwargs: Kwargs for ``init_process_group``\r\n\r\n    Raises:\r\n        RuntimeError:\r\n            If ``torch.distributed`` is not available", "docstring_tokens": ["utility", "function", "to", "initialize", "distributed", "connection", "by", "setting", "env", "variables", "and", "initializing", "the", "distributed", "process", "group", "args", "cluster_environment", "clusterenvironment", "instance", "torch_distributed_backend", "backend", "to", "use", "includes", "nccl", "and", "gloo", "global_rank", "rank", "of", "the", "current", "process", "world_size", "number", "of", "processes", "in", "the", "group", "kwargs", "kwargs", "for", "init_process_group", "raises", "runtimeerror", "if", "torch", "distributed", "is", "not", "available"], "docstring_summary": "Utility function to initialize distributed connection by setting env variables and initializing the distributed", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 265, "end_line": 309, "hash": "7b745c4ac31b758aab437424bf4dcc01", "complexity": 6, "parameters": ["cluster_environment", "torch_distributed_backend", "global_rank", "world_size", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\distributed.py", "func_name": "_get_default_process_group_backend_for_device", "original_string": "def _get_default_process_group_backend_for_device(device: torch.device) -> str:\r\n    \"\"\"Return corresponding distributed backend for a given device.\"\"\"\r\n    device_backend_map = torch.distributed.Backend.default_device_backend_map\r\n    if device.type in device_backend_map:\r\n        return device_backend_map[device.type]\r\n    return \"gloo\"", "language": "python", "code": "def _get_default_process_group_backend_for_device(device: torch.device) -> str:\r\n    \"\"\"Return corresponding distributed backend for a given device.\"\"\"\r\n    device_backend_map = torch.distributed.Backend.default_device_backend_map\r\n    if device.type in device_backend_map:\r\n        return device_backend_map[device.type]\r\n    return \"gloo\"", "code_tokens": ["def", "_get_default_process_group_backend_for_device", "(", "device", ":", "torch", ".", "device", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Return", "corresponding", "distributed", "backend", "for", "a", "given", "device", ".", "\"", "\"", "\"", "device_backend_map", "=", "torch", ".", "distributed", ".", "Backend", ".", "default_device_backend_map", "if", "device", ".", "type", "in", "device_backend_map", ":", "return", "device_backend_map", "[", "device", ".", "type", "]", "return", "\"", "gloo", "\""], "docstring": "Return corresponding distributed backend for a given device.", "docstring_tokens": ["return", "corresponding", "distributed", "backend", "for", "a", "given", "device"], "docstring_summary": "Return corresponding distributed backend for a given device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\distributed.py", "partition": "train", "function_type": "function", "start_line": 320, "end_line": 325, "hash": "438fc5c968401f2e22249ae94a7e9154", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\init.py", "func_name": "_materialize", "original_string": "def _materialize(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize a module.\"\"\"\r\n    module.to_empty(device=device, recurse=False)\r\n    if not hasattr(module, \"reset_parameters\"):\r\n        raise TypeError(\r\n            f\"Materialization requires that the `{type(module).__name__}.reset_parameters` method is implemented.\"\r\n            \" This method is used to initialize any children parameters or buffers in this module.\"\r\n        )\r\n    if callable(module.reset_parameters):\r\n        module.reset_parameters()", "language": "python", "code": "def _materialize(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize a module.\"\"\"\r\n    module.to_empty(device=device, recurse=False)\r\n    if not hasattr(module, \"reset_parameters\"):\r\n        raise TypeError(\r\n            f\"Materialization requires that the `{type(module).__name__}.reset_parameters` method is implemented.\"\r\n            \" This method is used to initialize any children parameters or buffers in this module.\"\r\n        )\r\n    if callable(module.reset_parameters):\r\n        module.reset_parameters()", "code_tokens": ["def", "_materialize", "(", "module", ":", "Module", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Materialize", "a", "module", ".", "\"", "\"", "\"", "module", ".", "to_empty", "(", "device", "=", "device", ",", "recurse", "=", "False", ")", "if", "not", "hasattr", "(", "module", ",", "\"", "reset_parameters", "\"", ")", ":", "raise", "TypeError", "(", "f", "\"", "Materialization", "requires", "that", "the", "`", "{", "type", "(", "module", ")", ".", "__name__", "}", ".", "reset_parameters", "`", "method", "is", "implemented", ".", "\"", "\"", "This", "method", "is", "used", "to", "initialize", "any", "children", "parameters", "or", "buffers", "in", "this", "module", ".", "\"", ")", "if", "callable", "(", "module", ".", "reset_parameters", ")", ":", "module", ".", "reset_parameters", "(", ")"], "docstring": "Materialize a module.", "docstring_tokens": ["materialize", "a", "module"], "docstring_summary": "Materialize a module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\init.py", "partition": "train", "function_type": "function", "start_line": 61, "end_line": 70, "hash": "d46b1f3d64bec631679701a7d11068fe", "complexity": 3, "parameters": ["module", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\init.py", "func_name": "_materialize_meta_tensors", "original_string": "def _materialize_meta_tensors(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize all tensors in a given module.\"\"\"\r\n    for module in module.modules():\r\n        if _has_meta_device_parameters_or_buffers(module, recurse=False):\r\n            _materialize(module, device)", "language": "python", "code": "def _materialize_meta_tensors(module: Module, device: _DEVICE) -> None:\r\n    \"\"\"Materialize all tensors in a given module.\"\"\"\r\n    for module in module.modules():\r\n        if _has_meta_device_parameters_or_buffers(module, recurse=False):\r\n            _materialize(module, device)", "code_tokens": ["def", "_materialize_meta_tensors", "(", "module", ":", "Module", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Materialize", "all", "tensors", "in", "a", "given", "module", ".", "\"", "\"", "\"", "for", "module", "in", "module", ".", "modules", "(", ")", ":", "if", "_has_meta_device_parameters_or_buffers", "(", "module", ",", "recurse", "=", "False", ")", ":", "_materialize", "(", "module", ",", "device", ")"], "docstring": "Materialize all tensors in a given module.", "docstring_tokens": ["materialize", "all", "tensors", "in", "a", "given", "module"], "docstring_summary": "Materialize all tensors in a given module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\init.py", "partition": "train", "function_type": "function", "start_line": 73, "end_line": 77, "hash": "6e1cb8ad54168e6150421b05cacb6498", "complexity": 3, "parameters": ["module", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\load.py", "func_name": "_move_state_into", "original_string": "def _move_state_into(\r\n    source: dict[str, Any], destination: dict[str, Union[Any, _Stateful]], keys: Optional[set[str]] = None\r\n) -> None:\r\n    \"\"\"Takes the state from the source destination and moves it into the destination dictionary.\r\n\r\n    If an object in the destination follows the stateful protocol, it loads the source state via ``load_state_dict``.\r\n\r\n    \"\"\"\r\n    keys = set(source) if keys is None else keys & set(source)\r\n    for key in keys:\r\n        state = source.pop(key)\r\n        if key in destination and isinstance(destination[key], _Stateful):\r\n            destination[key].load_state_dict(state)\r\n        else:\r\n            destination[key] = state", "language": "python", "code": "def _move_state_into(\r\n    source: dict[str, Any], destination: dict[str, Union[Any, _Stateful]], keys: Optional[set[str]] = None\r\n) -> None:\r\n    \"\"\"Takes the state from the source destination and moves it into the destination dictionary.\r\n\r\n    If an object in the destination follows the stateful protocol, it loads the source state via ``load_state_dict``.\r\n\r\n    \"\"\"\r\n    keys = set(source) if keys is None else keys & set(source)\r\n    for key in keys:\r\n        state = source.pop(key)\r\n        if key in destination and isinstance(destination[key], _Stateful):\r\n            destination[key].load_state_dict(state)\r\n        else:\r\n            destination[key] = state", "code_tokens": ["def", "_move_state_into", "(", "source", ":", "dict", "[", "str", ",", "Any", "]", ",", "destination", ":", "dict", "[", "str", ",", "Union", "[", "Any", ",", "_Stateful", "]", "]", ",", "keys", ":", "Optional", "[", "set", "[", "str", "]", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Takes", "the", "state", "from", "the", "source", "destination", "and", "moves", "it", "into", "the", "destination", "dictionary", ".", "If", "an", "object", "in", "the", "destination", "follows", "the", "stateful", "protocol", ",", "it", "loads", "the", "source", "state", "via", "`", "`", "load_state_dict", "`", "`", ".", "\"", "\"", "\"", "keys", "=", "set", "(", "source", ")", "if", "keys", "is", "None", "else", "keys", "&", "set", "(", "source", ")", "for", "key", "in", "keys", ":", "state", "=", "source", ".", "pop", "(", "key", ")", "if", "key", "in", "destination", "and", "isinstance", "(", "destination", "[", "key", "]", ",", "_Stateful", ")", ":", "destination", "[", "key", "]", ".", "load_state_dict", "(", "state", ")", "else", ":", "destination", "[", "key", "]", "=", "state"], "docstring": "Takes the state from the source destination and moves it into the destination dictionary.\r\n\r\n    If an object in the destination follows the stateful protocol, it loads the source state via ``load_state_dict``.", "docstring_tokens": ["takes", "the", "state", "from", "the", "source", "destination", "and", "moves", "it", "into", "the", "destination", "dictionary", "if", "an", "object", "in", "the", "destination", "follows", "the", "stateful", "protocol", "it", "loads", "the", "source", "state", "via", "load_state_dict"], "docstring_summary": "Takes the state from the source destination and moves it into the destination dictionary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\load.py", "partition": "train", "function_type": "function", "start_line": 222, "end_line": 236, "hash": "c97e67551038dabded2a5fe0b918025f", "complexity": 5, "parameters": ["source", "Any]", "destination", "Union[Any", "_Stateful]]", "keys"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\load.py", "func_name": "_load_distributed_checkpoint", "original_string": "def _load_distributed_checkpoint(checkpoint_folder: Path) -> dict[str, Any]:\r\n    \"\"\"Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.\r\n\r\n    The current implementation assumes that the entire checkpoint fits in CPU memory.\r\n\r\n    \"\"\"\r\n    if not _TORCH_GREATER_EQUAL_2_3:\r\n        raise ImportError(\"Processing distributed checkpoints requires PyTorch >= 2.3.\")\r\n\r\n    from torch.distributed.checkpoint import FileSystemReader\r\n    from torch.distributed.checkpoint.format_utils import _EmptyStateDictLoadPlanner\r\n    from torch.distributed.checkpoint.state_dict_loader import _load_state_dict\r\n\r\n    checkpoint: dict[str, Any] = {}\r\n    _load_state_dict(\r\n        checkpoint,\r\n        storage_reader=FileSystemReader(checkpoint_folder),\r\n        planner=_EmptyStateDictLoadPlanner(),\r\n        no_dist=True,\r\n    )\r\n\r\n    # This is the extra file saved by Fabric, with user data separate from weights and optimizer states\r\n    extra_file = checkpoint_folder / _METADATA_FILENAME\r\n    extra = torch.load(extra_file, map_location=\"cpu\") if extra_file.is_file() else {}\r\n    checkpoint.update(extra)\r\n\r\n    return checkpoint", "language": "python", "code": "def _load_distributed_checkpoint(checkpoint_folder: Path) -> dict[str, Any]:\r\n    \"\"\"Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.\r\n\r\n    The current implementation assumes that the entire checkpoint fits in CPU memory.\r\n\r\n    \"\"\"\r\n    if not _TORCH_GREATER_EQUAL_2_3:\r\n        raise ImportError(\"Processing distributed checkpoints requires PyTorch >= 2.3.\")\r\n\r\n    from torch.distributed.checkpoint import FileSystemReader\r\n    from torch.distributed.checkpoint.format_utils import _EmptyStateDictLoadPlanner\r\n    from torch.distributed.checkpoint.state_dict_loader import _load_state_dict\r\n\r\n    checkpoint: dict[str, Any] = {}\r\n    _load_state_dict(\r\n        checkpoint,\r\n        storage_reader=FileSystemReader(checkpoint_folder),\r\n        planner=_EmptyStateDictLoadPlanner(),\r\n        no_dist=True,\r\n    )\r\n\r\n    # This is the extra file saved by Fabric, with user data separate from weights and optimizer states\r\n    extra_file = checkpoint_folder / _METADATA_FILENAME\r\n    extra = torch.load(extra_file, map_location=\"cpu\") if extra_file.is_file() else {}\r\n    checkpoint.update(extra)\r\n\r\n    return checkpoint", "code_tokens": ["def", "_load_distributed_checkpoint", "(", "checkpoint_folder", ":", "Path", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Loads", "a", "sharded", "checkpoint", "saved", "with", "the", "`", "torch", ".", "distributed", ".", "checkpoint", "`", "into", "a", "full", "state", "dict", ".", "The", "current", "implementation", "assumes", "that", "the", "entire", "checkpoint", "fits", "in", "CPU", "memory", ".", "\"", "\"", "\"", "if", "not", "_TORCH_GREATER_EQUAL_2_3", ":", "raise", "ImportError", "(", "\"", "Processing", "distributed", "checkpoints", "requires", "PyTorch", ">", "=", "2", ".", "3", ".", "\"", ")", "from", "torch", ".", "distributed", ".", "checkpoint", "import", "FileSystemReader", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "format_utils", "import", "_EmptyStateDictLoadPlanner", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict_loader", "import", "_load_state_dict", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "_load_state_dict", "(", "checkpoint", ",", "storage_reader", "=", "FileSystemReader", "(", "checkpoint_folder", ")", ",", "planner", "=", "_EmptyStateDictLoadPlanner", "(", ")", ",", "no_dist", "=", "True", ",", ")", "extra_file", "=", "checkpoint_folder", "/", "_METADATA_FILENAME", "extra", "=", "torch", ".", "load", "(", "extra_file", ",", "map_location", "=", "\"", "cpu", "\"", ")", "if", "extra_file", ".", "is_file", "(", ")", "else", "{", "}", "checkpoint", ".", "update", "(", "extra", ")", "return", "checkpoint"], "docstring": "Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.\r\n\r\n    The current implementation assumes that the entire checkpoint fits in CPU memory.", "docstring_tokens": ["loads", "a", "sharded", "checkpoint", "saved", "with", "the", "torch", "distributed", "checkpoint", "into", "a", "full", "state", "dict", "the", "current", "implementation", "assumes", "that", "the", "entire", "checkpoint", "fits", "in", "cpu", "memory"], "docstring_summary": "Loads a sharded checkpoint saved with the `torch.distributed.checkpoint` into a full state dict.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\load.py", "partition": "train", "function_type": "function", "start_line": 239, "end_line": 265, "hash": "10f0bb0cfbdc6107033723cee73817e4", "complexity": 3, "parameters": ["checkpoint_folder"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "_convert_params", "original_string": "def _convert_params(params: Optional[Union[dict[str, Any], Namespace]]) -> dict[str, Any]:\r\n    \"\"\"Ensure parameters are a dict or convert to dict if necessary.\r\n\r\n    Args:\r\n        params: Target to be converted to a dictionary\r\n\r\n    Returns:\r\n        params as a dictionary\r\n\r\n    \"\"\"\r\n    # in case converting from namespace\r\n    if isinstance(params, Namespace):\r\n        params = vars(params)\r\n\r\n    if params is None:\r\n        params = {}\r\n\r\n    return params", "language": "python", "code": "def _convert_params(params: Optional[Union[dict[str, Any], Namespace]]) -> dict[str, Any]:\r\n    \"\"\"Ensure parameters are a dict or convert to dict if necessary.\r\n\r\n    Args:\r\n        params: Target to be converted to a dictionary\r\n\r\n    Returns:\r\n        params as a dictionary\r\n\r\n    \"\"\"\r\n    # in case converting from namespace\r\n    if isinstance(params, Namespace):\r\n        params = vars(params)\r\n\r\n    if params is None:\r\n        params = {}\r\n\r\n    return params", "code_tokens": ["def", "_convert_params", "(", "params", ":", "Optional", "[", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Ensure", "parameters", "are", "a", "dict", "or", "convert", "to", "dict", "if", "necessary", ".", "Args", ":", "params", ":", "Target", "to", "be", "converted", "to", "a", "dictionary", "Returns", ":", "params", "as", "a", "dictionary", "\"", "\"", "\"", "if", "isinstance", "(", "params", ",", "Namespace", ")", ":", "params", "=", "vars", "(", "params", ")", "if", "params", "is", "None", ":", "params", "=", "{", "}", "return", "params"], "docstring": "Ensure parameters are a dict or convert to dict if necessary.\r\n\r\n    Args:\r\n        params: Target to be converted to a dictionary\r\n\r\n    Returns:\r\n        params as a dictionary", "docstring_tokens": ["ensure", "parameters", "are", "a", "dict", "or", "convert", "to", "dict", "if", "necessary", "args", "params", "target", "to", "be", "converted", "to", "a", "dictionary", "returns", "params", "as", "a", "dictionary"], "docstring_summary": "Ensure parameters are a dict or convert to dict if necessary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\logger.py", "partition": "train", "function_type": "function", "start_line": 26, "end_line": 43, "hash": "7931c340364bf2fe949270c0645abd7c", "complexity": 3, "parameters": ["params", "Any]", "Namespace]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "_sanitize_callable_params", "original_string": "def _sanitize_callable_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n\r\n    Returns:\r\n        dictionary with all callables sanitized\r\n\r\n    \"\"\"\r\n\r\n    def _sanitize_callable(val: Any) -> Any:\r\n        if inspect.isclass(val):\r\n            # If it's a class, don't try to instantiate it, just return the name\r\n            return val.__name__\r\n        if callable(val):\r\n            # Callables get a chance to return a name\r\n            try:\r\n                _val = val()\r\n                if callable(_val):\r\n                    return val.__name__\r\n                return _val\r\n            # todo: specify the possible exception\r\n            except Exception:\r\n                return getattr(val, \"__name__\", None)\r\n        return val\r\n\r\n    return {key: _sanitize_callable(val) for key, val in params.items()}", "language": "python", "code": "def _sanitize_callable_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n\r\n    Returns:\r\n        dictionary with all callables sanitized\r\n\r\n    \"\"\"\r\n\r\n    def _sanitize_callable(val: Any) -> Any:\r\n        if inspect.isclass(val):\r\n            # If it's a class, don't try to instantiate it, just return the name\r\n            return val.__name__\r\n        if callable(val):\r\n            # Callables get a chance to return a name\r\n            try:\r\n                _val = val()\r\n                if callable(_val):\r\n                    return val.__name__\r\n                return _val\r\n            # todo: specify the possible exception\r\n            except Exception:\r\n                return getattr(val, \"__name__\", None)\r\n        return val\r\n\r\n    return {key: _sanitize_callable(val) for key, val in params.items()}", "code_tokens": ["def", "_sanitize_callable_params", "(", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Sanitize", "callable", "params", "dict", ",", "e", ".", "g", ".", "`", "`", "{", "'", "a", "'", ":", "<", "function_", "*", "*", "*", "*", "at", "0x", "*", "*", "*", "*", ">", "}", "-", ">", "{", "'", "a", "'", ":", "'", "function_", "*", "*", "*", "*", "'", "}", "`", "`", ".", "Args", ":", "params", ":", "Dictionary", "containing", "the", "hyperparameters", "Returns", ":", "dictionary", "with", "all", "callables", "sanitized", "\"", "\"", "\"", "def", "_sanitize_callable", "(", "val", ":", "Any", ")", "-", ">", "Any", ":", "if", "inspect", ".", "isclass", "(", "val", ")", ":", "return", "val", ".", "__name__", "if", "callable", "(", "val", ")", ":", "try", ":", "_val", "=", "val", "(", ")", "if", "callable", "(", "_val", ")", ":", "return", "val", ".", "__name__", "return", "_val", "except", "Exception", ":", "return", "getattr", "(", "val", ",", "\"", "__name__", "\"", ",", "None", ")", "return", "val", "return", "{", "key", ":", "_sanitize_callable", "(", "val", ")", "for", "key", ",", "val", "in", "params", ".", "items", "(", ")", "}"], "docstring": "Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n\r\n    Returns:\r\n        dictionary with all callables sanitized", "docstring_tokens": ["sanitize", "callable", "params", "dict", "e", "g", "a", "function_", "at", "0x", "a", "function_", "args", "params", "dictionary", "containing", "the", "hyperparameters", "returns", "dictionary", "with", "all", "callables", "sanitized"], "docstring_summary": "Sanitize callable params dict, e.g. ``{'a': <function_**** at 0x****>} -> {'a': 'function_****'}``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\logger.py", "partition": "train", "function_type": "function", "start_line": 46, "end_line": 73, "hash": "7ab1476ae95a7595287ce2c1289f38ca", "complexity": 6, "parameters": ["params", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "_flatten_dict", "original_string": "def _flatten_dict(params: MutableMapping[Any, Any], delimiter: str = \"/\", parent_key: str = \"\") -> dict[str, Any]:\r\n    \"\"\"Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n        delimiter: Delimiter to express the hierarchy. Defaults to ``'/'``.\r\n\r\n    Returns:\r\n        Flattened dict.\r\n\r\n    Examples:\r\n        >>> _flatten_dict({'a': {'b': 'c'}})\r\n        {'a/b': 'c'}\r\n        >>> _flatten_dict({'a': {'b': 123}})\r\n        {'a/b': 123}\r\n        >>> _flatten_dict({5: {'a': 123}})\r\n        {'5/a': 123}\r\n        >>> _flatten_dict({\"dl\": [{\"a\": 1, \"c\": 3}, {\"b\": 2, \"d\": 5}], \"l\": [1, 2, 3, 4]})\r\n        {'dl/0/a': 1, 'dl/0/c': 3, 'dl/1/b': 2, 'dl/1/d': 5, 'l': [1, 2, 3, 4]}\r\n\r\n    \"\"\"\r\n    result: dict[str, Any] = {}\r\n    for k, v in params.items():\r\n        new_key = parent_key + delimiter + str(k) if parent_key else str(k)\r\n        if is_dataclass(v) and not isinstance(v, type):\r\n            v = asdict(v)\r\n        elif isinstance(v, Namespace):\r\n            v = vars(v)\r\n\r\n        if isinstance(v, MutableMapping):\r\n            result = {**result, **_flatten_dict(v, parent_key=new_key, delimiter=delimiter)}\r\n        # Also handle the case where v is a list of dictionaries\r\n        elif isinstance(v, list) and all(isinstance(item, MutableMapping) for item in v):\r\n            for i, item in enumerate(v):\r\n                result = {**result, **_flatten_dict(item, parent_key=f\"{new_key}/{i}\", delimiter=delimiter)}\r\n        else:\r\n            result[new_key] = v\r\n    return result", "language": "python", "code": "def _flatten_dict(params: MutableMapping[Any, Any], delimiter: str = \"/\", parent_key: str = \"\") -> dict[str, Any]:\r\n    \"\"\"Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n        delimiter: Delimiter to express the hierarchy. Defaults to ``'/'``.\r\n\r\n    Returns:\r\n        Flattened dict.\r\n\r\n    Examples:\r\n        >>> _flatten_dict({'a': {'b': 'c'}})\r\n        {'a/b': 'c'}\r\n        >>> _flatten_dict({'a': {'b': 123}})\r\n        {'a/b': 123}\r\n        >>> _flatten_dict({5: {'a': 123}})\r\n        {'5/a': 123}\r\n        >>> _flatten_dict({\"dl\": [{\"a\": 1, \"c\": 3}, {\"b\": 2, \"d\": 5}], \"l\": [1, 2, 3, 4]})\r\n        {'dl/0/a': 1, 'dl/0/c': 3, 'dl/1/b': 2, 'dl/1/d': 5, 'l': [1, 2, 3, 4]}\r\n\r\n    \"\"\"\r\n    result: dict[str, Any] = {}\r\n    for k, v in params.items():\r\n        new_key = parent_key + delimiter + str(k) if parent_key else str(k)\r\n        if is_dataclass(v) and not isinstance(v, type):\r\n            v = asdict(v)\r\n        elif isinstance(v, Namespace):\r\n            v = vars(v)\r\n\r\n        if isinstance(v, MutableMapping):\r\n            result = {**result, **_flatten_dict(v, parent_key=new_key, delimiter=delimiter)}\r\n        # Also handle the case where v is a list of dictionaries\r\n        elif isinstance(v, list) and all(isinstance(item, MutableMapping) for item in v):\r\n            for i, item in enumerate(v):\r\n                result = {**result, **_flatten_dict(item, parent_key=f\"{new_key}/{i}\", delimiter=delimiter)}\r\n        else:\r\n            result[new_key] = v\r\n    return result", "code_tokens": ["def", "_flatten_dict", "(", "params", ":", "MutableMapping", "[", "Any", ",", "Any", "]", ",", "delimiter", ":", "str", "=", "\"", "/", "\"", ",", "parent_key", ":", "str", "=", "\"", "\"", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Flatten", "hierarchical", "dict", ",", "e", ".", "g", ".", "`", "`", "{", "'", "a", "'", ":", "{", "'", "b", "'", ":", "'", "c", "'", "}", "}", "-", ">", "{", "'", "a", "/", "b", "'", ":", "'", "c", "'", "}", "`", "`", ".", "Args", ":", "params", ":", "Dictionary", "containing", "the", "hyperparameters", "delimiter", ":", "Delimiter", "to", "express", "the", "hierarchy", ".", "Defaults", "to", "`", "`", "'", "/", "'", "`", "`", ".", "Returns", ":", "Flattened", "dict", ".", "Examples", ":", ">", ">", ">", "_flatten_dict", "(", "{", "'", "a", "'", ":", "{", "'", "b", "'", ":", "'", "c", "'", "}", "}", ")", "{", "'", "a", "/", "b", "'", ":", "'", "c", "'", "}", ">", ">", ">", "_flatten_dict", "(", "{", "'", "a", "'", ":", "{", "'", "b", "'", ":", "123", "}", "}", ")", "{", "'", "a", "/", "b", "'", ":", "123", "}", ">", ">", ">", "_flatten_dict", "(", "{", "5", ":", "{", "'", "a", "'", ":", "123", "}", "}", ")", "{", "'", "5", "/", "a", "'", ":", "123", "}", ">", ">", ">", "_flatten_dict", "(", "{", "\"", "dl", "\"", ":", "[", "{", "\"", "a", "\"", ":", "1", ",", "\"", "c", "\"", ":", "3", "}", ",", "{", "\"", "b", "\"", ":", "2", ",", "\"", "d", "\"", ":", "5", "}", "]", ",", "\"", "l", "\"", ":", "[", "1", ",", "2", ",", "3", ",", "4", "]", "}", ")", "{", "'", "dl", "/", "0", "/", "a", "'", ":", "1", ",", "'", "dl", "/", "0", "/", "c", "'", ":", "3", ",", "'", "dl", "/", "1", "/", "b", "'", ":", "2", ",", "'", "dl", "/", "1", "/", "d", "'", ":", "5", ",", "'", "l", "'", ":", "[", "1", ",", "2", ",", "3", ",", "4", "]", "}", "\"", "\"", "\"", "result", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "}", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "new_key", "=", "parent_key", "+", "delimiter", "+", "str", "(", "k", ")", "if", "parent_key", "else", "str", "(", "k", ")", "if", "is_dataclass", "(", "v", ")", "and", "not", "isinstance", "(", "v", ",", "type", ")", ":", "v", "=", "asdict", "(", "v", ")", "elif", "isinstance", "(", "v", ",", "Namespace", ")", ":", "v", "=", "vars", "(", "v", ")", "if", "isinstance", "(", "v", ",", "MutableMapping", ")", ":", "result", "=", "{", "*", "*", "result", ",", "*", "*", "_flatten_dict", "(", "v", ",", "parent_key", "=", "new_key", ",", "delimiter", "=", "delimiter", ")", "}", "elif", "isinstance", "(", "v", ",", "list", ")", "and", "all", "(", "isinstance", "(", "item", ",", "MutableMapping", ")", "for", "item", "in", "v", ")", ":", "for", "i", ",", "item", "in", "enumerate", "(", "v", ")", ":", "result", "=", "{", "*", "*", "result", ",", "*", "*", "_flatten_dict", "(", "item", ",", "parent_key", "=", "f", "\"", "{", "new_key", "}", "/", "{", "i", "}", "\"", ",", "delimiter", "=", "delimiter", ")", "}", "else", ":", "result", "[", "new_key", "]", "=", "v", "return", "result"], "docstring": "Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.\r\n\r\n    Args:\r\n        params: Dictionary containing the hyperparameters\r\n        delimiter: Delimiter to express the hierarchy. Defaults to ``'/'``.\r\n\r\n    Returns:\r\n        Flattened dict.\r\n\r\n    Examples:\r\n        >>> _flatten_dict({'a': {'b': 'c'}})\r\n        {'a/b': 'c'}\r\n        >>> _flatten_dict({'a': {'b': 123}})\r\n        {'a/b': 123}\r\n        >>> _flatten_dict({5: {'a': 123}})\r\n        {'5/a': 123}\r\n        >>> _flatten_dict({\"dl\": [{\"a\": 1, \"c\": 3}, {\"b\": 2, \"d\": 5}], \"l\": [1, 2, 3, 4]})\r\n        {'dl/0/a': 1, 'dl/0/c': 3, 'dl/1/b': 2, 'dl/1/d': 5, 'l': [1, 2, 3, 4]}", "docstring_tokens": ["flatten", "hierarchical", "dict", "e", "g", "a", "b", "c", "a", "b", "c", "args", "params", "dictionary", "containing", "the", "hyperparameters", "delimiter", "delimiter", "to", "express", "the", "hierarchy", "defaults", "to", "returns", "flattened", "dict", "examples", "_flatten_dict", "a", "b", "c", "a", "b", "c", "_flatten_dict", "a", "b", "123", "a", "b", "123", "_flatten_dict", "5", "a", "123", "5", "a", "123", "_flatten_dict", "dl", "a", "1", "c", "3", "b", "2", "d", "5", "l", "1", "2", "3", "4", "dl", "0", "a", "1", "dl", "0", "c", "3", "dl", "1", "b", "2", "dl", "1", "d", "5", "l", "1", "2", "3", "4"], "docstring_summary": "Flatten hierarchical dict, e.g. ``{'a': {'b': 'c'}} -> {'a/b': 'c'}``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\logger.py", "partition": "train", "function_type": "function", "start_line": 76, "end_line": 113, "hash": "d9c2f746f064004533558af61e8286d2", "complexity": 11, "parameters": ["params", "Any]", "delimiter", "parent_key"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "_sanitize_params", "original_string": "def _sanitize_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Returns params with non-primitvies converted to strings for logging.\r\n\r\n    >>> import torch\r\n    >>> params = {\"float\": 0.3,\r\n    ...           \"int\": 1,\r\n    ...           \"string\": \"abc\",\r\n    ...           \"bool\": True,\r\n    ...           \"list\": [1, 2, 3],\r\n    ...           \"namespace\": Namespace(foo=3),\r\n    ...           \"layer\": torch.nn.BatchNorm1d}\r\n    >>> import pprint\r\n    >>> pprint.pprint(_sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE\r\n    {'bool': True,\r\n        'float': 0.3,\r\n        'int': 1,\r\n        'layer': \"<class 'torch.nn.modules.batchnorm.BatchNorm1d'>\",\r\n        'list': '[1, 2, 3]',\r\n        'namespace': 'Namespace(foo=3)',\r\n        'string': 'abc'}\r\n\r\n    \"\"\"\r\n    for k in params:\r\n        if _NUMPY_AVAILABLE:\r\n            import numpy as np\r\n\r\n            if isinstance(params[k], (np.bool_, np.integer, np.floating)):\r\n                params[k] = params[k].item()\r\n        if type(params[k]) not in [bool, int, float, str, Tensor]:\r\n            params[k] = str(params[k])\r\n    return params", "language": "python", "code": "def _sanitize_params(params: dict[str, Any]) -> dict[str, Any]:\r\n    \"\"\"Returns params with non-primitvies converted to strings for logging.\r\n\r\n    >>> import torch\r\n    >>> params = {\"float\": 0.3,\r\n    ...           \"int\": 1,\r\n    ...           \"string\": \"abc\",\r\n    ...           \"bool\": True,\r\n    ...           \"list\": [1, 2, 3],\r\n    ...           \"namespace\": Namespace(foo=3),\r\n    ...           \"layer\": torch.nn.BatchNorm1d}\r\n    >>> import pprint\r\n    >>> pprint.pprint(_sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE\r\n    {'bool': True,\r\n        'float': 0.3,\r\n        'int': 1,\r\n        'layer': \"<class 'torch.nn.modules.batchnorm.BatchNorm1d'>\",\r\n        'list': '[1, 2, 3]',\r\n        'namespace': 'Namespace(foo=3)',\r\n        'string': 'abc'}\r\n\r\n    \"\"\"\r\n    for k in params:\r\n        if _NUMPY_AVAILABLE:\r\n            import numpy as np\r\n\r\n            if isinstance(params[k], (np.bool_, np.integer, np.floating)):\r\n                params[k] = params[k].item()\r\n        if type(params[k]) not in [bool, int, float, str, Tensor]:\r\n            params[k] = str(params[k])\r\n    return params", "code_tokens": ["def", "_sanitize_params", "(", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Returns", "params", "with", "non", "-", "primitvies", "converted", "to", "strings", "for", "logging", ".", ">", ">", ">", "import", "torch", ">", ">", ">", "params", "=", "{", "\"", "float", "\"", ":", "0", ".", "3", ",", ".", ".", ".", "\"", "int", "\"", ":", "1", ",", ".", ".", ".", "\"", "string", "\"", ":", "\"", "abc", "\"", ",", ".", ".", ".", "\"", "bool", "\"", ":", "True", ",", ".", ".", ".", "\"", "list", "\"", ":", "[", "1", ",", "2", ",", "3", "]", ",", ".", ".", ".", "\"", "namespace", "\"", ":", "Namespace", "(", "foo", "=", "3", ")", ",", ".", ".", ".", "\"", "layer", "\"", ":", "torch", ".", "nn", ".", "BatchNorm1d", "}", ">", ">", ">", "import", "pprint", ">", ">", ">", "pprint", ".", "pprint", "(", "_sanitize_params", "(", "params", ")", ")", "{", "'", "bool", "'", ":", "True", ",", "'", "float", "'", ":", "0", ".", "3", ",", "'", "int", "'", ":", "1", ",", "'", "layer", "'", ":", "\"", "<", "class", "'", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm1d", "'", ">", "\"", ",", "'", "list", "'", ":", "'", "[", "1", ",", "2", ",", "3", "]", "'", ",", "'", "namespace", "'", ":", "'", "Namespace", "(", "foo", "=", "3", ")", "'", ",", "'", "string", "'", ":", "'", "abc", "'", "}", "\"", "\"", "\"", "for", "k", "in", "params", ":", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "if", "isinstance", "(", "params", "[", "k", "]", ",", "(", "np", ".", "bool_", ",", "np", ".", "integer", ",", "np", ".", "floating", ")", ")", ":", "params", "[", "k", "]", "=", "params", "[", "k", "]", ".", "item", "(", ")", "if", "type", "(", "params", "[", "k", "]", ")", "not", "in", "[", "bool", ",", "int", ",", "float", ",", "str", ",", "Tensor", "]", ":", "params", "[", "k", "]", "=", "str", "(", "params", "[", "k", "]", ")", "return", "params"], "docstring": "Returns params with non-primitvies converted to strings for logging.\r\n\r\n    >>> import torch\r\n    >>> params = {\"float\": 0.3,\r\n    ...           \"int\": 1,\r\n    ...           \"string\": \"abc\",\r\n    ...           \"bool\": True,\r\n    ...           \"list\": [1, 2, 3],\r\n    ...           \"namespace\": Namespace(foo=3),\r\n    ...           \"layer\": torch.nn.BatchNorm1d}\r\n    >>> import pprint\r\n    >>> pprint.pprint(_sanitize_params(params))  # doctest: +NORMALIZE_WHITESPACE\r\n    {'bool': True,\r\n        'float': 0.3,\r\n        'int': 1,\r\n        'layer': \"<class 'torch.nn.modules.batchnorm.BatchNorm1d'>\",\r\n        'list': '[1, 2, 3]',\r\n        'namespace': 'Namespace(foo=3)',\r\n        'string': 'abc'}", "docstring_tokens": ["returns", "params", "with", "non", "primitvies", "converted", "to", "strings", "for", "logging", "import", "torch", "params", "float", "0", "3", "int", "1", "string", "abc", "bool", "true", "list", "1", "2", "3", "namespace", "namespace", "foo", "3", "layer", "torch", "nn", "batchnorm1d", "import", "pprint", "pprint", "pprint", "_sanitize_params", "params", "doctest", "normalize_whitespace", "bool", "true", "float", "0", "3", "int", "1", "layer", "class", "torch", "nn", "modules", "batchnorm", "batchnorm1d", "list", "1", "2", "3", "namespace", "namespace", "foo", "3", "string", "abc"], "docstring_summary": "Returns params with non-primitvies converted to strings for logging.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\logger.py", "partition": "train", "function_type": "function", "start_line": 116, "end_line": 146, "hash": "98c2037a275fa643f1bcce439f43a806", "complexity": 5, "parameters": ["params", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "_is_json_serializable", "original_string": "def _is_json_serializable(value: Any) -> bool:\r\n    \"\"\"Test whether a variable can be encoded as json.\"\"\"\r\n    if value is None or isinstance(value, (bool, int, float, str, list, dict)):  # fast path\r\n        return True\r\n    try:\r\n        json.dumps(value)\r\n        return True\r\n    except (TypeError, OverflowError):\r\n        # OverflowError is raised if number is too large to encode\r\n        return False", "language": "python", "code": "def _is_json_serializable(value: Any) -> bool:\r\n    \"\"\"Test whether a variable can be encoded as json.\"\"\"\r\n    if value is None or isinstance(value, (bool, int, float, str, list, dict)):  # fast path\r\n        return True\r\n    try:\r\n        json.dumps(value)\r\n        return True\r\n    except (TypeError, OverflowError):\r\n        # OverflowError is raised if number is too large to encode\r\n        return False", "code_tokens": ["def", "_is_json_serializable", "(", "value", ":", "Any", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Test", "whether", "a", "variable", "can", "be", "encoded", "as", "json", ".", "\"", "\"", "\"", "if", "value", "is", "None", "or", "isinstance", "(", "value", ",", "(", "bool", ",", "int", ",", "float", ",", "str", ",", "list", ",", "dict", ")", ")", ":", "return", "True", "try", ":", "json", ".", "dumps", "(", "value", ")", "return", "True", "except", "(", "TypeError", ",", "OverflowError", ")", ":", "return", "False"], "docstring": "Test whether a variable can be encoded as json.", "docstring_tokens": ["test", "whether", "a", "variable", "can", "be", "encoded", "as", "json"], "docstring_summary": "Test whether a variable can be encoded as json.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\logger.py", "partition": "train", "function_type": "function", "start_line": 154, "end_line": 163, "hash": "e80e3701d686b550b4df2bf5a69f5ba4", "complexity": 4, "parameters": ["value"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\logger.py", "func_name": "_add_prefix", "original_string": "def _add_prefix(\r\n    metrics: Mapping[str, Union[Tensor, float]], prefix: str, separator: str\r\n) -> Mapping[str, Union[Tensor, float]]:\r\n    \"\"\"Insert prefix before each key in a dict, separated by the separator.\r\n\r\n    Args:\r\n        metrics: Dictionary with metric names as keys and measured quantities as values\r\n        prefix: Prefix to insert before each key\r\n        separator: Separates prefix and original key name\r\n\r\n    Returns:\r\n        Dictionary with prefix and separator inserted before each key\r\n\r\n    \"\"\"\r\n    if not prefix:\r\n        return metrics\r\n    return {f\"{prefix}{separator}{k}\": v for k, v in metrics.items()}", "language": "python", "code": "def _add_prefix(\r\n    metrics: Mapping[str, Union[Tensor, float]], prefix: str, separator: str\r\n) -> Mapping[str, Union[Tensor, float]]:\r\n    \"\"\"Insert prefix before each key in a dict, separated by the separator.\r\n\r\n    Args:\r\n        metrics: Dictionary with metric names as keys and measured quantities as values\r\n        prefix: Prefix to insert before each key\r\n        separator: Separates prefix and original key name\r\n\r\n    Returns:\r\n        Dictionary with prefix and separator inserted before each key\r\n\r\n    \"\"\"\r\n    if not prefix:\r\n        return metrics\r\n    return {f\"{prefix}{separator}{k}\": v for k, v in metrics.items()}", "code_tokens": ["def", "_add_prefix", "(", "metrics", ":", "Mapping", "[", "str", ",", "Union", "[", "Tensor", ",", "float", "]", "]", ",", "prefix", ":", "str", ",", "separator", ":", "str", ")", "-", ">", "Mapping", "[", "str", ",", "Union", "[", "Tensor", ",", "float", "]", "]", ":", "\"", "\"", "\"", "Insert", "prefix", "before", "each", "key", "in", "a", "dict", ",", "separated", "by", "the", "separator", ".", "Args", ":", "metrics", ":", "Dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "prefix", ":", "Prefix", "to", "insert", "before", "each", "key", "separator", ":", "Separates", "prefix", "and", "original", "key", "name", "Returns", ":", "Dictionary", "with", "prefix", "and", "separator", "inserted", "before", "each", "key", "\"", "\"", "\"", "if", "not", "prefix", ":", "return", "metrics", "return", "{", "f", "\"", "{", "prefix", "}", "{", "separator", "}", "{", "k", "}", "\"", ":", "v", "for", "k", ",", "v", "in", "metrics", ".", "items", "(", ")", "}"], "docstring": "Insert prefix before each key in a dict, separated by the separator.\r\n\r\n    Args:\r\n        metrics: Dictionary with metric names as keys and measured quantities as values\r\n        prefix: Prefix to insert before each key\r\n        separator: Separates prefix and original key name\r\n\r\n    Returns:\r\n        Dictionary with prefix and separator inserted before each key", "docstring_tokens": ["insert", "prefix", "before", "each", "key", "in", "a", "dict", "separated", "by", "the", "separator", "args", "metrics", "dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "prefix", "prefix", "to", "insert", "before", "each", "key", "separator", "separates", "prefix", "and", "original", "key", "name", "returns", "dictionary", "with", "prefix", "and", "separator", "inserted", "before", "each", "key"], "docstring_summary": "Insert prefix before each key in a dict, separated by the separator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\logger.py", "partition": "train", "function_type": "function", "start_line": 166, "end_line": 182, "hash": "f38ef86d88513172b6c490a7e91e3877", "complexity": 3, "parameters": ["metrics", "Union[Tensor", "float]]", "prefix", "separator"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\optimizer.py", "func_name": "_optimizers_to_device", "original_string": "def _optimizers_to_device(optimizers: Iterable[Optimizer], device: _DEVICE) -> None:\r\n    \"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\r\n    for opt in optimizers:\r\n        _optimizer_to_device(opt, device)", "language": "python", "code": "def _optimizers_to_device(optimizers: Iterable[Optimizer], device: _DEVICE) -> None:\r\n    \"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\r\n    for opt in optimizers:\r\n        _optimizer_to_device(opt, device)", "code_tokens": ["def", "_optimizers_to_device", "(", "optimizers", ":", "Iterable", "[", "Optimizer", "]", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Moves", "optimizer", "states", "for", "a", "sequence", "of", "optimizers", "to", "the", "device", ".", "\"", "\"", "\"", "for", "opt", "in", "optimizers", ":", "_optimizer_to_device", "(", "opt", ",", "device", ")"], "docstring": "Moves optimizer states for a sequence of optimizers to the device.", "docstring_tokens": ["moves", "optimizer", "states", "for", "a", "sequence", "of", "optimizers", "to", "the", "device"], "docstring_summary": "Moves optimizer states for a sequence of optimizers to the device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\optimizer.py", "partition": "train", "function_type": "function", "start_line": 23, "end_line": 26, "hash": "46886af905cf941932b22a3ca11d4270", "complexity": 2, "parameters": ["optimizers", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\optimizer.py", "func_name": "_optimizer_to_device", "original_string": "def _optimizer_to_device(optimizer: Optimizer, device: _DEVICE) -> None:\r\n    \"\"\"Moves the state of a single optimizer to the device.\"\"\"\r\n    for p, v in optimizer.state.items():\r\n        if not isinstance(v, MutableMapping):\r\n            # Support for custom optimizers\r\n            optimizer.state[p] = apply_to_collection(v, Tensor, move_data_to_device, device, allow_frozen=True)\r\n            continue\r\n        for key, val in v.items():\r\n            # The 'step' parameter needs to remain unmoved (possibly on the CPU) since that is where the optimizer\r\n            # needs it. See https://github.com/pytorch/pytorch/issues/74424\r\n            if key != \"step\":\r\n                v[key] = move_data_to_device(val, device)", "language": "python", "code": "def _optimizer_to_device(optimizer: Optimizer, device: _DEVICE) -> None:\r\n    \"\"\"Moves the state of a single optimizer to the device.\"\"\"\r\n    for p, v in optimizer.state.items():\r\n        if not isinstance(v, MutableMapping):\r\n            # Support for custom optimizers\r\n            optimizer.state[p] = apply_to_collection(v, Tensor, move_data_to_device, device, allow_frozen=True)\r\n            continue\r\n        for key, val in v.items():\r\n            # The 'step' parameter needs to remain unmoved (possibly on the CPU) since that is where the optimizer\r\n            # needs it. See https://github.com/pytorch/pytorch/issues/74424\r\n            if key != \"step\":\r\n                v[key] = move_data_to_device(val, device)", "code_tokens": ["def", "_optimizer_to_device", "(", "optimizer", ":", "Optimizer", ",", "device", ":", "_DEVICE", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Moves", "the", "state", "of", "a", "single", "optimizer", "to", "the", "device", ".", "\"", "\"", "\"", "for", "p", ",", "v", "in", "optimizer", ".", "state", ".", "items", "(", ")", ":", "if", "not", "isinstance", "(", "v", ",", "MutableMapping", ")", ":", "optimizer", ".", "state", "[", "p", "]", "=", "apply_to_collection", "(", "v", ",", "Tensor", ",", "move_data_to_device", ",", "device", ",", "allow_frozen", "=", "True", ")", "continue", "for", "key", ",", "val", "in", "v", ".", "items", "(", ")", ":", "if", "key", "!", "=", "\"", "step", "\"", ":", "v", "[", "key", "]", "=", "move_data_to_device", "(", "val", ",", "device", ")"], "docstring": "Moves the state of a single optimizer to the device.", "docstring_tokens": ["moves", "the", "state", "of", "a", "single", "optimizer", "to", "the", "device"], "docstring_summary": "Moves the state of a single optimizer to the device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\optimizer.py", "partition": "train", "function_type": "function", "start_line": 29, "end_line": 40, "hash": "fbb504b1235b714ef599856d9d8effe4", "complexity": 5, "parameters": ["optimizer", "device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\registry.py", "func_name": "_load_external_callbacks", "original_string": "def _load_external_callbacks(group: str) -> list[Any]:\r\n    \"\"\"Collect external callbacks registered through entry points.\r\n\r\n    The entry points are expected to be functions returning a list of callbacks.\r\n\r\n    Args:\r\n        group: The entry point group name to load callbacks from.\r\n\r\n    Return:\r\n        A list of all callbacks collected from external factories.\r\n\r\n    \"\"\"\r\n    factories = (\r\n        entry_points(group=group) if _PYTHON_GREATER_EQUAL_3_10_0 else entry_points().get(group, {})  # type: ignore[arg-type]\r\n    )\r\n\r\n    external_callbacks: list[Any] = []\r\n    for factory in factories:\r\n        callback_factory = factory.load()\r\n        callbacks_list: Union[list[Any], Any] = callback_factory()\r\n        callbacks_list = [callbacks_list] if not isinstance(callbacks_list, list) else callbacks_list\r\n        if callbacks_list:\r\n            _log.info(\r\n                f\"Adding {len(callbacks_list)} callbacks from entry point '{factory.name}':\"\r\n                f\" {', '.join(type(cb).__name__ for cb in callbacks_list)}\"\r\n            )\r\n        external_callbacks.extend(callbacks_list)\r\n    return external_callbacks", "language": "python", "code": "def _load_external_callbacks(group: str) -> list[Any]:\r\n    \"\"\"Collect external callbacks registered through entry points.\r\n\r\n    The entry points are expected to be functions returning a list of callbacks.\r\n\r\n    Args:\r\n        group: The entry point group name to load callbacks from.\r\n\r\n    Return:\r\n        A list of all callbacks collected from external factories.\r\n\r\n    \"\"\"\r\n    factories = (\r\n        entry_points(group=group) if _PYTHON_GREATER_EQUAL_3_10_0 else entry_points().get(group, {})  # type: ignore[arg-type]\r\n    )\r\n\r\n    external_callbacks: list[Any] = []\r\n    for factory in factories:\r\n        callback_factory = factory.load()\r\n        callbacks_list: Union[list[Any], Any] = callback_factory()\r\n        callbacks_list = [callbacks_list] if not isinstance(callbacks_list, list) else callbacks_list\r\n        if callbacks_list:\r\n            _log.info(\r\n                f\"Adding {len(callbacks_list)} callbacks from entry point '{factory.name}':\"\r\n                f\" {', '.join(type(cb).__name__ for cb in callbacks_list)}\"\r\n            )\r\n        external_callbacks.extend(callbacks_list)\r\n    return external_callbacks", "code_tokens": ["def", "_load_external_callbacks", "(", "group", ":", "str", ")", "-", ">", "list", "[", "Any", "]", ":", "\"", "\"", "\"", "Collect", "external", "callbacks", "registered", "through", "entry", "points", ".", "The", "entry", "points", "are", "expected", "to", "be", "functions", "returning", "a", "list", "of", "callbacks", ".", "Args", ":", "group", ":", "The", "entry", "point", "group", "name", "to", "load", "callbacks", "from", ".", "Return", ":", "A", "list", "of", "all", "callbacks", "collected", "from", "external", "factories", ".", "\"", "\"", "\"", "factories", "=", "(", "entry_points", "(", "group", "=", "group", ")", "if", "_PYTHON_GREATER_EQUAL_3_10_0", "else", "entry_points", "(", ")", ".", "get", "(", "group", ",", "{", "}", ")", ")", "external_callbacks", ":", "list", "[", "Any", "]", "=", "[", "]", "for", "factory", "in", "factories", ":", "callback_factory", "=", "factory", ".", "load", "(", ")", "callbacks_list", ":", "Union", "[", "list", "[", "Any", "]", ",", "Any", "]", "=", "callback_factory", "(", ")", "callbacks_list", "=", "[", "callbacks_list", "]", "if", "not", "isinstance", "(", "callbacks_list", ",", "list", ")", "else", "callbacks_list", "if", "callbacks_list", ":", "_log", ".", "info", "(", "f", "\"", "Adding", "{", "len", "(", "callbacks_list", ")", "}", "callbacks", "from", "entry", "point", "'", "{", "factory", ".", "name", "}", "'", ":", "\"", "f", "\"", "{", "'", ",", "'", ".", "join", "(", "type", "(", "cb", ")", ".", "__name__", "for", "cb", "in", "callbacks_list", ")", "}", "\"", ")", "external_callbacks", ".", "extend", "(", "callbacks_list", ")", "return", "external_callbacks"], "docstring": "Collect external callbacks registered through entry points.\r\n\r\n    The entry points are expected to be functions returning a list of callbacks.\r\n\r\n    Args:\r\n        group: The entry point group name to load callbacks from.\r\n\r\n    Return:\r\n        A list of all callbacks collected from external factories.", "docstring_tokens": ["collect", "external", "callbacks", "registered", "through", "entry", "points", "the", "entry", "points", "are", "expected", "to", "be", "functions", "returning", "a", "list", "of", "callbacks", "args", "group", "the", "entry", "point", "group", "name", "to", "load", "callbacks", "from", "return", "a", "list", "of", "all", "callbacks", "collected", "from", "external", "factories"], "docstring_summary": "Collect external callbacks registered through entry points.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\registry.py", "partition": "train", "function_type": "function", "start_line": 26, "end_line": 53, "hash": "ae9922e1d5afb7e749da4c7c33f592bd", "complexity": 6, "parameters": ["group"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "seed_everything", "original_string": "def seed_everything(seed: Optional[int] = None, workers: bool = False, verbose: bool = True) -> int:\r\n    r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.\r\n    In addition, sets the following environment variables:\r\n\r\n    - ``PL_GLOBAL_SEED``: will be passed to spawned subprocesses (e.g. ddp_spawn backend).\r\n    - ``PL_SEED_WORKERS``: (optional) is set to 1 if ``workers=True``.\r\n\r\n    Args:\r\n        seed: the integer value seed for global random state in Lightning.\r\n            If ``None``, it will read the seed from ``PL_GLOBAL_SEED`` env variable. If ``None`` and the\r\n            ``PL_GLOBAL_SEED`` env variable is not set, then the seed defaults to 0. If seed is\r\n            not in bounds or cannot be cast to int, a ValueError is raised.\r\n        workers: if set to ``True``, will properly configure all dataloaders passed to the\r\n            Trainer with a ``worker_init_fn``. If the user already provides such a function\r\n            for their dataloaders, setting this argument will have no influence. See also:\r\n            :func:`~lightning.fabric.utilities.seed.pl_worker_init_function`.\r\n        verbose: Whether to print a message on each rank with the seed being set.\r\n\r\n    \"\"\"\r\n    if seed is None:\r\n        env_seed = os.environ.get(\"PL_GLOBAL_SEED\")\r\n        if env_seed is None:\r\n            seed = 0\r\n            if verbose:\r\n                rank_zero_warn(f\"No seed found, seed set to {seed}\")\r\n        else:\r\n            try:\r\n                seed = int(env_seed)\r\n            except ValueError:\r\n                raise ValueError(f\"Invalid seed specified via PL_GLOBAL_SEED: {repr(env_seed)}\")\r\n    elif not isinstance(seed, int):\r\n        seed = int(seed)\r\n\r\n    if not (min_seed_value <= seed <= max_seed_value):\r\n        raise ValueError(f\"{seed} is not in bounds, numpy accepts from {min_seed_value} to {max_seed_value}\")\r\n\r\n    if verbose:\r\n        log.info(rank_prefixed_message(f\"Seed set to {seed}\", _get_rank()))\r\n\r\n    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\r\n    random.seed(seed)\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n\r\n    os.environ[\"PL_SEED_WORKERS\"] = f\"{int(workers)}\"\r\n\r\n    return seed", "language": "python", "code": "def seed_everything(seed: Optional[int] = None, workers: bool = False, verbose: bool = True) -> int:\r\n    r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.\r\n    In addition, sets the following environment variables:\r\n\r\n    - ``PL_GLOBAL_SEED``: will be passed to spawned subprocesses (e.g. ddp_spawn backend).\r\n    - ``PL_SEED_WORKERS``: (optional) is set to 1 if ``workers=True``.\r\n\r\n    Args:\r\n        seed: the integer value seed for global random state in Lightning.\r\n            If ``None``, it will read the seed from ``PL_GLOBAL_SEED`` env variable. If ``None`` and the\r\n            ``PL_GLOBAL_SEED`` env variable is not set, then the seed defaults to 0. If seed is\r\n            not in bounds or cannot be cast to int, a ValueError is raised.\r\n        workers: if set to ``True``, will properly configure all dataloaders passed to the\r\n            Trainer with a ``worker_init_fn``. If the user already provides such a function\r\n            for their dataloaders, setting this argument will have no influence. See also:\r\n            :func:`~lightning.fabric.utilities.seed.pl_worker_init_function`.\r\n        verbose: Whether to print a message on each rank with the seed being set.\r\n\r\n    \"\"\"\r\n    if seed is None:\r\n        env_seed = os.environ.get(\"PL_GLOBAL_SEED\")\r\n        if env_seed is None:\r\n            seed = 0\r\n            if verbose:\r\n                rank_zero_warn(f\"No seed found, seed set to {seed}\")\r\n        else:\r\n            try:\r\n                seed = int(env_seed)\r\n            except ValueError:\r\n                raise ValueError(f\"Invalid seed specified via PL_GLOBAL_SEED: {repr(env_seed)}\")\r\n    elif not isinstance(seed, int):\r\n        seed = int(seed)\r\n\r\n    if not (min_seed_value <= seed <= max_seed_value):\r\n        raise ValueError(f\"{seed} is not in bounds, numpy accepts from {min_seed_value} to {max_seed_value}\")\r\n\r\n    if verbose:\r\n        log.info(rank_prefixed_message(f\"Seed set to {seed}\", _get_rank()))\r\n\r\n    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\r\n    random.seed(seed)\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n\r\n    os.environ[\"PL_SEED_WORKERS\"] = f\"{int(workers)}\"\r\n\r\n    return seed", "code_tokens": ["def", "seed_everything", "(", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "workers", ":", "bool", "=", "False", ",", "verbose", ":", "bool", "=", "True", ")", "-", ">", "int", ":", "r", "\"", "\"", "\"", "Function", "that", "sets", "the", "seed", "for", "pseudo", "-", "random", "number", "generators", "in", ":", "torch", ",", "numpy", ",", "and", "Python", "'", "s", "random", "module", ".", "In", "addition", ",", "sets", "the", "following", "environment", "variables", ":", "-", "`", "`", "PL_GLOBAL_SEED", "`", "`", ":", "will", "be", "passed", "to", "spawned", "subprocesses", "(", "e", ".", "g", ".", "ddp_spawn", "backend", ")", ".", "-", "`", "`", "PL_SEED_WORKERS", "`", "`", ":", "(", "optional", ")", "is", "set", "to", "1", "if", "`", "`", "workers", "=", "True", "`", "`", ".", "Args", ":", "seed", ":", "the", "integer", "value", "seed", "for", "global", "random", "state", "in", "Lightning", ".", "If", "`", "`", "None", "`", "`", ",", "it", "will", "read", "the", "seed", "from", "`", "`", "PL_GLOBAL_SEED", "`", "`", "env", "variable", ".", "If", "`", "`", "None", "`", "`", "and", "the", "`", "`", "PL_GLOBAL_SEED", "`", "`", "env", "variable", "is", "not", "set", ",", "then", "the", "seed", "defaults", "to", "0", ".", "If", "seed", "is", "not", "in", "bounds", "or", "cannot", "be", "cast", "to", "int", ",", "a", "ValueError", "is", "raised", ".", "workers", ":", "if", "set", "to", "`", "`", "True", "`", "`", ",", "will", "properly", "configure", "all", "dataloaders", "passed", "to", "the", "Trainer", "with", "a", "`", "`", "worker_init_fn", "`", "`", ".", "If", "the", "user", "already", "provides", "such", "a", "function", "for", "their", "dataloaders", ",", "setting", "this", "argument", "will", "have", "no", "influence", ".", "See", "also", ":", ":", "func", ":", "`", "~", "lightning", ".", "fabric", ".", "utilities", ".", "seed", ".", "pl_worker_init_function", "`", ".", "verbose", ":", "Whether", "to", "print", "a", "message", "on", "each", "rank", "with", "the", "seed", "being", "set", ".", "\"", "\"", "\"", "if", "seed", "is", "None", ":", "env_seed", "=", "os", ".", "environ", ".", "get", "(", "\"", "PL_GLOBAL_SEED", "\"", ")", "if", "env_seed", "is", "None", ":", "seed", "=", "0", "if", "verbose", ":", "rank_zero_warn", "(", "f", "\"", "No", "seed", "found", ",", "seed", "set", "to", "{", "seed", "}", "\"", ")", "else", ":", "try", ":", "seed", "=", "int", "(", "env_seed", ")", "except", "ValueError", ":", "raise", "ValueError", "(", "f", "\"", "Invalid", "seed", "specified", "via", "PL_GLOBAL_SEED", ":", "{", "repr", "(", "env_seed", ")", "}", "\"", ")", "elif", "not", "isinstance", "(", "seed", ",", "int", ")", ":", "seed", "=", "int", "(", "seed", ")", "if", "not", "(", "min_seed_value", "<", "=", "seed", "<", "=", "max_seed_value", ")", ":", "raise", "ValueError", "(", "f", "\"", "{", "seed", "}", "is", "not", "in", "bounds", ",", "numpy", "accepts", "from", "{", "min_seed_value", "}", "to", "{", "max_seed_value", "}", "\"", ")", "if", "verbose", ":", "log", ".", "info", "(", "rank_prefixed_message", "(", "f", "\"", "Seed", "set", "to", "{", "seed", "}", "\"", ",", "_get_rank", "(", ")", ")", ")", "os", ".", "environ", "[", "\"", "PL_GLOBAL_SEED", "\"", "]", "=", "str", "(", "seed", ")", "random", ".", "seed", "(", "seed", ")", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "np", ".", "random", ".", "seed", "(", "seed", ")", "torch", ".", "manual_seed", "(", "seed", ")", "os", ".", "environ", "[", "\"", "PL_SEED_WORKERS", "\"", "]", "=", "f", "\"", "{", "int", "(", "workers", ")", "}", "\"", "return", "seed"], "docstring": "r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.\r\n    In addition, sets the following environment variables:\r\n\r\n    - ``PL_GLOBAL_SEED``: will be passed to spawned subprocesses (e.g. ddp_spawn backend).\r\n    - ``PL_SEED_WORKERS``: (optional) is set to 1 if ``workers=True``.\r\n\r\n    Args:\r\n        seed: the integer value seed for global random state in Lightning.\r\n            If ``None``, it will read the seed from ``PL_GLOBAL_SEED`` env variable. If ``None`` and the\r\n            ``PL_GLOBAL_SEED`` env variable is not set, then the seed defaults to 0. If seed is\r\n            not in bounds or cannot be cast to int, a ValueError is raised.\r\n        workers: if set to ``True``, will properly configure all dataloaders passed to the\r\n            Trainer with a ``worker_init_fn``. If the user already provides such a function\r\n            for their dataloaders, setting this argument will have no influence. See also:\r\n            :func:`~lightning.fabric.utilities.seed.pl_worker_init_function`.\r\n        verbose: Whether to print a message on each rank with the seed being set.\r\n\r\n    \"\"\"", "docstring_tokens": ["r", "function", "that", "sets", "the", "seed", "for", "pseudo", "random", "number", "generators", "in", "torch", "numpy", "and", "python", "s", "random", "module", "in", "addition", "sets", "the", "following", "environment", "variables", "pl_global_seed", "will", "be", "passed", "to", "spawned", "subprocesses", "e", "g", "ddp_spawn", "backend", "pl_seed_workers", "optional", "is", "set", "to", "1", "if", "workers", "true", "args", "seed", "the", "integer", "value", "seed", "for", "global", "random", "state", "in", "lightning", "if", "none", "it", "will", "read", "the", "seed", "from", "pl_global_seed", "env", "variable", "if", "none", "and", "the", "pl_global_seed", "env", "variable", "is", "not", "set", "then", "the", "seed", "defaults", "to", "0", "if", "seed", "is", "not", "in", "bounds", "or", "cannot", "be", "cast", "to", "int", "a", "valueerror", "is", "raised", "workers", "if", "set", "to", "true", "will", "properly", "configure", "all", "dataloaders", "passed", "to", "the", "trainer", "with", "a", "worker_init_fn", "if", "the", "user", "already", "provides", "such", "a", "function", "for", "their", "dataloaders", "setting", "this", "argument", "will", "have", "no", "influence", "see", "also", "func", "lightning", "fabric", "utilities", "seed", "pl_worker_init_function", "verbose", "whether", "to", "print", "a", "message", "on", "each", "rank", "with", "the", "seed", "being", "set"], "docstring_summary": "r\"\"\"Function that sets the seed for pseudo-random number generators in: torch, numpy, and Python's random module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\seed.py", "partition": "train", "function_type": "function", "start_line": 19, "end_line": 68, "hash": "4a8acc19a3419023c7b4c88fa642ef8d", "complexity": 9, "parameters": ["seed", "workers", "verbose"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "reset_seed", "original_string": "def reset_seed() -> None:\r\n    r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.\r\n\r\n    If :func:`~lightning.fabric.utilities.seed.seed_everything` is unused, this function will do nothing.\r\n\r\n    \"\"\"\r\n    seed = os.environ.get(\"PL_GLOBAL_SEED\", None)\r\n    if seed is None:\r\n        return\r\n    workers = os.environ.get(\"PL_SEED_WORKERS\", \"0\")\r\n    seed_everything(int(seed), workers=bool(int(workers)), verbose=False)", "language": "python", "code": "def reset_seed() -> None:\r\n    r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.\r\n\r\n    If :func:`~lightning.fabric.utilities.seed.seed_everything` is unused, this function will do nothing.\r\n\r\n    \"\"\"\r\n    seed = os.environ.get(\"PL_GLOBAL_SEED\", None)\r\n    if seed is None:\r\n        return\r\n    workers = os.environ.get(\"PL_SEED_WORKERS\", \"0\")\r\n    seed_everything(int(seed), workers=bool(int(workers)), verbose=False)", "code_tokens": ["def", "reset_seed", "(", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Reset", "the", "seed", "to", "the", "value", "that", ":", "func", ":", "`", "~", "lightning", ".", "fabric", ".", "utilities", ".", "seed", ".", "seed_everything", "`", "previously", "set", ".", "If", ":", "func", ":", "`", "~", "lightning", ".", "fabric", ".", "utilities", ".", "seed", ".", "seed_everything", "`", "is", "unused", ",", "this", "function", "will", "do", "nothing", ".", "\"", "\"", "\"", "seed", "=", "os", ".", "environ", ".", "get", "(", "\"", "PL_GLOBAL_SEED", "\"", ",", "None", ")", "if", "seed", "is", "None", ":", "return", "workers", "=", "os", ".", "environ", ".", "get", "(", "\"", "PL_SEED_WORKERS", "\"", ",", "\"", "0", "\"", ")", "seed_everything", "(", "int", "(", "seed", ")", ",", "workers", "=", "bool", "(", "int", "(", "workers", ")", ")", ",", "verbose", "=", "False", ")"], "docstring": "r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.\r\n\r\n    If :func:`~lightning.fabric.utilities.seed.seed_everything` is unused, this function will do nothing.\r\n\r\n    \"\"\"", "docstring_tokens": ["r", "reset", "the", "seed", "to", "the", "value", "that", "func", "lightning", "fabric", "utilities", "seed", "seed_everything", "previously", "set", "if", "func", "lightning", "fabric", "utilities", "seed", "seed_everything", "is", "unused", "this", "function", "will", "do", "nothing"], "docstring_summary": "r\"\"\"Reset the seed to the value that :func:`~lightning.fabric.utilities.seed.seed_everything` previously set.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\seed.py", "partition": "train", "function_type": "function", "start_line": 71, "end_line": 81, "hash": "c91900fd95a6a15b8ff703130ca2196b", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "pl_worker_init_function", "original_string": "def pl_worker_init_function(worker_id: int, rank: Optional[int] = None) -> None:  # pragma: no cover\r\n    r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with\r\n    ``seed_everything(seed, workers=True)``.\r\n\r\n    See also the PyTorch documentation on\r\n    `randomness in DataLoaders <https://pytorch.org/docs/stable/notes/randomness.html#dataloader>`_.\r\n\r\n    \"\"\"\r\n    # implementation notes: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817392562\r\n    global_rank = rank if rank is not None else rank_zero_only.rank\r\n    process_seed = torch.initial_seed()\r\n    # back out the base seed so we can use all the bits\r\n    base_seed = process_seed - worker_id\r\n    log.debug(\r\n        f\"Initializing random number generators of process {global_rank} worker {worker_id} with base seed {base_seed}\"\r\n    )\r\n    seed_sequence = _generate_seed_sequence(base_seed, worker_id, global_rank, count=4)\r\n    torch.manual_seed(seed_sequence[0])  # torch takes a 64-bit seed\r\n    random.seed((seed_sequence[1] << 32) | seed_sequence[2])  # combine two 64-bit seeds\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        ss = np.random.SeedSequence([base_seed, worker_id, global_rank])\r\n        np_rng_seed = ss.generate_state(4)\r\n\r\n        np.random.seed(np_rng_seed)", "language": "python", "code": "def pl_worker_init_function(worker_id: int, rank: Optional[int] = None) -> None:  # pragma: no cover\r\n    r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with\r\n    ``seed_everything(seed, workers=True)``.\r\n\r\n    See also the PyTorch documentation on\r\n    `randomness in DataLoaders <https://pytorch.org/docs/stable/notes/randomness.html#dataloader>`_.\r\n\r\n    \"\"\"\r\n    # implementation notes: https://github.com/pytorch/pytorch/issues/5059#issuecomment-817392562\r\n    global_rank = rank if rank is not None else rank_zero_only.rank\r\n    process_seed = torch.initial_seed()\r\n    # back out the base seed so we can use all the bits\r\n    base_seed = process_seed - worker_id\r\n    log.debug(\r\n        f\"Initializing random number generators of process {global_rank} worker {worker_id} with base seed {base_seed}\"\r\n    )\r\n    seed_sequence = _generate_seed_sequence(base_seed, worker_id, global_rank, count=4)\r\n    torch.manual_seed(seed_sequence[0])  # torch takes a 64-bit seed\r\n    random.seed((seed_sequence[1] << 32) | seed_sequence[2])  # combine two 64-bit seeds\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        ss = np.random.SeedSequence([base_seed, worker_id, global_rank])\r\n        np_rng_seed = ss.generate_state(4)\r\n\r\n        np.random.seed(np_rng_seed)", "code_tokens": ["def", "pl_worker_init_function", "(", "worker_id", ":", "int", ",", "rank", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "The", "worker_init_fn", "that", "Lightning", "automatically", "adds", "to", "your", "dataloader", "if", "you", "previously", "set", "the", "seed", "with", "`", "`", "seed_everything", "(", "seed", ",", "workers", "=", "True", ")", "`", "`", ".", "See", "also", "the", "PyTorch", "documentation", "on", "`", "randomness", "in", "DataLoaders", "<", "https", ":", "/", "/", "pytorch", ".", "org", "/", "docs", "/", "stable", "/", "notes", "/", "randomness", ".", "html", "\"", "\"", "\"", "global_rank", "=", "rank", "if", "rank", "is", "not", "None", "else", "rank_zero_only", ".", "rank", "process_seed", "=", "torch", ".", "initial_seed", "(", ")", "base_seed", "=", "process_seed", "-", "worker_id", "log", ".", "debug", "(", "f", "\"", "Initializing", "random", "number", "generators", "of", "process", "{", "global_rank", "}", "worker", "{", "worker_id", "}", "with", "base", "seed", "{", "base_seed", "}", "\"", ")", "seed_sequence", "=", "_generate_seed_sequence", "(", "base_seed", ",", "worker_id", ",", "global_rank", ",", "count", "=", "4", ")", "torch", ".", "manual_seed", "(", "seed_sequence", "[", "0", "]", ")", "random", ".", "seed", "(", "(", "seed_sequence", "[", "1", "]", "<", "<", "32", ")", "|", "seed_sequence", "[", "2", "]", ")", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "ss", "=", "np", ".", "random", ".", "SeedSequence", "(", "[", "base_seed", ",", "worker_id", ",", "global_rank", "]", ")", "np_rng_seed", "=", "ss", ".", "generate_state", "(", "4", ")", "np", ".", "random", ".", "seed", "(", "np_rng_seed", ")"], "docstring": "r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with\r\n    ``seed_everything(seed, workers=True)``.\r\n\r\n    See also the PyTorch documentation on\r\n    `randomness in DataLoaders <https://pytorch.org/docs/stable/notes/randomness.html#dataloader>`_.\r\n\r\n    \"\"\"", "docstring_tokens": ["r", "the", "worker_init_fn", "that", "lightning", "automatically", "adds", "to", "your", "dataloader", "if", "you", "previously", "set", "the", "seed", "with", "seed_everything", "seed", "workers", "true", "see", "also", "the", "pytorch", "documentation", "on", "randomness", "in", "dataloaders", "https", "pytorch", "org", "docs", "stable", "notes", "randomness", "html", "dataloader", "_"], "docstring_summary": "r\"\"\"The worker_init_fn that Lightning automatically adds to your dataloader if you previously set the seed with", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\seed.py", "partition": "train", "function_type": "function", "start_line": 84, "end_line": 109, "hash": "e47c0d1390d3a2055bb870f72f5da7f7", "complexity": 3, "parameters": ["worker_id", "rank"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "_generate_seed_sequence", "original_string": "def _generate_seed_sequence(base_seed: int, worker_id: int, global_rank: int, count: int) -> list[int]:\r\n    \"\"\"Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)\r\n    algorithm.\"\"\"\r\n    # Combine base seed, worker id and rank into a unique 64-bit number\r\n    combined_seed = (base_seed << 32) | (worker_id << 16) | global_rank\r\n    seeds = []\r\n    for _ in range(count):\r\n        # x_(n+1) = (a * x_n + c) mod m. With c=1, m=2^64 and a is D. Knuth's constant\r\n        combined_seed = (combined_seed * 6364136223846793005 + 1) & ((1 << 64) - 1)\r\n        seeds.append(combined_seed)\r\n    return seeds", "language": "python", "code": "def _generate_seed_sequence(base_seed: int, worker_id: int, global_rank: int, count: int) -> list[int]:\r\n    \"\"\"Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)\r\n    algorithm.\"\"\"\r\n    # Combine base seed, worker id and rank into a unique 64-bit number\r\n    combined_seed = (base_seed << 32) | (worker_id << 16) | global_rank\r\n    seeds = []\r\n    for _ in range(count):\r\n        # x_(n+1) = (a * x_n + c) mod m. With c=1, m=2^64 and a is D. Knuth's constant\r\n        combined_seed = (combined_seed * 6364136223846793005 + 1) & ((1 << 64) - 1)\r\n        seeds.append(combined_seed)\r\n    return seeds", "code_tokens": ["def", "_generate_seed_sequence", "(", "base_seed", ":", "int", ",", "worker_id", ":", "int", ",", "global_rank", ":", "int", ",", "count", ":", "int", ")", "-", ">", "list", "[", "int", "]", ":", "\"", "\"", "\"", "Generates", "a", "sequence", "of", "seeds", "from", "a", "base", "seed", ",", "worker", "id", "and", "rank", "using", "the", "linear", "congruential", "generator", "(", "LCG", ")", "algorithm", ".", "\"", "\"", "\"", "combined_seed", "=", "(", "base_seed", "<", "<", "32", ")", "|", "(", "worker_id", "<", "<", "16", ")", "|", "global_rank", "seeds", "=", "[", "]", "for", "_", "in", "range", "(", "count", ")", ":", "combined_seed", "=", "(", "combined_seed", "*", "6364136223846793005", "+", "1", ")", "&", "(", "(", "1", "<", "<", "64", ")", "-", "1", ")", "seeds", ".", "append", "(", "combined_seed", ")", "return", "seeds"], "docstring": "Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)\r\n    algorithm.", "docstring_tokens": ["generates", "a", "sequence", "of", "seeds", "from", "a", "base", "seed", "worker", "id", "and", "rank", "using", "the", "linear", "congruential", "generator", "lcg", "algorithm"], "docstring_summary": "Generates a sequence of seeds from a base seed, worker id and rank using the linear congruential generator (LCG)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\seed.py", "partition": "train", "function_type": "function", "start_line": 112, "end_line": 122, "hash": "5311c6b5c969d16958d40d4fbf014a5f", "complexity": 2, "parameters": ["base_seed", "worker_id", "global_rank", "count"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "_collect_rng_states", "original_string": "def _collect_rng_states(include_cuda: bool = True) -> dict[str, Any]:\r\n    r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"\r\n    states = {\r\n        \"torch\": torch.get_rng_state(),\r\n        \"python\": python_get_rng_state(),\r\n    }\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        states[\"numpy\"] = np.random.get_state()\r\n    if include_cuda:\r\n        states[\"torch.cuda\"] = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else []\r\n    return states", "language": "python", "code": "def _collect_rng_states(include_cuda: bool = True) -> dict[str, Any]:\r\n    r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"\r\n    states = {\r\n        \"torch\": torch.get_rng_state(),\r\n        \"python\": python_get_rng_state(),\r\n    }\r\n    if _NUMPY_AVAILABLE:\r\n        import numpy as np\r\n\r\n        states[\"numpy\"] = np.random.get_state()\r\n    if include_cuda:\r\n        states[\"torch.cuda\"] = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else []\r\n    return states", "code_tokens": ["def", "_collect_rng_states", "(", "include_cuda", ":", "bool", "=", "True", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "r", "\"", "\"", "\"", "Collect", "the", "global", "random", "state", "of", ":", "mod", ":", "`", "torch", "`", ",", ":", "mod", ":", "`", "torch", ".", "cuda", "`", ",", ":", "mod", ":", "`", "numpy", "`", "and", "Python", ".", "\"", "\"", "\"", "states", "=", "{", "\"", "torch", "\"", ":", "torch", ".", "get_rng_state", "(", ")", ",", "\"", "python", "\"", ":", "python_get_rng_state", "(", ")", ",", "}", "if", "_NUMPY_AVAILABLE", ":", "import", "numpy", "as", "np", "states", "[", "\"", "numpy", "\"", "]", "=", "np", ".", "random", ".", "get_state", "(", ")", "if", "include_cuda", ":", "states", "[", "\"", "torch", ".", "cuda", "\"", "]", "=", "torch", ".", "cuda", ".", "get_rng_state_all", "(", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", "return", "states"], "docstring": "r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"", "docstring_tokens": ["r", "collect", "the", "global", "random", "state", "of", "mod", "torch", "mod", "torch", "cuda", "mod", "numpy", "and", "python"], "docstring_summary": "r\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\seed.py", "partition": "train", "function_type": "function", "start_line": 125, "end_line": 137, "hash": "245fcdf457714b2f7287ae3590d41384", "complexity": 4, "parameters": ["include_cuda"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\seed.py", "func_name": "_set_rng_states", "original_string": "def _set_rng_states(rng_state_dict: dict[str, Any]) -> None:\r\n    r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current\r\n    process.\"\"\"\r\n    torch.set_rng_state(rng_state_dict[\"torch\"])\r\n    # torch.cuda rng_state is only included since v1.8.\r\n    if \"torch.cuda\" in rng_state_dict:\r\n        torch.cuda.set_rng_state_all(rng_state_dict[\"torch.cuda\"])\r\n    if _NUMPY_AVAILABLE and \"numpy\" in rng_state_dict:\r\n        import numpy as np\r\n\r\n        np.random.set_state(rng_state_dict[\"numpy\"])\r\n    version, state, gauss = rng_state_dict[\"python\"]\r\n    python_set_rng_state((version, tuple(state), gauss))", "language": "python", "code": "def _set_rng_states(rng_state_dict: dict[str, Any]) -> None:\r\n    r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current\r\n    process.\"\"\"\r\n    torch.set_rng_state(rng_state_dict[\"torch\"])\r\n    # torch.cuda rng_state is only included since v1.8.\r\n    if \"torch.cuda\" in rng_state_dict:\r\n        torch.cuda.set_rng_state_all(rng_state_dict[\"torch.cuda\"])\r\n    if _NUMPY_AVAILABLE and \"numpy\" in rng_state_dict:\r\n        import numpy as np\r\n\r\n        np.random.set_state(rng_state_dict[\"numpy\"])\r\n    version, state, gauss = rng_state_dict[\"python\"]\r\n    python_set_rng_state((version, tuple(state), gauss))", "code_tokens": ["def", "_set_rng_states", "(", "rng_state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Set", "the", "global", "random", "state", "of", ":", "mod", ":", "`", "torch", "`", ",", ":", "mod", ":", "`", "torch", ".", "cuda", "`", ",", ":", "mod", ":", "`", "numpy", "`", "and", "Python", "in", "the", "current", "process", ".", "\"", "\"", "\"", "torch", ".", "set_rng_state", "(", "rng_state_dict", "[", "\"", "torch", "\"", "]", ")", "if", "\"", "torch", ".", "cuda", "\"", "in", "rng_state_dict", ":", "torch", ".", "cuda", ".", "set_rng_state_all", "(", "rng_state_dict", "[", "\"", "torch", ".", "cuda", "\"", "]", ")", "if", "_NUMPY_AVAILABLE", "and", "\"", "numpy", "\"", "in", "rng_state_dict", ":", "import", "numpy", "as", "np", "np", ".", "random", ".", "set_state", "(", "rng_state_dict", "[", "\"", "numpy", "\"", "]", ")", "version", ",", "state", ",", "gauss", "=", "rng_state_dict", "[", "\"", "python", "\"", "]", "python_set_rng_state", "(", "(", "version", ",", "tuple", "(", "state", ")", ",", "gauss", ")", ")"], "docstring": "r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current\r\n    process.\"\"\"", "docstring_tokens": ["r", "set", "the", "global", "random", "state", "of", "mod", "torch", "mod", "torch", "cuda", "mod", "numpy", "and", "python", "in", "the", "current", "process"], "docstring_summary": "r\"\"\"Set the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python in the current", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\seed.py", "partition": "train", "function_type": "function", "start_line": 140, "end_line": 152, "hash": "a75f8e34554d998f1a7c904250fc0d5f", "complexity": 4, "parameters": ["rng_state_dict", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\spike.py", "func_name": "on_train_batch_end", "original_string": "def on_train_batch_end(self, fabric: \"Fabric\", loss: torch.Tensor, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Checks if we currently have a loss-spike.\"\"\"\r\n        if batch_idx == 0:\r\n            self.running_mean.to(fabric.strategy.root_device)\r\n\r\n        if self.exclude_batches_path is None:\r\n            self.exclude_batches_path = os.getcwd()\r\n\r\n        if not str(self.exclude_batches_path).endswith(\".json\"):\r\n            self.exclude_batches_path = os.path.join(self.exclude_batches_path, \"skip_batches.json\")\r\n\r\n        is_spike = bool(batch_idx >= self.warmup and self._is_spike(loss))\r\n        fabric.strategy.barrier()\r\n\r\n        # While spike-detection happens on a per-rank level, we need to fail all ranks if any rank detected a spike\r\n        is_spike_global = fabric.strategy.reduce_boolean_decision(is_spike, all=False)\r\n\r\n        if is_spike_global:\r\n            self._handle_spike(fabric, batch_idx)\r\n        else:\r\n            is_finite_all = self.finite_only or fabric.strategy.reduce_boolean_decision(\r\n                bool(torch.isfinite(loss).all()), all=True\r\n            )\r\n            if is_finite_all:\r\n                self._update_stats(loss)", "language": "python", "code": "def on_train_batch_end(self, fabric: \"Fabric\", loss: torch.Tensor, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Checks if we currently have a loss-spike.\"\"\"\r\n        if batch_idx == 0:\r\n            self.running_mean.to(fabric.strategy.root_device)\r\n\r\n        if self.exclude_batches_path is None:\r\n            self.exclude_batches_path = os.getcwd()\r\n\r\n        if not str(self.exclude_batches_path).endswith(\".json\"):\r\n            self.exclude_batches_path = os.path.join(self.exclude_batches_path, \"skip_batches.json\")\r\n\r\n        is_spike = bool(batch_idx >= self.warmup and self._is_spike(loss))\r\n        fabric.strategy.barrier()\r\n\r\n        # While spike-detection happens on a per-rank level, we need to fail all ranks if any rank detected a spike\r\n        is_spike_global = fabric.strategy.reduce_boolean_decision(is_spike, all=False)\r\n\r\n        if is_spike_global:\r\n            self._handle_spike(fabric, batch_idx)\r\n        else:\r\n            is_finite_all = self.finite_only or fabric.strategy.reduce_boolean_decision(\r\n                bool(torch.isfinite(loss).all()), all=True\r\n            )\r\n            if is_finite_all:\r\n                self._update_stats(loss)", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "fabric", ":", "\"", "Fabric", "\"", ",", "loss", ":", "torch", ".", "Tensor", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "if", "we", "currently", "have", "a", "loss", "-", "spike", ".", "\"", "\"", "\"", "if", "batch_idx", "=", "=", "0", ":", "self", ".", "running_mean", ".", "to", "(", "fabric", ".", "strategy", ".", "root_device", ")", "if", "self", ".", "exclude_batches_path", "is", "None", ":", "self", ".", "exclude_batches_path", "=", "os", ".", "getcwd", "(", ")", "if", "not", "str", "(", "self", ".", "exclude_batches_path", ")", ".", "endswith", "(", "\"", ".", "json", "\"", ")", ":", "self", ".", "exclude_batches_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "exclude_batches_path", ",", "\"", "skip_batches", ".", "json", "\"", ")", "is_spike", "=", "bool", "(", "batch_idx", ">", "=", "self", ".", "warmup", "and", "self", ".", "_is_spike", "(", "loss", ")", ")", "fabric", ".", "strategy", ".", "barrier", "(", ")", "is_spike_global", "=", "fabric", ".", "strategy", ".", "reduce_boolean_decision", "(", "is_spike", ",", "all", "=", "False", ")", "if", "is_spike_global", ":", "self", ".", "_handle_spike", "(", "fabric", ",", "batch_idx", ")", "else", ":", "is_finite_all", "=", "self", ".", "finite_only", "or", "fabric", ".", "strategy", ".", "reduce_boolean_decision", "(", "bool", "(", "torch", ".", "isfinite", "(", "loss", ")", ".", "all", "(", ")", ")", ",", "all", "=", "True", ")", "if", "is_finite_all", ":", "self", ".", "_update_stats", "(", "loss", ")"], "docstring": "Checks if we currently have a loss-spike.", "docstring_tokens": ["checks", "if", "we", "currently", "have", "a", "loss", "spike"], "docstring_summary": "Checks if we currently have a loss-spike.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\spike.py", "partition": "train", "function_type": "class_method", "class_name": "SpikeDetection", "start_line": 70, "end_line": 94, "hash": "c4c56cec0f6fe387413e2062e5bd8647", "complexity": 8, "parameters": ["fabric", "loss", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "update", "original_string": "def update(\r\n        self,\r\n        *,\r\n        time: float,\r\n        batches: int,\r\n        samples: int,\r\n        lengths: Optional[int] = None,\r\n        flops: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Update throughput metrics.\r\n\r\n        Args:\r\n            time: Total elapsed time in seconds. It should monotonically increase by the iteration time with each\r\n                call.\r\n            batches: Total batches seen per device. It should monotonically increase with each call.\r\n            samples: Total samples seen per device. It should monotonically increase by the batch size with each call.\r\n            lengths: Total length of the samples seen. It should monotonically increase by the lengths of a batch with\r\n                each call.\r\n            flops: Flops elapased per device since last ``update()`` call. You can easily compute this by using\r\n                :func:`measure_flops` and multiplying it by the number of batches that have been processed.\r\n                The value might be different in each device if the batch size is not the same.\r\n\r\n        \"\"\"\r\n        self._time.append(time)\r\n        if samples < batches:\r\n            raise ValueError(f\"Expected samples ({samples}) to be greater or equal than batches ({batches})\")\r\n        self._batches.append(batches)\r\n        self._samples.append(samples)\r\n        if lengths is not None:\r\n            if lengths < samples:\r\n                raise ValueError(f\"Expected lengths ({lengths}) to be greater or equal than samples ({samples})\")\r\n            self._lengths.append(lengths)\r\n            if len(self._samples) != len(self._lengths):\r\n                raise RuntimeError(\r\n                    f\"If lengths are passed ({len(self._lengths)}), there needs to be the same number of samples\"\r\n                    f\" ({len(self._samples)})\"\r\n                )\r\n        if flops is not None:\r\n            # sum of flops across ranks\r\n            self._flops.append(flops * self.world_size)", "language": "python", "code": "def update(\r\n        self,\r\n        *,\r\n        time: float,\r\n        batches: int,\r\n        samples: int,\r\n        lengths: Optional[int] = None,\r\n        flops: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Update throughput metrics.\r\n\r\n        Args:\r\n            time: Total elapsed time in seconds. It should monotonically increase by the iteration time with each\r\n                call.\r\n            batches: Total batches seen per device. It should monotonically increase with each call.\r\n            samples: Total samples seen per device. It should monotonically increase by the batch size with each call.\r\n            lengths: Total length of the samples seen. It should monotonically increase by the lengths of a batch with\r\n                each call.\r\n            flops: Flops elapased per device since last ``update()`` call. You can easily compute this by using\r\n                :func:`measure_flops` and multiplying it by the number of batches that have been processed.\r\n                The value might be different in each device if the batch size is not the same.\r\n\r\n        \"\"\"\r\n        self._time.append(time)\r\n        if samples < batches:\r\n            raise ValueError(f\"Expected samples ({samples}) to be greater or equal than batches ({batches})\")\r\n        self._batches.append(batches)\r\n        self._samples.append(samples)\r\n        if lengths is not None:\r\n            if lengths < samples:\r\n                raise ValueError(f\"Expected lengths ({lengths}) to be greater or equal than samples ({samples})\")\r\n            self._lengths.append(lengths)\r\n            if len(self._samples) != len(self._lengths):\r\n                raise RuntimeError(\r\n                    f\"If lengths are passed ({len(self._lengths)}), there needs to be the same number of samples\"\r\n                    f\" ({len(self._samples)})\"\r\n                )\r\n        if flops is not None:\r\n            # sum of flops across ranks\r\n            self._flops.append(flops * self.world_size)", "code_tokens": ["def", "update", "(", "self", ",", "*", ",", "time", ":", "float", ",", "batches", ":", "int", ",", "samples", ":", "int", ",", "lengths", ":", "Optional", "[", "int", "]", "=", "None", ",", "flops", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Update", "throughput", "metrics", ".", "Args", ":", "time", ":", "Total", "elapsed", "time", "in", "seconds", ".", "It", "should", "monotonically", "increase", "by", "the", "iteration", "time", "with", "each", "call", ".", "batches", ":", "Total", "batches", "seen", "per", "device", ".", "It", "should", "monotonically", "increase", "with", "each", "call", ".", "samples", ":", "Total", "samples", "seen", "per", "device", ".", "It", "should", "monotonically", "increase", "by", "the", "batch", "size", "with", "each", "call", ".", "lengths", ":", "Total", "length", "of", "the", "samples", "seen", ".", "It", "should", "monotonically", "increase", "by", "the", "lengths", "of", "a", "batch", "with", "each", "call", ".", "flops", ":", "Flops", "elapased", "per", "device", "since", "last", "`", "`", "update", "(", ")", "`", "`", "call", ".", "You", "can", "easily", "compute", "this", "by", "using", ":", "func", ":", "`", "measure_flops", "`", "and", "multiplying", "it", "by", "the", "number", "of", "batches", "that", "have", "been", "processed", ".", "The", "value", "might", "be", "different", "in", "each", "device", "if", "the", "batch", "size", "is", "not", "the", "same", ".", "\"", "\"", "\"", "self", ".", "_time", ".", "append", "(", "time", ")", "if", "samples", "<", "batches", ":", "raise", "ValueError", "(", "f", "\"", "Expected", "samples", "(", "{", "samples", "}", ")", "to", "be", "greater", "or", "equal", "than", "batches", "(", "{", "batches", "}", ")", "\"", ")", "self", ".", "_batches", ".", "append", "(", "batches", ")", "self", ".", "_samples", ".", "append", "(", "samples", ")", "if", "lengths", "is", "not", "None", ":", "if", "lengths", "<", "samples", ":", "raise", "ValueError", "(", "f", "\"", "Expected", "lengths", "(", "{", "lengths", "}", ")", "to", "be", "greater", "or", "equal", "than", "samples", "(", "{", "samples", "}", ")", "\"", ")", "self", ".", "_lengths", ".", "append", "(", "lengths", ")", "if", "len", "(", "self", ".", "_samples", ")", "!", "=", "len", "(", "self", ".", "_lengths", ")", ":", "raise", "RuntimeError", "(", "f", "\"", "If", "lengths", "are", "passed", "(", "{", "len", "(", "self", ".", "_lengths", ")", "}", ")", ",", "there", "needs", "to", "be", "the", "same", "number", "of", "samples", "\"", "f", "\"", "(", "{", "len", "(", "self", ".", "_samples", ")", "}", ")", "\"", ")", "if", "flops", "is", "not", "None", ":", "self", ".", "_flops", ".", "append", "(", "flops", "*", "self", ".", "world_size", ")"], "docstring": "Update throughput metrics.\r\n\r\n        Args:\r\n            time: Total elapsed time in seconds. It should monotonically increase by the iteration time with each\r\n                call.\r\n            batches: Total batches seen per device. It should monotonically increase with each call.\r\n            samples: Total samples seen per device. It should monotonically increase by the batch size with each call.\r\n            lengths: Total length of the samples seen. It should monotonically increase by the lengths of a batch with\r\n                each call.\r\n            flops: Flops elapased per device since last ``update()`` call. You can easily compute this by using\r\n                :func:`measure_flops` and multiplying it by the number of batches that have been processed.\r\n                The value might be different in each device if the batch size is not the same.", "docstring_tokens": ["update", "throughput", "metrics", "args", "time", "total", "elapsed", "time", "in", "seconds", "it", "should", "monotonically", "increase", "by", "the", "iteration", "time", "with", "each", "call", "batches", "total", "batches", "seen", "per", "device", "it", "should", "monotonically", "increase", "with", "each", "call", "samples", "total", "samples", "seen", "per", "device", "it", "should", "monotonically", "increase", "by", "the", "batch", "size", "with", "each", "call", "lengths", "total", "length", "of", "the", "samples", "seen", "it", "should", "monotonically", "increase", "by", "the", "lengths", "of", "a", "batch", "with", "each", "call", "flops", "flops", "elapased", "per", "device", "since", "last", "update", "call", "you", "can", "easily", "compute", "this", "by", "using", "func", "measure_flops", "and", "multiplying", "it", "by", "the", "number", "of", "batches", "that", "have", "been", "processed", "the", "value", "might", "be", "different", "in", "each", "device", "if", "the", "batch", "size", "is", "not", "the", "same"], "docstring_summary": "Update throughput metrics.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\throughput.py", "partition": "train", "function_type": "class_method", "class_name": "Throughput", "start_line": 112, "end_line": 151, "hash": "42e2411b0609ea972cc40fabe762a42a", "complexity": 6, "parameters": ["*", "time", "batches", "samples", "lengths", "flops"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "compute", "original_string": "def compute(self) -> _THROUGHPUT_METRICS:\r\n        \"\"\"Compute throughput metrics.\"\"\"\r\n        metrics = {\r\n            \"time\": self._time[-1],\r\n            \"batches\": self._batches[-1],\r\n            \"samples\": self._samples[-1],\r\n        }\r\n        if self._lengths:\r\n            metrics[\"lengths\"] = self._lengths[-1]\r\n\r\n        add_global_metrics = self.world_size > 1\r\n        # a different but valid design choice would be to still compute all these metrics even if the window of values\r\n        # has not been filled\r\n        if len(self._time) == self._time.maxlen:\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            elapsed_batches = self._batches[-1] - self._batches[0]\r\n            elapsed_samples = self._samples[-1] - self._samples[0]\r\n            # we are safe from ZeroDivisionError thanks to `_MonotonicWindow`\r\n            dev_samples_per_sec = elapsed_samples / elapsed_time\r\n            dev_batches_per_sec = elapsed_batches / elapsed_time\r\n            metrics.update({\r\n                f\"device{self.separator}batches_per_sec\": elapsed_batches / elapsed_time,\r\n                f\"device{self.separator}samples_per_sec\": dev_samples_per_sec,\r\n            })\r\n            if add_global_metrics:\r\n                samples_per_sec = dev_batches_per_sec * self.world_size\r\n                metrics.update({\r\n                    \"batches_per_sec\": samples_per_sec,\r\n                    \"samples_per_sec\": dev_samples_per_sec * self.world_size,\r\n                })\r\n\r\n            if len(self._lengths) == self._lengths.maxlen:\r\n                elapsed_lengths = self._lengths[-1] - self._lengths[0]\r\n                dev_items_per_sec = elapsed_lengths / elapsed_time\r\n                metrics[f\"device{self.separator}items_per_sec\"] = dev_items_per_sec\r\n                if add_global_metrics:\r\n                    items_per_sec = dev_items_per_sec * self.world_size\r\n                    metrics[\"items_per_sec\"] = items_per_sec\r\n\r\n        if len(self._flops) == self._flops.maxlen:\r\n            elapsed_flops = sum(self._flops) - self._flops[0]\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            flops_per_sec = elapsed_flops / elapsed_time\r\n            dev_flops_per_sec = flops_per_sec / self.world_size\r\n            if add_global_metrics:\r\n                metrics[\"flops_per_sec\"] = flops_per_sec\r\n            metrics[f\"device{self.separator}flops_per_sec\"] = dev_flops_per_sec\r\n            if self.available_flops:\r\n                metrics[f\"device{self.separator}mfu\"] = dev_flops_per_sec / self.available_flops\r\n\r\n        return metrics", "language": "python", "code": "def compute(self) -> _THROUGHPUT_METRICS:\r\n        \"\"\"Compute throughput metrics.\"\"\"\r\n        metrics = {\r\n            \"time\": self._time[-1],\r\n            \"batches\": self._batches[-1],\r\n            \"samples\": self._samples[-1],\r\n        }\r\n        if self._lengths:\r\n            metrics[\"lengths\"] = self._lengths[-1]\r\n\r\n        add_global_metrics = self.world_size > 1\r\n        # a different but valid design choice would be to still compute all these metrics even if the window of values\r\n        # has not been filled\r\n        if len(self._time) == self._time.maxlen:\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            elapsed_batches = self._batches[-1] - self._batches[0]\r\n            elapsed_samples = self._samples[-1] - self._samples[0]\r\n            # we are safe from ZeroDivisionError thanks to `_MonotonicWindow`\r\n            dev_samples_per_sec = elapsed_samples / elapsed_time\r\n            dev_batches_per_sec = elapsed_batches / elapsed_time\r\n            metrics.update({\r\n                f\"device{self.separator}batches_per_sec\": elapsed_batches / elapsed_time,\r\n                f\"device{self.separator}samples_per_sec\": dev_samples_per_sec,\r\n            })\r\n            if add_global_metrics:\r\n                samples_per_sec = dev_batches_per_sec * self.world_size\r\n                metrics.update({\r\n                    \"batches_per_sec\": samples_per_sec,\r\n                    \"samples_per_sec\": dev_samples_per_sec * self.world_size,\r\n                })\r\n\r\n            if len(self._lengths) == self._lengths.maxlen:\r\n                elapsed_lengths = self._lengths[-1] - self._lengths[0]\r\n                dev_items_per_sec = elapsed_lengths / elapsed_time\r\n                metrics[f\"device{self.separator}items_per_sec\"] = dev_items_per_sec\r\n                if add_global_metrics:\r\n                    items_per_sec = dev_items_per_sec * self.world_size\r\n                    metrics[\"items_per_sec\"] = items_per_sec\r\n\r\n        if len(self._flops) == self._flops.maxlen:\r\n            elapsed_flops = sum(self._flops) - self._flops[0]\r\n            elapsed_time = self._time[-1] - self._time[0]\r\n            flops_per_sec = elapsed_flops / elapsed_time\r\n            dev_flops_per_sec = flops_per_sec / self.world_size\r\n            if add_global_metrics:\r\n                metrics[\"flops_per_sec\"] = flops_per_sec\r\n            metrics[f\"device{self.separator}flops_per_sec\"] = dev_flops_per_sec\r\n            if self.available_flops:\r\n                metrics[f\"device{self.separator}mfu\"] = dev_flops_per_sec / self.available_flops\r\n\r\n        return metrics", "code_tokens": ["def", "compute", "(", "self", ")", "-", ">", "_THROUGHPUT_METRICS", ":", "\"", "\"", "\"", "Compute", "throughput", "metrics", ".", "\"", "\"", "\"", "metrics", "=", "{", "\"", "time", "\"", ":", "self", ".", "_time", "[", "-", "1", "]", ",", "\"", "batches", "\"", ":", "self", ".", "_batches", "[", "-", "1", "]", ",", "\"", "samples", "\"", ":", "self", ".", "_samples", "[", "-", "1", "]", ",", "}", "if", "self", ".", "_lengths", ":", "metrics", "[", "\"", "lengths", "\"", "]", "=", "self", ".", "_lengths", "[", "-", "1", "]", "add_global_metrics", "=", "self", ".", "world_size", ">", "1", "if", "len", "(", "self", ".", "_time", ")", "=", "=", "self", ".", "_time", ".", "maxlen", ":", "elapsed_time", "=", "self", ".", "_time", "[", "-", "1", "]", "-", "self", ".", "_time", "[", "0", "]", "elapsed_batches", "=", "self", ".", "_batches", "[", "-", "1", "]", "-", "self", ".", "_batches", "[", "0", "]", "elapsed_samples", "=", "self", ".", "_samples", "[", "-", "1", "]", "-", "self", ".", "_samples", "[", "0", "]", "dev_samples_per_sec", "=", "elapsed_samples", "/", "elapsed_time", "dev_batches_per_sec", "=", "elapsed_batches", "/", "elapsed_time", "metrics", ".", "update", "(", "{", "f", "\"", "device", "{", "self", ".", "separator", "}", "batches_per_sec", "\"", ":", "elapsed_batches", "/", "elapsed_time", ",", "f", "\"", "device", "{", "self", ".", "separator", "}", "samples_per_sec", "\"", ":", "dev_samples_per_sec", ",", "}", ")", "if", "add_global_metrics", ":", "samples_per_sec", "=", "dev_batches_per_sec", "*", "self", ".", "world_size", "metrics", ".", "update", "(", "{", "\"", "batches_per_sec", "\"", ":", "samples_per_sec", ",", "\"", "samples_per_sec", "\"", ":", "dev_samples_per_sec", "*", "self", ".", "world_size", ",", "}", ")", "if", "len", "(", "self", ".", "_lengths", ")", "=", "=", "self", ".", "_lengths", ".", "maxlen", ":", "elapsed_lengths", "=", "self", ".", "_lengths", "[", "-", "1", "]", "-", "self", ".", "_lengths", "[", "0", "]", "dev_items_per_sec", "=", "elapsed_lengths", "/", "elapsed_time", "metrics", "[", "f", "\"", "device", "{", "self", ".", "separator", "}", "items_per_sec", "\"", "]", "=", "dev_items_per_sec", "if", "add_global_metrics", ":", "items_per_sec", "=", "dev_items_per_sec", "*", "self", ".", "world_size", "metrics", "[", "\"", "items_per_sec", "\"", "]", "=", "items_per_sec", "if", "len", "(", "self", ".", "_flops", ")", "=", "=", "self", ".", "_flops", ".", "maxlen", ":", "elapsed_flops", "=", "sum", "(", "self", ".", "_flops", ")", "-", "self", ".", "_flops", "[", "0", "]", "elapsed_time", "=", "self", ".", "_time", "[", "-", "1", "]", "-", "self", ".", "_time", "[", "0", "]", "flops_per_sec", "=", "elapsed_flops", "/", "elapsed_time", "dev_flops_per_sec", "=", "flops_per_sec", "/", "self", ".", "world_size", "if", "add_global_metrics", ":", "metrics", "[", "\"", "flops_per_sec", "\"", "]", "=", "flops_per_sec", "metrics", "[", "f", "\"", "device", "{", "self", ".", "separator", "}", "flops_per_sec", "\"", "]", "=", "dev_flops_per_sec", "if", "self", ".", "available_flops", ":", "metrics", "[", "f", "\"", "device", "{", "self", ".", "separator", "}", "mfu", "\"", "]", "=", "dev_flops_per_sec", "/", "self", ".", "available_flops", "return", "metrics"], "docstring": "Compute throughput metrics.", "docstring_tokens": ["compute", "throughput", "metrics"], "docstring_summary": "Compute throughput metrics.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\throughput.py", "partition": "train", "function_type": "class_method", "class_name": "Throughput", "start_line": 153, "end_line": 203, "hash": "5847a64986519f2cd6ad42ce128610bf", "complexity": 9, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "compute_and_log", "original_string": "def compute_and_log(self, step: Optional[int] = None, **kwargs: Any) -> _THROUGHPUT_METRICS:\r\n        r\"\"\"See :meth:`Throughput.compute`\r\n\r\n        Args:\r\n            step: Can be used to override the logging step.\r\n            \\**kwargs: See available parameters in :meth:`Throughput.compute`\r\n\r\n        \"\"\"\r\n        self.step = (self.step + 1) if step is None else step\r\n        metrics = self.compute(**kwargs)\r\n        self._fabric.log_dict(metrics=metrics, step=self.step)\r\n        return metrics", "language": "python", "code": "def compute_and_log(self, step: Optional[int] = None, **kwargs: Any) -> _THROUGHPUT_METRICS:\r\n        r\"\"\"See :meth:`Throughput.compute`\r\n\r\n        Args:\r\n            step: Can be used to override the logging step.\r\n            \\**kwargs: See available parameters in :meth:`Throughput.compute`\r\n\r\n        \"\"\"\r\n        self.step = (self.step + 1) if step is None else step\r\n        metrics = self.compute(**kwargs)\r\n        self._fabric.log_dict(metrics=metrics, step=self.step)\r\n        return metrics", "code_tokens": ["def", "compute_and_log", "(", "self", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "_THROUGHPUT_METRICS", ":", "r", "\"", "\"", "\"", "See", ":", "meth", ":", "`", "Throughput", ".", "compute", "`", "Args", ":", "step", ":", "Can", "be", "used", "to", "override", "the", "logging", "step", ".", "\\", "*", "*", "kwargs", ":", "See", "available", "parameters", "in", ":", "meth", ":", "`", "Throughput", ".", "compute", "`", "\"", "\"", "\"", "self", ".", "step", "=", "(", "self", ".", "step", "+", "1", ")", "if", "step", "is", "None", "else", "step", "metrics", "=", "self", ".", "compute", "(", "*", "*", "kwargs", ")", "self", ".", "_fabric", ".", "log_dict", "(", "metrics", "=", "metrics", ",", "step", "=", "self", ".", "step", ")", "return", "metrics"], "docstring": "r\"\"\"See :meth:`Throughput.compute`\r\n\r\n        Args:\r\n            step: Can be used to override the logging step.\r\n            \\**kwargs: See available parameters in :meth:`Throughput.compute`\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "see", "meth", "throughput", "compute", "args", "step", "can", "be", "used", "to", "override", "the", "logging", "step", "kwargs", "see", "available", "parameters", "in", "meth", "throughput", "compute"], "docstring_summary": "r\"\"\"See :meth:`Throughput.compute`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\throughput.py", "partition": "train", "function_type": "class_method", "class_name": "ThroughputMonitor", "start_line": 251, "end_line": 262, "hash": "33e469dae3fad1f715c80aee66e18ada", "complexity": 2, "parameters": ["step", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\throughput.py", "func_name": "measure_flops", "original_string": "def measure_flops(\r\n    model: torch.nn.Module,\r\n    forward_fn: Callable[[], torch.Tensor],\r\n    loss_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\r\n) -> int:\r\n    \"\"\"Utility to compute the total number of FLOPs used by a module during training or during inference.\r\n\r\n    It's recommended to create a meta-device model for this:\r\n\r\n    Example::\r\n\r\n        with torch.device(\"meta\"):\r\n            model = MyModel()\r\n            x = torch.randn(2, 32)\r\n\r\n        model_fwd = lambda: model(x)\r\n        fwd_flops = measure_flops(model, model_fwd)\r\n\r\n        model_loss = lambda y: y.sum()\r\n        fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n\r\n    Args:\r\n        model: The model whose FLOPs should be measured.\r\n        forward_fn: A function that runs ``forward`` on the model and returns the result.\r\n        loss_fn: A function that computes the loss given the ``forward_fn`` output. If provided, the loss and `backward`\r\n            FLOPs will be included in the result.\r\n\r\n    \"\"\"\r\n    from torch.utils.flop_counter import FlopCounterMode\r\n\r\n    flop_counter = FlopCounterMode(display=False)\r\n    with flop_counter:\r\n        if loss_fn is None:\r\n            forward_fn()\r\n        else:\r\n            loss_fn(forward_fn()).backward()\r\n    return flop_counter.get_total_flops()", "language": "python", "code": "def measure_flops(\r\n    model: torch.nn.Module,\r\n    forward_fn: Callable[[], torch.Tensor],\r\n    loss_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\r\n) -> int:\r\n    \"\"\"Utility to compute the total number of FLOPs used by a module during training or during inference.\r\n\r\n    It's recommended to create a meta-device model for this:\r\n\r\n    Example::\r\n\r\n        with torch.device(\"meta\"):\r\n            model = MyModel()\r\n            x = torch.randn(2, 32)\r\n\r\n        model_fwd = lambda: model(x)\r\n        fwd_flops = measure_flops(model, model_fwd)\r\n\r\n        model_loss = lambda y: y.sum()\r\n        fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n\r\n    Args:\r\n        model: The model whose FLOPs should be measured.\r\n        forward_fn: A function that runs ``forward`` on the model and returns the result.\r\n        loss_fn: A function that computes the loss given the ``forward_fn`` output. If provided, the loss and `backward`\r\n            FLOPs will be included in the result.\r\n\r\n    \"\"\"\r\n    from torch.utils.flop_counter import FlopCounterMode\r\n\r\n    flop_counter = FlopCounterMode(display=False)\r\n    with flop_counter:\r\n        if loss_fn is None:\r\n            forward_fn()\r\n        else:\r\n            loss_fn(forward_fn()).backward()\r\n    return flop_counter.get_total_flops()", "code_tokens": ["def", "measure_flops", "(", "model", ":", "torch", ".", "nn", ".", "Module", ",", "forward_fn", ":", "Callable", "[", "[", "]", ",", "torch", ".", "Tensor", "]", ",", "loss_fn", ":", "Optional", "[", "Callable", "[", "[", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Utility", "to", "compute", "the", "total", "number", "of", "FLOPs", "used", "by", "a", "module", "during", "training", "or", "during", "inference", ".", "It", "'", "s", "recommended", "to", "create", "a", "meta", "-", "device", "model", "for", "this", ":", "Example", ":", ":", "with", "torch", ".", "device", "(", "\"", "meta", "\"", ")", ":", "model", "=", "MyModel", "(", ")", "x", "=", "torch", ".", "randn", "(", "2", ",", "32", ")", "model_fwd", "=", "lambda", ":", "model", "(", "x", ")", "fwd_flops", "=", "measure_flops", "(", "model", ",", "model_fwd", ")", "model_loss", "=", "lambda", "y", ":", "y", ".", "sum", "(", ")", "fwd_and_bwd_flops", "=", "measure_flops", "(", "model", ",", "model_fwd", ",", "model_loss", ")", "Args", ":", "model", ":", "The", "model", "whose", "FLOPs", "should", "be", "measured", ".", "forward_fn", ":", "A", "function", "that", "runs", "`", "`", "forward", "`", "`", "on", "the", "model", "and", "returns", "the", "result", ".", "loss_fn", ":", "A", "function", "that", "computes", "the", "loss", "given", "the", "`", "`", "forward_fn", "`", "`", "output", ".", "If", "provided", ",", "the", "loss", "and", "`", "backward", "`", "FLOPs", "will", "be", "included", "in", "the", "result", ".", "\"", "\"", "\"", "from", "torch", ".", "utils", ".", "flop_counter", "import", "FlopCounterMode", "flop_counter", "=", "FlopCounterMode", "(", "display", "=", "False", ")", "with", "flop_counter", ":", "if", "loss_fn", "is", "None", ":", "forward_fn", "(", ")", "else", ":", "loss_fn", "(", "forward_fn", "(", ")", ")", ".", "backward", "(", ")", "return", "flop_counter", ".", "get_total_flops", "(", ")"], "docstring": "Utility to compute the total number of FLOPs used by a module during training or during inference.\r\n\r\n    It's recommended to create a meta-device model for this:\r\n\r\n    Example::\r\n\r\n        with torch.device(\"meta\"):\r\n            model = MyModel()\r\n            x = torch.randn(2, 32)\r\n\r\n        model_fwd = lambda: model(x)\r\n        fwd_flops = measure_flops(model, model_fwd)\r\n\r\n        model_loss = lambda y: y.sum()\r\n        fwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n\r\n    Args:\r\n        model: The model whose FLOPs should be measured.\r\n        forward_fn: A function that runs ``forward`` on the model and returns the result.\r\n        loss_fn: A function that computes the loss given the ``forward_fn`` output. If provided, the loss and `backward`\r\n            FLOPs will be included in the result.", "docstring_tokens": ["utility", "to", "compute", "the", "total", "number", "of", "flops", "used", "by", "a", "module", "during", "training", "or", "during", "inference", "it", "s", "recommended", "to", "create", "a", "meta", "device", "model", "for", "this", "example", "with", "torch", "device", "meta", "model", "mymodel", "x", "torch", "randn", "2", "32", "model_fwd", "lambda", "model", "x", "fwd_flops", "measure_flops", "model", "model_fwd", "model_loss", "lambda", "y", "y", "sum", "fwd_and_bwd_flops", "measure_flops", "model", "model_fwd", "model_loss", "args", "model", "the", "model", "whose", "flops", "should", "be", "measured", "forward_fn", "a", "function", "that", "runs", "forward", "on", "the", "model", "and", "returns", "the", "result", "loss_fn", "a", "function", "that", "computes", "the", "loss", "given", "the", "forward_fn", "output", "if", "provided", "the", "loss", "and", "backward", "flops", "will", "be", "included", "in", "the", "result"], "docstring_summary": "Utility to compute the total number of FLOPs used by a module during training or during inference.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\throughput.py", "partition": "train", "function_type": "function", "start_line": 265, "end_line": 301, "hash": "d9f041b268a8af458d8b6a14e51e3064", "complexity": 3, "parameters": ["model", "forward_fn", "torch.Tensor]", "loss_fn", "torch.Tensor]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\warnings.py", "func_name": "disable_possible_user_warnings", "original_string": "def disable_possible_user_warnings(module: str = \"\") -> None:\r\n    \"\"\"Ignore warnings of the category ``PossibleUserWarning`` from Lightning.\r\n\r\n    For more granular control over which warnings to ignore, use :func:`warnings.filterwarnings` directly.\r\n\r\n    Args:\r\n        module: Name of the module for which the warnings should be ignored (e.g., ``'lightning.pytorch.strategies'``).\r\n            Default: Disables warnings from all modules.\r\n\r\n    \"\"\"\r\n    warnings.filterwarnings(\"ignore\", module=module, category=PossibleUserWarning)", "language": "python", "code": "def disable_possible_user_warnings(module: str = \"\") -> None:\r\n    \"\"\"Ignore warnings of the category ``PossibleUserWarning`` from Lightning.\r\n\r\n    For more granular control over which warnings to ignore, use :func:`warnings.filterwarnings` directly.\r\n\r\n    Args:\r\n        module: Name of the module for which the warnings should be ignored (e.g., ``'lightning.pytorch.strategies'``).\r\n            Default: Disables warnings from all modules.\r\n\r\n    \"\"\"\r\n    warnings.filterwarnings(\"ignore\", module=module, category=PossibleUserWarning)", "code_tokens": ["def", "disable_possible_user_warnings", "(", "module", ":", "str", "=", "\"", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Ignore", "warnings", "of", "the", "category", "`", "`", "PossibleUserWarning", "`", "`", "from", "Lightning", ".", "For", "more", "granular", "control", "over", "which", "warnings", "to", "ignore", ",", "use", ":", "func", ":", "`", "warnings", ".", "filterwarnings", "`", "directly", ".", "Args", ":", "module", ":", "Name", "of", "the", "module", "for", "which", "the", "warnings", "should", "be", "ignored", "(", "e", ".", "g", ".", ",", "`", "`", "'", "lightning", ".", "pytorch", ".", "strategies", "'", "`", "`", ")", ".", "Default", ":", "Disables", "warnings", "from", "all", "modules", ".", "\"", "\"", "\"", "warnings", ".", "filterwarnings", "(", "\"", "ignore", "\"", ",", "module", "=", "module", ",", "category", "=", "PossibleUserWarning", ")"], "docstring": "Ignore warnings of the category ``PossibleUserWarning`` from Lightning.\r\n\r\n    For more granular control over which warnings to ignore, use :func:`warnings.filterwarnings` directly.\r\n\r\n    Args:\r\n        module: Name of the module for which the warnings should be ignored (e.g., ``'lightning.pytorch.strategies'``).\r\n            Default: Disables warnings from all modules.", "docstring_tokens": ["ignore", "warnings", "of", "the", "category", "possibleuserwarning", "from", "lightning", "for", "more", "granular", "control", "over", "which", "warnings", "to", "ignore", "use", "func", "warnings", "filterwarnings", "directly", "args", "module", "name", "of", "the", "module", "for", "which", "the", "warnings", "should", "be", "ignored", "e", "g", "lightning", "pytorch", "strategies", "default", "disables", "warnings", "from", "all", "modules"], "docstring_summary": "Ignore warnings of the category ``PossibleUserWarning`` from Lightning.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\warnings.py", "partition": "train", "function_type": "function", "start_line": 26, "end_line": 36, "hash": "725e382724fbe2b3af2a974bfe92c81e", "complexity": 1, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\fabric\\utilities\\warnings.py", "func_name": "_custom_format_warning", "original_string": "def _custom_format_warning(\r\n    message: Union[Warning, str], category: type[Warning], filename: str, lineno: int, line: Optional[str] = None\r\n) -> str:\r\n    \"\"\"Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.\"\"\"\r\n    if _is_path_in_lightning(Path(filename)):\r\n        # The warning originates from the Lightning package\r\n        return f\"{filename}:{lineno}: {message}\\n\"\r\n    return _default_format_warning(message, category, filename, lineno, line)", "language": "python", "code": "def _custom_format_warning(\r\n    message: Union[Warning, str], category: type[Warning], filename: str, lineno: int, line: Optional[str] = None\r\n) -> str:\r\n    \"\"\"Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.\"\"\"\r\n    if _is_path_in_lightning(Path(filename)):\r\n        # The warning originates from the Lightning package\r\n        return f\"{filename}:{lineno}: {message}\\n\"\r\n    return _default_format_warning(message, category, filename, lineno, line)", "code_tokens": ["def", "_custom_format_warning", "(", "message", ":", "Union", "[", "Warning", ",", "str", "]", ",", "category", ":", "type", "[", "Warning", "]", ",", "filename", ":", "str", ",", "lineno", ":", "int", ",", "line", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Custom", "formatting", "that", "avoids", "an", "extra", "line", "in", "case", "warnings", "are", "emitted", "from", "the", "`", "rank_zero", "`", "-", "functions", ".", "\"", "\"", "\"", "if", "_is_path_in_lightning", "(", "Path", "(", "filename", ")", ")", ":", "return", "f", "\"", "{", "filename", "}", ":", "{", "lineno", "}", ":", "{", "message", "}", "\\", "n", "\"", "return", "_default_format_warning", "(", "message", ",", "category", ",", "filename", ",", "lineno", ",", "line", ")"], "docstring": "Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.", "docstring_tokens": ["custom", "formatting", "that", "avoids", "an", "extra", "line", "in", "case", "warnings", "are", "emitted", "from", "the", "rank_zero", "functions"], "docstring_summary": "Custom formatting that avoids an extra line in case warnings are emitted from the `rank_zero`-functions.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\fabric\\utilities\\warnings.py", "partition": "train", "function_type": "function", "start_line": 39, "end_line": 46, "hash": "050f4cea408a58cf9bffb98d6e965a76", "complexity": 2, "parameters": ["message", "str]", "category", "filename", "lineno", "line"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        *args: Any,\r\n        description: str = \"Lightning Trainer command line tool\",\r\n        env_prefix: str = \"PL\",\r\n        default_env: bool = False,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        \"\"\"Initialize argument parser that supports configuration file input.\r\n\r\n        For full details of accepted arguments see `ArgumentParser.__init__\r\n        <https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentParser.__init__>`_.\r\n\r\n        Args:\r\n            description: Description of the tool shown when running ``--help``.\r\n            env_prefix: Prefix for environment variables. Set ``default_env=True`` to enable env parsing.\r\n            default_env: Whether to parse environment variables.\r\n\r\n        \"\"\"\r\n        if not _JSONARGPARSE_SIGNATURES_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"{_JSONARGPARSE_SIGNATURES_AVAILABLE}\")\r\n        super().__init__(*args, description=description, env_prefix=env_prefix, default_env=default_env, **kwargs)\r\n        self.callback_keys: list[str] = []\r\n        # separate optimizers and lr schedulers to know which were added\r\n        self._optimizers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}\r\n        self._lr_schedulers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}", "language": "python", "code": "def __init__(\r\n        self,\r\n        *args: Any,\r\n        description: str = \"Lightning Trainer command line tool\",\r\n        env_prefix: str = \"PL\",\r\n        default_env: bool = False,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        \"\"\"Initialize argument parser that supports configuration file input.\r\n\r\n        For full details of accepted arguments see `ArgumentParser.__init__\r\n        <https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentParser.__init__>`_.\r\n\r\n        Args:\r\n            description: Description of the tool shown when running ``--help``.\r\n            env_prefix: Prefix for environment variables. Set ``default_env=True`` to enable env parsing.\r\n            default_env: Whether to parse environment variables.\r\n\r\n        \"\"\"\r\n        if not _JSONARGPARSE_SIGNATURES_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"{_JSONARGPARSE_SIGNATURES_AVAILABLE}\")\r\n        super().__init__(*args, description=description, env_prefix=env_prefix, default_env=default_env, **kwargs)\r\n        self.callback_keys: list[str] = []\r\n        # separate optimizers and lr schedulers to know which were added\r\n        self._optimizers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}\r\n        self._lr_schedulers: dict[str, tuple[Union[type, tuple[type, ...]], str]] = {}", "code_tokens": ["def", "__init__", "(", "self", ",", "*", "args", ":", "Any", ",", "description", ":", "str", "=", "\"", "Lightning", "Trainer", "command", "line", "tool", "\"", ",", "env_prefix", ":", "str", "=", "\"", "PL", "\"", ",", "default_env", ":", "bool", "=", "False", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Initialize", "argument", "parser", "that", "supports", "configuration", "file", "input", ".", "For", "full", "details", "of", "accepted", "arguments", "see", "`", "ArgumentParser", ".", "__init__", "<", "https", ":", "/", "/", "jsonargparse", ".", "readthedocs", ".", "io", "/", "en", "/", "stable", "/", "Args", ":", "description", ":", "Description", "of", "the", "tool", "shown", "when", "running", "`", "`", "-", "-", "help", "`", "`", ".", "env_prefix", ":", "Prefix", "for", "environment", "variables", ".", "Set", "`", "`", "default_env", "=", "True", "`", "`", "to", "enable", "env", "parsing", ".", "default_env", ":", "Whether", "to", "parse", "environment", "variables", ".", "\"", "\"", "\"", "if", "not", "_JSONARGPARSE_SIGNATURES_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "f", "\"", "{", "_JSONARGPARSE_SIGNATURES_AVAILABLE", "}", "\"", ")", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "description", "=", "description", ",", "env_prefix", "=", "env_prefix", ",", "default_env", "=", "default_env", ",", "*", "*", "kwargs", ")", "self", ".", "callback_keys", ":", "list", "[", "str", "]", "=", "[", "]", "self", ".", "_optimizers", ":", "dict", "[", "str", ",", "tuple", "[", "Union", "[", "type", ",", "tuple", "[", "type", ",", ".", ".", ".", "]", "]", ",", "str", "]", "]", "=", "{", "}", "self", ".", "_lr_schedulers", ":", "dict", "[", "str", ",", "tuple", "[", "Union", "[", "type", ",", "tuple", "[", "type", ",", ".", ".", ".", "]", "]", ",", "str", "]", "]", "=", "{", "}"], "docstring": "Initialize argument parser that supports configuration file input.\r\n\r\n        For full details of accepted arguments see `ArgumentParser.__init__\r\n        <https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.ArgumentParser.__init__>`_.\r\n\r\n        Args:\r\n            description: Description of the tool shown when running ``--help``.\r\n            env_prefix: Prefix for environment variables. Set ``default_env=True`` to enable env parsing.\r\n            default_env: Whether to parse environment variables.", "docstring_tokens": ["initialize", "argument", "parser", "that", "supports", "configuration", "file", "input", "for", "full", "details", "of", "accepted", "arguments", "see", "argumentparser", "__init__", "https", "jsonargparse", "readthedocs", "io", "en", "stable", "jsonargparse", "argumentparser", "__init__", "_", "args", "description", "description", "of", "the", "tool", "shown", "when", "running", "help", "env_prefix", "prefix", "for", "environment", "variables", "set", "default_env", "true", "to", "enable", "env", "parsing", "default_env", "whether", "to", "parse", "environment", "variables"], "docstring_summary": "Initialize argument parser that supports configuration file input.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningArgumentParser", "start_line": 88, "end_line": 113, "hash": "25cbc63a50f164607bcc3a81d806c610", "complexity": 2, "parameters": ["*args", "description", "env_prefix", "default_env", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "add_lightning_class_args", "original_string": "def add_lightning_class_args(\r\n        self,\r\n        lightning_class: Union[\r\n            Callable[..., Union[Trainer, LightningModule, LightningDataModule, Callback]],\r\n            type[Trainer],\r\n            type[LightningModule],\r\n            type[LightningDataModule],\r\n            type[Callback],\r\n        ],\r\n        nested_key: str,\r\n        subclass_mode: bool = False,\r\n        required: bool = True,\r\n    ) -> list[str]:\r\n        \"\"\"Adds arguments from a lightning class to a nested key of the parser.\r\n\r\n        Args:\r\n            lightning_class: A callable or any subclass of {Trainer, LightningModule, LightningDataModule, Callback}.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            subclass_mode: Whether allow any subclass of the given class.\r\n            required: Whether the argument group is required.\r\n\r\n        Returns:\r\n            A list with the names of the class arguments added.\r\n\r\n        \"\"\"\r\n        if callable(lightning_class) and not isinstance(lightning_class, type):\r\n            lightning_class = class_from_function(lightning_class)\r\n\r\n        if isinstance(lightning_class, type) and issubclass(\r\n            lightning_class, (Trainer, LightningModule, LightningDataModule, Callback)\r\n        ):\r\n            if issubclass(lightning_class, Callback):\r\n                self.callback_keys.append(nested_key)\r\n            if subclass_mode:\r\n                return self.add_subclass_arguments(lightning_class, nested_key, fail_untyped=False, required=required)\r\n            return self.add_class_arguments(\r\n                lightning_class,\r\n                nested_key,\r\n                fail_untyped=False,\r\n                instantiate=not issubclass(lightning_class, Trainer),\r\n                sub_configs=True,\r\n            )\r\n        raise MisconfigurationException(\r\n            f\"Cannot add arguments from: {lightning_class}. You should provide either a callable or a subclass of: \"\r\n            \"Trainer, LightningModule, LightningDataModule, or Callback.\"\r\n        )", "language": "python", "code": "def add_lightning_class_args(\r\n        self,\r\n        lightning_class: Union[\r\n            Callable[..., Union[Trainer, LightningModule, LightningDataModule, Callback]],\r\n            type[Trainer],\r\n            type[LightningModule],\r\n            type[LightningDataModule],\r\n            type[Callback],\r\n        ],\r\n        nested_key: str,\r\n        subclass_mode: bool = False,\r\n        required: bool = True,\r\n    ) -> list[str]:\r\n        \"\"\"Adds arguments from a lightning class to a nested key of the parser.\r\n\r\n        Args:\r\n            lightning_class: A callable or any subclass of {Trainer, LightningModule, LightningDataModule, Callback}.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            subclass_mode: Whether allow any subclass of the given class.\r\n            required: Whether the argument group is required.\r\n\r\n        Returns:\r\n            A list with the names of the class arguments added.\r\n\r\n        \"\"\"\r\n        if callable(lightning_class) and not isinstance(lightning_class, type):\r\n            lightning_class = class_from_function(lightning_class)\r\n\r\n        if isinstance(lightning_class, type) and issubclass(\r\n            lightning_class, (Trainer, LightningModule, LightningDataModule, Callback)\r\n        ):\r\n            if issubclass(lightning_class, Callback):\r\n                self.callback_keys.append(nested_key)\r\n            if subclass_mode:\r\n                return self.add_subclass_arguments(lightning_class, nested_key, fail_untyped=False, required=required)\r\n            return self.add_class_arguments(\r\n                lightning_class,\r\n                nested_key,\r\n                fail_untyped=False,\r\n                instantiate=not issubclass(lightning_class, Trainer),\r\n                sub_configs=True,\r\n            )\r\n        raise MisconfigurationException(\r\n            f\"Cannot add arguments from: {lightning_class}. You should provide either a callable or a subclass of: \"\r\n            \"Trainer, LightningModule, LightningDataModule, or Callback.\"\r\n        )", "code_tokens": ["def", "add_lightning_class_args", "(", "self", ",", "lightning_class", ":", "Union", "[", "Callable", "[", ".", ".", ".", ",", "Union", "[", "Trainer", ",", "LightningModule", ",", "LightningDataModule", ",", "Callback", "]", "]", ",", "type", "[", "Trainer", "]", ",", "type", "[", "LightningModule", "]", ",", "type", "[", "LightningDataModule", "]", ",", "type", "[", "Callback", "]", ",", "]", ",", "nested_key", ":", "str", ",", "subclass_mode", ":", "bool", "=", "False", ",", "required", ":", "bool", "=", "True", ",", ")", "-", ">", "list", "[", "str", "]", ":", "\"", "\"", "\"", "Adds", "arguments", "from", "a", "lightning", "class", "to", "a", "nested", "key", "of", "the", "parser", ".", "Args", ":", "lightning_class", ":", "A", "callable", "or", "any", "subclass", "of", "{", "Trainer", ",", "LightningModule", ",", "LightningDataModule", ",", "Callback", "}", ".", "nested_key", ":", "Name", "of", "the", "nested", "namespace", "to", "store", "arguments", ".", "subclass_mode", ":", "Whether", "allow", "any", "subclass", "of", "the", "given", "class", ".", "required", ":", "Whether", "the", "argument", "group", "is", "required", ".", "Returns", ":", "A", "list", "with", "the", "names", "of", "the", "class", "arguments", "added", ".", "\"", "\"", "\"", "if", "callable", "(", "lightning_class", ")", "and", "not", "isinstance", "(", "lightning_class", ",", "type", ")", ":", "lightning_class", "=", "class_from_function", "(", "lightning_class", ")", "if", "isinstance", "(", "lightning_class", ",", "type", ")", "and", "issubclass", "(", "lightning_class", ",", "(", "Trainer", ",", "LightningModule", ",", "LightningDataModule", ",", "Callback", ")", ")", ":", "if", "issubclass", "(", "lightning_class", ",", "Callback", ")", ":", "self", ".", "callback_keys", ".", "append", "(", "nested_key", ")", "if", "subclass_mode", ":", "return", "self", ".", "add_subclass_arguments", "(", "lightning_class", ",", "nested_key", ",", "fail_untyped", "=", "False", ",", "required", "=", "required", ")", "return", "self", ".", "add_class_arguments", "(", "lightning_class", ",", "nested_key", ",", "fail_untyped", "=", "False", ",", "instantiate", "=", "not", "issubclass", "(", "lightning_class", ",", "Trainer", ")", ",", "sub_configs", "=", "True", ",", ")", "raise", "MisconfigurationException", "(", "f", "\"", "Cannot", "add", "arguments", "from", ":", "{", "lightning_class", "}", ".", "You", "should", "provide", "either", "a", "callable", "or", "a", "subclass", "of", ":", "\"", "\"", "Trainer", ",", "LightningModule", ",", "LightningDataModule", ",", "or", "Callback", ".", "\"", ")"], "docstring": "Adds arguments from a lightning class to a nested key of the parser.\r\n\r\n        Args:\r\n            lightning_class: A callable or any subclass of {Trainer, LightningModule, LightningDataModule, Callback}.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            subclass_mode: Whether allow any subclass of the given class.\r\n            required: Whether the argument group is required.\r\n\r\n        Returns:\r\n            A list with the names of the class arguments added.", "docstring_tokens": ["adds", "arguments", "from", "a", "lightning", "class", "to", "a", "nested", "key", "of", "the", "parser", "args", "lightning_class", "a", "callable", "or", "any", "subclass", "of", "trainer", "lightningmodule", "lightningdatamodule", "callback", "nested_key", "name", "of", "the", "nested", "namespace", "to", "store", "arguments", "subclass_mode", "whether", "allow", "any", "subclass", "of", "the", "given", "class", "required", "whether", "the", "argument", "group", "is", "required", "returns", "a", "list", "with", "the", "names", "of", "the", "class", "arguments", "added"], "docstring_summary": "Adds arguments from a lightning class to a nested key of the parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningArgumentParser", "start_line": 115, "end_line": 160, "hash": "9cabdc97bee46f5805d6fbdf4263d066", "complexity": 7, "parameters": ["lightning_class", "Union[Trainer", "LightningModule", "LightningDataModule", "Callback]]", "type[Trainer]", "type[LightningModule]", "type[LightningDataModule]", "type[Callback]", "]", "nested_key", "subclass_mode", "required"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "add_optimizer_args", "original_string": "def add_optimizer_args(\r\n        self,\r\n        optimizer_class: Union[type[Optimizer], tuple[type[Optimizer], ...]] = (Optimizer,),\r\n        nested_key: str = \"optimizer\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from an optimizer class to a nested key of the parser.\r\n\r\n        Args:\r\n            optimizer_class: Any subclass of :class:`torch.optim.Optimizer`. Use tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer_class, tuple):\r\n            assert all(issubclass(o, Optimizer) for o in optimizer_class)\r\n        else:\r\n            assert issubclass(optimizer_class, Optimizer)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"params\"}}\r\n        if isinstance(optimizer_class, tuple):\r\n            self.add_subclass_arguments(optimizer_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(optimizer_class, nested_key, sub_configs=True, **kwargs)\r\n        self._optimizers[nested_key] = (optimizer_class, link_to)", "language": "python", "code": "def add_optimizer_args(\r\n        self,\r\n        optimizer_class: Union[type[Optimizer], tuple[type[Optimizer], ...]] = (Optimizer,),\r\n        nested_key: str = \"optimizer\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from an optimizer class to a nested key of the parser.\r\n\r\n        Args:\r\n            optimizer_class: Any subclass of :class:`torch.optim.Optimizer`. Use tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer_class, tuple):\r\n            assert all(issubclass(o, Optimizer) for o in optimizer_class)\r\n        else:\r\n            assert issubclass(optimizer_class, Optimizer)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"params\"}}\r\n        if isinstance(optimizer_class, tuple):\r\n            self.add_subclass_arguments(optimizer_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(optimizer_class, nested_key, sub_configs=True, **kwargs)\r\n        self._optimizers[nested_key] = (optimizer_class, link_to)", "code_tokens": ["def", "add_optimizer_args", "(", "self", ",", "optimizer_class", ":", "Union", "[", "type", "[", "Optimizer", "]", ",", "tuple", "[", "type", "[", "Optimizer", "]", ",", ".", ".", ".", "]", "]", "=", "(", "Optimizer", ",", ")", ",", "nested_key", ":", "str", "=", "\"", "optimizer", "\"", ",", "link_to", ":", "str", "=", "\"", "AUTOMATIC", "\"", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adds", "arguments", "from", "an", "optimizer", "class", "to", "a", "nested", "key", "of", "the", "parser", ".", "Args", ":", "optimizer_class", ":", "Any", "subclass", "of", ":", "class", ":", "`", "torch", ".", "optim", ".", "Optimizer", "`", ".", "Use", "tuple", "to", "allow", "subclasses", ".", "nested_key", ":", "Name", "of", "the", "nested", "namespace", "to", "store", "arguments", ".", "link_to", ":", "Dot", "notation", "of", "a", "parser", "key", "to", "set", "arguments", "or", "AUTOMATIC", ".", "\"", "\"", "\"", "if", "isinstance", "(", "optimizer_class", ",", "tuple", ")", ":", "assert", "all", "(", "issubclass", "(", "o", ",", "Optimizer", ")", "for", "o", "in", "optimizer_class", ")", "else", ":", "assert", "issubclass", "(", "optimizer_class", ",", "Optimizer", ")", "kwargs", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "\"", "instantiate", "\"", ":", "False", ",", "\"", "fail_untyped", "\"", ":", "False", ",", "\"", "skip", "\"", ":", "{", "\"", "params", "\"", "}", "}", "if", "isinstance", "(", "optimizer_class", ",", "tuple", ")", ":", "self", ".", "add_subclass_arguments", "(", "optimizer_class", ",", "nested_key", ",", "*", "*", "kwargs", ")", "else", ":", "self", ".", "add_class_arguments", "(", "optimizer_class", ",", "nested_key", ",", "sub_configs", "=", "True", ",", "*", "*", "kwargs", ")", "self", ".", "_optimizers", "[", "nested_key", "]", "=", "(", "optimizer_class", ",", "link_to", ")"], "docstring": "Adds arguments from an optimizer class to a nested key of the parser.\r\n\r\n        Args:\r\n            optimizer_class: Any subclass of :class:`torch.optim.Optimizer`. Use tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.", "docstring_tokens": ["adds", "arguments", "from", "an", "optimizer", "class", "to", "a", "nested", "key", "of", "the", "parser", "args", "optimizer_class", "any", "subclass", "of", "class", "torch", "optim", "optimizer", "use", "tuple", "to", "allow", "subclasses", "nested_key", "name", "of", "the", "nested", "namespace", "to", "store", "arguments", "link_to", "dot", "notation", "of", "a", "parser", "key", "to", "set", "arguments", "or", "automatic"], "docstring_summary": "Adds arguments from an optimizer class to a nested key of the parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningArgumentParser", "start_line": 162, "end_line": 185, "hash": "821c663759742508d879ccab212e906e", "complexity": 4, "parameters": ["optimizer_class", "tuple[type[Optimizer]", "...]]", ")", "nested_key", "link_to"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "add_lr_scheduler_args", "original_string": "def add_lr_scheduler_args(\r\n        self,\r\n        lr_scheduler_class: Union[LRSchedulerType, tuple[LRSchedulerType, ...]] = LRSchedulerTypeTuple,\r\n        nested_key: str = \"lr_scheduler\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from a learning rate scheduler class to a nested key of the parser.\r\n\r\n        Args:\r\n            lr_scheduler_class: Any subclass of ``torch.optim.lr_scheduler.{_LRScheduler, ReduceLROnPlateau}``. Use\r\n                tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            assert all(issubclass(o, LRSchedulerTypeTuple) for o in lr_scheduler_class)\r\n        else:\r\n            assert issubclass(lr_scheduler_class, LRSchedulerTypeTuple)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"optimizer\"}}\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            self.add_subclass_arguments(lr_scheduler_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(lr_scheduler_class, nested_key, sub_configs=True, **kwargs)\r\n        self._lr_schedulers[nested_key] = (lr_scheduler_class, link_to)", "language": "python", "code": "def add_lr_scheduler_args(\r\n        self,\r\n        lr_scheduler_class: Union[LRSchedulerType, tuple[LRSchedulerType, ...]] = LRSchedulerTypeTuple,\r\n        nested_key: str = \"lr_scheduler\",\r\n        link_to: str = \"AUTOMATIC\",\r\n    ) -> None:\r\n        \"\"\"Adds arguments from a learning rate scheduler class to a nested key of the parser.\r\n\r\n        Args:\r\n            lr_scheduler_class: Any subclass of ``torch.optim.lr_scheduler.{_LRScheduler, ReduceLROnPlateau}``. Use\r\n                tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.\r\n\r\n        \"\"\"\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            assert all(issubclass(o, LRSchedulerTypeTuple) for o in lr_scheduler_class)\r\n        else:\r\n            assert issubclass(lr_scheduler_class, LRSchedulerTypeTuple)\r\n        kwargs: dict[str, Any] = {\"instantiate\": False, \"fail_untyped\": False, \"skip\": {\"optimizer\"}}\r\n        if isinstance(lr_scheduler_class, tuple):\r\n            self.add_subclass_arguments(lr_scheduler_class, nested_key, **kwargs)\r\n        else:\r\n            self.add_class_arguments(lr_scheduler_class, nested_key, sub_configs=True, **kwargs)\r\n        self._lr_schedulers[nested_key] = (lr_scheduler_class, link_to)", "code_tokens": ["def", "add_lr_scheduler_args", "(", "self", ",", "lr_scheduler_class", ":", "Union", "[", "LRSchedulerType", ",", "tuple", "[", "LRSchedulerType", ",", ".", ".", ".", "]", "]", "=", "LRSchedulerTypeTuple", ",", "nested_key", ":", "str", "=", "\"", "lr_scheduler", "\"", ",", "link_to", ":", "str", "=", "\"", "AUTOMATIC", "\"", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adds", "arguments", "from", "a", "learning", "rate", "scheduler", "class", "to", "a", "nested", "key", "of", "the", "parser", ".", "Args", ":", "lr_scheduler_class", ":", "Any", "subclass", "of", "`", "`", "torch", ".", "optim", ".", "lr_scheduler", ".", "{", "_LRScheduler", ",", "ReduceLROnPlateau", "}", "`", "`", ".", "Use", "tuple", "to", "allow", "subclasses", ".", "nested_key", ":", "Name", "of", "the", "nested", "namespace", "to", "store", "arguments", ".", "link_to", ":", "Dot", "notation", "of", "a", "parser", "key", "to", "set", "arguments", "or", "AUTOMATIC", ".", "\"", "\"", "\"", "if", "isinstance", "(", "lr_scheduler_class", ",", "tuple", ")", ":", "assert", "all", "(", "issubclass", "(", "o", ",", "LRSchedulerTypeTuple", ")", "for", "o", "in", "lr_scheduler_class", ")", "else", ":", "assert", "issubclass", "(", "lr_scheduler_class", ",", "LRSchedulerTypeTuple", ")", "kwargs", ":", "dict", "[", "str", ",", "Any", "]", "=", "{", "\"", "instantiate", "\"", ":", "False", ",", "\"", "fail_untyped", "\"", ":", "False", ",", "\"", "skip", "\"", ":", "{", "\"", "optimizer", "\"", "}", "}", "if", "isinstance", "(", "lr_scheduler_class", ",", "tuple", ")", ":", "self", ".", "add_subclass_arguments", "(", "lr_scheduler_class", ",", "nested_key", ",", "*", "*", "kwargs", ")", "else", ":", "self", ".", "add_class_arguments", "(", "lr_scheduler_class", ",", "nested_key", ",", "sub_configs", "=", "True", ",", "*", "*", "kwargs", ")", "self", ".", "_lr_schedulers", "[", "nested_key", "]", "=", "(", "lr_scheduler_class", ",", "link_to", ")"], "docstring": "Adds arguments from a learning rate scheduler class to a nested key of the parser.\r\n\r\n        Args:\r\n            lr_scheduler_class: Any subclass of ``torch.optim.lr_scheduler.{_LRScheduler, ReduceLROnPlateau}``. Use\r\n                tuple to allow subclasses.\r\n            nested_key: Name of the nested namespace to store arguments.\r\n            link_to: Dot notation of a parser key to set arguments or AUTOMATIC.", "docstring_tokens": ["adds", "arguments", "from", "a", "learning", "rate", "scheduler", "class", "to", "a", "nested", "key", "of", "the", "parser", "args", "lr_scheduler_class", "any", "subclass", "of", "torch", "optim", "lr_scheduler", "_lrscheduler", "reducelronplateau", "use", "tuple", "to", "allow", "subclasses", "nested_key", "name", "of", "the", "nested", "namespace", "to", "store", "arguments", "link_to", "dot", "notation", "of", "a", "parser", "key", "to", "set", "arguments", "or", "automatic"], "docstring_summary": "Adds arguments from a learning rate scheduler class to a nested key of the parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningArgumentParser", "start_line": 187, "end_line": 211, "hash": "13521c01f082d304d9adf3744878bbe9", "complexity": 4, "parameters": ["lr_scheduler_class", "tuple[LRSchedulerType", "...]]", "nested_key", "link_to"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "save_config", "original_string": "def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\r\n        \"\"\"Implement to save the config in some other place additional to the standard log_dir.\r\n\r\n        Example:\r\n            def save_config(self, trainer, pl_module, stage):\r\n                if isinstance(trainer.logger, Logger):\r\n                    config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\r\n                    trainer.logger.log_hyperparams({\"config\": config})\r\n\r\n        Note:\r\n            This method is only called on rank zero. This allows to implement a custom save config without having to\r\n            worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the\r\n            process hang waiting for a broadcast. If you need to make collective calls, implement the setup method\r\n            instead.\r\n\r\n        \"\"\"", "language": "python", "code": "def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\r\n        \"\"\"Implement to save the config in some other place additional to the standard log_dir.\r\n\r\n        Example:\r\n            def save_config(self, trainer, pl_module, stage):\r\n                if isinstance(trainer.logger, Logger):\r\n                    config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\r\n                    trainer.logger.log_hyperparams({\"config\": config})\r\n\r\n        Note:\r\n            This method is only called on rank zero. This allows to implement a custom save config without having to\r\n            worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the\r\n            process hang waiting for a broadcast. If you need to make collective calls, implement the setup method\r\n            instead.\r\n\r\n        \"\"\"", "code_tokens": ["def", "save_config", "(", "self", ",", "trainer", ":", "Trainer", ",", "pl_module", ":", "LightningModule", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Implement", "to", "save", "the", "config", "in", "some", "other", "place", "additional", "to", "the", "standard", "log_dir", ".", "Example", ":", "def", "save_config", "(", "self", ",", "trainer", ",", "pl_module", ",", "stage", ")", ":", "if", "isinstance", "(", "trainer", ".", "logger", ",", "Logger", ")", ":", "config", "=", "self", ".", "parser", ".", "dump", "(", "self", ".", "config", ",", "skip_none", "=", "False", ")", "trainer", ".", "logger", ".", "log_hyperparams", "(", "{", "\"", "config", "\"", ":", "config", "}", ")", "Note", ":", "This", "method", "is", "only", "called", "on", "rank", "zero", ".", "This", "allows", "to", "implement", "a", "custom", "save", "config", "without", "having", "to", "worry", "about", "ranks", "or", "race", "conditions", ".", "Since", "it", "only", "runs", "on", "rank", "zero", ",", "any", "collective", "call", "will", "make", "the", "process", "hang", "waiting", "for", "a", "broadcast", ".", "If", "you", "need", "to", "make", "collective", "calls", ",", "implement", "the", "setup", "method", "instead", ".", "\"", "\"", "\""], "docstring": "Implement to save the config in some other place additional to the standard log_dir.\r\n\r\n        Example:\r\n            def save_config(self, trainer, pl_module, stage):\r\n                if isinstance(trainer.logger, Logger):\r\n                    config = self.parser.dump(self.config, skip_none=False)  # Required for proper reproducibility\r\n                    trainer.logger.log_hyperparams({\"config\": config})\r\n\r\n        Note:\r\n            This method is only called on rank zero. This allows to implement a custom save config without having to\r\n            worry about ranks or race conditions. Since it only runs on rank zero, any collective call will make the\r\n            process hang waiting for a broadcast. If you need to make collective calls, implement the setup method\r\n            instead.", "docstring_tokens": ["implement", "to", "save", "the", "config", "in", "some", "other", "place", "additional", "to", "the", "standard", "log_dir", "example", "def", "save_config", "self", "trainer", "pl_module", "stage", "if", "isinstance", "trainer", "logger", "logger", "config", "self", "parser", "dump", "self", "config", "skip_none", "false", "required", "for", "proper", "reproducibility", "trainer", "logger", "log_hyperparams", "config", "config", "note", "this", "method", "is", "only", "called", "on", "rank", "zero", "this", "allows", "to", "implement", "a", "custom", "save", "config", "without", "having", "to", "worry", "about", "ranks", "or", "race", "conditions", "since", "it", "only", "runs", "on", "rank", "zero", "any", "collective", "call", "will", "make", "the", "process", "hang", "waiting", "for", "a", "broadcast", "if", "you", "need", "to", "make", "collective", "calls", "implement", "the", "setup", "method", "instead"], "docstring_summary": "Implement to save the config in some other place additional to the standard log_dir.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "SaveConfigCallback", "start_line": 293, "end_line": 308, "hash": "f46557c0a95ac871605e7c502f1f1ef9", "complexity": 1, "parameters": ["trainer", "pl_module", "stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        model_class: Optional[Union[type[LightningModule], Callable[..., LightningModule]]] = None,\r\n        datamodule_class: Optional[Union[type[LightningDataModule], Callable[..., LightningDataModule]]] = None,\r\n        save_config_callback: Optional[type[SaveConfigCallback]] = SaveConfigCallback,\r\n        save_config_kwargs: Optional[dict[str, Any]] = None,\r\n        trainer_class: Union[type[Trainer], Callable[..., Trainer]] = Trainer,\r\n        trainer_defaults: Optional[dict[str, Any]] = None,\r\n        seed_everything_default: Union[bool, int] = True,\r\n        parser_kwargs: Optional[Union[dict[str, Any], dict[str, dict[str, Any]]]] = None,\r\n        parser_class: type[LightningArgumentParser] = LightningArgumentParser,\r\n        subclass_mode_model: bool = False,\r\n        subclass_mode_data: bool = False,\r\n        args: ArgsType = None,\r\n        run: bool = True,\r\n        auto_configure_optimizers: bool = True,\r\n        load_from_checkpoint_support: bool = True,\r\n    ) -> None:\r\n        \"\"\"Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are\r\n        called / instantiated using a parsed configuration file and / or command line args.\r\n\r\n        Parsing of configuration from environment variables can be enabled by setting ``parser_kwargs={\"default_env\":\r\n        True}``. A full configuration yaml would be parsed from ``PL_CONFIG`` if set. Individual settings are so parsed\r\n        from variables named for example ``PL_TRAINER__MAX_EPOCHS``.\r\n\r\n        For more info, read :ref:`the CLI docs <lightning-cli>`.\r\n\r\n        Args:\r\n            model_class: An optional :class:`~lightning.pytorch.core.LightningModule` class to train on or a\r\n                callable which returns a :class:`~lightning.pytorch.core.LightningModule` instance when\r\n                called. If ``None``, you can pass a registered model with ``--model=MyModel``.\r\n            datamodule_class: An optional :class:`~lightning.pytorch.core.datamodule.LightningDataModule` class or a\r\n                callable which returns a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` instance when\r\n                called. If ``None``, you can pass a registered datamodule with ``--data=MyDataModule``.\r\n            save_config_callback: A callback class to save the config.\r\n            save_config_kwargs: Parameters that will be used to instantiate the save_config_callback.\r\n            trainer_class: An optional subclass of the :class:`~lightning.pytorch.trainer.trainer.Trainer` class or a\r\n                callable which returns a :class:`~lightning.pytorch.trainer.trainer.Trainer` instance when called.\r\n            trainer_defaults: Set to override Trainer defaults or add persistent callbacks. The callbacks added through\r\n                this argument will not be configurable from a configuration file and will always be present for\r\n                this particular CLI. Alternatively, configurable callbacks can be added as explained in\r\n                :ref:`the CLI docs <lightning-cli>`.\r\n            seed_everything_default: Number for the :func:`~lightning.fabric.utilities.seed.seed_everything`\r\n                seed value. Set to True to automatically choose a seed value.\r\n                Setting it to False will avoid calling ``seed_everything``.\r\n            parser_kwargs: Additional arguments to instantiate each ``LightningArgumentParser``.\r\n            subclass_mode_model: Whether model can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            subclass_mode_data: Whether datamodule can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            args: Arguments to parse. If ``None`` the arguments are taken from ``sys.argv``. Command line style\r\n                arguments can be given in a ``list``. Alternatively, structured config options can be given in a\r\n                ``dict`` or ``jsonargparse.Namespace``.\r\n            run: Whether subcommands should be added to run a :class:`~lightning.pytorch.trainer.trainer.Trainer`\r\n                method. If set to ``False``, the trainer and model classes will be instantiated only.\r\n            auto_configure_optimizers: Whether to automatically add default optimizer and lr_scheduler arguments.\r\n            load_from_checkpoint_support: Whether ``save_hyperparameters`` should save the original parsed\r\n                hyperparameters (instead of what ``__init__`` receives), such that it is possible for\r\n                ``load_from_checkpoint`` to correctly instantiate classes even when using complex nesting and\r\n                dependency injection.\r\n\r\n        \"\"\"\r\n        self.save_config_callback = save_config_callback\r\n        self.save_config_kwargs = save_config_kwargs or {}\r\n        self.trainer_class = trainer_class\r\n        self.trainer_defaults = trainer_defaults or {}\r\n        self.seed_everything_default = seed_everything_default\r\n        self.parser_kwargs = parser_kwargs or {}\r\n        self.parser_class = parser_class\r\n        self.auto_configure_optimizers = auto_configure_optimizers\r\n\r\n        self.model_class = model_class\r\n        # used to differentiate between the original value and the processed value\r\n        self._model_class = model_class or LightningModule\r\n        self.subclass_mode_model = (model_class is None) or subclass_mode_model\r\n\r\n        self.datamodule_class = datamodule_class\r\n        # used to differentiate between the original value and the processed value\r\n        self._datamodule_class = datamodule_class or LightningDataModule\r\n        self.subclass_mode_data = (datamodule_class is None) or subclass_mode_data\r\n\r\n        main_kwargs, subparser_kwargs = self._setup_parser_kwargs(self.parser_kwargs)\r\n        self.setup_parser(run, main_kwargs, subparser_kwargs)\r\n        self.parse_arguments(self.parser, args)\r\n        self._parse_ckpt_path()\r\n\r\n        self.subcommand = self.config[\"subcommand\"] if run else None\r\n\r\n        self._set_seed()\r\n\r\n        if load_from_checkpoint_support:\r\n            self._add_instantiators()\r\n        self.before_instantiate_classes()\r\n        self.instantiate_classes()\r\n        self.after_instantiate_classes()\r\n\r\n        if self.subcommand is not None:\r\n            self._run_subcommand(self.subcommand)", "language": "python", "code": "def __init__(\r\n        self,\r\n        model_class: Optional[Union[type[LightningModule], Callable[..., LightningModule]]] = None,\r\n        datamodule_class: Optional[Union[type[LightningDataModule], Callable[..., LightningDataModule]]] = None,\r\n        save_config_callback: Optional[type[SaveConfigCallback]] = SaveConfigCallback,\r\n        save_config_kwargs: Optional[dict[str, Any]] = None,\r\n        trainer_class: Union[type[Trainer], Callable[..., Trainer]] = Trainer,\r\n        trainer_defaults: Optional[dict[str, Any]] = None,\r\n        seed_everything_default: Union[bool, int] = True,\r\n        parser_kwargs: Optional[Union[dict[str, Any], dict[str, dict[str, Any]]]] = None,\r\n        parser_class: type[LightningArgumentParser] = LightningArgumentParser,\r\n        subclass_mode_model: bool = False,\r\n        subclass_mode_data: bool = False,\r\n        args: ArgsType = None,\r\n        run: bool = True,\r\n        auto_configure_optimizers: bool = True,\r\n        load_from_checkpoint_support: bool = True,\r\n    ) -> None:\r\n        \"\"\"Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are\r\n        called / instantiated using a parsed configuration file and / or command line args.\r\n\r\n        Parsing of configuration from environment variables can be enabled by setting ``parser_kwargs={\"default_env\":\r\n        True}``. A full configuration yaml would be parsed from ``PL_CONFIG`` if set. Individual settings are so parsed\r\n        from variables named for example ``PL_TRAINER__MAX_EPOCHS``.\r\n\r\n        For more info, read :ref:`the CLI docs <lightning-cli>`.\r\n\r\n        Args:\r\n            model_class: An optional :class:`~lightning.pytorch.core.LightningModule` class to train on or a\r\n                callable which returns a :class:`~lightning.pytorch.core.LightningModule` instance when\r\n                called. If ``None``, you can pass a registered model with ``--model=MyModel``.\r\n            datamodule_class: An optional :class:`~lightning.pytorch.core.datamodule.LightningDataModule` class or a\r\n                callable which returns a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` instance when\r\n                called. If ``None``, you can pass a registered datamodule with ``--data=MyDataModule``.\r\n            save_config_callback: A callback class to save the config.\r\n            save_config_kwargs: Parameters that will be used to instantiate the save_config_callback.\r\n            trainer_class: An optional subclass of the :class:`~lightning.pytorch.trainer.trainer.Trainer` class or a\r\n                callable which returns a :class:`~lightning.pytorch.trainer.trainer.Trainer` instance when called.\r\n            trainer_defaults: Set to override Trainer defaults or add persistent callbacks. The callbacks added through\r\n                this argument will not be configurable from a configuration file and will always be present for\r\n                this particular CLI. Alternatively, configurable callbacks can be added as explained in\r\n                :ref:`the CLI docs <lightning-cli>`.\r\n            seed_everything_default: Number for the :func:`~lightning.fabric.utilities.seed.seed_everything`\r\n                seed value. Set to True to automatically choose a seed value.\r\n                Setting it to False will avoid calling ``seed_everything``.\r\n            parser_kwargs: Additional arguments to instantiate each ``LightningArgumentParser``.\r\n            subclass_mode_model: Whether model can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            subclass_mode_data: Whether datamodule can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            args: Arguments to parse. If ``None`` the arguments are taken from ``sys.argv``. Command line style\r\n                arguments can be given in a ``list``. Alternatively, structured config options can be given in a\r\n                ``dict`` or ``jsonargparse.Namespace``.\r\n            run: Whether subcommands should be added to run a :class:`~lightning.pytorch.trainer.trainer.Trainer`\r\n                method. If set to ``False``, the trainer and model classes will be instantiated only.\r\n            auto_configure_optimizers: Whether to automatically add default optimizer and lr_scheduler arguments.\r\n            load_from_checkpoint_support: Whether ``save_hyperparameters`` should save the original parsed\r\n                hyperparameters (instead of what ``__init__`` receives), such that it is possible for\r\n                ``load_from_checkpoint`` to correctly instantiate classes even when using complex nesting and\r\n                dependency injection.\r\n\r\n        \"\"\"\r\n        self.save_config_callback = save_config_callback\r\n        self.save_config_kwargs = save_config_kwargs or {}\r\n        self.trainer_class = trainer_class\r\n        self.trainer_defaults = trainer_defaults or {}\r\n        self.seed_everything_default = seed_everything_default\r\n        self.parser_kwargs = parser_kwargs or {}\r\n        self.parser_class = parser_class\r\n        self.auto_configure_optimizers = auto_configure_optimizers\r\n\r\n        self.model_class = model_class\r\n        # used to differentiate between the original value and the processed value\r\n        self._model_class = model_class or LightningModule\r\n        self.subclass_mode_model = (model_class is None) or subclass_mode_model\r\n\r\n        self.datamodule_class = datamodule_class\r\n        # used to differentiate between the original value and the processed value\r\n        self._datamodule_class = datamodule_class or LightningDataModule\r\n        self.subclass_mode_data = (datamodule_class is None) or subclass_mode_data\r\n\r\n        main_kwargs, subparser_kwargs = self._setup_parser_kwargs(self.parser_kwargs)\r\n        self.setup_parser(run, main_kwargs, subparser_kwargs)\r\n        self.parse_arguments(self.parser, args)\r\n        self._parse_ckpt_path()\r\n\r\n        self.subcommand = self.config[\"subcommand\"] if run else None\r\n\r\n        self._set_seed()\r\n\r\n        if load_from_checkpoint_support:\r\n            self._add_instantiators()\r\n        self.before_instantiate_classes()\r\n        self.instantiate_classes()\r\n        self.after_instantiate_classes()\r\n\r\n        if self.subcommand is not None:\r\n            self._run_subcommand(self.subcommand)", "code_tokens": ["def", "__init__", "(", "self", ",", "model_class", ":", "Optional", "[", "Union", "[", "type", "[", "LightningModule", "]", ",", "Callable", "[", ".", ".", ".", ",", "LightningModule", "]", "]", "]", "=", "None", ",", "datamodule_class", ":", "Optional", "[", "Union", "[", "type", "[", "LightningDataModule", "]", ",", "Callable", "[", ".", ".", ".", ",", "LightningDataModule", "]", "]", "]", "=", "None", ",", "save_config_callback", ":", "Optional", "[", "type", "[", "SaveConfigCallback", "]", "]", "=", "SaveConfigCallback", ",", "save_config_kwargs", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "trainer_class", ":", "Union", "[", "type", "[", "Trainer", "]", ",", "Callable", "[", ".", ".", ".", ",", "Trainer", "]", "]", "=", "Trainer", ",", "trainer_defaults", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "seed_everything_default", ":", "Union", "[", "bool", ",", "int", "]", "=", "True", ",", "parser_kwargs", ":", "Optional", "[", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "dict", "[", "str", ",", "dict", "[", "str", ",", "Any", "]", "]", "]", "]", "=", "None", ",", "parser_class", ":", "type", "[", "LightningArgumentParser", "]", "=", "LightningArgumentParser", ",", "subclass_mode_model", ":", "bool", "=", "False", ",", "subclass_mode_data", ":", "bool", "=", "False", ",", "args", ":", "ArgsType", "=", "None", ",", "run", ":", "bool", "=", "True", ",", "auto_configure_optimizers", ":", "bool", "=", "True", ",", "load_from_checkpoint_support", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Receives", "as", "input", "pytorch", "-", "lightning", "classes", "(", "or", "callables", "which", "return", "pytorch", "-", "lightning", "classes", ")", ",", "which", "are", "called", "/", "instantiated", "using", "a", "parsed", "configuration", "file", "and", "/", "or", "command", "line", "args", ".", "Parsing", "of", "configuration", "from", "environment", "variables", "can", "be", "enabled", "by", "setting", "`", "`", "parser_kwargs", "=", "{", "\"", "default_env", "\"", ":", "True", "}", "`", "`", ".", "A", "full", "configuration", "yaml", "would", "be", "parsed", "from", "`", "`", "PL_CONFIG", "`", "`", "if", "set", ".", "Individual", "settings", "are", "so", "parsed", "from", "variables", "named", "for", "example", "`", "`", "PL_TRAINER__MAX_EPOCHS", "`", "`", ".", "For", "more", "info", ",", "read", ":", "ref", ":", "`", "the", "CLI", "docs", "<", "lightning", "-", "cli", ">", "`", ".", "Args", ":", "model_class", ":", "An", "optional", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "class", "to", "train", "on", "or", "a", "callable", "which", "returns", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", "when", "called", ".", "If", "`", "`", "None", "`", "`", ",", "you", "can", "pass", "a", "registered", "model", "with", "`", "`", "-", "-", "model", "=", "MyModel", "`", "`", ".", "datamodule_class", ":", "An", "optional", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "class", "or", "a", "callable", "which", "returns", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "datamodule", ".", "LightningDataModule", "`", "instance", "when", "called", ".", "If", "`", "`", "None", "`", "`", ",", "you", "can", "pass", "a", "registered", "datamodule", "with", "`", "`", "-", "-", "data", "=", "MyDataModule", "`", "`", ".", "save_config_callback", ":", "A", "callback", "class", "to", "save", "the", "config", ".", "save_config_kwargs", ":", "Parameters", "that", "will", "be", "used", "to", "instantiate", "the", "save_config_callback", ".", "trainer_class", ":", "An", "optional", "subclass", "of", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "class", "or", "a", "callable", "which", "returns", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", "when", "called", ".", "trainer_defaults", ":", "Set", "to", "override", "Trainer", "defaults", "or", "add", "persistent", "callbacks", ".", "The", "callbacks", "added", "through", "this", "argument", "will", "not", "be", "configurable", "from", "a", "configuration", "file", "and", "will", "always", "be", "present", "for", "this", "particular", "CLI", ".", "Alternatively", ",", "configurable", "callbacks", "can", "be", "added", "as", "explained", "in", ":", "ref", ":", "`", "the", "CLI", "docs", "<", "lightning", "-", "cli", ">", "`", ".", "seed_everything_default", ":", "Number", "for", "the", ":", "func", ":", "`", "~", "lightning", ".", "fabric", ".", "utilities", ".", "seed", ".", "seed_everything", "`", "seed", "value", ".", "Set", "to", "True", "to", "automatically", "choose", "a", "seed", "value", ".", "Setting", "it", "to", "False", "will", "avoid", "calling", "`", "`", "seed_everything", "`", "`", ".", "parser_kwargs", ":", "Additional", "arguments", "to", "instantiate", "each", "`", "`", "LightningArgumentParser", "`", "`", ".", "subclass_mode_model", ":", "Whether", "model", "can", "be", "any", "`", "subclass", "<", "https", ":", "/", "/", "jsonargparse", ".", "readthedocs", ".", "io", "/", "en", "/", "stable", "/", "of", "the", "given", "class", ".", "subclass_mode_data", ":", "Whether", "datamodule", "can", "be", "any", "`", "subclass", "<", "https", ":", "/", "/", "jsonargparse", ".", "readthedocs", ".", "io", "/", "en", "/", "stable", "/", "of", "the", "given", "class", ".", "args", ":", "Arguments", "to", "parse", ".", "If", "`", "`", "None", "`", "`", "the", "arguments", "are", "taken", "from", "`", "`", "sys", ".", "argv", "`", "`", ".", "Command", "line", "style", "arguments", "can", "be", "given", "in", "a", "`", "`", "list", "`", "`", ".", "Alternatively", ",", "structured", "config", "options", "can", "be", "given", "in", "a", "`", "`", "dict", "`", "`", "or", "`", "`", "jsonargparse", ".", "Namespace", "`", "`", ".", "run", ":", "Whether", "subcommands", "should", "be", "added", "to", "run", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "method", ".", "If", "set", "to", "`", "`", "False", "`", "`", ",", "the", "trainer", "and", "model", "classes", "will", "be", "instantiated", "only", ".", "auto_configure_optimizers", ":", "Whether", "to", "automatically", "add", "default", "optimizer", "and", "lr_scheduler", "arguments", ".", "load_from_checkpoint_support", ":", "Whether", "`", "`", "save_hyperparameters", "`", "`", "should", "save", "the", "original", "parsed", "hyperparameters", "(", "instead", "of", "what", "`", "`", "__init__", "`", "`", "receives", ")", ",", "such", "that", "it", "is", "possible", "for", "`", "`", "load_from_checkpoint", "`", "`", "to", "correctly", "instantiate", "classes", "even", "when", "using", "complex", "nesting", "and", "dependency", "injection", ".", "\"", "\"", "\"", "self", ".", "save_config_callback", "=", "save_config_callback", "self", ".", "save_config_kwargs", "=", "save_config_kwargs", "or", "{", "}", "self", ".", "trainer_class", "=", "trainer_class", "self", ".", "trainer_defaults", "=", "trainer_defaults", "or", "{", "}", "self", ".", "seed_everything_default", "=", "seed_everything_default", "self", ".", "parser_kwargs", "=", "parser_kwargs", "or", "{", "}", "self", ".", "parser_class", "=", "parser_class", "self", ".", "auto_configure_optimizers", "=", "auto_configure_optimizers", "self", ".", "model_class", "=", "model_class", "self", ".", "_model_class", "=", "model_class", "or", "LightningModule", "self", ".", "subclass_mode_model", "=", "(", "model_class", "is", "None", ")", "or", "subclass_mode_model", "self", ".", "datamodule_class", "=", "datamodule_class", "self", ".", "_datamodule_class", "=", "datamodule_class", "or", "LightningDataModule", "self", ".", "subclass_mode_data", "=", "(", "datamodule_class", "is", "None", ")", "or", "subclass_mode_data", "main_kwargs", ",", "subparser_kwargs", "=", "self", ".", "_setup_parser_kwargs", "(", "self", ".", "parser_kwargs", ")", "self", ".", "setup_parser", "(", "run", ",", "main_kwargs", ",", "subparser_kwargs", ")", "self", ".", "parse_arguments", "(", "self", ".", "parser", ",", "args", ")", "self", ".", "_parse_ckpt_path", "(", ")", "self", ".", "subcommand", "=", "self", ".", "config", "[", "\"", "subcommand", "\"", "]", "if", "run", "else", "None", "self", ".", "_set_seed", "(", ")", "if", "load_from_checkpoint_support", ":", "self", ".", "_add_instantiators", "(", ")", "self", ".", "before_instantiate_classes", "(", ")", "self", ".", "instantiate_classes", "(", ")", "self", ".", "after_instantiate_classes", "(", ")", "if", "self", ".", "subcommand", "is", "not", "None", ":", "self", ".", "_run_subcommand", "(", "self", ".", "subcommand", ")"], "docstring": "Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are\r\n        called / instantiated using a parsed configuration file and / or command line args.\r\n\r\n        Parsing of configuration from environment variables can be enabled by setting ``parser_kwargs={\"default_env\":\r\n        True}``. A full configuration yaml would be parsed from ``PL_CONFIG`` if set. Individual settings are so parsed\r\n        from variables named for example ``PL_TRAINER__MAX_EPOCHS``.\r\n\r\n        For more info, read :ref:`the CLI docs <lightning-cli>`.\r\n\r\n        Args:\r\n            model_class: An optional :class:`~lightning.pytorch.core.LightningModule` class to train on or a\r\n                callable which returns a :class:`~lightning.pytorch.core.LightningModule` instance when\r\n                called. If ``None``, you can pass a registered model with ``--model=MyModel``.\r\n            datamodule_class: An optional :class:`~lightning.pytorch.core.datamodule.LightningDataModule` class or a\r\n                callable which returns a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` instance when\r\n                called. If ``None``, you can pass a registered datamodule with ``--data=MyDataModule``.\r\n            save_config_callback: A callback class to save the config.\r\n            save_config_kwargs: Parameters that will be used to instantiate the save_config_callback.\r\n            trainer_class: An optional subclass of the :class:`~lightning.pytorch.trainer.trainer.Trainer` class or a\r\n                callable which returns a :class:`~lightning.pytorch.trainer.trainer.Trainer` instance when called.\r\n            trainer_defaults: Set to override Trainer defaults or add persistent callbacks. The callbacks added through\r\n                this argument will not be configurable from a configuration file and will always be present for\r\n                this particular CLI. Alternatively, configurable callbacks can be added as explained in\r\n                :ref:`the CLI docs <lightning-cli>`.\r\n            seed_everything_default: Number for the :func:`~lightning.fabric.utilities.seed.seed_everything`\r\n                seed value. Set to True to automatically choose a seed value.\r\n                Setting it to False will avoid calling ``seed_everything``.\r\n            parser_kwargs: Additional arguments to instantiate each ``LightningArgumentParser``.\r\n            subclass_mode_model: Whether model can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            subclass_mode_data: Whether datamodule can be any `subclass\r\n                <https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes>`_\r\n                of the given class.\r\n            args: Arguments to parse. If ``None`` the arguments are taken from ``sys.argv``. Command line style\r\n                arguments can be given in a ``list``. Alternatively, structured config options can be given in a\r\n                ``dict`` or ``jsonargparse.Namespace``.\r\n            run: Whether subcommands should be added to run a :class:`~lightning.pytorch.trainer.trainer.Trainer`\r\n                method. If set to ``False``, the trainer and model classes will be instantiated only.\r\n            auto_configure_optimizers: Whether to automatically add default optimizer and lr_scheduler arguments.\r\n            load_from_checkpoint_support: Whether ``save_hyperparameters`` should save the original parsed\r\n                hyperparameters (instead of what ``__init__`` receives), such that it is possible for\r\n                ``load_from_checkpoint`` to correctly instantiate classes even when using complex nesting and\r\n                dependency injection.", "docstring_tokens": ["receives", "as", "input", "pytorch", "lightning", "classes", "or", "callables", "which", "return", "pytorch", "lightning", "classes", "which", "are", "called", "instantiated", "using", "a", "parsed", "configuration", "file", "and", "or", "command", "line", "args", "parsing", "of", "configuration", "from", "environment", "variables", "can", "be", "enabled", "by", "setting", "parser_kwargs", "default_env", "true", "a", "full", "configuration", "yaml", "would", "be", "parsed", "from", "pl_config", "if", "set", "individual", "settings", "are", "so", "parsed", "from", "variables", "named", "for", "example", "pl_trainer__max_epochs", "for", "more", "info", "read", "ref", "the", "cli", "docs", "lightning", "cli", "args", "model_class", "an", "optional", "class", "lightning", "pytorch", "core", "lightningmodule", "class", "to", "train", "on", "or", "a", "callable", "which", "returns", "a", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "when", "called", "if", "none", "you", "can", "pass", "a", "registered", "model", "with", "model", "mymodel", "datamodule_class", "an", "optional", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "class", "or", "a", "callable", "which", "returns", "a", "class", "lightning", "pytorch", "core", "datamodule", "lightningdatamodule", "instance", "when", "called", "if", "none", "you", "can", "pass", "a", "registered", "datamodule", "with", "data", "mydatamodule", "save_config_callback", "a", "callback", "class", "to", "save", "the", "config", "save_config_kwargs", "parameters", "that", "will", "be", "used", "to", "instantiate", "the", "save_config_callback", "trainer_class", "an", "optional", "subclass", "of", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "class", "or", "a", "callable", "which", "returns", "a", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "when", "called", "trainer_defaults", "set", "to", "override", "trainer", "defaults", "or", "add", "persistent", "callbacks", "the", "callbacks", "added", "through", "this", "argument", "will", "not", "be", "configurable", "from", "a", "configuration", "file", "and", "will", "always", "be", "present", "for", "this", "particular", "cli", "alternatively", "configurable", "callbacks", "can", "be", "added", "as", "explained", "in", "ref", "the", "cli", "docs", "lightning", "cli", "seed_everything_default", "number", "for", "the", "func", "lightning", "fabric", "utilities", "seed", "seed_everything", "seed", "value", "set", "to", "true", "to", "automatically", "choose", "a", "seed", "value", "setting", "it", "to", "false", "will", "avoid", "calling", "seed_everything", "parser_kwargs", "additional", "arguments", "to", "instantiate", "each", "lightningargumentparser", "subclass_mode_model", "whether", "model", "can", "be", "any", "subclass", "https", "jsonargparse", "readthedocs", "io", "en", "stable", "class", "type", "and", "sub", "classes", "_", "of", "the", "given", "class", "subclass_mode_data", "whether", "datamodule", "can", "be", "any", "subclass", "https", "jsonargparse", "readthedocs", "io", "en", "stable", "class", "type", "and", "sub", "classes", "_", "of", "the", "given", "class", "args", "arguments", "to", "parse", "if", "none", "the", "arguments", "are", "taken", "from", "sys", "argv", "command", "line", "style", "arguments", "can", "be", "given", "in", "a", "list", "alternatively", "structured", "config", "options", "can", "be", "given", "in", "a", "dict", "or", "jsonargparse", "namespace", "run", "whether", "subcommands", "should", "be", "added", "to", "run", "a", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "method", "if", "set", "to", "false", "the", "trainer", "and", "model", "classes", "will", "be", "instantiated", "only", "auto_configure_optimizers", "whether", "to", "automatically", "add", "default", "optimizer", "and", "lr_scheduler", "arguments", "load_from_checkpoint_support", "whether", "save_hyperparameters", "should", "save", "the", "original", "parsed", "hyperparameters", "instead", "of", "what", "__init__", "receives", "such", "that", "it", "is", "possible", "for", "load_from_checkpoint", "to", "correctly", "instantiate", "classes", "even", "when", "using", "complex", "nesting", "and", "dependency", "injection"], "docstring_summary": "Receives as input pytorch-lightning classes (or callables which return pytorch-lightning classes), which are", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 314, "end_line": 413, "hash": "fb3f44edc7950c4b02d67785709371b0", "complexity": 11, "parameters": ["model_class", "Callable[...", "LightningModule]]]", "datamodule_class", "Callable[...", "LightningDataModule]]]", "save_config_callback", "save_config_kwargs", "Any]]", "trainer_class", "Callable[...", "Trainer]]", "trainer_defaults", "Any]]", "seed_everything_default", "int]", "parser_kwargs", "Any]", "dict[str", "dict[str", "Any]]]]", "parser_class", "subclass_mode_model", "subclass_mode_data", "args", "run", "auto_configure_optimizers", "load_from_checkpoint_support"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "init_parser", "original_string": "def init_parser(self, **kwargs: Any) -> LightningArgumentParser:\r\n        \"\"\"Method that instantiates the argument parser.\"\"\"\r\n        kwargs.setdefault(\"dump_header\", [f\"lightning.pytorch=={pl.__version__}\"])\r\n        parser = self.parser_class(**kwargs)\r\n        parser.add_argument(\r\n            \"-c\", \"--config\", action=ActionConfigFile, help=\"Path to a configuration file in json or yaml format.\"\r\n        )\r\n        return parser", "language": "python", "code": "def init_parser(self, **kwargs: Any) -> LightningArgumentParser:\r\n        \"\"\"Method that instantiates the argument parser.\"\"\"\r\n        kwargs.setdefault(\"dump_header\", [f\"lightning.pytorch=={pl.__version__}\"])\r\n        parser = self.parser_class(**kwargs)\r\n        parser.add_argument(\r\n            \"-c\", \"--config\", action=ActionConfigFile, help=\"Path to a configuration file in json or yaml format.\"\r\n        )\r\n        return parser", "code_tokens": ["def", "init_parser", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "LightningArgumentParser", ":", "\"", "\"", "\"", "Method", "that", "instantiates", "the", "argument", "parser", ".", "\"", "\"", "\"", "kwargs", ".", "setdefault", "(", "\"", "dump_header", "\"", ",", "[", "f", "\"", "lightning", ".", "pytorch", "=", "=", "{", "pl", ".", "__version__", "}", "\"", "]", ")", "parser", "=", "self", ".", "parser_class", "(", "*", "*", "kwargs", ")", "parser", ".", "add_argument", "(", "\"", "-", "c", "\"", ",", "\"", "-", "-", "config", "\"", ",", "action", "=", "ActionConfigFile", ",", "help", "=", "\"", "Path", "to", "a", "configuration", "file", "in", "json", "or", "yaml", "format", ".", "\"", ")", "return", "parser"], "docstring": "Method that instantiates the argument parser.", "docstring_tokens": ["method", "that", "instantiates", "the", "argument", "parser"], "docstring_summary": "Method that instantiates the argument parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 421, "end_line": 428, "hash": "4e2c863bd6e996701627a25c3f59e61b", "complexity": 1, "parameters": ["**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "setup_parser", "original_string": "def setup_parser(\r\n        self, add_subcommands: bool, main_kwargs: dict[str, Any], subparser_kwargs: dict[str, Any]\r\n    ) -> None:\r\n        \"\"\"Initialize and setup the parser, subcommands, and arguments.\"\"\"\r\n        self.parser = self.init_parser(**main_kwargs)\r\n        if add_subcommands:\r\n            self._subcommand_method_arguments: dict[str, list[str]] = {}\r\n            self._add_subcommands(self.parser, **subparser_kwargs)\r\n        else:\r\n            self._add_arguments(self.parser)", "language": "python", "code": "def setup_parser(\r\n        self, add_subcommands: bool, main_kwargs: dict[str, Any], subparser_kwargs: dict[str, Any]\r\n    ) -> None:\r\n        \"\"\"Initialize and setup the parser, subcommands, and arguments.\"\"\"\r\n        self.parser = self.init_parser(**main_kwargs)\r\n        if add_subcommands:\r\n            self._subcommand_method_arguments: dict[str, list[str]] = {}\r\n            self._add_subcommands(self.parser, **subparser_kwargs)\r\n        else:\r\n            self._add_arguments(self.parser)", "code_tokens": ["def", "setup_parser", "(", "self", ",", "add_subcommands", ":", "bool", ",", "main_kwargs", ":", "dict", "[", "str", ",", "Any", "]", ",", "subparser_kwargs", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Initialize", "and", "setup", "the", "parser", ",", "subcommands", ",", "and", "arguments", ".", "\"", "\"", "\"", "self", ".", "parser", "=", "self", ".", "init_parser", "(", "*", "*", "main_kwargs", ")", "if", "add_subcommands", ":", "self", ".", "_subcommand_method_arguments", ":", "dict", "[", "str", ",", "list", "[", "str", "]", "]", "=", "{", "}", "self", ".", "_add_subcommands", "(", "self", ".", "parser", ",", "*", "*", "subparser_kwargs", ")", "else", ":", "self", ".", "_add_arguments", "(", "self", ".", "parser", ")"], "docstring": "Initialize and setup the parser, subcommands, and arguments.", "docstring_tokens": ["initialize", "and", "setup", "the", "parser", "subcommands", "and", "arguments"], "docstring_summary": "Initialize and setup the parser, subcommands, and arguments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 430, "end_line": 439, "hash": "3fa821f2e876928ddb565669da581fdf", "complexity": 2, "parameters": ["add_subcommands", "main_kwargs", "Any]", "subparser_kwargs", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "add_default_arguments_to_parser", "original_string": "def add_default_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds default arguments to the parser.\"\"\"\r\n        parser.add_argument(\r\n            \"--seed_everything\",\r\n            type=Union[bool, int],\r\n            default=self.seed_everything_default,\r\n            help=(\r\n                \"Set to an int to run seed_everything with this value before classes instantiation.\"\r\n                \"Set to True to use a random seed.\"\r\n            ),\r\n        )", "language": "python", "code": "def add_default_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds default arguments to the parser.\"\"\"\r\n        parser.add_argument(\r\n            \"--seed_everything\",\r\n            type=Union[bool, int],\r\n            default=self.seed_everything_default,\r\n            help=(\r\n                \"Set to an int to run seed_everything with this value before classes instantiation.\"\r\n                \"Set to True to use a random seed.\"\r\n            ),\r\n        )", "code_tokens": ["def", "add_default_arguments_to_parser", "(", "self", ",", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adds", "default", "arguments", "to", "the", "parser", ".", "\"", "\"", "\"", "parser", ".", "add_argument", "(", "\"", "-", "-", "seed_everything", "\"", ",", "type", "=", "Union", "[", "bool", ",", "int", "]", ",", "default", "=", "self", ".", "seed_everything_default", ",", "help", "=", "(", "\"", "Set", "to", "an", "int", "to", "run", "seed_everything", "with", "this", "value", "before", "classes", "instantiation", ".", "\"", "\"", "Set", "to", "True", "to", "use", "a", "random", "seed", ".", "\"", ")", ",", ")"], "docstring": "Adds default arguments to the parser.", "docstring_tokens": ["adds", "default", "arguments", "to", "the", "parser"], "docstring_summary": "Adds default arguments to the parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 441, "end_line": 451, "hash": "04159c91c2c4d2016c501d2acb5070f8", "complexity": 1, "parameters": ["parser"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "add_core_arguments_to_parser", "original_string": "def add_core_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds arguments from the core classes to the parser.\"\"\"\r\n        parser.add_lightning_class_args(self.trainer_class, \"trainer\")\r\n        trainer_defaults = {\"trainer.\" + k: v for k, v in self.trainer_defaults.items() if k != \"callbacks\"}\r\n        parser.set_defaults(trainer_defaults)\r\n\r\n        parser.add_lightning_class_args(self._model_class, \"model\", subclass_mode=self.subclass_mode_model)\r\n\r\n        if self.datamodule_class is not None:\r\n            parser.add_lightning_class_args(self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data)\r\n        else:\r\n            # this should not be required because the user might want to use the `LightningModule` dataloaders\r\n            parser.add_lightning_class_args(\r\n                self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data, required=False\r\n            )", "language": "python", "code": "def add_core_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Adds arguments from the core classes to the parser.\"\"\"\r\n        parser.add_lightning_class_args(self.trainer_class, \"trainer\")\r\n        trainer_defaults = {\"trainer.\" + k: v for k, v in self.trainer_defaults.items() if k != \"callbacks\"}\r\n        parser.set_defaults(trainer_defaults)\r\n\r\n        parser.add_lightning_class_args(self._model_class, \"model\", subclass_mode=self.subclass_mode_model)\r\n\r\n        if self.datamodule_class is not None:\r\n            parser.add_lightning_class_args(self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data)\r\n        else:\r\n            # this should not be required because the user might want to use the `LightningModule` dataloaders\r\n            parser.add_lightning_class_args(\r\n                self._datamodule_class, \"data\", subclass_mode=self.subclass_mode_data, required=False\r\n            )", "code_tokens": ["def", "add_core_arguments_to_parser", "(", "self", ",", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adds", "arguments", "from", "the", "core", "classes", "to", "the", "parser", ".", "\"", "\"", "\"", "parser", ".", "add_lightning_class_args", "(", "self", ".", "trainer_class", ",", "\"", "trainer", "\"", ")", "trainer_defaults", "=", "{", "\"", "trainer", ".", "\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "trainer_defaults", ".", "items", "(", ")", "if", "k", "!", "=", "\"", "callbacks", "\"", "}", "parser", ".", "set_defaults", "(", "trainer_defaults", ")", "parser", ".", "add_lightning_class_args", "(", "self", ".", "_model_class", ",", "\"", "model", "\"", ",", "subclass_mode", "=", "self", ".", "subclass_mode_model", ")", "if", "self", ".", "datamodule_class", "is", "not", "None", ":", "parser", ".", "add_lightning_class_args", "(", "self", ".", "_datamodule_class", ",", "\"", "data", "\"", ",", "subclass_mode", "=", "self", ".", "subclass_mode_data", ")", "else", ":", "parser", ".", "add_lightning_class_args", "(", "self", ".", "_datamodule_class", ",", "\"", "data", "\"", ",", "subclass_mode", "=", "self", ".", "subclass_mode_data", ",", "required", "=", "False", ")"], "docstring": "Adds arguments from the core classes to the parser.", "docstring_tokens": ["adds", "arguments", "from", "the", "core", "classes", "to", "the", "parser"], "docstring_summary": "Adds arguments from the core classes to the parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 453, "end_line": 467, "hash": "3a3a9d5c6f2c4becdfdeff416a6828c3", "complexity": 4, "parameters": ["parser"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "add_arguments_to_parser", "original_string": "def add_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Implement to add extra arguments to the parser or link arguments.\r\n\r\n        Args:\r\n            parser: The parser object to which arguments can be added\r\n\r\n        \"\"\"", "language": "python", "code": "def add_arguments_to_parser(self, parser: LightningArgumentParser) -> None:\r\n        \"\"\"Implement to add extra arguments to the parser or link arguments.\r\n\r\n        Args:\r\n            parser: The parser object to which arguments can be added\r\n\r\n        \"\"\"", "code_tokens": ["def", "add_arguments_to_parser", "(", "self", ",", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Implement", "to", "add", "extra", "arguments", "to", "the", "parser", "or", "link", "arguments", ".", "Args", ":", "parser", ":", "The", "parser", "object", "to", "which", "arguments", "can", "be", "added", "\"", "\"", "\""], "docstring": "Implement to add extra arguments to the parser or link arguments.\r\n\r\n        Args:\r\n            parser: The parser object to which arguments can be added", "docstring_tokens": ["implement", "to", "add", "extra", "arguments", "to", "the", "parser", "or", "link", "arguments", "args", "parser", "the", "parser", "object", "to", "which", "arguments", "can", "be", "added"], "docstring_summary": "Implement to add extra arguments to the parser or link arguments.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 482, "end_line": 488, "hash": "d324fac59d93e0040cfc958fc5eb3d76", "complexity": 1, "parameters": ["parser"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "subcommands", "original_string": "def subcommands() -> dict[str, set[str]]:\r\n        \"\"\"Defines the list of available subcommands and the arguments to skip.\"\"\"\r\n        return {\r\n            \"fit\": {\"model\", \"train_dataloaders\", \"val_dataloaders\", \"datamodule\"},\r\n            \"validate\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"test\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"predict\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n        }", "language": "python", "code": "def subcommands() -> dict[str, set[str]]:\r\n        \"\"\"Defines the list of available subcommands and the arguments to skip.\"\"\"\r\n        return {\r\n            \"fit\": {\"model\", \"train_dataloaders\", \"val_dataloaders\", \"datamodule\"},\r\n            \"validate\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"test\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n            \"predict\": {\"model\", \"dataloaders\", \"datamodule\"},\r\n        }", "code_tokens": ["def", "subcommands", "(", ")", "-", ">", "dict", "[", "str", ",", "set", "[", "str", "]", "]", ":", "\"", "\"", "\"", "Defines", "the", "list", "of", "available", "subcommands", "and", "the", "arguments", "to", "skip", ".", "\"", "\"", "\"", "return", "{", "\"", "fit", "\"", ":", "{", "\"", "model", "\"", ",", "\"", "train_dataloaders", "\"", ",", "\"", "val_dataloaders", "\"", ",", "\"", "datamodule", "\"", "}", ",", "\"", "validate", "\"", ":", "{", "\"", "model", "\"", ",", "\"", "dataloaders", "\"", ",", "\"", "datamodule", "\"", "}", ",", "\"", "test", "\"", ":", "{", "\"", "model", "\"", ",", "\"", "dataloaders", "\"", ",", "\"", "datamodule", "\"", "}", ",", "\"", "predict", "\"", ":", "{", "\"", "model", "\"", ",", "\"", "dataloaders", "\"", ",", "\"", "datamodule", "\"", "}", ",", "}"], "docstring": "Defines the list of available subcommands and the arguments to skip.", "docstring_tokens": ["defines", "the", "list", "of", "available", "subcommands", "and", "the", "arguments", "to", "skip"], "docstring_summary": "Defines the list of available subcommands and the arguments to skip.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 491, "end_line": 498, "hash": "537fc5346d076ada95782af9196f8bbc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "_add_subcommands", "original_string": "def _add_subcommands(self, parser: LightningArgumentParser, **kwargs: Any) -> None:\r\n        \"\"\"Adds subcommands to the input parser.\"\"\"\r\n        self._subcommand_parsers: dict[str, LightningArgumentParser] = {}\r\n        parser_subcommands = parser.add_subcommands()\r\n        # the user might have passed a builder function\r\n        trainer_class = (\r\n            self.trainer_class if isinstance(self.trainer_class, type) else class_from_function(self.trainer_class)\r\n        )\r\n        # register all subcommands in separate subcommand parsers under the main parser\r\n        for subcommand in self.subcommands():\r\n            fn = getattr(trainer_class, subcommand)\r\n            # extract the first line description in the docstring for the subcommand help message\r\n            description = _get_short_description(fn)\r\n            subparser_kwargs = kwargs.get(subcommand, {})\r\n            subparser_kwargs.setdefault(\"description\", description)\r\n            subcommand_parser = self._prepare_subcommand_parser(trainer_class, subcommand, **subparser_kwargs)\r\n            self._subcommand_parsers[subcommand] = subcommand_parser\r\n            parser_subcommands.add_subcommand(subcommand, subcommand_parser, help=description)", "language": "python", "code": "def _add_subcommands(self, parser: LightningArgumentParser, **kwargs: Any) -> None:\r\n        \"\"\"Adds subcommands to the input parser.\"\"\"\r\n        self._subcommand_parsers: dict[str, LightningArgumentParser] = {}\r\n        parser_subcommands = parser.add_subcommands()\r\n        # the user might have passed a builder function\r\n        trainer_class = (\r\n            self.trainer_class if isinstance(self.trainer_class, type) else class_from_function(self.trainer_class)\r\n        )\r\n        # register all subcommands in separate subcommand parsers under the main parser\r\n        for subcommand in self.subcommands():\r\n            fn = getattr(trainer_class, subcommand)\r\n            # extract the first line description in the docstring for the subcommand help message\r\n            description = _get_short_description(fn)\r\n            subparser_kwargs = kwargs.get(subcommand, {})\r\n            subparser_kwargs.setdefault(\"description\", description)\r\n            subcommand_parser = self._prepare_subcommand_parser(trainer_class, subcommand, **subparser_kwargs)\r\n            self._subcommand_parsers[subcommand] = subcommand_parser\r\n            parser_subcommands.add_subcommand(subcommand, subcommand_parser, help=description)", "code_tokens": ["def", "_add_subcommands", "(", "self", ",", "parser", ":", "LightningArgumentParser", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adds", "subcommands", "to", "the", "input", "parser", ".", "\"", "\"", "\"", "self", ".", "_subcommand_parsers", ":", "dict", "[", "str", ",", "LightningArgumentParser", "]", "=", "{", "}", "parser_subcommands", "=", "parser", ".", "add_subcommands", "(", ")", "trainer_class", "=", "(", "self", ".", "trainer_class", "if", "isinstance", "(", "self", ".", "trainer_class", ",", "type", ")", "else", "class_from_function", "(", "self", ".", "trainer_class", ")", ")", "for", "subcommand", "in", "self", ".", "subcommands", "(", ")", ":", "fn", "=", "getattr", "(", "trainer_class", ",", "subcommand", ")", "description", "=", "_get_short_description", "(", "fn", ")", "subparser_kwargs", "=", "kwargs", ".", "get", "(", "subcommand", ",", "{", "}", ")", "subparser_kwargs", ".", "setdefault", "(", "\"", "description", "\"", ",", "description", ")", "subcommand_parser", "=", "self", ".", "_prepare_subcommand_parser", "(", "trainer_class", ",", "subcommand", ",", "*", "*", "subparser_kwargs", ")", "self", ".", "_subcommand_parsers", "[", "subcommand", "]", "=", "subcommand_parser", "parser_subcommands", ".", "add_subcommand", "(", "subcommand", ",", "subcommand_parser", ",", "help", "=", "description", ")"], "docstring": "Adds subcommands to the input parser.", "docstring_tokens": ["adds", "subcommands", "to", "the", "input", "parser"], "docstring_summary": "Adds subcommands to the input parser.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 500, "end_line": 517, "hash": "761e8568f201d0fdd9b968786ff640f3", "complexity": 3, "parameters": ["parser", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "link_optimizers_and_lr_schedulers", "original_string": "def link_optimizers_and_lr_schedulers(parser: LightningArgumentParser) -> None:\r\n        \"\"\"Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.\"\"\"\r\n        optimizers_and_lr_schedulers = {**parser._optimizers, **parser._lr_schedulers}\r\n        for key, (class_type, link_to) in optimizers_and_lr_schedulers.items():\r\n            if link_to == \"AUTOMATIC\":\r\n                continue\r\n            if isinstance(class_type, tuple):\r\n                parser.link_arguments(key, link_to)\r\n            else:\r\n                add_class_path = _add_class_path_generator(class_type)\r\n                parser.link_arguments(key, link_to, compute_fn=add_class_path)", "language": "python", "code": "def link_optimizers_and_lr_schedulers(parser: LightningArgumentParser) -> None:\r\n        \"\"\"Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.\"\"\"\r\n        optimizers_and_lr_schedulers = {**parser._optimizers, **parser._lr_schedulers}\r\n        for key, (class_type, link_to) in optimizers_and_lr_schedulers.items():\r\n            if link_to == \"AUTOMATIC\":\r\n                continue\r\n            if isinstance(class_type, tuple):\r\n                parser.link_arguments(key, link_to)\r\n            else:\r\n                add_class_path = _add_class_path_generator(class_type)\r\n                parser.link_arguments(key, link_to, compute_fn=add_class_path)", "code_tokens": ["def", "link_optimizers_and_lr_schedulers", "(", "parser", ":", "LightningArgumentParser", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Creates", "argument", "links", "for", "optimizers", "and", "learning", "rate", "schedulers", "that", "specified", "a", "`", "`", "link_to", "`", "`", ".", "\"", "\"", "\"", "optimizers_and_lr_schedulers", "=", "{", "*", "*", "parser", ".", "_optimizers", ",", "*", "*", "parser", ".", "_lr_schedulers", "}", "for", "key", ",", "(", "class_type", ",", "link_to", ")", "in", "optimizers_and_lr_schedulers", ".", "items", "(", ")", ":", "if", "link_to", "=", "=", "\"", "AUTOMATIC", "\"", ":", "continue", "if", "isinstance", "(", "class_type", ",", "tuple", ")", ":", "parser", ".", "link_arguments", "(", "key", ",", "link_to", ")", "else", ":", "add_class_path", "=", "_add_class_path_generator", "(", "class_type", ")", "parser", ".", "link_arguments", "(", "key", ",", "link_to", ",", "compute_fn", "=", "add_class_path", ")"], "docstring": "Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.", "docstring_tokens": ["creates", "argument", "links", "for", "optimizers", "and", "learning", "rate", "schedulers", "that", "specified", "a", "link_to"], "docstring_summary": "Creates argument links for optimizers and learning rate schedulers that specified a ``link_to``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 530, "end_line": 540, "hash": "5c86b7288877ece9518dfe69977d8995", "complexity": 4, "parameters": ["parser"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "parse_arguments", "original_string": "def parse_arguments(self, parser: LightningArgumentParser, args: ArgsType) -> None:\r\n        \"\"\"Parses command line arguments and stores it in ``self.config``.\"\"\"\r\n        if args is not None and len(sys.argv) > 1:\r\n            rank_zero_warn(\r\n                \"LightningCLI's args parameter is intended to run from within Python like if it were from the command \"\r\n                \"line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: \"\r\n                f\"sys.argv[1:]={sys.argv[1:]}, args={args}.\"\r\n            )\r\n        if isinstance(args, (dict, Namespace)):\r\n            self.config = parser.parse_object(args)\r\n        else:\r\n            self.config = parser.parse_args(args)", "language": "python", "code": "def parse_arguments(self, parser: LightningArgumentParser, args: ArgsType) -> None:\r\n        \"\"\"Parses command line arguments and stores it in ``self.config``.\"\"\"\r\n        if args is not None and len(sys.argv) > 1:\r\n            rank_zero_warn(\r\n                \"LightningCLI's args parameter is intended to run from within Python like if it were from the command \"\r\n                \"line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: \"\r\n                f\"sys.argv[1:]={sys.argv[1:]}, args={args}.\"\r\n            )\r\n        if isinstance(args, (dict, Namespace)):\r\n            self.config = parser.parse_object(args)\r\n        else:\r\n            self.config = parser.parse_args(args)", "code_tokens": ["def", "parse_arguments", "(", "self", ",", "parser", ":", "LightningArgumentParser", ",", "args", ":", "ArgsType", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Parses", "command", "line", "arguments", "and", "stores", "it", "in", "`", "`", "self", ".", "config", "`", "`", ".", "\"", "\"", "\"", "if", "args", "is", "not", "None", "and", "len", "(", "sys", ".", "argv", ")", ">", "1", ":", "rank_zero_warn", "(", "\"", "LightningCLI", "'", "s", "args", "parameter", "is", "intended", "to", "run", "from", "within", "Python", "like", "if", "it", "were", "from", "the", "command", "\"", "\"", "line", ".", "To", "prevent", "mistakes", "it", "is", "not", "recommended", "to", "provide", "both", "args", "and", "command", "line", "arguments", ",", "got", ":", "\"", "f", "\"", "sys", ".", "argv", "[", "1", ":", "]", "=", "{", "sys", ".", "argv", "[", "1", ":", "]", "}", ",", "args", "=", "{", "args", "}", ".", "\"", ")", "if", "isinstance", "(", "args", ",", "(", "dict", ",", "Namespace", ")", ")", ":", "self", ".", "config", "=", "parser", ".", "parse_object", "(", "args", ")", "else", ":", "self", ".", "config", "=", "parser", ".", "parse_args", "(", "args", ")"], "docstring": "Parses command line arguments and stores it in ``self.config``.", "docstring_tokens": ["parses", "command", "line", "arguments", "and", "stores", "it", "in", "self", "config"], "docstring_summary": "Parses command line arguments and stores it in ``self.config``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 542, "end_line": 553, "hash": "48329f52d3e21d3e0b6b23f2d818b10e", "complexity": 4, "parameters": ["parser", "args"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "_parse_ckpt_path", "original_string": "def _parse_ckpt_path(self) -> None:\r\n        \"\"\"If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.\"\"\"\r\n        if not self.config.get(\"subcommand\"):\r\n            return\r\n        ckpt_path = self.config[self.config.subcommand].get(\"ckpt_path\")\r\n        if ckpt_path and Path(ckpt_path).is_file():\r\n            ckpt = torch.load(ckpt_path, weights_only=True, map_location=\"cpu\")\r\n            hparams = ckpt.get(\"hyper_parameters\", {})\r\n            hparams.pop(\"_instantiator\", None)\r\n            if not hparams:\r\n                return\r\n            if \"_class_path\" in hparams:\r\n                hparams = {\r\n                    \"class_path\": hparams.pop(\"_class_path\"),\r\n                    \"dict_kwargs\": hparams,\r\n                }\r\n            hparams = {self.config.subcommand: {\"model\": hparams}}\r\n            try:\r\n                self.config = self.parser.parse_object(hparams, self.config)\r\n            except SystemExit:\r\n                sys.stderr.write(\"Parsing of ckpt_path hyperparameters failed!\\n\")\r\n                raise", "language": "python", "code": "def _parse_ckpt_path(self) -> None:\r\n        \"\"\"If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.\"\"\"\r\n        if not self.config.get(\"subcommand\"):\r\n            return\r\n        ckpt_path = self.config[self.config.subcommand].get(\"ckpt_path\")\r\n        if ckpt_path and Path(ckpt_path).is_file():\r\n            ckpt = torch.load(ckpt_path, weights_only=True, map_location=\"cpu\")\r\n            hparams = ckpt.get(\"hyper_parameters\", {})\r\n            hparams.pop(\"_instantiator\", None)\r\n            if not hparams:\r\n                return\r\n            if \"_class_path\" in hparams:\r\n                hparams = {\r\n                    \"class_path\": hparams.pop(\"_class_path\"),\r\n                    \"dict_kwargs\": hparams,\r\n                }\r\n            hparams = {self.config.subcommand: {\"model\": hparams}}\r\n            try:\r\n                self.config = self.parser.parse_object(hparams, self.config)\r\n            except SystemExit:\r\n                sys.stderr.write(\"Parsing of ckpt_path hyperparameters failed!\\n\")\r\n                raise", "code_tokens": ["def", "_parse_ckpt_path", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "If", "a", "checkpoint", "path", "is", "given", ",", "parse", "the", "hyperparameters", "from", "the", "checkpoint", "and", "update", "the", "config", ".", "\"", "\"", "\"", "if", "not", "self", ".", "config", ".", "get", "(", "\"", "subcommand", "\"", ")", ":", "return", "ckpt_path", "=", "self", ".", "config", "[", "self", ".", "config", ".", "subcommand", "]", ".", "get", "(", "\"", "ckpt_path", "\"", ")", "if", "ckpt_path", "and", "Path", "(", "ckpt_path", ")", ".", "is_file", "(", ")", ":", "ckpt", "=", "torch", ".", "load", "(", "ckpt_path", ",", "weights_only", "=", "True", ",", "map_location", "=", "\"", "cpu", "\"", ")", "hparams", "=", "ckpt", ".", "get", "(", "\"", "hyper_parameters", "\"", ",", "{", "}", ")", "hparams", ".", "pop", "(", "\"", "_instantiator", "\"", ",", "None", ")", "if", "not", "hparams", ":", "return", "if", "\"", "_class_path", "\"", "in", "hparams", ":", "hparams", "=", "{", "\"", "class_path", "\"", ":", "hparams", ".", "pop", "(", "\"", "_class_path", "\"", ")", ",", "\"", "dict_kwargs", "\"", ":", "hparams", ",", "}", "hparams", "=", "{", "self", ".", "config", ".", "subcommand", ":", "{", "\"", "model", "\"", ":", "hparams", "}", "}", "try", ":", "self", ".", "config", "=", "self", ".", "parser", ".", "parse_object", "(", "hparams", ",", "self", ".", "config", ")", "except", "SystemExit", ":", "sys", ".", "stderr", ".", "write", "(", "\"", "Parsing", "of", "ckpt_path", "hyperparameters", "failed", "!", "\\", "n", "\"", ")", "raise"], "docstring": "If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.", "docstring_tokens": ["if", "a", "checkpoint", "path", "is", "given", "parse", "the", "hyperparameters", "from", "the", "checkpoint", "and", "update", "the", "config"], "docstring_summary": "If a checkpoint path is given, parse the hyperparameters from the checkpoint and update the config.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 555, "end_line": 576, "hash": "17353e2cea8094bdf66f879abfd632ca", "complexity": 7, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "instantiate_classes", "original_string": "def instantiate_classes(self) -> None:\r\n        \"\"\"Instantiates the classes and sets their attributes.\"\"\"\r\n        self.config_init = self.parser.instantiate_classes(self.config)\r\n        self.datamodule = self._get(self.config_init, \"data\")\r\n        self.model = self._get(self.config_init, \"model\")\r\n        self._add_configure_optimizers_method_to_model(self.subcommand)\r\n        self.trainer = self.instantiate_trainer()", "language": "python", "code": "def instantiate_classes(self) -> None:\r\n        \"\"\"Instantiates the classes and sets their attributes.\"\"\"\r\n        self.config_init = self.parser.instantiate_classes(self.config)\r\n        self.datamodule = self._get(self.config_init, \"data\")\r\n        self.model = self._get(self.config_init, \"model\")\r\n        self._add_configure_optimizers_method_to_model(self.subcommand)\r\n        self.trainer = self.instantiate_trainer()", "code_tokens": ["def", "instantiate_classes", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Instantiates", "the", "classes", "and", "sets", "their", "attributes", ".", "\"", "\"", "\"", "self", ".", "config_init", "=", "self", ".", "parser", ".", "instantiate_classes", "(", "self", ".", "config", ")", "self", ".", "datamodule", "=", "self", ".", "_get", "(", "self", ".", "config_init", ",", "\"", "data", "\"", ")", "self", ".", "model", "=", "self", ".", "_get", "(", "self", ".", "config_init", ",", "\"", "model", "\"", ")", "self", ".", "_add_configure_optimizers_method_to_model", "(", "self", ".", "subcommand", ")", "self", ".", "trainer", "=", "self", ".", "instantiate_trainer", "(", ")"], "docstring": "Instantiates the classes and sets their attributes.", "docstring_tokens": ["instantiates", "the", "classes", "and", "sets", "their", "attributes"], "docstring_summary": "Instantiates the classes and sets their attributes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 602, "end_line": 608, "hash": "bdb1a7037b0590b7bb7764e555aa96ff", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "instantiate_trainer", "original_string": "def instantiate_trainer(self, **kwargs: Any) -> Trainer:\r\n        \"\"\"Instantiates the trainer.\r\n\r\n        Args:\r\n            kwargs: Any custom trainer arguments.\r\n\r\n        \"\"\"\r\n        extra_callbacks = [self._get(self.config_init, c) for c in self._parser(self.subcommand).callback_keys]\r\n        trainer_config = {**self._get(self.config_init, \"trainer\", default={}), **kwargs}\r\n        return self._instantiate_trainer(trainer_config, extra_callbacks)", "language": "python", "code": "def instantiate_trainer(self, **kwargs: Any) -> Trainer:\r\n        \"\"\"Instantiates the trainer.\r\n\r\n        Args:\r\n            kwargs: Any custom trainer arguments.\r\n\r\n        \"\"\"\r\n        extra_callbacks = [self._get(self.config_init, c) for c in self._parser(self.subcommand).callback_keys]\r\n        trainer_config = {**self._get(self.config_init, \"trainer\", default={}), **kwargs}\r\n        return self._instantiate_trainer(trainer_config, extra_callbacks)", "code_tokens": ["def", "instantiate_trainer", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Trainer", ":", "\"", "\"", "\"", "Instantiates", "the", "trainer", ".", "Args", ":", "kwargs", ":", "Any", "custom", "trainer", "arguments", ".", "\"", "\"", "\"", "extra_callbacks", "=", "[", "self", ".", "_get", "(", "self", ".", "config_init", ",", "c", ")", "for", "c", "in", "self", ".", "_parser", "(", "self", ".", "subcommand", ")", ".", "callback_keys", "]", "trainer_config", "=", "{", "*", "*", "self", ".", "_get", "(", "self", ".", "config_init", ",", "\"", "trainer", "\"", ",", "default", "=", "{", "}", ")", ",", "*", "*", "kwargs", "}", "return", "self", ".", "_instantiate_trainer", "(", "trainer_config", ",", "extra_callbacks", ")"], "docstring": "Instantiates the trainer.\r\n\r\n        Args:\r\n            kwargs: Any custom trainer arguments.", "docstring_tokens": ["instantiates", "the", "trainer", "args", "kwargs", "any", "custom", "trainer", "arguments"], "docstring_summary": "Instantiates the trainer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 613, "end_line": 622, "hash": "25344a2dc9d4746ffedef58e0969babe", "complexity": 2, "parameters": ["**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "configure_optimizers", "original_string": "def configure_optimizers(\r\n        lightning_module: LightningModule, optimizer: Optimizer, lr_scheduler: Optional[LRSchedulerTypeUnion] = None\r\n    ) -> Any:\r\n        \"\"\"Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.\r\n\r\n        Args:\r\n            lightning_module: A reference to the model.\r\n            optimizer: The optimizer.\r\n            lr_scheduler: The learning rate scheduler (if used).\r\n\r\n        \"\"\"\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        if isinstance(lr_scheduler, ReduceLROnPlateau):\r\n            return {\r\n                \"optimizer\": optimizer,\r\n                \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"monitor\": lr_scheduler.monitor},\r\n            }\r\n        return [optimizer], [lr_scheduler]", "language": "python", "code": "def configure_optimizers(\r\n        lightning_module: LightningModule, optimizer: Optimizer, lr_scheduler: Optional[LRSchedulerTypeUnion] = None\r\n    ) -> Any:\r\n        \"\"\"Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.\r\n\r\n        Args:\r\n            lightning_module: A reference to the model.\r\n            optimizer: The optimizer.\r\n            lr_scheduler: The learning rate scheduler (if used).\r\n\r\n        \"\"\"\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        if isinstance(lr_scheduler, ReduceLROnPlateau):\r\n            return {\r\n                \"optimizer\": optimizer,\r\n                \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"monitor\": lr_scheduler.monitor},\r\n            }\r\n        return [optimizer], [lr_scheduler]", "code_tokens": ["def", "configure_optimizers", "(", "lightning_module", ":", "LightningModule", ",", "optimizer", ":", "Optimizer", ",", "lr_scheduler", ":", "Optional", "[", "LRSchedulerTypeUnion", "]", "=", "None", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Override", "to", "customize", "the", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "configure_optimizers", "`", "method", ".", "Args", ":", "lightning_module", ":", "A", "reference", "to", "the", "model", ".", "optimizer", ":", "The", "optimizer", ".", "lr_scheduler", ":", "The", "learning", "rate", "scheduler", "(", "if", "used", ")", ".", "\"", "\"", "\"", "if", "lr_scheduler", "is", "None", ":", "return", "optimizer", "if", "isinstance", "(", "lr_scheduler", ",", "ReduceLROnPlateau", ")", ":", "return", "{", "\"", "optimizer", "\"", ":", "optimizer", ",", "\"", "lr_scheduler", "\"", ":", "{", "\"", "scheduler", "\"", ":", "lr_scheduler", ",", "\"", "monitor", "\"", ":", "lr_scheduler", ".", "monitor", "}", ",", "}", "return", "[", "optimizer", "]", ",", "[", "lr_scheduler", "]"], "docstring": "Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.\r\n\r\n        Args:\r\n            lightning_module: A reference to the model.\r\n            optimizer: The optimizer.\r\n            lr_scheduler: The learning rate scheduler (if used).", "docstring_tokens": ["override", "to", "customize", "the", "meth", "lightning", "pytorch", "core", "lightningmodule", "configure_optimizers", "method", "args", "lightning_module", "a", "reference", "to", "the", "model", "optimizer", "the", "optimizer", "lr_scheduler", "the", "learning", "rate", "scheduler", "if", "used"], "docstring_summary": "Override to customize the :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers` method.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 656, "end_line": 674, "hash": "e8ee4a2460c28e5f19f5731a8d0f7b20", "complexity": 3, "parameters": ["lightning_module", "optimizer", "lr_scheduler"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "_run_subcommand", "original_string": "def _run_subcommand(self, subcommand: str) -> None:\r\n        \"\"\"Run the chosen subcommand.\"\"\"\r\n        before_fn = getattr(self, f\"before_{subcommand}\", None)\r\n        if callable(before_fn):\r\n            before_fn()\r\n\r\n        default = getattr(self.trainer, subcommand)\r\n        fn = getattr(self, subcommand, default)\r\n        fn_kwargs = self._prepare_subcommand_kwargs(subcommand)\r\n        fn(**fn_kwargs)\r\n\r\n        after_fn = getattr(self, f\"after_{subcommand}\", None)\r\n        if callable(after_fn):\r\n            after_fn()", "language": "python", "code": "def _run_subcommand(self, subcommand: str) -> None:\r\n        \"\"\"Run the chosen subcommand.\"\"\"\r\n        before_fn = getattr(self, f\"before_{subcommand}\", None)\r\n        if callable(before_fn):\r\n            before_fn()\r\n\r\n        default = getattr(self.trainer, subcommand)\r\n        fn = getattr(self, subcommand, default)\r\n        fn_kwargs = self._prepare_subcommand_kwargs(subcommand)\r\n        fn(**fn_kwargs)\r\n\r\n        after_fn = getattr(self, f\"after_{subcommand}\", None)\r\n        if callable(after_fn):\r\n            after_fn()", "code_tokens": ["def", "_run_subcommand", "(", "self", ",", "subcommand", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Run", "the", "chosen", "subcommand", ".", "\"", "\"", "\"", "before_fn", "=", "getattr", "(", "self", ",", "f", "\"", "before_", "{", "subcommand", "}", "\"", ",", "None", ")", "if", "callable", "(", "before_fn", ")", ":", "before_fn", "(", ")", "default", "=", "getattr", "(", "self", ".", "trainer", ",", "subcommand", ")", "fn", "=", "getattr", "(", "self", ",", "subcommand", ",", "default", ")", "fn_kwargs", "=", "self", ".", "_prepare_subcommand_kwargs", "(", "subcommand", ")", "fn", "(", "*", "*", "fn_kwargs", ")", "after_fn", "=", "getattr", "(", "self", ",", "f", "\"", "after_", "{", "subcommand", "}", "\"", ",", "None", ")", "if", "callable", "(", "after_fn", ")", ":", "after_fn", "(", ")"], "docstring": "Run the chosen subcommand.", "docstring_tokens": ["run", "the", "chosen", "subcommand"], "docstring_summary": "Run the chosen subcommand.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 742, "end_line": 755, "hash": "f64973144b8e12b5f6f9b56a84b2ddc1", "complexity": 3, "parameters": ["subcommand"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "_prepare_subcommand_kwargs", "original_string": "def _prepare_subcommand_kwargs(self, subcommand: str) -> dict[str, Any]:\r\n        \"\"\"Prepares the keyword arguments to pass to the subcommand to run.\"\"\"\r\n        fn_kwargs = {\r\n            k: v for k, v in self.config_init[subcommand].items() if k in self._subcommand_method_arguments[subcommand]\r\n        }\r\n        fn_kwargs[\"model\"] = self.model\r\n        if self.datamodule is not None:\r\n            fn_kwargs[\"datamodule\"] = self.datamodule\r\n        return fn_kwargs", "language": "python", "code": "def _prepare_subcommand_kwargs(self, subcommand: str) -> dict[str, Any]:\r\n        \"\"\"Prepares the keyword arguments to pass to the subcommand to run.\"\"\"\r\n        fn_kwargs = {\r\n            k: v for k, v in self.config_init[subcommand].items() if k in self._subcommand_method_arguments[subcommand]\r\n        }\r\n        fn_kwargs[\"model\"] = self.model\r\n        if self.datamodule is not None:\r\n            fn_kwargs[\"datamodule\"] = self.datamodule\r\n        return fn_kwargs", "code_tokens": ["def", "_prepare_subcommand_kwargs", "(", "self", ",", "subcommand", ":", "str", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Prepares", "the", "keyword", "arguments", "to", "pass", "to", "the", "subcommand", "to", "run", ".", "\"", "\"", "\"", "fn_kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "config_init", "[", "subcommand", "]", ".", "items", "(", ")", "if", "k", "in", "self", ".", "_subcommand_method_arguments", "[", "subcommand", "]", "}", "fn_kwargs", "[", "\"", "model", "\"", "]", "=", "self", ".", "model", "if", "self", ".", "datamodule", "is", "not", "None", ":", "fn_kwargs", "[", "\"", "datamodule", "\"", "]", "=", "self", ".", "datamodule", "return", "fn_kwargs"], "docstring": "Prepares the keyword arguments to pass to the subcommand to run.", "docstring_tokens": ["prepares", "the", "keyword", "arguments", "to", "pass", "to", "the", "subcommand", "to", "run"], "docstring_summary": "Prepares the keyword arguments to pass to the subcommand to run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "class_method", "class_name": "LightningCLI", "start_line": 757, "end_line": 765, "hash": "ec6b4e74c76f10c1a518e3f30cab234a", "complexity": 4, "parameters": ["subcommand"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\cli.py", "func_name": "instantiate_class", "original_string": "def instantiate_class(args: Union[Any, tuple[Any, ...]], init: dict[str, Any]) -> Any:\r\n    \"\"\"Instantiates a class with the given args and init.\r\n\r\n    Args:\r\n        args: Positional arguments required for instantiation.\r\n        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\r\n\r\n    Returns:\r\n        The instantiated class object.\r\n\r\n    \"\"\"\r\n    kwargs = init.get(\"init_args\", {})\r\n    if not isinstance(args, tuple):\r\n        args = (args,)\r\n    class_module, class_name = init[\"class_path\"].rsplit(\".\", 1)\r\n    module = __import__(class_module, fromlist=[class_name])\r\n    args_class = getattr(module, class_name)\r\n    return args_class(*args, **kwargs)", "language": "python", "code": "def instantiate_class(args: Union[Any, tuple[Any, ...]], init: dict[str, Any]) -> Any:\r\n    \"\"\"Instantiates a class with the given args and init.\r\n\r\n    Args:\r\n        args: Positional arguments required for instantiation.\r\n        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\r\n\r\n    Returns:\r\n        The instantiated class object.\r\n\r\n    \"\"\"\r\n    kwargs = init.get(\"init_args\", {})\r\n    if not isinstance(args, tuple):\r\n        args = (args,)\r\n    class_module, class_name = init[\"class_path\"].rsplit(\".\", 1)\r\n    module = __import__(class_module, fromlist=[class_name])\r\n    args_class = getattr(module, class_name)\r\n    return args_class(*args, **kwargs)", "code_tokens": ["def", "instantiate_class", "(", "args", ":", "Union", "[", "Any", ",", "tuple", "[", "Any", ",", ".", ".", ".", "]", "]", ",", "init", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Instantiates", "a", "class", "with", "the", "given", "args", "and", "init", ".", "Args", ":", "args", ":", "Positional", "arguments", "required", "for", "instantiation", ".", "init", ":", "Dict", "of", "the", "form", "{", "\"", "class_path", "\"", ":", ".", ".", ".", ",", "\"", "init_args", "\"", ":", ".", ".", ".", "}", ".", "Returns", ":", "The", "instantiated", "class", "object", ".", "\"", "\"", "\"", "kwargs", "=", "init", ".", "get", "(", "\"", "init_args", "\"", ",", "{", "}", ")", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "args", "=", "(", "args", ",", ")", "class_module", ",", "class_name", "=", "init", "[", "\"", "class_path", "\"", "]", ".", "rsplit", "(", "\"", ".", "\"", ",", "1", ")", "module", "=", "__import__", "(", "class_module", ",", "fromlist", "=", "[", "class_name", "]", ")", "args_class", "=", "getattr", "(", "module", ",", "class_name", ")", "return", "args_class", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Instantiates a class with the given args and init.\r\n\r\n    Args:\r\n        args: Positional arguments required for instantiation.\r\n        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\r\n\r\n    Returns:\r\n        The instantiated class object.", "docstring_tokens": ["instantiates", "a", "class", "with", "the", "given", "args", "and", "init", "args", "args", "positional", "arguments", "required", "for", "instantiation", "init", "dict", "of", "the", "form", "class_path", "init_args", "returns", "the", "instantiated", "class", "object"], "docstring_summary": "Instantiates a class with the given args and init.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\cli.py", "partition": "train", "function_type": "function", "start_line": 802, "end_line": 819, "hash": "961e20310e2ab1cb1205c4eb9902688d", "complexity": 2, "parameters": ["args", "tuple[Any", "...]]", "init", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\accelerator.py", "func_name": "setup", "original_string": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Called by the Trainer to set up the accelerator before the model starts running on the device.\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Called by the Trainer to set up the accelerator before the model starts running on the device.\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "by", "the", "Trainer", "to", "set", "up", "the", "accelerator", "before", "the", "model", "starts", "running", "on", "the", "device", ".", "Args", ":", "trainer", ":", "the", "trainer", "instance", "\"", "\"", "\""], "docstring": "Called by the Trainer to set up the accelerator before the model starts running on the device.\r\n\r\n        Args:\r\n            trainer: the trainer instance", "docstring_tokens": ["called", "by", "the", "trainer", "to", "set", "up", "the", "accelerator", "before", "the", "model", "starts", "running", "on", "the", "device", "args", "trainer", "the", "trainer", "instance"], "docstring_summary": "Called by the Trainer to set up the accelerator before the model starts running on the device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\accelerator.py", "partition": "train", "function_type": "class_method", "class_name": "Accelerator", "start_line": 28, "end_line": 34, "hash": "7cd0695d9c3cf448a78c5f90371764f7", "complexity": 1, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\accelerator.py", "func_name": "get_device_stats", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get stats for a given device.\r\n\r\n        Args:\r\n            device: device for which to get stats\r\n\r\n        Returns:\r\n            Dictionary of device stats\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Get stats for a given device.\r\n\r\n        Args:\r\n            device: device for which to get stats\r\n\r\n        Returns:\r\n            Dictionary of device stats\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Get", "stats", "for", "a", "given", "device", ".", "Args", ":", "device", ":", "device", "for", "which", "to", "get", "stats", "Returns", ":", "Dictionary", "of", "device", "stats", "\"", "\"", "\"", "raise", "NotImplementedError"], "docstring": "Get stats for a given device.\r\n\r\n        Args:\r\n            device: device for which to get stats\r\n\r\n        Returns:\r\n            Dictionary of device stats", "docstring_tokens": ["get", "stats", "for", "a", "given", "device", "args", "device", "device", "for", "which", "to", "get", "stats", "returns", "dictionary", "of", "device", "stats"], "docstring_summary": "Get stats for a given device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\accelerator.py", "partition": "train", "function_type": "class_method", "class_name": "Accelerator", "start_line": 36, "end_line": 46, "hash": "dc2b302486dea10808eb29c69ce93e21", "complexity": 1, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "setup_device", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise MisconfigurationException(f\"Device should be CPU, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not CPU.\r\n        \"\"\"\r\n        if device.type != \"cpu\":\r\n            raise MisconfigurationException(f\"Device should be CPU, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "MisconfigurationException", ":", "If", "the", "selected", "device", "is", "not", "CPU", ".", "\"", "\"", "\"", "if", "device", ".", "type", "!", "=", "\"", "cpu", "\"", ":", "raise", "MisconfigurationException", "(", "f", "\"", "Device", "should", "be", "CPU", ",", "got", "{", "device", "}", "instead", ".", "\"", ")"], "docstring": "Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not CPU.", "docstring_tokens": ["raises", "misconfigurationexception", "if", "the", "selected", "device", "is", "not", "cpu"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cpu.py", "partition": "train", "function_type": "class_method", "class_name": "CPUAccelerator", "start_line": 30, "end_line": 37, "hash": "fc4bb111e1709b96afb685988680f845", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cpu.py", "func_name": "get_parallel_devices", "original_string": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        devices = _parse_cpu_cores(devices)\r\n        return [torch.device(\"cpu\")] * devices", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "\"", "\"", "\"", "Gets", "parallel", "devices", "for", "the", "Accelerator", ".", "\"", "\"", "\"", "devices", "=", "_parse_cpu_cores", "(", "devices", ")", "return", "[", "torch", ".", "device", "(", "\"", "cpu", "\"", ")", "]", "*", "devices"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "docstring_summary": "Gets parallel devices for the Accelerator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cpu.py", "partition": "train", "function_type": "class_method", "class_name": "CPUAccelerator", "start_line": 56, "end_line": 59, "hash": "bb916317d3eb3b151f30cf9194f5dd0a", "complexity": 1, "parameters": ["devices", "str]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "setup_device", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not GPU.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise MisconfigurationException(f\"Device should be GPU, got {device} instead\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not GPU.\r\n        \"\"\"\r\n        if device.type != \"cuda\":\r\n            raise MisconfigurationException(f\"Device should be GPU, got {device} instead\")\r\n        _check_cuda_matmul_precision(device)\r\n        torch.cuda.set_device(device)", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "MisconfigurationException", ":", "If", "the", "selected", "device", "is", "not", "GPU", ".", "\"", "\"", "\"", "if", "device", ".", "type", "!", "=", "\"", "cuda", "\"", ":", "raise", "MisconfigurationException", "(", "f", "\"", "Device", "should", "be", "GPU", ",", "got", "{", "device", "}", "instead", "\"", ")", "_check_cuda_matmul_precision", "(", "device", ")", "torch", ".", "cuda", ".", "set_device", "(", "device", ")"], "docstring": "Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not GPU.", "docstring_tokens": ["raises", "misconfigurationexception", "if", "the", "selected", "device", "is", "not", "gpu"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cuda.py", "partition": "train", "function_type": "class_method", "class_name": "CUDAAccelerator", "start_line": 37, "end_line": 46, "hash": "d068185e1207aacd0ee043920d3e676f", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "get_device_stats", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given GPU device.\r\n\r\n        Args:\r\n            device: GPU device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics to their values.\r\n\r\n        Raises:\r\n            FileNotFoundError:\r\n                If nvidia-smi installation not found\r\n\r\n        \"\"\"\r\n        return torch.cuda.memory_stats(device)", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given GPU device.\r\n\r\n        Args:\r\n            device: GPU device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics to their values.\r\n\r\n        Raises:\r\n            FileNotFoundError:\r\n                If nvidia-smi installation not found\r\n\r\n        \"\"\"\r\n        return torch.cuda.memory_stats(device)", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Gets", "stats", "for", "the", "given", "GPU", "device", ".", "Args", ":", "device", ":", "GPU", "device", "for", "which", "to", "get", "stats", "Returns", ":", "A", "dictionary", "mapping", "the", "metrics", "to", "their", "values", ".", "Raises", ":", "FileNotFoundError", ":", "If", "nvidia", "-", "smi", "installation", "not", "found", "\"", "\"", "\"", "return", "torch", ".", "cuda", ".", "memory_stats", "(", "device", ")"], "docstring": "Gets stats for the given GPU device.\r\n\r\n        Args:\r\n            device: GPU device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics to their values.\r\n\r\n        Raises:\r\n            FileNotFoundError:\r\n                If nvidia-smi installation not found", "docstring_tokens": ["gets", "stats", "for", "the", "given", "gpu", "device", "args", "device", "gpu", "device", "for", "which", "to", "get", "stats", "returns", "a", "dictionary", "mapping", "the", "metrics", "to", "their", "values", "raises", "filenotfounderror", "if", "nvidia", "smi", "installation", "not", "found"], "docstring_summary": "Gets stats for the given GPU device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cuda.py", "partition": "train", "function_type": "class_method", "class_name": "CUDAAccelerator", "start_line": 63, "end_line": 77, "hash": "90cf0b69d016568e182aef37ace001c0", "complexity": 1, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "get_nvidia_gpu_stats", "original_string": "def get_nvidia_gpu_stats(device: _DEVICE) -> dict[str, float]:  # pragma: no-cover\r\n    \"\"\"Get GPU stats including memory, fan speed, and temperature from nvidia-smi.\r\n\r\n    Args:\r\n        device: GPU device for which to get stats\r\n\r\n    Returns:\r\n        A dictionary mapping the metrics to their values.\r\n\r\n    Raises:\r\n        FileNotFoundError:\r\n            If nvidia-smi installation not found\r\n\r\n    \"\"\"\r\n    nvidia_smi_path = shutil.which(\"nvidia-smi\")\r\n    if nvidia_smi_path is None:\r\n        raise FileNotFoundError(\"nvidia-smi: command not found\")\r\n\r\n    gpu_stat_metrics = [\r\n        (\"utilization.gpu\", \"%\"),\r\n        (\"memory.used\", \"MB\"),\r\n        (\"memory.free\", \"MB\"),\r\n        (\"utilization.memory\", \"%\"),\r\n        (\"fan.speed\", \"%\"),\r\n        (\"temperature.gpu\", \"\u00b0C\"),\r\n        (\"temperature.memory\", \"\u00b0C\"),\r\n    ]\r\n    gpu_stat_keys = [k for k, _ in gpu_stat_metrics]\r\n    gpu_query = \",\".join(gpu_stat_keys)\r\n\r\n    index = torch._utils._get_device_index(device)\r\n    gpu_id = _get_gpu_id(index)\r\n    result = subprocess.run(\r\n        [nvidia_smi_path, f\"--query-gpu={gpu_query}\", \"--format=csv,nounits,noheader\", f\"--id={gpu_id}\"],\r\n        encoding=\"utf-8\",\r\n        capture_output=True,\r\n        check=True,\r\n    )\r\n\r\n    def _to_float(x: str) -> float:\r\n        try:\r\n            return float(x)\r\n        except ValueError:\r\n            return 0.0\r\n\r\n    s = result.stdout.strip()\r\n    stats = [_to_float(x) for x in s.split(\", \")]\r\n    return {f\"{x} ({unit})\": stat for (x, unit), stat in zip(gpu_stat_metrics, stats)}", "language": "python", "code": "def get_nvidia_gpu_stats(device: _DEVICE) -> dict[str, float]:  # pragma: no-cover\r\n    \"\"\"Get GPU stats including memory, fan speed, and temperature from nvidia-smi.\r\n\r\n    Args:\r\n        device: GPU device for which to get stats\r\n\r\n    Returns:\r\n        A dictionary mapping the metrics to their values.\r\n\r\n    Raises:\r\n        FileNotFoundError:\r\n            If nvidia-smi installation not found\r\n\r\n    \"\"\"\r\n    nvidia_smi_path = shutil.which(\"nvidia-smi\")\r\n    if nvidia_smi_path is None:\r\n        raise FileNotFoundError(\"nvidia-smi: command not found\")\r\n\r\n    gpu_stat_metrics = [\r\n        (\"utilization.gpu\", \"%\"),\r\n        (\"memory.used\", \"MB\"),\r\n        (\"memory.free\", \"MB\"),\r\n        (\"utilization.memory\", \"%\"),\r\n        (\"fan.speed\", \"%\"),\r\n        (\"temperature.gpu\", \"\u00b0C\"),\r\n        (\"temperature.memory\", \"\u00b0C\"),\r\n    ]\r\n    gpu_stat_keys = [k for k, _ in gpu_stat_metrics]\r\n    gpu_query = \",\".join(gpu_stat_keys)\r\n\r\n    index = torch._utils._get_device_index(device)\r\n    gpu_id = _get_gpu_id(index)\r\n    result = subprocess.run(\r\n        [nvidia_smi_path, f\"--query-gpu={gpu_query}\", \"--format=csv,nounits,noheader\", f\"--id={gpu_id}\"],\r\n        encoding=\"utf-8\",\r\n        capture_output=True,\r\n        check=True,\r\n    )\r\n\r\n    def _to_float(x: str) -> float:\r\n        try:\r\n            return float(x)\r\n        except ValueError:\r\n            return 0.0\r\n\r\n    s = result.stdout.strip()\r\n    stats = [_to_float(x) for x in s.split(\", \")]\r\n    return {f\"{x} ({unit})\": stat for (x, unit), stat in zip(gpu_stat_metrics, stats)}", "code_tokens": ["def", "get_nvidia_gpu_stats", "(", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "float", "]", ":", "\"", "\"", "\"", "Get", "GPU", "stats", "including", "memory", ",", "fan", "speed", ",", "and", "temperature", "from", "nvidia", "-", "smi", ".", "Args", ":", "device", ":", "GPU", "device", "for", "which", "to", "get", "stats", "Returns", ":", "A", "dictionary", "mapping", "the", "metrics", "to", "their", "values", ".", "Raises", ":", "FileNotFoundError", ":", "If", "nvidia", "-", "smi", "installation", "not", "found", "\"", "\"", "\"", "nvidia_smi_path", "=", "shutil", ".", "which", "(", "\"", "nvidia", "-", "smi", "\"", ")", "if", "nvidia_smi_path", "is", "None", ":", "raise", "FileNotFoundError", "(", "\"", "nvidia", "-", "smi", ":", "command", "not", "found", "\"", ")", "gpu_stat_metrics", "=", "[", "(", "\"", "utilization", ".", "gpu", "\"", ",", "\"", "%", "\"", ")", ",", "(", "\"", "memory", ".", "used", "\"", ",", "\"", "MB", "\"", ")", ",", "(", "\"", "memory", ".", "free", "\"", ",", "\"", "MB", "\"", ")", ",", "(", "\"", "utilization", ".", "memory", "\"", ",", "\"", "%", "\"", ")", ",", "(", "\"", "fan", ".", "speed", "\"", ",", "\"", "%", "\"", ")", ",", "(", "\"", "temperature", ".", "gpu", "\"", ",", "\"", "\u00b0", "C", "\"", ")", ",", "(", "\"", "temperature", ".", "memory", "\"", ",", "\"", "\u00b0", "C", "\"", ")", ",", "]", "gpu_stat_keys", "=", "[", "k", "for", "k", ",", "_", "in", "gpu_stat_metrics", "]", "gpu_query", "=", "\"", ",", "\"", ".", "join", "(", "gpu_stat_keys", ")", "index", "=", "torch", ".", "_utils", ".", "_get_device_index", "(", "device", ")", "gpu_id", "=", "_get_gpu_id", "(", "index", ")", "result", "=", "subprocess", ".", "run", "(", "[", "nvidia_smi_path", ",", "f", "\"", "-", "-", "query", "-", "gpu", "=", "{", "gpu_query", "}", "\"", ",", "\"", "-", "-", "format", "=", "csv", ",", "nounits", ",", "noheader", "\"", ",", "f", "\"", "-", "-", "id", "=", "{", "gpu_id", "}", "\"", "]", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ",", "capture_output", "=", "True", ",", "check", "=", "True", ",", ")", "def", "_to_float", "(", "x", ":", "str", ")", "-", ">", "float", ":", "try", ":", "return", "float", "(", "x", ")", "except", "ValueError", ":", "return", "0", ".", "0", "s", "=", "result", ".", "stdout", ".", "strip", "(", ")", "stats", "=", "[", "_to_float", "(", "x", ")", "for", "x", "in", "s", ".", "split", "(", "\"", ",", "\"", ")", "]", "return", "{", "f", "\"", "{", "x", "}", "(", "{", "unit", "}", ")", "\"", ":", "stat", "for", "(", "x", ",", "unit", ")", ",", "stat", "in", "zip", "(", "gpu_stat_metrics", ",", "stats", ")", "}"], "docstring": "Get GPU stats including memory, fan speed, and temperature from nvidia-smi.\r\n\r\n    Args:\r\n        device: GPU device for which to get stats\r\n\r\n    Returns:\r\n        A dictionary mapping the metrics to their values.\r\n\r\n    Raises:\r\n        FileNotFoundError:\r\n            If nvidia-smi installation not found", "docstring_tokens": ["get", "gpu", "stats", "including", "memory", "fan", "speed", "and", "temperature", "from", "nvidia", "smi", "args", "device", "gpu", "device", "for", "which", "to", "get", "stats", "returns", "a", "dictionary", "mapping", "the", "metrics", "to", "their", "values", "raises", "filenotfounderror", "if", "nvidia", "smi", "installation", "not", "found"], "docstring_summary": "Get GPU stats including memory, fan speed, and temperature from nvidia-smi.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cuda.py", "partition": "train", "function_type": "function", "start_line": 116, "end_line": 163, "hash": "cf0242cb3988edd73bcda3aac864448f", "complexity": 6, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\cuda.py", "func_name": "_get_gpu_id", "original_string": "def _get_gpu_id(device_id: int) -> str:\r\n    \"\"\"Get the unmasked real GPU IDs.\"\"\"\r\n    # All devices if `CUDA_VISIBLE_DEVICES` unset\r\n    default = \",\".join(str(i) for i in range(num_cuda_devices()))\r\n    cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", default=default).split(\",\")\r\n    return cuda_visible_devices[device_id].strip()", "language": "python", "code": "def _get_gpu_id(device_id: int) -> str:\r\n    \"\"\"Get the unmasked real GPU IDs.\"\"\"\r\n    # All devices if `CUDA_VISIBLE_DEVICES` unset\r\n    default = \",\".join(str(i) for i in range(num_cuda_devices()))\r\n    cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", default=default).split(\",\")\r\n    return cuda_visible_devices[device_id].strip()", "code_tokens": ["def", "_get_gpu_id", "(", "device_id", ":", "int", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Get", "the", "unmasked", "real", "GPU", "IDs", ".", "\"", "\"", "\"", "default", "=", "\"", ",", "\"", ".", "join", "(", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_cuda_devices", "(", ")", ")", ")", "cuda_visible_devices", "=", "os", ".", "getenv", "(", "\"", "CUDA_VISIBLE_DEVICES", "\"", ",", "default", "=", "default", ")", ".", "split", "(", "\"", ",", "\"", ")", "return", "cuda_visible_devices", "[", "device_id", "]", ".", "strip", "(", ")"], "docstring": "Get the unmasked real GPU IDs.", "docstring_tokens": ["get", "the", "unmasked", "real", "gpu", "ids"], "docstring_summary": "Get the unmasked real GPU IDs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\cuda.py", "partition": "train", "function_type": "function", "start_line": 166, "end_line": 171, "hash": "96acd05b3c45583cd4f150528bc20d9a", "complexity": 2, "parameters": ["device_id"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "setup_device", "original_string": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise MisconfigurationException(f\"Device should be MPS, got {device} instead.\")", "language": "python", "code": "def setup_device(self, device: torch.device) -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not MPS.\r\n        \"\"\"\r\n        if device.type != \"mps\":\r\n            raise MisconfigurationException(f\"Device should be MPS, got {device} instead.\")", "code_tokens": ["def", "setup_device", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "MisconfigurationException", ":", "If", "the", "selected", "device", "is", "not", "MPS", ".", "\"", "\"", "\"", "if", "device", ".", "type", "!", "=", "\"", "mps", "\"", ":", "raise", "MisconfigurationException", "(", "f", "\"", "Device", "should", "be", "MPS", ",", "got", "{", "device", "}", "instead", ".", "\"", ")"], "docstring": "Raises:\r\n            MisconfigurationException:\r\n                If the selected device is not MPS.", "docstring_tokens": ["raises", "misconfigurationexception", "if", "the", "selected", "device", "is", "not", "mps"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\mps.py", "partition": "train", "function_type": "class_method", "class_name": "MPSAccelerator", "start_line": 35, "end_line": 42, "hash": "010e2e27ac1807a4212c0dfc23376315", "complexity": 2, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\mps.py", "func_name": "get_parallel_devices", "original_string": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "language": "python", "code": "def get_parallel_devices(devices: Union[int, str, list[int]]) -> list[torch.device]:\r\n        \"\"\"Gets parallel devices for the Accelerator.\"\"\"\r\n        parsed_devices = MPSAccelerator.parse_devices(devices)\r\n        assert parsed_devices is not None\r\n\r\n        return [torch.device(\"mps\", i) for i in range(len(parsed_devices))]", "code_tokens": ["def", "get_parallel_devices", "(", "devices", ":", "Union", "[", "int", ",", "str", ",", "list", "[", "int", "]", "]", ")", "-", ">", "list", "[", "torch", ".", "device", "]", ":", "\"", "\"", "\"", "Gets", "parallel", "devices", "for", "the", "Accelerator", ".", "\"", "\"", "\"", "parsed_devices", "=", "MPSAccelerator", ".", "parse_devices", "(", "devices", ")", "assert", "parsed_devices", "is", "not", "None", "return", "[", "torch", ".", "device", "(", "\"", "mps", "\"", ",", "i", ")", "for", "i", "in", "range", "(", "len", "(", "parsed_devices", ")", ")", "]"], "docstring": "Gets parallel devices for the Accelerator.", "docstring_tokens": ["gets", "parallel", "devices", "for", "the", "accelerator"], "docstring_summary": "Gets parallel devices for the Accelerator.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\mps.py", "partition": "train", "function_type": "class_method", "class_name": "MPSAccelerator", "start_line": 61, "end_line": 66, "hash": "739332f59523cf0f27bb8d6c58057d43", "complexity": 2, "parameters": ["devices", "str", "list[int]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\accelerators\\xla.py", "func_name": "get_device_stats", "original_string": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given XLA device.\r\n\r\n        Args:\r\n            device: XLA device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics (free memory and peak memory) to their values.\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        memory_info = xm.get_memory_info(device)\r\n        free_memory = memory_info[\"kb_free\"]\r\n        peak_memory = memory_info[\"kb_total\"] - free_memory\r\n        return {\r\n            \"avg. free memory (MB)\": free_memory,\r\n            \"avg. peak memory (MB)\": peak_memory,\r\n        }", "language": "python", "code": "def get_device_stats(self, device: _DEVICE) -> dict[str, Any]:\r\n        \"\"\"Gets stats for the given XLA device.\r\n\r\n        Args:\r\n            device: XLA device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics (free memory and peak memory) to their values.\r\n\r\n        \"\"\"\r\n        import torch_xla.core.xla_model as xm\r\n\r\n        memory_info = xm.get_memory_info(device)\r\n        free_memory = memory_info[\"kb_free\"]\r\n        peak_memory = memory_info[\"kb_total\"] - free_memory\r\n        return {\r\n            \"avg. free memory (MB)\": free_memory,\r\n            \"avg. peak memory (MB)\": peak_memory,\r\n        }", "code_tokens": ["def", "get_device_stats", "(", "self", ",", "device", ":", "_DEVICE", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Gets", "stats", "for", "the", "given", "XLA", "device", ".", "Args", ":", "device", ":", "XLA", "device", "for", "which", "to", "get", "stats", "Returns", ":", "A", "dictionary", "mapping", "the", "metrics", "(", "free", "memory", "and", "peak", "memory", ")", "to", "their", "values", ".", "\"", "\"", "\"", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "memory_info", "=", "xm", ".", "get_memory_info", "(", "device", ")", "free_memory", "=", "memory_info", "[", "\"", "kb_free", "\"", "]", "peak_memory", "=", "memory_info", "[", "\"", "kb_total", "\"", "]", "-", "free_memory", "return", "{", "\"", "avg", ".", "free", "memory", "(", "MB", ")", "\"", ":", "free_memory", ",", "\"", "avg", ".", "peak", "memory", "(", "MB", ")", "\"", ":", "peak_memory", ",", "}"], "docstring": "Gets stats for the given XLA device.\r\n\r\n        Args:\r\n            device: XLA device for which to get stats\r\n\r\n        Returns:\r\n            A dictionary mapping the metrics (free memory and peak memory) to their values.", "docstring_tokens": ["gets", "stats", "for", "the", "given", "xla", "device", "args", "device", "xla", "device", "for", "which", "to", "get", "stats", "returns", "a", "dictionary", "mapping", "the", "metrics", "free", "memory", "and", "peak", "memory", "to", "their", "values"], "docstring_summary": "Gets stats for the given XLA device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\accelerators\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAAccelerator", "start_line": 31, "end_line": 49, "hash": "2626bc0bcf764416d7c11beda6105794", "complexity": 1, "parameters": ["device"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "state_key", "original_string": "def state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__", "language": "python", "code": "def state_key(self) -> str:\r\n        \"\"\"Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.\r\n\r\n        \"\"\"\r\n        return self.__class__.__qualname__", "code_tokens": ["def", "state_key", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Identifier", "for", "the", "state", "of", "the", "callback", ".", "Used", "to", "store", "and", "retrieve", "a", "callback", "'", "s", "state", "from", "the", "checkpoint", "dictionary", "by", "`", "`", "checkpoint", "[", "\"", "callbacks", "\"", "]", "[", "state_key", "]", "`", "`", ".", "Implementations", "of", "a", "callback", "need", "to", "provide", "a", "unique", "state", "key", "if", "1", ")", "the", "callback", "has", "state", "and", "2", ")", "it", "is", "desired", "to", "maintain", "the", "state", "of", "multiple", "instances", "of", "that", "callback", ".", "\"", "\"", "\"", "return", "self", ".", "__class__", ".", "__qualname__"], "docstring": "Identifier for the state of the callback.\r\n\r\n        Used to store and retrieve a callback's state from the checkpoint dictionary by\r\n        ``checkpoint[\"callbacks\"][state_key]``. Implementations of a callback need to provide a unique state key if 1)\r\n        the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.", "docstring_tokens": ["identifier", "for", "the", "state", "of", "the", "callback", "used", "to", "store", "and", "retrieve", "a", "callback", "s", "state", "from", "the", "checkpoint", "dictionary", "by", "checkpoint", "callbacks", "state_key", "implementations", "of", "a", "callback", "need", "to", "provide", "a", "unique", "state", "key", "if", "1", "the", "callback", "has", "state", "and", "2", "it", "is", "desired", "to", "maintain", "the", "state", "of", "multiple", "instances", "of", "that", "callback"], "docstring_summary": "Identifier for the state of the callback.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 32, "end_line": 40, "hash": "b05442e1dda9a21bd462a97afa6af803", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "_generate_state_key", "original_string": "def _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"", "language": "python", "code": "def _generate_state_key(self, **kwargs: Any) -> str:\r\n        \"\"\"Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.\r\n\r\n        \"\"\"\r\n        return f\"{self.__class__.__qualname__}{repr(kwargs)}\"", "code_tokens": ["def", "_generate_state_key", "(", "self", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Formats", "a", "set", "of", "key", "-", "value", "pairs", "into", "a", "state", "key", "string", "with", "the", "callback", "class", "name", "prefixed", ".", "Useful", "for", "defining", "a", ":", "attr", ":", "`", "state_key", "`", ".", "Args", ":", "*", "*", "kwargs", ":", "A", "set", "of", "key", "-", "value", "pairs", ".", "Must", "be", "serializable", "to", ":", "class", ":", "`", "str", "`", ".", "\"", "\"", "\"", "return", "f", "\"", "{", "self", ".", "__class__", ".", "__qualname__", "}", "{", "repr", "(", "kwargs", ")", "}", "\""], "docstring": "Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for\r\n        defining a :attr:`state_key`.\r\n\r\n        Args:\r\n            **kwargs: A set of key-value pairs. Must be serializable to :class:`str`.", "docstring_tokens": ["formats", "a", "set", "of", "key", "value", "pairs", "into", "a", "state", "key", "string", "with", "the", "callback", "class", "name", "prefixed", "useful", "for", "defining", "a", "attr", "state_key", "args", "kwargs", "a", "set", "of", "key", "value", "pairs", "must", "be", "serializable", "to", "class", "str"], "docstring_summary": "Formats a set of key-value pairs into a state key string with the callback class name prefixed. Useful for", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 47, "end_line": 55, "hash": "451ae967d90458d2bd9c03546a0c0183", "complexity": 1, "parameters": ["**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_train_batch_start", "original_string": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"", "language": "python", "code": "def on_train_batch_start(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch begins.\"\"\"", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "train", "batch", "begins", ".", "\"", "\"", "\""], "docstring": "Called when the train batch begins.", "docstring_tokens": ["called", "when", "the", "train", "batch", "begins"], "docstring_summary": "Called when the train batch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 75, "end_line": 78, "hash": "691dc8232f02e7a0b65eeeeee4cb7764", "complexity": 1, "parameters": ["trainer", "pl_module", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_train_batch_end", "original_string": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "train", "batch", "ends", ".", "Note", ":", "The", "value", "`", "`", "outputs", "[", "\"", "loss", "\"", "]", "`", "`", "here", "will", "be", "the", "normalized", "value", "w", ".", "r", ".", "t", "`", "`", "accumulate_grad_batches", "`", "`", "of", "the", "loss", "returned", "from", "`", "`", "training_step", "`", "`", ".", "\"", "\"", "\""], "docstring": "Called when the train batch ends.\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.", "docstring_tokens": ["called", "when", "the", "train", "batch", "ends", "note", "the", "value", "outputs", "loss", "here", "will", "be", "the", "normalized", "value", "w", "r", "t", "accumulate_grad_batches", "of", "the", "loss", "returned", "from", "training_step"], "docstring_summary": "Called when the train batch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 80, "end_line": 89, "hash": "7d09335b7709611ffbfd5444068330a2", "complexity": 1, "parameters": ["trainer", "pl_module", "outputs", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_train_epoch_end", "original_string": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    pl_module.training_step_outputs.clear()\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "train", "epoch", "ends", ".", "To", "access", "all", "batch", "outputs", "at", "the", "end", "of", "the", "epoch", ",", "you", "can", "cache", "step", "outputs", "as", "an", "attribute", "of", "the", ":", "class", ":", "`", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "and", "access", "them", "in", "this", "hook", ":", ".", ".", "code", "-", "block", ":", ":", "python", "class", "MyLightningModule", "(", "L", ".", "LightningModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "training_step_outputs", "=", "[", "]", "def", "training_step", "(", "self", ")", ":", "loss", "=", ".", ".", ".", "self", ".", "training_step_outputs", ".", "append", "(", "loss", ")", "return", "loss", "class", "MyCallback", "(", "L", ".", "Callback", ")", ":", "def", "on_train_epoch_end", "(", "self", ",", "trainer", ",", "pl_module", ")", ":", "epoch_mean", "=", "torch", ".", "stack", "(", "pl_module", ".", "training_step_outputs", ")", ".", "mean", "(", ")", "pl_module", ".", "log", "(", "\"", "training_epoch_mean", "\"", ",", "epoch_mean", ")", "pl_module", ".", "training_step_outputs", ".", "clear", "(", ")", "\"", "\"", "\""], "docstring": "Called when the train epoch ends.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`lightning.pytorch.core.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n\r\n            class MyCallback(L.Callback):\r\n                def on_train_epoch_end(self, trainer, pl_module):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(pl_module.training_step_outputs).mean()\r\n                    pl_module.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    pl_module.training_step_outputs.clear()", "docstring_tokens": ["called", "when", "the", "train", "epoch", "ends", "to", "access", "all", "batch", "outputs", "at", "the", "end", "of", "the", "epoch", "you", "can", "cache", "step", "outputs", "as", "an", "attribute", "of", "the", "class", "lightning", "pytorch", "core", "lightningmodule", "and", "access", "them", "in", "this", "hook", "code", "block", "python", "class", "mylightningmodule", "l", "lightningmodule", "def", "__init__", "self", "super", "__init__", "self", "training_step_outputs", "def", "training_step", "self", "loss", "self", "training_step_outputs", "append", "loss", "return", "loss", "class", "mycallback", "l", "callback", "def", "on_train_epoch_end", "self", "trainer", "pl_module", "do", "something", "with", "all", "training_step", "outputs", "for", "example", "epoch_mean", "torch", "stack", "pl_module", "training_step_outputs", "mean", "pl_module", "log", "training_epoch_mean", "epoch_mean", "free", "up", "the", "memory", "pl_module", "training_step_outputs", "clear"], "docstring_summary": "Called when the train epoch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 94, "end_line": 121, "hash": "818363aca85b724c8c91fb9b09841dca", "complexity": 1, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_validation_batch_start", "original_string": "def on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"", "language": "python", "code": "def on_validation_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch begins.\"\"\"", "code_tokens": ["def", "on_validation_batch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "validation", "batch", "begins", ".", "\"", "\"", "\""], "docstring": "Called when the validation batch begins.", "docstring_tokens": ["called", "when", "the", "validation", "batch", "begins"], "docstring_summary": "Called when the validation batch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 141, "end_line": 149, "hash": "3ebd1f85e6b3cb4210e7df8a7a72cc04", "complexity": 1, "parameters": ["trainer", "pl_module", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_validation_batch_end", "original_string": "def on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"", "language": "python", "code": "def on_validation_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the validation batch ends.\"\"\"", "code_tokens": ["def", "on_validation_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "validation", "batch", "ends", ".", "\"", "\"", "\""], "docstring": "Called when the validation batch ends.", "docstring_tokens": ["called", "when", "the", "validation", "batch", "ends"], "docstring_summary": "Called when the validation batch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 151, "end_line": 160, "hash": "1b172838de2832c7931979913cb48336", "complexity": 1, "parameters": ["trainer", "pl_module", "outputs", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_test_batch_start", "original_string": "def on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"", "language": "python", "code": "def on_test_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch begins.\"\"\"", "code_tokens": ["def", "on_test_batch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "test", "batch", "begins", ".", "\"", "\"", "\""], "docstring": "Called when the test batch begins.", "docstring_tokens": ["called", "when", "the", "test", "batch", "begins"], "docstring_summary": "Called when the test batch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 162, "end_line": 170, "hash": "5416369f5c6aa93a926389cbb0d85d96", "complexity": 1, "parameters": ["trainer", "pl_module", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_test_batch_end", "original_string": "def on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"", "language": "python", "code": "def on_test_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: STEP_OUTPUT,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the test batch ends.\"\"\"", "code_tokens": ["def", "on_test_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "test", "batch", "ends", ".", "\"", "\"", "\""], "docstring": "Called when the test batch ends.", "docstring_tokens": ["called", "when", "the", "test", "batch", "ends"], "docstring_summary": "Called when the test batch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 172, "end_line": 181, "hash": "7805496d0dbf59e707f174fb0d34d9ff", "complexity": 1, "parameters": ["trainer", "pl_module", "outputs", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_predict_batch_start", "original_string": "def on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"", "language": "python", "code": "def on_predict_batch_start(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch begins.\"\"\"", "code_tokens": ["def", "on_predict_batch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "predict", "batch", "begins", ".", "\"", "\"", "\""], "docstring": "Called when the predict batch begins.", "docstring_tokens": ["called", "when", "the", "predict", "batch", "begins"], "docstring_summary": "Called when the predict batch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 183, "end_line": 191, "hash": "f2d0916c08b7ab59c38bfae300fadd34", "complexity": 1, "parameters": ["trainer", "pl_module", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_predict_batch_end", "original_string": "def on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"", "language": "python", "code": "def on_predict_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        outputs: Any,\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int = 0,\r\n    ) -> None:\r\n        \"\"\"Called when the predict batch ends.\"\"\"", "code_tokens": ["def", "on_predict_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "outputs", ":", "Any", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "predict", "batch", "ends", ".", "\"", "\"", "\""], "docstring": "Called when the predict batch ends.", "docstring_tokens": ["called", "when", "the", "predict", "batch", "ends"], "docstring_summary": "Called when the predict batch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 193, "end_line": 202, "hash": "2546c7e9e9e9dc01f1c746b0ba4a0475", "complexity": 1, "parameters": ["trainer", "pl_module", "outputs", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "state_dict", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "checkpoint", ",", "implement", "to", "generate", "callback", "'", "s", "`", "`", "state_dict", "`", "`", ".", "Returns", ":", "A", "dictionary", "containing", "callback", "state", ".", "\"", "\"", "\"", "return", "{", "}"], "docstring": "Called when saving a checkpoint, implement to generate callback's ``state_dict``.\r\n\r\n        Returns:\r\n            A dictionary containing callback state.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "implement", "to", "generate", "callback", "s", "state_dict", "returns", "a", "dictionary", "containing", "callback", "state"], "docstring_summary": "Called when saving a checkpoint, implement to generate callback's ``state_dict``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 231, "end_line": 238, "hash": "1f7a2bd0a1e81e5b2010559f423d32dc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "load_state_dict", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "loading", "a", "checkpoint", ",", "implement", "to", "reload", "callback", "state", "given", "callback", "'", "s", "`", "`", "state_dict", "`", "`", ".", "Args", ":", "state_dict", ":", "the", "callback", "state", "returned", "by", "`", "`", "state_dict", "`", "`", ".", "\"", "\"", "\"", "pass"], "docstring": "Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: the callback state returned by ``state_dict``.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "implement", "to", "reload", "callback", "state", "given", "callback", "s", "state_dict", "args", "state_dict", "the", "callback", "state", "returned", "by", "state_dict"], "docstring_summary": "Called when loading a checkpoint, implement to reload callback state given callback's ``state_dict``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 240, "end_line": 247, "hash": "77ecf1dcb512a43f5c8d7d13be8699bb", "complexity": 1, "parameters": ["state_dict", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_save_checkpoint", "original_string": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Called", "when", "saving", "a", "checkpoint", "to", "give", "you", "a", "chance", "to", "store", "anything", "else", "you", "might", "want", "to", "save", ".", "Args", ":", "trainer", ":", "the", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "the", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "checkpoint", ":", "the", "checkpoint", "dictionary", "that", "will", "be", "saved", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "called", "when", "saving", "a", "checkpoint", "to", "give", "you", "a", "chance", "to", "store", "anything", "else", "you", "might", "want", "to", "save", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "checkpoint", "the", "checkpoint", "dictionary", "that", "will", "be", "saved"], "docstring_summary": "r\"\"\"Called when saving a checkpoint to give you a chance to store anything else you might want to save.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 249, "end_line": 259, "hash": "312d629ba022d5aa8a609fdd0f954b8b", "complexity": 1, "parameters": ["trainer", "pl_module", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_load_checkpoint", "original_string": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Called", "when", "loading", "a", "model", "checkpoint", ",", "use", "to", "reload", "state", ".", "Args", ":", "trainer", ":", "the", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "the", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "checkpoint", ":", "the", "full", "checkpoint", "dictionary", "that", "got", "loaded", "by", "the", "Trainer", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Called when loading a model checkpoint, use to reload state.\r\n\r\n        Args:\r\n            trainer: the current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: the current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: the full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "called", "when", "loading", "a", "model", "checkpoint", "use", "to", "reload", "state", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "checkpoint", "the", "full", "checkpoint", "dictionary", "that", "got", "loaded", "by", "the", "trainer"], "docstring_summary": "r\"\"\"Called when loading a model checkpoint, use to reload state.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 261, "end_line": 271, "hash": "979467e6b76b3b26b2fe9ae0c9735f0f", "complexity": 1, "parameters": ["trainer", "pl_module", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\callback.py", "func_name": "on_before_optimizer_step", "original_string": "def on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"", "language": "python", "code": "def on_before_optimizer_step(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", optimizer: Optimizer\r\n    ) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\"\"\"", "code_tokens": ["def", "on_before_optimizer_step", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "before", "`", "`", "optimizer", ".", "step", "(", ")", "`", "`", ".", "\"", "\"", "\""], "docstring": "Called before ``optimizer.step()``.", "docstring_tokens": ["called", "before", "optimizer", "step"], "docstring_summary": "Called before ``optimizer.step()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\callback.py", "partition": "train", "function_type": "class_method", "class_name": "Callback", "start_line": 279, "end_line": 282, "hash": "eb38169540c71f9adfd3e1988f96f685", "complexity": 1, "parameters": ["trainer", "pl_module", "optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py", "func_name": "_run_early_stopping_check", "original_string": "def _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        # stop every ddp process if any world process decides to stop\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)", "language": "python", "code": "def _run_early_stopping_check(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\r\n        logs = trainer.callback_metrics\r\n\r\n        if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run\r\n            logs\r\n        ):  # short circuit if metric not present\r\n            return\r\n\r\n        current = logs[self.monitor].squeeze()\r\n        should_stop, reason = self._evaluate_stopping_criteria(current)\r\n\r\n        # stop every ddp process if any world process decides to stop\r\n        should_stop = trainer.strategy.reduce_boolean_decision(should_stop, all=False)\r\n        trainer.should_stop = trainer.should_stop or should_stop\r\n        if should_stop:\r\n            self.stopped_epoch = trainer.current_epoch\r\n            self.stopping_reason_message = reason\r\n        if reason and self.verbose:\r\n            self._log_info(trainer, reason, self.log_rank_zero_only)", "code_tokens": ["def", "_run_early_stopping_check", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "whether", "the", "early", "stopping", "condition", "is", "met", "and", "if", "so", "tells", "the", "trainer", "to", "stop", "the", "training", ".", "\"", "\"", "\"", "logs", "=", "trainer", ".", "callback_metrics", "if", "trainer", ".", "fast_dev_run", "or", "not", "self", ".", "_validate_condition_metric", "(", "logs", ")", ":", "return", "current", "=", "logs", "[", "self", ".", "monitor", "]", ".", "squeeze", "(", ")", "should_stop", ",", "reason", "=", "self", ".", "_evaluate_stopping_criteria", "(", "current", ")", "should_stop", "=", "trainer", ".", "strategy", ".", "reduce_boolean_decision", "(", "should_stop", ",", "all", "=", "False", ")", "trainer", ".", "should_stop", "=", "trainer", ".", "should_stop", "or", "should_stop", "if", "should_stop", ":", "self", ".", "stopped_epoch", "=", "trainer", ".", "current_epoch", "self", ".", "stopping_reason_message", "=", "reason", "if", "reason", "and", "self", ".", "verbose", ":", "self", ".", "_log_info", "(", "trainer", ",", "reason", ",", "self", ".", "log_rank_zero_only", ")"], "docstring": "Checks whether the early stopping condition is met and if so tells the trainer to stop the training.", "docstring_tokens": ["checks", "whether", "the", "early", "stopping", "condition", "is", "met", "and", "if", "so", "tells", "the", "trainer", "to", "stop", "the", "training"], "docstring_summary": "Checks whether the early stopping condition is met and if so tells the trainer to stop the training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\early_stopping.py", "partition": "train", "function_type": "class_method", "class_name": "EarlyStopping", "start_line": 224, "end_line": 243, "hash": "6d24ac5ab430f749b2c73248eeabd14b", "complexity": 7, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\early_stopping.py", "func_name": "_improvement_message", "original_string": "def _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg", "language": "python", "code": "def _improvement_message(self, current: Tensor) -> str:\r\n        \"\"\"Formats a log message that informs the user about an improvement in the monitored score.\"\"\"\r\n        if torch.isfinite(self.best_score):\r\n            msg = (\r\n                f\"Metric {self.monitor} improved by {abs(self.best_score - current):.3f} >=\"\r\n                f\" min_delta = {abs(self.min_delta)}. New best score: {current:.3f}\"\r\n            )\r\n        else:\r\n            msg = f\"Metric {self.monitor} improved. New best score: {current:.3f}\"\r\n        return msg", "code_tokens": ["def", "_improvement_message", "(", "self", ",", "current", ":", "Tensor", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Formats", "a", "log", "message", "that", "informs", "the", "user", "about", "an", "improvement", "in", "the", "monitored", "score", ".", "\"", "\"", "\"", "if", "torch", ".", "isfinite", "(", "self", ".", "best_score", ")", ":", "msg", "=", "(", "f", "\"", "Metric", "{", "self", ".", "monitor", "}", "improved", "by", "{", "abs", "(", "self", ".", "best_score", "-", "current", ")", ":", ".", "3f", "}", ">", "=", "\"", "f", "\"", "min_delta", "=", "{", "abs", "(", "self", ".", "min_delta", ")", "}", ".", "New", "best", "score", ":", "{", "current", ":", ".", "3f", "}", "\"", ")", "else", ":", "msg", "=", "f", "\"", "Metric", "{", "self", ".", "monitor", "}", "improved", ".", "New", "best", "score", ":", "{", "current", ":", ".", "3f", "}", "\"", "return", "msg"], "docstring": "Formats a log message that informs the user about an improvement in the monitored score.", "docstring_tokens": ["formats", "a", "log", "message", "that", "informs", "the", "user", "about", "an", "improvement", "in", "the", "monitored", "score"], "docstring_summary": "Formats a log message that informs the user about an improvement in the monitored score.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\early_stopping.py", "partition": "train", "function_type": "class_method", "class_name": "EarlyStopping", "start_line": 288, "end_line": 297, "hash": "b08965a7e4eb7654d1069b12126f97b8", "complexity": 2, "parameters": ["current"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "flatten_modules", "original_string": "def flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        # Capture all leaf modules as well as parent modules that have parameters directly themselves\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]", "language": "python", "code": "def flatten_modules(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> list[Module]:\r\n        \"\"\"This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules\r\n\r\n        \"\"\"\r\n        if isinstance(modules, ModuleDict):\r\n            modules = modules.values()\r\n\r\n        if isinstance(modules, Iterable):\r\n            _flatten_modules = []\r\n            for m in modules:\r\n                _flatten_modules.extend(BaseFinetuning.flatten_modules(m))\r\n\r\n            _modules = iter(_flatten_modules)\r\n        else:\r\n            _modules = modules.modules()\r\n\r\n        # Capture all leaf modules as well as parent modules that have parameters directly themselves\r\n        return [m for m in _modules if not list(m.children()) or m._parameters]", "code_tokens": ["def", "flatten_modules", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ")", "-", ">", "list", "[", "Module", "]", ":", "\"", "\"", "\"", "This", "function", "is", "used", "to", "flatten", "a", "module", "or", "an", "iterable", "of", "modules", "into", "a", "list", "of", "its", "leaf", "modules", "(", "modules", "with", "no", "children", ")", "and", "parent", "modules", "that", "have", "parameters", "directly", "themselves", ".", "Args", ":", "modules", ":", "A", "given", "module", "or", "an", "iterable", "of", "modules", "Returns", ":", "List", "of", "modules", "\"", "\"", "\"", "if", "isinstance", "(", "modules", ",", "ModuleDict", ")", ":", "modules", "=", "modules", ".", "values", "(", ")", "if", "isinstance", "(", "modules", ",", "Iterable", ")", ":", "_flatten_modules", "=", "[", "]", "for", "m", "in", "modules", ":", "_flatten_modules", ".", "extend", "(", "BaseFinetuning", ".", "flatten_modules", "(", "m", ")", ")", "_modules", "=", "iter", "(", "_flatten_modules", ")", "else", ":", "_modules", "=", "modules", ".", "modules", "(", ")", "return", "[", "m", "for", "m", "in", "_modules", "if", "not", "list", "(", "m", ".", "children", "(", ")", ")", "or", "m", ".", "_parameters", "]"], "docstring": "This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules\r\n        with no children) and parent modules that have parameters directly themselves.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        Returns:\r\n            List of modules", "docstring_tokens": ["this", "function", "is", "used", "to", "flatten", "a", "module", "or", "an", "iterable", "of", "modules", "into", "a", "list", "of", "its", "leaf", "modules", "modules", "with", "no", "children", "and", "parent", "modules", "that", "have", "parameters", "directly", "themselves", "args", "modules", "a", "given", "module", "or", "an", "iterable", "of", "modules", "returns", "list", "of", "modules"], "docstring_summary": "This function is used to flatten a module or an iterable of modules into a list of its leaf modules (modules", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 119, "end_line": 143, "hash": "377d02fd084fef9bffac10c3adb17455", "complexity": 7, "parameters": ["modules", "Iterable[Union[Module", "Iterable]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "filter_params", "original_string": "def filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param", "language": "python", "code": "def filter_params(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True, requires_grad: bool = True\r\n    ) -> Generator:\r\n        \"\"\"Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and not train_bn:\r\n                continue\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in mod.parameters(recurse=False):\r\n                if param.requires_grad == requires_grad:\r\n                    yield param", "code_tokens": ["def", "filter_params", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ",", "train_bn", ":", "bool", "=", "True", ",", "requires_grad", ":", "bool", "=", "True", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Yields", "the", "`", "requires_grad", "`", "parameters", "of", "a", "given", "module", "or", "list", "of", "modules", ".", "Args", ":", "modules", ":", "A", "given", "module", "or", "an", "iterable", "of", "modules", "train_bn", ":", "Whether", "not", "to", "train", "the", "BatchNorm", "module", "requires_grad", ":", "Whether", "to", "create", "a", "generator", "for", "trainable", "or", "non", "-", "trainable", "parameters", ".", "Returns", ":", "Generator", "\"", "\"", "\"", "modules", "=", "BaseFinetuning", ".", "flatten_modules", "(", "modules", ")", "for", "mod", "in", "modules", ":", "if", "isinstance", "(", "mod", ",", "_BatchNorm", ")", "and", "not", "train_bn", ":", "continue", "for", "param", "in", "mod", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "if", "param", ".", "requires_grad", "=", "=", "requires_grad", ":", "yield", "param"], "docstring": "Yields the `requires_grad` parameters of a given module or list of modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: Whether not to train the BatchNorm module\r\n            requires_grad: Whether to create a generator for trainable or non-trainable parameters.\r\n        Returns:\r\n            Generator", "docstring_tokens": ["yields", "the", "requires_grad", "parameters", "of", "a", "given", "module", "or", "list", "of", "modules", "args", "modules", "a", "given", "module", "or", "an", "iterable", "of", "modules", "train_bn", "whether", "not", "to", "train", "the", "batchnorm", "module", "requires_grad", "whether", "to", "create", "a", "generator", "for", "trainable", "or", "non", "trainable", "parameters", "returns", "generator"], "docstring_summary": "Yields the `requires_grad` parameters of a given module or list of modules.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 146, "end_line": 166, "hash": "2ae136d5656bf86cc986b91bf9b6ebe9", "complexity": 6, "parameters": ["modules", "Iterable[Union[Module", "Iterable]]]", "train_bn", "requires_grad"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "make_trainable", "original_string": "def make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True", "language": "python", "code": "def make_trainable(modules: Union[Module, Iterable[Union[Module, Iterable]]]) -> None:\r\n        \"\"\"Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for module in modules:\r\n            if isinstance(module, _BatchNorm):\r\n                module.track_running_stats = True\r\n            # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n            for param in module.parameters(recurse=False):\r\n                param.requires_grad = True", "code_tokens": ["def", "make_trainable", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Unfreezes", "the", "parameters", "of", "the", "provided", "modules", ".", "Args", ":", "modules", ":", "A", "given", "module", "or", "an", "iterable", "of", "modules", "\"", "\"", "\"", "modules", "=", "BaseFinetuning", ".", "flatten_modules", "(", "modules", ")", "for", "module", "in", "modules", ":", "if", "isinstance", "(", "module", ",", "_BatchNorm", ")", ":", "module", ".", "track_running_stats", "=", "True", "for", "param", "in", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "param", ".", "requires_grad", "=", "True"], "docstring": "Unfreezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules", "docstring_tokens": ["unfreezes", "the", "parameters", "of", "the", "provided", "modules", "args", "modules", "a", "given", "module", "or", "an", "iterable", "of", "modules"], "docstring_summary": "Unfreezes the parameters of the provided modules.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 169, "end_line": 182, "hash": "638b40f02695d6825961a4b79557c099", "complexity": 4, "parameters": ["modules", "Iterable[Union[Module", "Iterable]]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "freeze_module", "original_string": "def freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False", "language": "python", "code": "def freeze_module(module: Module) -> None:\r\n        \"\"\"Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module\r\n\r\n        \"\"\"\r\n        if isinstance(module, _BatchNorm):\r\n            module.track_running_stats = False\r\n        # recursion could yield duplicate parameters for parent modules w/ parameters so disabling it\r\n        for param in module.parameters(recurse=False):\r\n            param.requires_grad = False", "code_tokens": ["def", "freeze_module", "(", "module", ":", "Module", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Freezes", "the", "parameters", "of", "the", "provided", "module", ".", "Args", ":", "module", ":", "A", "given", "module", "\"", "\"", "\"", "if", "isinstance", "(", "module", ",", "_BatchNorm", ")", ":", "module", ".", "track_running_stats", "=", "False", "for", "param", "in", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ":", "param", ".", "requires_grad", "=", "False"], "docstring": "Freezes the parameters of the provided module.\r\n\r\n        Args:\r\n            module: A given module", "docstring_tokens": ["freezes", "the", "parameters", "of", "the", "provided", "module", "args", "module", "a", "given", "module"], "docstring_summary": "Freezes the parameters of the provided module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 185, "end_line": 196, "hash": "0f2a640a279474ce241555a4b04c2c62", "complexity": 3, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "freeze", "original_string": "def freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)", "language": "python", "code": "def freeze(modules: Union[Module, Iterable[Union[Module, Iterable]]], train_bn: bool = True) -> None:\r\n        \"\"\"Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None\r\n\r\n        \"\"\"\r\n        modules = BaseFinetuning.flatten_modules(modules)\r\n        for mod in modules:\r\n            if isinstance(mod, _BatchNorm) and train_bn:\r\n                BaseFinetuning.make_trainable(mod)\r\n            else:\r\n                BaseFinetuning.freeze_module(mod)", "code_tokens": ["def", "freeze", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ",", "train_bn", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Freezes", "the", "parameters", "of", "the", "provided", "modules", ".", "Args", ":", "modules", ":", "A", "given", "module", "or", "an", "iterable", "of", "modules", "train_bn", ":", "If", "True", ",", "leave", "the", "BatchNorm", "layers", "in", "training", "mode", "Returns", ":", "None", "\"", "\"", "\"", "modules", "=", "BaseFinetuning", ".", "flatten_modules", "(", "modules", ")", "for", "mod", "in", "modules", ":", "if", "isinstance", "(", "mod", ",", "_BatchNorm", ")", "and", "train_bn", ":", "BaseFinetuning", ".", "make_trainable", "(", "mod", ")", "else", ":", "BaseFinetuning", ".", "freeze_module", "(", "mod", ")"], "docstring": "Freezes the parameters of the provided modules.\r\n\r\n        Args:\r\n            modules: A given module or an iterable of modules\r\n            train_bn: If True, leave the BatchNorm layers in training mode\r\n\r\n        Returns:\r\n            None", "docstring_tokens": ["freezes", "the", "parameters", "of", "the", "provided", "modules", "args", "modules", "a", "given", "module", "or", "an", "iterable", "of", "modules", "train_bn", "if", "true", "leave", "the", "batchnorm", "layers", "in", "training", "mode", "returns", "none"], "docstring_summary": "Freezes the parameters of the provided modules.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 199, "end_line": 215, "hash": "a8cfb1739a8fe49b84173efd75e6ee7b", "complexity": 4, "parameters": ["modules", "Iterable[Union[Module", "Iterable]]]", "train_bn"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "filter_on_optimizer", "original_string": "def filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params", "language": "python", "code": "def filter_on_optimizer(optimizer: Optimizer, params: Iterable) -> list:\r\n        \"\"\"This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups\r\n\r\n        \"\"\"\r\n        out_params = []\r\n        removed_params = []\r\n        for param in params:\r\n            if not any(torch.equal(p, param) for group in optimizer.param_groups for p in group[\"params\"]):\r\n                out_params.append(param)\r\n            else:\r\n                removed_params.append(param)\r\n\r\n        if removed_params:\r\n            rank_zero_warn(\r\n                \"The provided params to be frozen already exist within another group of this optimizer.\"\r\n                \" Those parameters will be skipped.\\n\"\r\n                \"HINT: Did you init your optimizer in `configure_optimizer` as such:\\n\"\r\n                f\" {type(optimizer)}(filter(lambda p: p.requires_grad, self.parameters()), ...) \",\r\n            )\r\n        return out_params", "code_tokens": ["def", "filter_on_optimizer", "(", "optimizer", ":", "Optimizer", ",", "params", ":", "Iterable", ")", "-", ">", "list", ":", "\"", "\"", "\"", "This", "function", "is", "used", "to", "exclude", "any", "parameter", "which", "already", "exists", "in", "this", "optimizer", ".", "Args", ":", "optimizer", ":", "Optimizer", "used", "for", "parameter", "exclusion", "params", ":", "Iterable", "of", "parameters", "used", "to", "check", "against", "the", "provided", "optimizer", "Returns", ":", "List", "of", "parameters", "not", "contained", "in", "this", "optimizer", "param", "groups", "\"", "\"", "\"", "out_params", "=", "[", "]", "removed_params", "=", "[", "]", "for", "param", "in", "params", ":", "if", "not", "any", "(", "torch", ".", "equal", "(", "p", ",", "param", ")", "for", "group", "in", "optimizer", ".", "param_groups", "for", "p", "in", "group", "[", "\"", "params", "\"", "]", ")", ":", "out_params", ".", "append", "(", "param", ")", "else", ":", "removed_params", ".", "append", "(", "param", ")", "if", "removed_params", ":", "rank_zero_warn", "(", "\"", "The", "provided", "params", "to", "be", "frozen", "already", "exist", "within", "another", "group", "of", "this", "optimizer", ".", "\"", "\"", "Those", "parameters", "will", "be", "skipped", ".", "\\", "n", "\"", "\"", "HINT", ":", "Did", "you", "init", "your", "optimizer", "in", "`", "configure_optimizer", "`", "as", "such", ":", "\\", "n", "\"", "f", "\"", "{", "type", "(", "optimizer", ")", "}", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "self", ".", "parameters", "(", ")", ")", ",", ".", ".", ".", ")", "\"", ",", ")", "return", "out_params"], "docstring": "This function is used to exclude any parameter which already exists in this optimizer.\r\n\r\n        Args:\r\n            optimizer: Optimizer used for parameter exclusion\r\n            params: Iterable of parameters used to check against the provided optimizer\r\n\r\n        Returns:\r\n            List of parameters not contained in this optimizer param groups", "docstring_tokens": ["this", "function", "is", "used", "to", "exclude", "any", "parameter", "which", "already", "exists", "in", "this", "optimizer", "args", "optimizer", "optimizer", "used", "for", "parameter", "exclusion", "params", "iterable", "of", "parameters", "used", "to", "check", "against", "the", "provided", "optimizer", "returns", "list", "of", "parameters", "not", "contained", "in", "this", "optimizer", "param", "groups"], "docstring_summary": "This function is used to exclude any parameter which already exists in this optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 218, "end_line": 244, "hash": "ca2e687b3f9cb6d6f2e8dc62f41e8816", "complexity": 6, "parameters": ["optimizer", "params"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "unfreeze_and_add_param_group", "original_string": "def unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})", "language": "python", "code": "def unfreeze_and_add_param_group(\r\n        modules: Union[Module, Iterable[Union[Module, Iterable]]],\r\n        optimizer: Optimizer,\r\n        lr: Optional[float] = None,\r\n        initial_denom_lr: float = 10.0,\r\n        train_bn: bool = True,\r\n    ) -> None:\r\n        \"\"\"Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.\r\n\r\n        \"\"\"\r\n        BaseFinetuning.make_trainable(modules)\r\n        params_lr = optimizer.param_groups[0][\"lr\"] if lr is None else float(lr)\r\n        denom_lr = initial_denom_lr if lr is None else 1.0\r\n        params = BaseFinetuning.filter_params(modules, train_bn=train_bn, requires_grad=True)\r\n        params = BaseFinetuning.filter_on_optimizer(optimizer, params)\r\n        if params:\r\n            optimizer.add_param_group({\"params\": params, \"lr\": params_lr / denom_lr})", "code_tokens": ["def", "unfreeze_and_add_param_group", "(", "modules", ":", "Union", "[", "Module", ",", "Iterable", "[", "Union", "[", "Module", ",", "Iterable", "]", "]", "]", ",", "optimizer", ":", "Optimizer", ",", "lr", ":", "Optional", "[", "float", "]", "=", "None", ",", "initial_denom_lr", ":", "float", "=", "10", ".", "0", ",", "train_bn", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Unfreezes", "a", "module", "and", "adds", "its", "parameters", "to", "an", "optimizer", ".", "Args", ":", "modules", ":", "A", "module", "or", "iterable", "of", "modules", "to", "unfreeze", ".", "Their", "parameters", "will", "be", "added", "to", "an", "optimizer", "as", "a", "new", "param", "group", ".", "optimizer", ":", "The", "provided", "optimizer", "will", "receive", "new", "parameters", "and", "will", "add", "them", "to", "`", "add_param_group", "`", "lr", ":", "Learning", "rate", "for", "the", "new", "param", "group", ".", "initial_denom_lr", ":", "If", "no", "lr", "is", "provided", ",", "the", "learning", "from", "the", "first", "param", "group", "will", "be", "used", "and", "divided", "by", "`", "initial_denom_lr", "`", ".", "train_bn", ":", "Whether", "to", "train", "the", "BatchNormalization", "layers", ".", "\"", "\"", "\"", "BaseFinetuning", ".", "make_trainable", "(", "modules", ")", "params_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"", "lr", "\"", "]", "if", "lr", "is", "None", "else", "float", "(", "lr", ")", "denom_lr", "=", "initial_denom_lr", "if", "lr", "is", "None", "else", "1", ".", "0", "params", "=", "BaseFinetuning", ".", "filter_params", "(", "modules", ",", "train_bn", "=", "train_bn", ",", "requires_grad", "=", "True", ")", "params", "=", "BaseFinetuning", ".", "filter_on_optimizer", "(", "optimizer", ",", "params", ")", "if", "params", ":", "optimizer", ".", "add_param_group", "(", "{", "\"", "params", "\"", ":", "params", ",", "\"", "lr", "\"", ":", "params_lr", "/", "denom_lr", "}", ")"], "docstring": "Unfreezes a module and adds its parameters to an optimizer.\r\n\r\n        Args:\r\n            modules: A module or iterable of modules to unfreeze.\r\n                Their parameters will be added to an optimizer as a new param group.\r\n            optimizer: The provided optimizer will receive new parameters and will add them to\r\n                `add_param_group`\r\n            lr: Learning rate for the new param group.\r\n            initial_denom_lr: If no lr is provided, the learning from the first param group will be used\r\n                and divided by `initial_denom_lr`.\r\n            train_bn: Whether to train the BatchNormalization layers.", "docstring_tokens": ["unfreezes", "a", "module", "and", "adds", "its", "parameters", "to", "an", "optimizer", "args", "modules", "a", "module", "or", "iterable", "of", "modules", "to", "unfreeze", "their", "parameters", "will", "be", "added", "to", "an", "optimizer", "as", "a", "new", "param", "group", "optimizer", "the", "provided", "optimizer", "will", "receive", "new", "parameters", "and", "will", "add", "them", "to", "add_param_group", "lr", "learning", "rate", "for", "the", "new", "param", "group", "initial_denom_lr", "if", "no", "lr", "is", "provided", "the", "learning", "from", "the", "first", "param", "group", "will", "be", "used", "and", "divided", "by", "initial_denom_lr", "train_bn", "whether", "to", "train", "the", "batchnormalization", "layers"], "docstring_summary": "Unfreezes a module and adds its parameters to an optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 247, "end_line": 273, "hash": "2f1a1752ea8ed41872957b7f1d46d86a", "complexity": 4, "parameters": ["modules", "Iterable[Union[Module", "Iterable]]]", "optimizer", "lr", "initial_denom_lr", "train_bn"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "on_train_epoch_start", "original_string": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)", "language": "python", "code": "def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        for opt_idx, optimizer in enumerate(trainer.optimizers):\r\n            num_param_groups = len(optimizer.param_groups)\r\n            self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\n            current_param_groups = optimizer.param_groups\r\n            self._store(pl_module, opt_idx, num_param_groups, current_param_groups)", "code_tokens": ["def", "on_train_epoch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "epoch", "begins", ".", "\"", "\"", "\"", "for", "opt_idx", ",", "optimizer", "in", "enumerate", "(", "trainer", ".", "optimizers", ")", ":", "num_param_groups", "=", "len", "(", "optimizer", ".", "param_groups", ")", "self", ".", "finetune_function", "(", "pl_module", ",", "trainer", ".", "current_epoch", ",", "optimizer", ")", "current_param_groups", "=", "optimizer", ".", "param_groups", "self", ".", "_store", "(", "pl_module", ",", "opt_idx", ",", "num_param_groups", ",", "current_param_groups", ")"], "docstring": "Called when the epoch begins.", "docstring_tokens": ["called", "when", "the", "epoch", "begins"], "docstring_summary": "Called when the epoch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BaseFinetuning", "start_line": 316, "end_line": 322, "hash": "51b58db00a0b8a3e94ba7f5e3bdba137", "complexity": 2, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "on_fit_start", "original_string": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")", "language": "python", "code": "def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"\r\n        Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.\r\n        \"\"\"\r\n        if hasattr(pl_module, \"backbone\") and isinstance(pl_module.backbone, Module):\r\n            return super().on_fit_start(trainer, pl_module)\r\n        raise MisconfigurationException(\"The LightningModule should have a nn.Module `backbone` attribute\")", "code_tokens": ["def", "on_fit_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Raises", ":", "MisconfigurationException", ":", "If", "LightningModule", "has", "no", "nn", ".", "Module", "`", "backbone", "`", "attribute", ".", "\"", "\"", "\"", "if", "hasattr", "(", "pl_module", ",", "\"", "backbone", "\"", ")", "and", "isinstance", "(", "pl_module", ".", "backbone", ",", "Module", ")", ":", "return", "super", "(", ")", ".", "on_fit_start", "(", "trainer", ",", "pl_module", ")", "raise", "MisconfigurationException", "(", "\"", "The", "LightningModule", "should", "have", "a", "nn", ".", "Module", "`", "backbone", "`", "attribute", "\"", ")"], "docstring": "Raises:\r\n            MisconfigurationException:\r\n                If LightningModule has no nn.Module `backbone` attribute.", "docstring_tokens": ["raises", "misconfigurationexception", "if", "lightningmodule", "has", "no", "nn", "module", "backbone", "attribute"], "docstring_summary": "Raises:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BackboneFinetuning", "start_line": 402, "end_line": 410, "hash": "ad27706d1d2e1837a26d8efb5680a25a", "complexity": 3, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\finetuning.py", "func_name": "finetune_function", "original_string": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )", "language": "python", "code": "def finetune_function(self, pl_module: \"pl.LightningModule\", epoch: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Called when the epoch begins.\"\"\"\r\n        if epoch == self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            initial_backbone_lr = (\r\n                self.backbone_initial_lr\r\n                if self.backbone_initial_lr is not None\r\n                else current_lr * self.backbone_initial_ratio_lr\r\n            )\r\n            self.previous_backbone_lr = initial_backbone_lr\r\n            self.unfreeze_and_add_param_group(\r\n                pl_module.backbone,\r\n                optimizer,\r\n                initial_backbone_lr,\r\n                train_bn=self.train_bn,\r\n                initial_denom_lr=self.initial_denom_lr,\r\n            )\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(initial_backbone_lr, self.rounding)}\"\r\n                )\r\n\r\n        elif epoch > self.unfreeze_backbone_at_epoch:\r\n            current_lr = optimizer.param_groups[0][\"lr\"]\r\n            next_current_backbone_lr = self.lambda_func(epoch + 1) * self.previous_backbone_lr\r\n            next_current_backbone_lr = (\r\n                current_lr\r\n                if (self.should_align and next_current_backbone_lr > current_lr)\r\n                else next_current_backbone_lr\r\n            )\r\n            optimizer.param_groups[-1][\"lr\"] = next_current_backbone_lr\r\n            self.previous_backbone_lr = next_current_backbone_lr\r\n            if self.verbose:\r\n                log.info(\r\n                    f\"Current lr: {round(current_lr, self.rounding)}, \"\r\n                    f\"Backbone lr: {round(next_current_backbone_lr, self.rounding)}\"\r\n                )", "code_tokens": ["def", "finetune_function", "(", "self", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "epoch", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "epoch", "begins", ".", "\"", "\"", "\"", "if", "epoch", "=", "=", "self", ".", "unfreeze_backbone_at_epoch", ":", "current_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"", "lr", "\"", "]", "initial_backbone_lr", "=", "(", "self", ".", "backbone_initial_lr", "if", "self", ".", "backbone_initial_lr", "is", "not", "None", "else", "current_lr", "*", "self", ".", "backbone_initial_ratio_lr", ")", "self", ".", "previous_backbone_lr", "=", "initial_backbone_lr", "self", ".", "unfreeze_and_add_param_group", "(", "pl_module", ".", "backbone", ",", "optimizer", ",", "initial_backbone_lr", ",", "train_bn", "=", "self", ".", "train_bn", ",", "initial_denom_lr", "=", "self", ".", "initial_denom_lr", ",", ")", "if", "self", ".", "verbose", ":", "log", ".", "info", "(", "f", "\"", "Current", "lr", ":", "{", "round", "(", "current_lr", ",", "self", ".", "rounding", ")", "}", ",", "\"", "f", "\"", "Backbone", "lr", ":", "{", "round", "(", "initial_backbone_lr", ",", "self", ".", "rounding", ")", "}", "\"", ")", "elif", "epoch", ">", "self", ".", "unfreeze_backbone_at_epoch", ":", "current_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"", "lr", "\"", "]", "next_current_backbone_lr", "=", "self", ".", "lambda_func", "(", "epoch", "+", "1", ")", "*", "self", ".", "previous_backbone_lr", "next_current_backbone_lr", "=", "(", "current_lr", "if", "(", "self", ".", "should_align", "and", "next_current_backbone_lr", ">", "current_lr", ")", "else", "next_current_backbone_lr", ")", "optimizer", ".", "param_groups", "[", "-", "1", "]", "[", "\"", "lr", "\"", "]", "=", "next_current_backbone_lr", "self", ".", "previous_backbone_lr", "=", "next_current_backbone_lr", "if", "self", ".", "verbose", ":", "log", ".", "info", "(", "f", "\"", "Current", "lr", ":", "{", "round", "(", "current_lr", ",", "self", ".", "rounding", ")", "}", ",", "\"", "f", "\"", "Backbone", "lr", ":", "{", "round", "(", "next_current_backbone_lr", ",", "self", ".", "rounding", ")", "}", "\"", ")"], "docstring": "Called when the epoch begins.", "docstring_tokens": ["called", "when", "the", "epoch", "begins"], "docstring_summary": "Called when the epoch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\finetuning.py", "partition": "train", "function_type": "class_method", "class_name": "BackboneFinetuning", "start_line": 417, "end_line": 454, "hash": "4b095e01092dc262bc60bf018461bc3f", "complexity": 8, "parameters": ["pl_module", "epoch", "optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py", "func_name": "on_train_start", "original_string": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )", "language": "python", "code": "def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Performns a configuration validation before training starts and raises errors for incompatible settings.\"\"\"\r\n\r\n        if not pl_module.automatic_optimization:\r\n            raise RuntimeError(\r\n                \"\"\"Automatic gradient accumulation and the `GradientAccumulationScheduler` is not supported for\r\n                manual optimization. Please remove the callback or switch to automatic optimization.\"\"\"\r\n            )\r\n\r\n        overridden_optimizer_step = is_overridden(\"optimizer_step\", pl_module)\r\n        overridden_optimizer_zero_grad = is_overridden(\"optimizer_zero_grad\", pl_module)\r\n        going_to_accumulate_grad_batches = self.going_to_accumulate_grad_batches()\r\n        has_overridden_optimization_functions = overridden_optimizer_step or overridden_optimizer_zero_grad\r\n        if has_overridden_optimization_functions and going_to_accumulate_grad_batches:\r\n            rank_zero_warn(\r\n                \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\r\n                \" `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch\"\r\n                \" (rather, they are called on every optimization step).\"\r\n            )\r\n\r\n        # local import to avoid circular import\r\n        from lightning.pytorch.strategies import DeepSpeedStrategy\r\n\r\n        if isinstance(trainer.strategy, DeepSpeedStrategy):\r\n            raise RuntimeError(\r\n                f\"The `{type(trainer.strategy).__name__}` does not support `accumulate_grad_batches` changing\"\r\n                \" between epochs.\"\r\n            )\r\n        if trainer.accumulate_grad_batches != 1:\r\n            raise ValueError(\r\n                \"You have set `accumulate_grad_batches` and are using the `GradientAccumulationScheduler`\"\r\n                \" callback. Either remove `accumulate_grad_batches` from the Trainer or remove the callback.\"\r\n            )", "code_tokens": ["def", "on_train_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Performns", "a", "configuration", "validation", "before", "training", "starts", "and", "raises", "errors", "for", "incompatible", "settings", ".", "\"", "\"", "\"", "if", "not", "pl_module", ".", "automatic_optimization", ":", "raise", "RuntimeError", "(", "\"", "\"", "\"", "Automatic", "gradient", "accumulation", "and", "the", "`", "GradientAccumulationScheduler", "`", "is", "not", "supported", "for", "manual", "optimization", ".", "Please", "remove", "the", "callback", "or", "switch", "to", "automatic", "optimization", ".", "\"", "\"", "\"", ")", "overridden_optimizer_step", "=", "is_overridden", "(", "\"", "optimizer_step", "\"", ",", "pl_module", ")", "overridden_optimizer_zero_grad", "=", "is_overridden", "(", "\"", "optimizer_zero_grad", "\"", ",", "pl_module", ")", "going_to_accumulate_grad_batches", "=", "self", ".", "going_to_accumulate_grad_batches", "(", ")", "has_overridden_optimization_functions", "=", "overridden_optimizer_step", "or", "overridden_optimizer_zero_grad", "if", "has_overridden_optimization_functions", "and", "going_to_accumulate_grad_batches", ":", "rank_zero_warn", "(", "\"", "When", "using", "`", "Trainer", "(", "accumulate_grad_batches", "!", "=", "1", ")", "`", "and", "overriding", "\"", "\"", "`", "LightningModule", ".", "optimizer_", "{", "step", ",", "zero_grad", "}", "`", ",", "the", "hooks", "will", "not", "be", "called", "on", "every", "batch", "\"", "\"", "(", "rather", ",", "they", "are", "called", "on", "every", "optimization", "step", ")", ".", "\"", ")", "from", "lightning", ".", "pytorch", ".", "strategies", "import", "DeepSpeedStrategy", "if", "isinstance", "(", "trainer", ".", "strategy", ",", "DeepSpeedStrategy", ")", ":", "raise", "RuntimeError", "(", "f", "\"", "The", "`", "{", "type", "(", "trainer", ".", "strategy", ")", ".", "__name__", "}", "`", "does", "not", "support", "`", "accumulate_grad_batches", "`", "changing", "\"", "\"", "between", "epochs", ".", "\"", ")", "if", "trainer", ".", "accumulate_grad_batches", "!", "=", "1", ":", "raise", "ValueError", "(", "\"", "You", "have", "set", "`", "accumulate_grad_batches", "`", "and", "are", "using", "the", "`", "GradientAccumulationScheduler", "`", "\"", "\"", "callback", ".", "Either", "remove", "`", "accumulate_grad_batches", "`", "from", "the", "Trainer", "or", "remove", "the", "callback", ".", "\"", ")"], "docstring": "Performns a configuration validation before training starts and raises errors for incompatible settings.", "docstring_tokens": ["performns", "a", "configuration", "validation", "before", "training", "starts", "and", "raises", "errors", "for", "incompatible", "settings"], "docstring_summary": "Performns a configuration validation before training starts and raises errors for incompatible settings.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\gradient_accumulation_scheduler.py", "partition": "train", "function_type": "class_method", "class_name": "GradientAccumulationScheduler", "start_line": 103, "end_line": 135, "hash": "ea1d94558c4429e38cff6fcfb2347db8", "complexity": 7, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "func_name": "on_train_start", "original_string": "def on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        # Find names for schedulers\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        # Find names for leftover optimizers\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        # Initialize for storing values\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}", "language": "python", "code": "def on_train_start(self, trainer: \"pl.Trainer\", *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.\r\n\r\n        \"\"\"\r\n        if not trainer.loggers:\r\n            raise MisconfigurationException(\r\n                \"Cannot use `LearningRateMonitor` callback with `Trainer` that has no logger.\"\r\n            )\r\n\r\n        if self.log_momentum:\r\n\r\n            def _check_no_key(key: str) -> bool:\r\n                if trainer.lr_scheduler_configs:\r\n                    return any(\r\n                        key not in config.scheduler.optimizer.defaults for config in trainer.lr_scheduler_configs\r\n                    )\r\n\r\n                return any(key not in optimizer.defaults for optimizer in trainer.optimizers)\r\n\r\n            if _check_no_key(\"momentum\") and _check_no_key(\"betas\"):\r\n                rank_zero_warn(\r\n                    \"You have set log_momentum=True, but some optimizers do not\"\r\n                    \" have momentum. This will log a value 0 for the momentum.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n        # Find names for schedulers\r\n        names: list[list[str]] = []\r\n        (\r\n            sched_hparam_keys,\r\n            optimizers_with_scheduler,\r\n            optimizers_with_scheduler_types,\r\n        ) = self._find_names_from_schedulers(trainer.lr_scheduler_configs)\r\n        names.extend(sched_hparam_keys)\r\n\r\n        # Find names for leftover optimizers\r\n        optimizer_hparam_keys, _ = self._find_names_from_optimizers(\r\n            trainer.optimizers,\r\n            seen_optimizers=optimizers_with_scheduler,\r\n            seen_optimizer_types=optimizers_with_scheduler_types,\r\n        )\r\n        names.extend(optimizer_hparam_keys)\r\n\r\n        # Initialize for storing values\r\n        names_flatten = list(itertools.chain.from_iterable(names))\r\n        self.lrs = {name: [] for name in names_flatten}\r\n        self.last_momentum_values = {name + \"-momentum\": None for name in names_flatten}\r\n        self.last_weight_decay_values = {name + \"-weight_decay\": None for name in names_flatten}", "code_tokens": ["def", "on_train_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "before", "training", ",", "determines", "unique", "names", "for", "all", "lr", "schedulers", "in", "the", "case", "of", "multiple", "of", "the", "same", "type", "or", "in", "the", "case", "of", "multiple", "parameter", "groups", ".", "Raises", ":", "MisconfigurationException", ":", "If", "`", "`", "Trainer", "`", "`", "has", "no", "`", "`", "logger", "`", "`", ".", "\"", "\"", "\"", "if", "not", "trainer", ".", "loggers", ":", "raise", "MisconfigurationException", "(", "\"", "Cannot", "use", "`", "LearningRateMonitor", "`", "callback", "with", "`", "Trainer", "`", "that", "has", "no", "logger", ".", "\"", ")", "if", "self", ".", "log_momentum", ":", "def", "_check_no_key", "(", "key", ":", "str", ")", "-", ">", "bool", ":", "if", "trainer", ".", "lr_scheduler_configs", ":", "return", "any", "(", "key", "not", "in", "config", ".", "scheduler", ".", "optimizer", ".", "defaults", "for", "config", "in", "trainer", ".", "lr_scheduler_configs", ")", "return", "any", "(", "key", "not", "in", "optimizer", ".", "defaults", "for", "optimizer", "in", "trainer", ".", "optimizers", ")", "if", "_check_no_key", "(", "\"", "momentum", "\"", ")", "and", "_check_no_key", "(", "\"", "betas", "\"", ")", ":", "rank_zero_warn", "(", "\"", "You", "have", "set", "log_momentum", "=", "True", ",", "but", "some", "optimizers", "do", "not", "\"", "\"", "have", "momentum", ".", "This", "will", "log", "a", "value", "0", "for", "the", "momentum", ".", "\"", ",", "category", "=", "RuntimeWarning", ",", ")", "names", ":", "list", "[", "list", "[", "str", "]", "]", "=", "[", "]", "(", "sched_hparam_keys", ",", "optimizers_with_scheduler", ",", "optimizers_with_scheduler_types", ",", ")", "=", "self", ".", "_find_names_from_schedulers", "(", "trainer", ".", "lr_scheduler_configs", ")", "names", ".", "extend", "(", "sched_hparam_keys", ")", "optimizer_hparam_keys", ",", "_", "=", "self", ".", "_find_names_from_optimizers", "(", "trainer", ".", "optimizers", ",", "seen_optimizers", "=", "optimizers_with_scheduler", ",", "seen_optimizer_types", "=", "optimizers_with_scheduler_types", ",", ")", "names", ".", "extend", "(", "optimizer_hparam_keys", ")", "names_flatten", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "names", ")", ")", "self", ".", "lrs", "=", "{", "name", ":", "[", "]", "for", "name", "in", "names_flatten", "}", "self", ".", "last_momentum_values", "=", "{", "name", "+", "\"", "-", "momentum", "\"", ":", "None", "for", "name", "in", "names_flatten", "}", "self", ".", "last_weight_decay_values", "=", "{", "name", "+", "\"", "-", "weight_decay", "\"", ":", "None", "for", "name", "in", "names_flatten", "}"], "docstring": "Called before training, determines unique names for all lr schedulers in the case of multiple of the same\r\n        type or in the case of multiple parameter groups.\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If ``Trainer`` has no ``logger``.", "docstring_tokens": ["called", "before", "training", "determines", "unique", "names", "for", "all", "lr", "schedulers", "in", "the", "case", "of", "multiple", "of", "the", "same", "type", "or", "in", "the", "case", "of", "multiple", "parameter", "groups", "raises", "misconfigurationexception", "if", "trainer", "has", "no", "logger"], "docstring_summary": "Called before training, determines unique names for all lr schedulers in the case of multiple of the same", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "partition": "train", "function_type": "class_method", "class_name": "LearningRateMonitor", "start_line": 111, "end_line": 163, "hash": "8354b8c3b2921fb0620d1b15908f5936", "complexity": 11, "parameters": ["trainer", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "func_name": "_remap_keys", "original_string": "def _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []", "language": "python", "code": "def _remap_keys(self, names: list[list[str]], token: str = \"/pg1\") -> None:\r\n        \"\"\"This function is used the remap the keys if param groups for a given optimizer increased.\"\"\"\r\n        for group_new_names in names:\r\n            for new_name in group_new_names:\r\n                old_name = new_name.replace(token, \"\")\r\n                if token in new_name and old_name in self.lrs:\r\n                    self.lrs[new_name] = self.lrs.pop(old_name)\r\n                elif new_name not in self.lrs:\r\n                    self.lrs[new_name] = []", "code_tokens": ["def", "_remap_keys", "(", "self", ",", "names", ":", "list", "[", "list", "[", "str", "]", "]", ",", "token", ":", "str", "=", "\"", "/", "pg1", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "function", "is", "used", "the", "remap", "the", "keys", "if", "param", "groups", "for", "a", "given", "optimizer", "increased", ".", "\"", "\"", "\"", "for", "group_new_names", "in", "names", ":", "for", "new_name", "in", "group_new_names", ":", "old_name", "=", "new_name", ".", "replace", "(", "token", ",", "\"", "\"", ")", "if", "token", "in", "new_name", "and", "old_name", "in", "self", ".", "lrs", ":", "self", ".", "lrs", "[", "new_name", "]", "=", "self", ".", "lrs", ".", "pop", "(", "old_name", ")", "elif", "new_name", "not", "in", "self", ".", "lrs", ":", "self", ".", "lrs", "[", "new_name", "]", "=", "[", "]"], "docstring": "This function is used the remap the keys if param groups for a given optimizer increased.", "docstring_tokens": ["this", "function", "is", "used", "the", "remap", "the", "keys", "if", "param", "groups", "for", "a", "given", "optimizer", "increased"], "docstring_summary": "This function is used the remap the keys if param groups for a given optimizer increased.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "partition": "train", "function_type": "class_method", "class_name": "LearningRateMonitor", "start_line": 243, "end_line": 251, "hash": "ddf50962fb12568115791d6cb70f1099", "complexity": 6, "parameters": ["names", "token"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "func_name": "_extract_weight_decay", "original_string": "def _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}", "language": "python", "code": "def _extract_weight_decay(self, param_group: dict[str, Any], name: str) -> dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        weight_decay = param_group[\"weight_decay\"]\r\n        self.last_weight_decay_values[name] = weight_decay\r\n        return {name: weight_decay}", "code_tokens": ["def", "_extract_weight_decay", "(", "self", ",", "param_group", ":", "dict", "[", "str", ",", "Any", "]", ",", "name", ":", "str", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Extracts", "the", "weight", "decay", "statistics", "from", "a", "parameter", "group", ".", "\"", "\"", "\"", "if", "not", "self", ".", "log_weight_decay", ":", "return", "{", "}", "weight_decay", "=", "param_group", "[", "\"", "weight_decay", "\"", "]", "self", ".", "last_weight_decay_values", "[", "name", "]", "=", "weight_decay", "return", "{", "name", ":", "weight_decay", "}"], "docstring": "Extracts the weight decay statistics from a parameter group.", "docstring_tokens": ["extracts", "the", "weight", "decay", "statistics", "from", "a", "parameter", "group"], "docstring_summary": "Extracts the weight decay statistics from a parameter group.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\lr_monitor.py", "partition": "train", "function_type": "class_method", "class_name": "LearningRateMonitor", "start_line": 261, "end_line": 268, "hash": "46831a994154d3c3d0def4f167b80bee", "complexity": 2, "parameters": ["param_group", "Any]", "name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "on_train_epoch_end", "original_string": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "a", "checkpoint", "at", "the", "end", "of", "the", "training", "epoch", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_should_skip_saving_checkpoint", "(", "trainer", ")", "and", "self", ".", "_should_save_on_train_epoch_end", "(", "trainer", ")", ":", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "if", "self", ".", "_every_n_epochs", ">", "=", "1", "and", "(", "trainer", ".", "current_epoch", "+", "1", ")", "%", "self", ".", "_every_n_epochs", "=", "=", "0", ":", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Save a checkpoint at the end of the training epoch.", "docstring_tokens": ["save", "a", "checkpoint", "at", "the", "end", "of", "the", "training", "epoch"], "docstring_summary": "Save a checkpoint at the end of the training epoch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 376, "end_line": 382, "hash": "45e49154623c51ce656fe7a5f7c07fe6", "complexity": 5, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "on_validation_end", "original_string": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            # If a step/time-triggered save was deferred due to a missing monitored metric,\r\n            # perform the save now that validation metrics are available.\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Save a checkpoint at the end of the validation stage.\"\"\"\r\n        if not self._should_skip_saving_checkpoint(trainer) and not self._should_save_on_train_epoch_end(trainer):\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            # If a step/time-triggered save was deferred due to a missing monitored metric,\r\n            # perform the save now that validation metrics are available.\r\n            if self._defer_save_until_validation:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n                self._save_last_checkpoint(trainer, monitor_candidates)\r\n                self._defer_save_until_validation = False\r\n                return\r\n\r\n            if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n                self._save_topk_checkpoint(trainer, monitor_candidates)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_validation_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "a", "checkpoint", "at", "the", "end", "of", "the", "validation", "stage", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_should_skip_saving_checkpoint", "(", "trainer", ")", "and", "not", "self", ".", "_should_save_on_train_epoch_end", "(", "trainer", ")", ":", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "if", "self", ".", "_defer_save_until_validation", ":", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_defer_save_until_validation", "=", "False", "return", "if", "self", ".", "_every_n_epochs", ">", "=", "1", "and", "(", "trainer", ".", "current_epoch", "+", "1", ")", "%", "self", ".", "_every_n_epochs", "=", "=", "0", ":", "self", ".", "_save_topk_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Save a checkpoint at the end of the validation stage.", "docstring_tokens": ["save", "a", "checkpoint", "at", "the", "end", "of", "the", "validation", "stage"], "docstring_summary": "Save a checkpoint at the end of the validation stage.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 385, "end_line": 399, "hash": "7edfe2e07ec0cab035fb946347abde4d", "complexity": 6, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "on_exception", "original_string": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )", "language": "python", "code": "def on_exception(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", exception: BaseException) -> None:\r\n        \"\"\"Save a checkpoint when an exception is raised.\"\"\"\r\n        if not self._should_save_on_exception(trainer):\r\n            return\r\n        monitor_candidates = self._monitor_candidates(trainer)\r\n        filepath = self.format_checkpoint_name(metrics=monitor_candidates)\r\n        self._save_checkpoint(trainer, filepath)\r\n        self._save_last_checkpoint(trainer, monitor_candidates)\r\n        rank_zero_info(\r\n            f\"An {type(exception).__name__} was raised with message: \\\r\n            {str(exception)}, saved checkpoint to {filepath}\"\r\n        )", "code_tokens": ["def", "on_exception", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "exception", ":", "BaseException", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "a", "checkpoint", "when", "an", "exception", "is", "raised", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_should_save_on_exception", "(", "trainer", ")", ":", "return", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "filepath", "=", "self", ".", "format_checkpoint_name", "(", "metrics", "=", "monitor_candidates", ")", "self", ".", "_save_checkpoint", "(", "trainer", ",", "filepath", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")", "rank_zero_info", "(", "f", "\"", "An", "{", "type", "(", "exception", ")", ".", "__name__", "}", "was", "raised", "with", "message", ":", "\\", "{", "str", "(", "exception", ")", "}", ",", "saved", "checkpoint", "to", "{", "filepath", "}", "\"", ")"], "docstring": "Save a checkpoint when an exception is raised.", "docstring_tokens": ["save", "a", "checkpoint", "when", "an", "exception", "is", "raised"], "docstring_summary": "Save a checkpoint when an exception is raised.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 402, "end_line": 413, "hash": "13e337712ef866c4460661a6e8f8efbf", "complexity": 2, "parameters": ["trainer", "pl_module", "exception"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "on_train_end", "original_string": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "language": "python", "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Ensure save_last=True is applied when training ends.\"\"\"\r\n        if self.save_last and not self._last_checkpoint_saved:\r\n            monitor_candidates = self._monitor_candidates(trainer)\r\n            self._save_last_checkpoint(trainer, monitor_candidates)", "code_tokens": ["def", "on_train_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Ensure", "save_last", "=", "True", "is", "applied", "when", "training", "ends", ".", "\"", "\"", "\"", "if", "self", ".", "save_last", "and", "not", "self", ".", "_last_checkpoint_saved", ":", "monitor_candidates", "=", "self", ".", "_monitor_candidates", "(", "trainer", ")", "self", ".", "_save_last_checkpoint", "(", "trainer", ",", "monitor_candidates", ")"], "docstring": "Ensure save_last=True is applied when training ends.", "docstring_tokens": ["ensure", "save_last", "true", "is", "applied", "when", "training", "ends"], "docstring_summary": "Ensure save_last=True is applied when training ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 415, "end_line": 419, "hash": "e28e1fc0a53c63a147d375501b99368e", "complexity": 3, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "format_checkpoint_name", "original_string": "def format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name", "language": "python", "code": "def format_checkpoint_name(\r\n        self,\r\n        metrics: dict[str, Tensor],\r\n        filename: Optional[str] = None,\r\n        ver: Optional[int] = None,\r\n        prefix: Optional[str] = None,\r\n    ) -> str:\r\n        \"\"\"Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'\r\n\r\n        \"\"\"\r\n        filename = filename or self.filename\r\n        filename = self._format_checkpoint_name(\r\n            filename, metrics, prefix=prefix, auto_insert_metric_name=self.auto_insert_metric_name\r\n        )\r\n\r\n        if ver is not None:\r\n            filename = self.CHECKPOINT_JOIN_CHAR.join((filename, f\"v{ver}\"))\r\n\r\n        ckpt_name = f\"{filename}{self.FILE_EXTENSION}\"\r\n        return os.path.join(self.dirpath, ckpt_name) if self.dirpath else ckpt_name", "code_tokens": ["def", "format_checkpoint_name", "(", "self", ",", "metrics", ":", "dict", "[", "str", ",", "Tensor", "]", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "ver", ":", "Optional", "[", "int", "]", "=", "None", ",", "prefix", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Generate", "a", "filename", "according", "to", "the", "defined", "template", ".", "Example", ":", ":", ">", ">", ">", "tmpdir", "=", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ">", ">", ">", "ckpt", "=", "ModelCheckpoint", "(", "dirpath", "=", "tmpdir", ",", "filename", "=", "'", "{", "epoch", "}", "'", ")", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "dict", "(", "epoch", "=", "0", ")", ")", ")", "'", "epoch", "=", "0", ".", "ckpt", "'", ">", ">", ">", "ckpt", "=", "ModelCheckpoint", "(", "dirpath", "=", "tmpdir", ",", "filename", "=", "'", "{", "epoch", ":", "03d", "}", "'", ")", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "dict", "(", "epoch", "=", "5", ")", ")", ")", "'", "epoch", "=", "005", ".", "ckpt", "'", ">", ">", ">", "ckpt", "=", "ModelCheckpoint", "(", "dirpath", "=", "tmpdir", ",", "filename", "=", "'", "{", "epoch", "}", "-", "{", "val_loss", ":", ".", "2f", "}", "'", ")", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "dict", "(", "epoch", "=", "2", ",", "val_loss", "=", "0", ".", "123456", ")", ")", ")", "'", "epoch", "=", "2", "-", "val_loss", "=", "0", ".", "12", ".", "ckpt", "'", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "dict", "(", "epoch", "=", "2", ",", "val_loss", "=", "0", ".", "12", ")", ",", "filename", "=", "'", "{", "epoch", ":", "d", "}", "'", ")", ")", "'", "epoch", "=", "2", ".", "ckpt", "'", ">", ">", ">", "ckpt", "=", "ModelCheckpoint", "(", "dirpath", "=", "tmpdir", ",", ".", ".", ".", "filename", "=", "'", "epoch", "=", "{", "epoch", "}", "-", "validation_loss", "=", "{", "val_loss", ":", ".", "2f", "}", "'", ",", ".", ".", ".", "auto_insert_metric_name", "=", "False", ")", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "dict", "(", "epoch", "=", "2", ",", "val_loss", "=", "0", ".", "123456", ")", ")", ")", "'", "epoch", "=", "2", "-", "validation_loss", "=", "0", ".", "12", ".", "ckpt", "'", ">", ">", ">", "ckpt", "=", "ModelCheckpoint", "(", "dirpath", "=", "tmpdir", ",", "filename", "=", "'", "{", "missing", ":", "d", "}", "'", ")", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "{", "}", ")", ")", "'", "missing", "=", "0", ".", "ckpt", "'", ">", ">", ">", "ckpt", "=", "ModelCheckpoint", "(", "filename", "=", "'", "{", "step", "}", "'", ")", ">", ">", ">", "os", ".", "path", ".", "basename", "(", "ckpt", ".", "format_checkpoint_name", "(", "dict", "(", "step", "=", "0", ")", ")", ")", "'", "step", "=", "0", ".", "ckpt", "'", "\"", "\"", "\"", "filename", "=", "filename", "or", "self", ".", "filename", "filename", "=", "self", ".", "_format_checkpoint_name", "(", "filename", ",", "metrics", ",", "prefix", "=", "prefix", ",", "auto_insert_metric_name", "=", "self", ".", "auto_insert_metric_name", ")", "if", "ver", "is", "not", "None", ":", "filename", "=", "self", ".", "CHECKPOINT_JOIN_CHAR", ".", "join", "(", "(", "filename", ",", "f", "\"", "v", "{", "ver", "}", "\"", ")", ")", "ckpt_name", "=", "f", "\"", "{", "filename", "}", "{", "self", ".", "FILE_EXTENSION", "}", "\"", "return", "os", ".", "path", ".", "join", "(", "self", ".", "dirpath", ",", "ckpt_name", ")", "if", "self", ".", "dirpath", "else", "ckpt_name"], "docstring": "Generate a filename according to the defined template.\r\n\r\n        Example::\r\n\r\n            >>> tmpdir = os.path.dirname(__file__)\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=0)))\r\n            'epoch=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch:03d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=5)))\r\n            'epoch=005.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{epoch}-{val_loss:.2f}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-val_loss=0.12.ckpt'\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.12), filename='{epoch:d}'))\r\n            'epoch=2.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir,\r\n            ... filename='epoch={epoch}-validation_loss={val_loss:.2f}',\r\n            ... auto_insert_metric_name=False)\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(epoch=2, val_loss=0.123456)))\r\n            'epoch=2-validation_loss=0.12.ckpt'\r\n            >>> ckpt = ModelCheckpoint(dirpath=tmpdir, filename='{missing:d}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name({}))\r\n            'missing=0.ckpt'\r\n            >>> ckpt = ModelCheckpoint(filename='{step}')\r\n            >>> os.path.basename(ckpt.format_checkpoint_name(dict(step=0)))\r\n            'step=0.ckpt'", "docstring_tokens": ["generate", "a", "filename", "according", "to", "the", "defined", "template", "example", "tmpdir", "os", "path", "dirname", "__file__", "ckpt", "modelcheckpoint", "dirpath", "tmpdir", "filename", "epoch", "os", "path", "basename", "ckpt", "format_checkpoint_name", "dict", "epoch", "0", "epoch", "0", "ckpt", "ckpt", "modelcheckpoint", "dirpath", "tmpdir", "filename", "epoch", "03d", "os", "path", "basename", "ckpt", "format_checkpoint_name", "dict", "epoch", "5", "epoch", "005", "ckpt", "ckpt", "modelcheckpoint", "dirpath", "tmpdir", "filename", "epoch", "val_loss", "2f", "os", "path", "basename", "ckpt", "format_checkpoint_name", "dict", "epoch", "2", "val_loss", "0", "123456", "epoch", "2", "val_loss", "0", "12", "ckpt", "os", "path", "basename", "ckpt", "format_checkpoint_name", "dict", "epoch", "2", "val_loss", "0", "12", "filename", "epoch", "d", "epoch", "2", "ckpt", "ckpt", "modelcheckpoint", "dirpath", "tmpdir", "filename", "epoch", "epoch", "validation_loss", "val_loss", "2f", "auto_insert_metric_name", "false", "os", "path", "basename", "ckpt", "format_checkpoint_name", "dict", "epoch", "2", "val_loss", "0", "123456", "epoch", "2", "validation_loss", "0", "12", "ckpt", "ckpt", "modelcheckpoint", "dirpath", "tmpdir", "filename", "missing", "d", "os", "path", "basename", "ckpt", "format_checkpoint_name", "missing", "0", "ckpt", "ckpt", "modelcheckpoint", "filename", "step", "os", "path", "basename", "ckpt", "format_checkpoint_name", "dict", "step", "0", "step", "0", "ckpt"], "docstring_summary": "Generate a filename according to the defined template.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 665, "end_line": 710, "hash": "11456a7b1a52001589415ef0dcb10069", "complexity": 4, "parameters": ["metrics", "Tensor]", "filename", "ver", "prefix"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "__resolve_ckpt_dir", "original_string": "def __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            # short circuit if dirpath was passed to ModelCheckpoint\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            # if no loggers, use default_root_dir\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path", "language": "python", "code": "def __resolve_ckpt_dir(self, trainer: \"pl.Trainer\") -> _PATH:\r\n        \"\"\"Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".\r\n\r\n        \"\"\"\r\n        if self.dirpath is not None:\r\n            # short circuit if dirpath was passed to ModelCheckpoint\r\n            return self.dirpath\r\n\r\n        if len(trainer.loggers) > 0:\r\n            if trainer.loggers[0].save_dir is not None:\r\n                save_dir = trainer.loggers[0].save_dir\r\n            else:\r\n                save_dir = trainer.default_root_dir\r\n            name = trainer.loggers[0].name\r\n            version = trainer.loggers[0].version\r\n            version = version if isinstance(version, str) else f\"version_{version}\"\r\n            ckpt_path = os.path.join(save_dir, str(name), version, \"checkpoints\")\r\n        else:\r\n            # if no loggers, use default_root_dir\r\n            ckpt_path = os.path.join(trainer.default_root_dir, \"checkpoints\")\r\n\r\n        return ckpt_path", "code_tokens": ["def", "__resolve_ckpt_dir", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "_PATH", ":", "\"", "\"", "\"", "Determines", "model", "checkpoint", "save", "directory", "at", "runtime", ".", "Reference", "attributes", "from", "the", "trainer", "'", "s", "logger", "to", "determine", "where", "to", "save", "checkpoints", ".", "The", "path", "for", "saving", "weights", "is", "set", "in", "this", "priority", ":", "1", ".", "The", "`", "`", "ModelCheckpoint", "`", "`", "'", "s", "`", "`", "dirpath", "`", "`", "if", "passed", "in", "2", ".", "The", "`", "`", "Logger", "`", "`", "'", "s", "`", "`", "log_dir", "`", "`", "if", "the", "trainer", "has", "loggers", "3", ".", "The", "`", "`", "Trainer", "`", "`", "'", "s", "`", "`", "default_root_dir", "`", "`", "if", "the", "trainer", "has", "no", "loggers", "The", "path", "gets", "extended", "with", "subdirectory", "\"", "checkpoints", "\"", ".", "\"", "\"", "\"", "if", "self", ".", "dirpath", "is", "not", "None", ":", "return", "self", ".", "dirpath", "if", "len", "(", "trainer", ".", "loggers", ")", ">", "0", ":", "if", "trainer", ".", "loggers", "[", "0", "]", ".", "save_dir", "is", "not", "None", ":", "save_dir", "=", "trainer", ".", "loggers", "[", "0", "]", ".", "save_dir", "else", ":", "save_dir", "=", "trainer", ".", "default_root_dir", "name", "=", "trainer", ".", "loggers", "[", "0", "]", ".", "name", "version", "=", "trainer", ".", "loggers", "[", "0", "]", ".", "version", "version", "=", "version", "if", "isinstance", "(", "version", ",", "str", ")", "else", "f", "\"", "version_", "{", "version", "}", "\"", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "str", "(", "name", ")", ",", "version", ",", "\"", "checkpoints", "\"", ")", "else", ":", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "trainer", ".", "default_root_dir", ",", "\"", "checkpoints", "\"", ")", "return", "ckpt_path"], "docstring": "Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to\r\n        determine where to save checkpoints. The path for saving weights is set in this priority:\r\n\r\n        1.  The ``ModelCheckpoint``'s ``dirpath`` if passed in\r\n        2.  The ``Logger``'s ``log_dir`` if the trainer has loggers\r\n        3.  The ``Trainer``'s ``default_root_dir`` if the trainer has no loggers\r\n\r\n        The path gets extended with subdirectory \"checkpoints\".", "docstring_tokens": ["determines", "model", "checkpoint", "save", "directory", "at", "runtime", "reference", "attributes", "from", "the", "trainer", "s", "logger", "to", "determine", "where", "to", "save", "checkpoints", "the", "path", "for", "saving", "weights", "is", "set", "in", "this", "priority", "1", "the", "modelcheckpoint", "s", "dirpath", "if", "passed", "in", "2", "the", "logger", "s", "log_dir", "if", "the", "trainer", "has", "loggers", "3", "the", "trainer", "s", "default_root_dir", "if", "the", "trainer", "has", "no", "loggers", "the", "path", "gets", "extended", "with", "subdirectory", "checkpoints"], "docstring_summary": "Determines model checkpoint save directory at runtime. Reference attributes from the trainer's logger to", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 712, "end_line": 740, "hash": "ff8c36d80daf3f30e5e93a0e47b993ef", "complexity": 5, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "to_yaml", "original_string": "def to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)", "language": "python", "code": "def to_yaml(self, filepath: Optional[_PATH] = None) -> None:\r\n        \"\"\"Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.\"\"\"\r\n        best_k = {k: v.item() for k, v in self.best_k_models.items()}\r\n        if filepath is None:\r\n            assert self.dirpath\r\n            filepath = os.path.join(self.dirpath, \"best_k_models.yaml\")\r\n        with self._fs.open(filepath, \"w\") as fp:\r\n            yaml.dump(best_k, fp)", "code_tokens": ["def", "to_yaml", "(", "self", ",", "filepath", ":", "Optional", "[", "_PATH", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Saves", "the", "`", "best_k_models", "`", "dict", "containing", "the", "checkpoint", "paths", "with", "the", "corresponding", "scores", "to", "a", "YAML", "file", ".", "\"", "\"", "\"", "best_k", "=", "{", "k", ":", "v", ".", "item", "(", ")", "for", "k", ",", "v", "in", "self", ".", "best_k_models", ".", "items", "(", ")", "}", "if", "filepath", "is", "None", ":", "assert", "self", ".", "dirpath", "filepath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dirpath", ",", "\"", "best_k_models", ".", "yaml", "\"", ")", "with", "self", ".", "_fs", ".", "open", "(", "filepath", ",", "\"", "w", "\"", ")", "as", "fp", ":", "yaml", ".", "dump", "(", "best_k", ",", "fp", ")"], "docstring": "Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML\r\n        file.", "docstring_tokens": ["saves", "the", "best_k_models", "dict", "containing", "the", "checkpoint", "paths", "with", "the", "corresponding", "scores", "to", "a", "yaml", "file"], "docstring_summary": "Saves the `best_k_models` dict containing the checkpoint paths with the corresponding scores to a YAML", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 864, "end_line": 872, "hash": "d475e57c208daa2e59c94ba74bdfdede", "complexity": 4, "parameters": ["filepath"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "file_exists", "original_string": "def file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)", "language": "python", "code": "def file_exists(self, filepath: _PATH, trainer: \"pl.Trainer\") -> bool:\r\n        \"\"\"Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.\"\"\"\r\n        exists = self._fs.exists(filepath)\r\n        return trainer.strategy.broadcast(exists)", "code_tokens": ["def", "file_exists", "(", "self", ",", "filepath", ":", "_PATH", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Checks", "if", "a", "file", "exists", "on", "rank", "0", "and", "broadcasts", "the", "result", "to", "all", "other", "ranks", ",", "preventing", "the", "internal", "state", "to", "diverge", "between", "ranks", ".", "\"", "\"", "\"", "exists", "=", "self", ".", "_fs", ".", "exists", "(", "filepath", ")", "return", "trainer", ".", "strategy", ".", "broadcast", "(", "exists", ")"], "docstring": "Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal\r\n        state to diverge between ranks.", "docstring_tokens": ["checks", "if", "a", "file", "exists", "on", "rank", "0", "and", "broadcasts", "the", "result", "to", "all", "other", "ranks", "preventing", "the", "internal", "state", "to", "diverge", "between", "ranks"], "docstring_summary": "Checks if a file exists on rank 0 and broadcasts the result to all other ranks, preventing the internal", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 874, "end_line": 878, "hash": "5ad15cd28fc0ca3e3b4544eb98badeba", "complexity": 1, "parameters": ["filepath", "trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "func_name": "_should_remove_checkpoint", "original_string": "def _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents", "language": "python", "code": "def _should_remove_checkpoint(self, trainer: \"pl.Trainer\", previous: str, current: str) -> bool:\r\n        \"\"\"Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local\r\n\r\n        \"\"\"\r\n        if previous == current:\r\n            return False\r\n        if not _is_local_file_protocol(previous):\r\n            return True\r\n        previous = Path(previous).absolute()\r\n        resume_path = Path(trainer.ckpt_path).absolute() if trainer.ckpt_path is not None else None\r\n        if resume_path is not None and previous == resume_path:\r\n            return False\r\n        assert self.dirpath is not None\r\n        dirpath = Path(self.dirpath).absolute()\r\n        return dirpath in previous.parents", "code_tokens": ["def", "_should_remove_checkpoint", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "previous", ":", "str", ",", "current", ":", "str", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Checks", "if", "the", "previous", "checkpoint", "should", "be", "deleted", ".", "A", "checkpoint", "won", "'", "t", "be", "deleted", "if", "any", "of", "the", "cases", "apply", ":", "-", "The", "previous", "checkpoint", "is", "the", "same", "as", "the", "current", "checkpoint", "(", "means", "the", "old", "was", "already", "overwritten", "by", "new", ")", "-", "The", "previous", "checkpoint", "is", "not", "in", "the", "current", "checkpoint", "directory", "and", "the", "filesystem", "is", "local", "-", "The", "previous", "checkpoint", "is", "the", "checkpoint", "the", "Trainer", "resumed", "from", "and", "the", "filesystem", "is", "local", "\"", "\"", "\"", "if", "previous", "=", "=", "current", ":", "return", "False", "if", "not", "_is_local_file_protocol", "(", "previous", ")", ":", "return", "True", "previous", "=", "Path", "(", "previous", ")", ".", "absolute", "(", ")", "resume_path", "=", "Path", "(", "trainer", ".", "ckpt_path", ")", ".", "absolute", "(", ")", "if", "trainer", ".", "ckpt_path", "is", "not", "None", "else", "None", "if", "resume_path", "is", "not", "None", "and", "previous", "=", "=", "resume_path", ":", "return", "False", "assert", "self", ".", "dirpath", "is", "not", "None", "dirpath", "=", "Path", "(", "self", ".", "dirpath", ")", ".", "absolute", "(", ")", "return", "dirpath", "in", "previous", ".", "parents"], "docstring": "Checks if the previous checkpoint should be deleted.\r\n\r\n        A checkpoint won't be deleted if any of the cases apply:\r\n        - The previous checkpoint is the same as the current checkpoint (means the old was already overwritten by new)\r\n        - The previous checkpoint is not in the current checkpoint directory and the filesystem is local\r\n        - The previous checkpoint is the checkpoint the Trainer resumed from and the filesystem is local", "docstring_tokens": ["checks", "if", "the", "previous", "checkpoint", "should", "be", "deleted", "a", "checkpoint", "won", "t", "be", "deleted", "if", "any", "of", "the", "cases", "apply", "the", "previous", "checkpoint", "is", "the", "same", "as", "the", "current", "checkpoint", "means", "the", "old", "was", "already", "overwritten", "by", "new", "the", "previous", "checkpoint", "is", "not", "in", "the", "current", "checkpoint", "directory", "and", "the", "filesystem", "is", "local", "the", "previous", "checkpoint", "is", "the", "checkpoint", "the", "trainer", "resumed", "from", "and", "the", "filesystem", "is", "local"], "docstring_summary": "Checks if the previous checkpoint should be deleted.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\model_checkpoint.py", "partition": "train", "function_type": "class_method", "class_name": "ModelCheckpoint", "start_line": 880, "end_line": 899, "hash": "1b79f2dbf2d0e43de7dee4cb364100d0", "complexity": 6, "parameters": ["trainer", "previous", "current"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "func_name": "write_on_batch_end", "original_string": "def write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()", "language": "python", "code": "def write_on_batch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        prediction: Any,\r\n        batch_indices: Optional[Sequence[int]],\r\n        batch: Any,\r\n        batch_idx: int,\r\n        dataloader_idx: int,\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write a single batch.\"\"\"\r\n        raise NotImplementedError()", "code_tokens": ["def", "write_on_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "prediction", ":", "Any", ",", "batch_indices", ":", "Optional", "[", "Sequence", "[", "int", "]", "]", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Override", "with", "the", "logic", "to", "write", "a", "single", "batch", ".", "\"", "\"", "\"", "raise", "NotImplementedError", "(", ")"], "docstring": "Override with the logic to write a single batch.", "docstring_tokens": ["override", "with", "the", "logic", "to", "write", "a", "single", "batch"], "docstring_summary": "Override with the logic to write a single batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "partition": "train", "function_type": "class_method", "class_name": "BasePredictionWriter", "start_line": 119, "end_line": 130, "hash": "b16b98a848e39c51f301e8923e3a3d48", "complexity": 1, "parameters": ["trainer", "pl_module", "prediction", "batch_indices", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "func_name": "write_on_epoch_end", "original_string": "def write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()", "language": "python", "code": "def write_on_epoch_end(\r\n        self,\r\n        trainer: \"pl.Trainer\",\r\n        pl_module: \"pl.LightningModule\",\r\n        predictions: Sequence[Any],\r\n        batch_indices: Sequence[Any],\r\n    ) -> None:\r\n        \"\"\"Override with the logic to write all batches.\"\"\"\r\n        raise NotImplementedError()", "code_tokens": ["def", "write_on_epoch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "predictions", ":", "Sequence", "[", "Any", "]", ",", "batch_indices", ":", "Sequence", "[", "Any", "]", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Override", "with", "the", "logic", "to", "write", "all", "batches", ".", "\"", "\"", "\"", "raise", "NotImplementedError", "(", ")"], "docstring": "Override with the logic to write all batches.", "docstring_tokens": ["override", "with", "the", "logic", "to", "write", "all", "batches"], "docstring_summary": "Override with the logic to write all batches.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\prediction_writer.py", "partition": "train", "function_type": "class_method", "class_name": "BasePredictionWriter", "start_line": 132, "end_line": 140, "hash": "2b2b41a5c3e51012c76c1db3f8314d6a", "complexity": 1, "parameters": ["trainer", "pl_module", "predictions", "batch_indices"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "_create_pruning_fn", "original_string": "def _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        # save the function __name__ now because partial does not include it\r\n        # and there are issues setting the attribute manually in ddp.\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)", "language": "python", "code": "def _create_pruning_fn(self, pruning_fn: str, **kwargs: Any) -> Union[Callable, pytorch_prune.BasePruningMethod]:\r\n        \"\"\"This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.\r\n\r\n        \"\"\"\r\n        pruning_meth = (\r\n            _PYTORCH_PRUNING_METHOD[pruning_fn]\r\n            if self._use_global_unstructured\r\n            else _PYTORCH_PRUNING_FUNCTIONS[pruning_fn]\r\n        )\r\n        assert callable(pruning_meth), \"Selected pruning method is not callable\"\r\n        if self._use_global_unstructured:\r\n            self._global_kwargs = kwargs\r\n        # save the function __name__ now because partial does not include it\r\n        # and there are issues setting the attribute manually in ddp.\r\n        self._pruning_method_name = pruning_meth.__name__\r\n        if self._use_global_unstructured:\r\n            return pruning_meth\r\n        return ModelPruning._wrap_pruning_fn(pruning_meth, **kwargs)", "code_tokens": ["def", "_create_pruning_fn", "(", "self", ",", "pruning_fn", ":", "str", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Union", "[", "Callable", ",", "pytorch_prune", ".", "BasePruningMethod", "]", ":", "\"", "\"", "\"", "This", "function", "takes", "`", "pruning_fn", "`", ",", "a", "function", "name", ".", "IF", "use_global_unstructured", ",", "pruning_fn", "will", "be", "resolved", "into", "its", "associated", "`", "`", "PyTorch", "BasePruningMethod", "`", "`", "ELSE", ",", "pruning_fn", "will", "be", "resolved", "into", "its", "function", "counterpart", "from", "`", "torch", ".", "nn", ".", "utils", ".", "prune", "`", ".", "\"", "\"", "\"", "pruning_meth", "=", "(", "_PYTORCH_PRUNING_METHOD", "[", "pruning_fn", "]", "if", "self", ".", "_use_global_unstructured", "else", "_PYTORCH_PRUNING_FUNCTIONS", "[", "pruning_fn", "]", ")", "assert", "callable", "(", "pruning_meth", ")", ",", "\"", "Selected", "pruning", "method", "is", "not", "callable", "\"", "if", "self", ".", "_use_global_unstructured", ":", "self", ".", "_global_kwargs", "=", "kwargs", "self", ".", "_pruning_method_name", "=", "pruning_meth", ".", "__name__", "if", "self", ".", "_use_global_unstructured", ":", "return", "pruning_meth", "return", "ModelPruning", ".", "_wrap_pruning_fn", "(", "pruning_meth", ",", "*", "*", "kwargs", ")"], "docstring": "This function takes `pruning_fn`, a function name.\r\n\r\n        IF use_global_unstructured, pruning_fn will be resolved into its associated ``PyTorch BasePruningMethod`` ELSE,\r\n        pruning_fn will be resolved into its function counterpart from `torch.nn.utils.prune`.", "docstring_tokens": ["this", "function", "takes", "pruning_fn", "a", "function", "name", "if", "use_global_unstructured", "pruning_fn", "will", "be", "resolved", "into", "its", "associated", "pytorch", "basepruningmethod", "else", "pruning_fn", "will", "be", "resolved", "into", "its", "function", "counterpart", "from", "torch", "nn", "utils", "prune"], "docstring_summary": "This function takes `pruning_fn`, a function name.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\pruning.py", "partition": "train", "function_type": "class_method", "class_name": "ModelPruning", "start_line": 238, "end_line": 258, "hash": "a95c9c2666e6ae1f296dabafb7eaf9cd", "complexity": 4, "parameters": ["pruning_fn", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "make_pruning_permanent", "original_string": "def make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]", "language": "python", "code": "def make_pruning_permanent(self, module: nn.Module) -> None:\r\n        \"\"\"Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122\r\n\r\n        \"\"\"\r\n        for _, module in module.named_modules():\r\n            for k in list(module._forward_pre_hooks):\r\n                hook = module._forward_pre_hooks[k]\r\n                if isinstance(hook, pytorch_prune.BasePruningMethod):\r\n                    hook.remove(module)\r\n                    del module._forward_pre_hooks[k]", "code_tokens": ["def", "make_pruning_permanent", "(", "self", ",", "module", ":", "nn", ".", "Module", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Removes", "pruning", "buffers", "from", "any", "pruned", "modules", ".", "Adapted", "from", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "blob", "/", "v1", ".", "7", ".", "1", "/", "torch", "/", "nn", "/", "utils", "/", "prune", ".", "py", "\"", "\"", "\"", "for", "_", ",", "module", "in", "module", ".", "named_modules", "(", ")", ":", "for", "k", "in", "list", "(", "module", ".", "_forward_pre_hooks", ")", ":", "hook", "=", "module", ".", "_forward_pre_hooks", "[", "k", "]", "if", "isinstance", "(", "hook", ",", "pytorch_prune", ".", "BasePruningMethod", ")", ":", "hook", ".", "remove", "(", "module", ")", "del", "module", ".", "_forward_pre_hooks", "[", "k", "]"], "docstring": "Removes pruning buffers from any pruned modules.\r\n\r\n        Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/nn/utils/prune.py#L1118-L1122", "docstring_tokens": ["removes", "pruning", "buffers", "from", "any", "pruned", "modules", "adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "nn", "utils", "prune", "py", "l1118", "l1122"], "docstring_summary": "Removes pruning buffers from any pruned modules.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\pruning.py", "partition": "train", "function_type": "class_method", "class_name": "ModelPruning", "start_line": 264, "end_line": 275, "hash": "41527e1fe77f73fe843e7f492e040a4b", "complexity": 4, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "apply_lottery_ticket_hypothesis", "original_string": "def apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)", "language": "python", "code": "def apply_lottery_ticket_hypothesis(self) -> None:\r\n        r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"  # noqa: E501\r\n        assert self._original_layers is not None\r\n        for d in self._original_layers.values():\r\n            copy = d[\"data\"]\r\n            names = d[\"names\"]\r\n            if self._resample_parameters and hasattr(copy, \"reset_parameters\") and callable(copy.reset_parameters):\r\n                copy = deepcopy(copy)  # keep the original parameters\r\n                copy.reset_parameters()\r\n            for i, name in names:\r\n                new, _ = self._parameters_to_prune[i]\r\n                self._copy_param(new, copy, name)", "code_tokens": ["def", "apply_lottery_ticket_hypothesis", "(", "self", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Lottery", "ticket", "hypothesis", "algorithm", "(", "see", "page", "2", "of", "the", "paper", ")", ":", "1", ".", "Randomly", "initialize", "a", "neural", "network", ":", "math", ":", "`", "f", "(", "x", ";", "\\", "theta_0", ")", "`", "(", "where", ":", "math", ":", "`", "\\", "theta_0", "\\", "sim", "\\", "mathcal", "{", "D", "}", "_", "\\", "theta", "`", ")", ".", "2", ".", "Train", "the", "network", "for", ":", "math", ":", "`", "j", "`", "iterations", ",", "arriving", "at", "parameters", ":", "math", ":", "`", "\\", "theta_j", "`", ".", "3", ".", "Prune", ":", "math", ":", "`", "p", "\\", "%", "`", "of", "the", "parameters", "in", ":", "math", ":", "`", "\\", "theta_j", "`", ",", "creating", "a", "mask", ":", "math", ":", "`", "m", "`", ".", "4", ".", "Reset", "the", "remaining", "parameters", "to", "their", "values", "in", ":", "math", ":", "`", "\\", "theta_0", "`", ",", "creating", "the", "winning", "ticket", ":", "math", ":", "`", "f", "(", "x", ";", "m", "\\", "odot", "\\", "theta_0", ")", "`", ".", "This", "function", "implements", "the", "step", "4", ".", "The", "`", "`", "resample_parameters", "`", "`", "argument", "can", "be", "used", "to", "reset", "the", "parameters", "with", "a", "new", ":", "math", ":", "`", "\\", "theta_z", "\\", "sim", "\\", "mathcal", "{", "D", "}", "_", "\\", "theta", "`", "\"", "\"", "\"", "assert", "self", ".", "_original_layers", "is", "not", "None", "for", "d", "in", "self", ".", "_original_layers", ".", "values", "(", ")", ":", "copy", "=", "d", "[", "\"", "data", "\"", "]", "names", "=", "d", "[", "\"", "names", "\"", "]", "if", "self", ".", "_resample_parameters", "and", "hasattr", "(", "copy", ",", "\"", "reset_parameters", "\"", ")", "and", "callable", "(", "copy", ".", "reset_parameters", ")", ":", "copy", "=", "deepcopy", "(", "copy", ")", "copy", ".", "reset_parameters", "(", ")", "for", "i", ",", "name", "in", "names", ":", "new", ",", "_", "=", "self", ".", "_parameters_to_prune", "[", "i", "]", "self", ".", "_copy_param", "(", "new", ",", "copy", ",", "name", ")"], "docstring": "r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):\r\n\r\n            1. Randomly initialize a neural network :math:`f(x; \\theta_0)` (where :math:`\\theta_0 \\sim \\mathcal{D}_\\theta`).\r\n            2. Train the network for :math:`j` iterations, arriving at parameters :math:`\\theta_j`.\r\n            3. Prune :math:`p\\%` of the parameters in :math:`\\theta_j`, creating a mask :math:`m`.\r\n            4. Reset the remaining parameters to their values in :math:`\\theta_0`, creating the winning ticket :math:`f(x; m \\odot \\theta_0)`.\r\n\r\n        This function implements the step 4.\r\n\r\n        The ``resample_parameters`` argument can be used to reset the parameters with a new :math:`\\theta_z \\sim \\mathcal{D}_\\theta`\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "lottery", "ticket", "hypothesis", "algorithm", "see", "page", "2", "of", "the", "paper", "1", "randomly", "initialize", "a", "neural", "network", "math", "f", "x", "theta_0", "where", "math", "theta_0", "sim", "mathcal", "d", "_", "theta", "2", "train", "the", "network", "for", "math", "j", "iterations", "arriving", "at", "parameters", "math", "theta_j", "3", "prune", "math", "p", "of", "the", "parameters", "in", "math", "theta_j", "creating", "a", "mask", "math", "m", "4", "reset", "the", "remaining", "parameters", "to", "their", "values", "in", "math", "theta_0", "creating", "the", "winning", "ticket", "math", "f", "x", "m", "odot", "theta_0", "this", "function", "implements", "the", "step", "4", "the", "resample_parameters", "argument", "can", "be", "used", "to", "reset", "the", "parameters", "with", "a", "new", "math", "theta_z", "sim", "mathcal", "d", "_", "theta"], "docstring_summary": "r\"\"\"Lottery ticket hypothesis algorithm (see page 2 of the paper):", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\pruning.py", "partition": "train", "function_type": "class_method", "class_name": "ModelPruning", "start_line": 286, "end_line": 308, "hash": "d49f7ec6e6fe10e0e9f20425aef0c806", "complexity": 6, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\pruning.py", "func_name": "apply_pruning", "original_string": "def apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)", "language": "python", "code": "def apply_pruning(self, amount: Union[int, float]) -> None:\r\n        \"\"\"Applies pruning to ``parameters_to_prune``.\"\"\"\r\n        if self._verbose:\r\n            prev_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n\r\n        if self._use_global_unstructured:\r\n            self._apply_global_pruning(amount)\r\n        else:\r\n            self._apply_local_pruning(amount)\r\n\r\n        if self._verbose:\r\n            curr_stats = [self._get_pruned_stats(m, n) for m, n in self._parameters_to_prune]\r\n            self._log_sparsity_stats(prev_stats, curr_stats, amount=amount)", "code_tokens": ["def", "apply_pruning", "(", "self", ",", "amount", ":", "Union", "[", "int", ",", "float", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Applies", "pruning", "to", "`", "`", "parameters_to_prune", "`", "`", ".", "\"", "\"", "\"", "if", "self", ".", "_verbose", ":", "prev_stats", "=", "[", "self", ".", "_get_pruned_stats", "(", "m", ",", "n", ")", "for", "m", ",", "n", "in", "self", ".", "_parameters_to_prune", "]", "if", "self", ".", "_use_global_unstructured", ":", "self", ".", "_apply_global_pruning", "(", "amount", ")", "else", ":", "self", ".", "_apply_local_pruning", "(", "amount", ")", "if", "self", ".", "_verbose", ":", "curr_stats", "=", "[", "self", ".", "_get_pruned_stats", "(", "m", ",", "n", ")", "for", "m", ",", "n", "in", "self", ".", "_parameters_to_prune", "]", "self", ".", "_log_sparsity_stats", "(", "prev_stats", ",", "curr_stats", ",", "amount", "=", "amount", ")"], "docstring": "Applies pruning to ``parameters_to_prune``.", "docstring_tokens": ["applies", "pruning", "to", "parameters_to_prune"], "docstring_summary": "Applies pruning to ``parameters_to_prune``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\pruning.py", "partition": "train", "function_type": "class_method", "class_name": "ModelPruning", "start_line": 333, "end_line": 345, "hash": "1378507b9a21deb3f28cab1dc7d4a233", "complexity": 6, "parameters": ["amount", "float]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "reset_batch_norm_and_save_state", "original_string": "def reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0", "language": "python", "code": "def reset_batch_norm_and_save_state(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.\"\"\"\r\n        self.momenta = {}\r\n        for module in pl_module.modules():\r\n            if not isinstance(module, nn.modules.batchnorm._BatchNorm):\r\n                continue\r\n            assert module.running_mean is not None\r\n            module.running_mean = torch.zeros_like(\r\n                module.running_mean,\r\n                device=pl_module.device,\r\n                dtype=module.running_mean.dtype,\r\n            )\r\n            assert module.running_var is not None\r\n            module.running_var = torch.ones_like(\r\n                module.running_var,\r\n                device=pl_module.device,\r\n                dtype=module.running_var.dtype,\r\n            )\r\n            self.momenta[module] = module.momentum\r\n            module.momentum = None\r\n            assert module.num_batches_tracked is not None\r\n            module.num_batches_tracked *= 0", "code_tokens": ["def", "reset_batch_norm_and_save_state", "(", "self", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adapted", "from", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "blob", "/", "v1", ".", "7", ".", "1", "/", "torch", "/", "optim", "/", "swa_utils", ".", "py", "self", ".", "momenta", "=", "{", "}", "for", "module", "in", "pl_module", ".", "modules", "(", ")", ":", "if", "not", "isinstance", "(", "module", ",", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", ":", "continue", "assert", "module", ".", "running_mean", "is", "not", "None", "module", ".", "running_mean", "=", "torch", ".", "zeros_like", "(", "module", ".", "running_mean", ",", "device", "=", "pl_module", ".", "device", ",", "dtype", "=", "module", ".", "running_mean", ".", "dtype", ",", ")", "assert", "module", ".", "running_var", "is", "not", "None", "module", ".", "running_var", "=", "torch", ".", "ones_like", "(", "module", ".", "running_var", ",", "device", "=", "pl_module", ".", "device", ",", "dtype", "=", "module", ".", "running_var", ".", "dtype", ",", ")", "self", ".", "momenta", "[", "module", "]", "=", "module", ".", "momentum", "module", ".", "momentum", "=", "None", "assert", "module", ".", "num_batches_tracked", "is", "not", "None", "module", ".", "num_batches_tracked", "*", "=", "0"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l140", "l154"], "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L140-L154.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "partition": "train", "function_type": "class_method", "class_name": "StochasticWeightAveraging", "start_line": 286, "end_line": 307, "hash": "1ee2000861451dfe6f2ab7a5a5215e51", "complexity": 3, "parameters": ["pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "reset_momenta", "original_string": "def reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]", "language": "python", "code": "def reset_momenta(self) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.\"\"\"\r\n        for bn_module in self.momenta:\r\n            bn_module.momentum = self.momenta[bn_module]", "code_tokens": ["def", "reset_momenta", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adapted", "from", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "blob", "/", "v1", ".", "7", ".", "1", "/", "torch", "/", "optim", "/", "swa_utils", ".", "py", "for", "bn_module", "in", "self", ".", "momenta", ":", "bn_module", ".", "momentum", "=", "self", ".", "momenta", "[", "bn_module", "]"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l164", "l165"], "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L164-L165.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "partition": "train", "function_type": "class_method", "class_name": "StochasticWeightAveraging", "start_line": 309, "end_line": 312, "hash": "a0f80e754d7b1dc810b6bf4ce03fb48b", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "func_name": "update_parameters", "original_string": "def update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1", "language": "python", "code": "def update_parameters(\r\n        average_model: \"pl.LightningModule\", model: \"pl.LightningModule\", n_averaged: Tensor, avg_fn: _AVG_FN\r\n    ) -> None:\r\n        \"\"\"Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.\"\"\"\r\n        for p_swa, p_model in zip(average_model.parameters(), model.parameters()):\r\n            device = p_swa.device\r\n            p_swa_ = p_swa.detach()\r\n            p_model_ = p_model.detach().to(device)\r\n            src = p_model_ if n_averaged == 0 else avg_fn(p_swa_, p_model_, n_averaged.to(device))\r\n            p_swa_.copy_(src)\r\n        n_averaged += 1", "code_tokens": ["def", "update_parameters", "(", "average_model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "n_averaged", ":", "Tensor", ",", "avg_fn", ":", "_AVG_FN", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Adapted", "from", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "blob", "/", "v1", ".", "7", ".", "1", "/", "torch", "/", "optim", "/", "swa_utils", ".", "py", "for", "p_swa", ",", "p_model", "in", "zip", "(", "average_model", ".", "parameters", "(", ")", ",", "model", ".", "parameters", "(", ")", ")", ":", "device", "=", "p_swa", ".", "device", "p_swa_", "=", "p_swa", ".", "detach", "(", ")", "p_model_", "=", "p_model", ".", "detach", "(", ")", ".", "to", "(", "device", ")", "src", "=", "p_model_", "if", "n_averaged", "=", "=", "0", "else", "avg_fn", "(", "p_swa_", ",", "p_model_", ",", "n_averaged", ".", "to", "(", "device", ")", ")", "p_swa_", ".", "copy_", "(", "src", ")", "n_averaged", "+", "=", "1"], "docstring": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.", "docstring_tokens": ["adapted", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v1", "7", "1", "torch", "optim", "swa_utils", "py", "l104", "l112"], "docstring_summary": "Adapted from https://github.com/pytorch/pytorch/blob/v1.7.1/torch/optim/swa_utils.py#L104-L112.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\stochastic_weight_avg.py", "partition": "train", "function_type": "class_method", "class_name": "StochasticWeightAveraging", "start_line": 315, "end_line": 325, "hash": "f8333446af47befa7450e18103650278", "complexity": 3, "parameters": ["average_model", "model", "n_averaged", "avg_fn"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "start_time", "original_string": "def start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]", "language": "python", "code": "def start_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the start time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._start_time[stage]", "code_tokens": ["def", "start_time", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "Optional", "[", "float", "]", ":", "\"", "\"", "\"", "Return", "the", "start", "time", "of", "a", "particular", "stage", "(", "in", "seconds", ")", "\"", "\"", "\"", "stage", "=", "RunningStage", "(", "stage", ")", "return", "self", ".", "_start_time", "[", "stage", "]"], "docstring": "Return the start time of a particular stage (in seconds)", "docstring_tokens": ["return", "the", "start", "time", "of", "a", "particular", "stage", "in", "seconds"], "docstring_summary": "Return the start time of a particular stage (in seconds)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\timer.py", "partition": "train", "function_type": "class_method", "class_name": "Timer", "start_line": 117, "end_line": 120, "hash": "0b7fc9b79fe86d3cd135c21b553b0be6", "complexity": 1, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "end_time", "original_string": "def end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]", "language": "python", "code": "def end_time(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the end time of a particular stage (in seconds)\"\"\"\r\n        stage = RunningStage(stage)\r\n        return self._end_time[stage]", "code_tokens": ["def", "end_time", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "Optional", "[", "float", "]", ":", "\"", "\"", "\"", "Return", "the", "end", "time", "of", "a", "particular", "stage", "(", "in", "seconds", ")", "\"", "\"", "\"", "stage", "=", "RunningStage", "(", "stage", ")", "return", "self", ".", "_end_time", "[", "stage", "]"], "docstring": "Return the end time of a particular stage (in seconds)", "docstring_tokens": ["return", "the", "end", "time", "of", "a", "particular", "stage", "in", "seconds"], "docstring_summary": "Return the end time of a particular stage (in seconds)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\timer.py", "partition": "train", "function_type": "class_method", "class_name": "Timer", "start_line": 122, "end_line": 125, "hash": "168cdcfa17b25f1c5691fa82029ae96e", "complexity": 1, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "time_elapsed", "original_string": "def time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset", "language": "python", "code": "def time_elapsed(self, stage: str = RunningStage.TRAINING) -> float:\r\n        \"\"\"Return the time elapsed for a particular stage (in seconds)\"\"\"\r\n        start = self.start_time(stage)\r\n        end = self.end_time(stage)\r\n        offset = self._offset if stage == RunningStage.TRAINING else 0\r\n        if start is None:\r\n            return offset\r\n        if end is None:\r\n            return time.monotonic() - start + offset\r\n        return end - start + offset", "code_tokens": ["def", "time_elapsed", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "float", ":", "\"", "\"", "\"", "Return", "the", "time", "elapsed", "for", "a", "particular", "stage", "(", "in", "seconds", ")", "\"", "\"", "\"", "start", "=", "self", ".", "start_time", "(", "stage", ")", "end", "=", "self", ".", "end_time", "(", "stage", ")", "offset", "=", "self", ".", "_offset", "if", "stage", "=", "=", "RunningStage", ".", "TRAINING", "else", "0", "if", "start", "is", "None", ":", "return", "offset", "if", "end", "is", "None", ":", "return", "time", ".", "monotonic", "(", ")", "-", "start", "+", "offset", "return", "end", "-", "start", "+", "offset"], "docstring": "Return the time elapsed for a particular stage (in seconds)", "docstring_tokens": ["return", "the", "time", "elapsed", "for", "a", "particular", "stage", "in", "seconds"], "docstring_summary": "Return the time elapsed for a particular stage (in seconds)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\timer.py", "partition": "train", "function_type": "class_method", "class_name": "Timer", "start_line": 127, "end_line": 136, "hash": "f9df60992678cea1fba9ac81559edab2", "complexity": 4, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\timer.py", "func_name": "time_remaining", "original_string": "def time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None", "language": "python", "code": "def time_remaining(self, stage: str = RunningStage.TRAINING) -> Optional[float]:\r\n        \"\"\"Return the time remaining for a particular stage (in seconds)\"\"\"\r\n        if self._duration is not None:\r\n            return self._duration - self.time_elapsed(stage)\r\n        return None", "code_tokens": ["def", "time_remaining", "(", "self", ",", "stage", ":", "str", "=", "RunningStage", ".", "TRAINING", ")", "-", ">", "Optional", "[", "float", "]", ":", "\"", "\"", "\"", "Return", "the", "time", "remaining", "for", "a", "particular", "stage", "(", "in", "seconds", ")", "\"", "\"", "\"", "if", "self", ".", "_duration", "is", "not", "None", ":", "return", "self", ".", "_duration", "-", "self", ".", "time_elapsed", "(", "stage", ")", "return", "None"], "docstring": "Return the time remaining for a particular stage (in seconds)", "docstring_tokens": ["return", "the", "time", "remaining", "for", "a", "particular", "stage", "in", "seconds"], "docstring_summary": "Return the time remaining for a particular stage (in seconds)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\timer.py", "partition": "train", "function_type": "class_method", "class_name": "Timer", "start_line": 138, "end_line": 142, "hash": "49eb664d5dd44774968f778a402c4efb", "complexity": 2, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "should_update", "original_string": "def should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None", "language": "python", "code": "def should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None) -> bool:\r\n        \"\"\"Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.\r\n\r\n        \"\"\"\r\n        return step_idx is not None", "code_tokens": ["def", "should_update", "(", "self", ",", "step_idx", ":", "Optional", "[", "int", "]", "=", "None", ",", "epoch_idx", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Called", "after", "every", "optimizer", "step", "and", "after", "every", "training", "epoch", "to", "check", "whether", "the", "average", "model", "should", "be", "updated", ".", "One", "of", "the", "arguments", "is", "set", "to", "the", "zero", "-", "based", "index", "of", "the", "last", "training", "step", "or", "epoch", ".", "The", "default", "implementation", "returns", "`", "`", "True", "`", "`", "when", "any", "`", "`", "step_idx", "`", "`", "is", "provided", ".", "The", "user", "can", "customize", "when", "the", "average", "model", "gets", "updated", "by", "overriding", "this", "method", ".", "Args", ":", "step_idx", ":", "Index", "of", "the", "last", "optimizer", "step", ",", "or", "`", "`", "None", "`", "`", "when", "called", "at", "the", "epoch", "end", ".", "epoch_idx", ":", "Index", "of", "the", "last", "epoch", ",", "or", "`", "`", "None", "`", "`", "when", "called", "after", "an", "optimizer", "step", ".", "Returns", ":", "`", "`", "True", "`", "`", "if", "the", "average", "model", "should", "be", "updated", "and", "`", "`", "False", "`", "`", "if", "not", ".", "\"", "\"", "\"", "return", "step_idx", "is", "not", "None"], "docstring": "Called after every optimizer step and after every training epoch to check whether the average model should\r\n        be updated.\r\n\r\n        One of the arguments is set to the zero-based index of the last training step or epoch. The default\r\n        implementation returns ``True`` when any ``step_idx`` is provided. The user can customize when the average model\r\n        gets updated by overriding this method.\r\n\r\n        Args:\r\n            step_idx: Index of the last optimizer step, or ``None`` when called at the epoch end.\r\n            epoch_idx: Index of the last epoch, or ``None`` when called after an optimizer step.\r\n\r\n        Returns:\r\n            ``True`` if the average model should be updated and ``False`` if not.", "docstring_tokens": ["called", "after", "every", "optimizer", "step", "and", "after", "every", "training", "epoch", "to", "check", "whether", "the", "average", "model", "should", "be", "updated", "one", "of", "the", "arguments", "is", "set", "to", "the", "zero", "based", "index", "of", "the", "last", "training", "step", "or", "epoch", "the", "default", "implementation", "returns", "true", "when", "any", "step_idx", "is", "provided", "the", "user", "can", "customize", "when", "the", "average", "model", "gets", "updated", "by", "overriding", "this", "method", "args", "step_idx", "index", "of", "the", "last", "optimizer", "step", "or", "none", "when", "called", "at", "the", "epoch", "end", "epoch_idx", "index", "of", "the", "last", "epoch", "or", "none", "when", "called", "after", "an", "optimizer", "step", "returns", "true", "if", "the", "average", "model", "should", "be", "updated", "and", "false", "if", "not"], "docstring_summary": "Called after every optimizer step and after every training epoch to check whether the average model should", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 114, "end_line": 130, "hash": "59d7015f0d8f23f3ce61bf3bdcf2444c", "complexity": 1, "parameters": ["step_idx", "epoch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "setup", "original_string": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            # If the configure_model hook is overridden, call it to create the layers before constructing the\r\n            # AveragedModel. However, sharding will not be done and a warning will be issued.\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str) -> None:\r\n        \"\"\"Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\r\n\r\n        \"\"\"\r\n        if stage == \"fit\":\r\n            device = self._device or pl_module.device\r\n\r\n            # If the configure_model hook is overridden, call it to create the layers before constructing the\r\n            # AveragedModel. However, sharding will not be done and a warning will be issued.\r\n            if is_overridden(\"configure_model\", pl_module):\r\n                rank_zero_warn(\r\n                    \"You're using the WeightAveraging callback with a model that overrides the configure_model \"\r\n                    \"callback. WeightAveraging doesn't support sharding model layers, so you may run out of memory.\"\r\n                )\r\n                pl_module.configure_model()\r\n\r\n            self._average_model = AveragedModel(\r\n                model=pl_module, device=device, use_buffers=self._use_buffers, **self._kwargs\r\n            )", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "fit", ",", "validate", ",", "test", ",", "predict", ",", "or", "tune", "begins", ".", "Creates", "an", ":", "class", ":", "`", "AveragedModel", "`", "when", "fit", "begins", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "stage", ":", "The", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "state", ".", "\"", "\"", "\"", "if", "stage", "=", "=", "\"", "fit", "\"", ":", "device", "=", "self", ".", "_device", "or", "pl_module", ".", "device", "if", "is_overridden", "(", "\"", "configure_model", "\"", ",", "pl_module", ")", ":", "rank_zero_warn", "(", "\"", "You", "'", "re", "using", "the", "WeightAveraging", "callback", "with", "a", "model", "that", "overrides", "the", "configure_model", "\"", "\"", "callback", ".", "WeightAveraging", "doesn", "'", "t", "support", "sharding", "model", "layers", ",", "so", "you", "may", "run", "out", "of", "memory", ".", "\"", ")", "pl_module", ".", "configure_model", "(", ")", "self", ".", "_average_model", "=", "AveragedModel", "(", "model", "=", "pl_module", ",", "device", "=", "device", ",", "use_buffers", "=", "self", ".", "_use_buffers", ",", "*", "*", "self", ".", "_kwargs", ")"], "docstring": "Called when fit, validate, test, predict, or tune begins.\r\n\r\n        Creates an :class:`AveragedModel` when fit begins.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.", "docstring_tokens": ["called", "when", "fit", "validate", "test", "predict", "or", "tune", "begins", "creates", "an", "class", "averagedmodel", "when", "fit", "begins", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "stage", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "state"], "docstring_summary": "Called when fit, validate, test, predict, or tune begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 133, "end_line": 158, "hash": "4b567d16f0d3f32c2ff4e02316ec57ca", "complexity": 4, "parameters": ["trainer", "pl_module", "stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_train_batch_end", "original_string": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        # trainer.global_step is the number of optimizer steps taken so far, i.e. 1 after the first optimizer step. To\r\n        # make step_idx consistent with epoch_idx, we'll pass a zero-based index.\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step", "language": "python", "code": "def on_train_batch_end(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\r\n    ) -> None:\r\n        \"\"\"Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.\r\n\r\n        \"\"\"\r\n        # trainer.global_step is the number of optimizer steps taken so far, i.e. 1 after the first optimizer step. To\r\n        # make step_idx consistent with epoch_idx, we'll pass a zero-based index.\r\n        step_idx = trainer.global_step - 1\r\n        if (trainer.global_step > self._latest_update_step) and self.should_update(step_idx=step_idx):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_step = trainer.global_step", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "a", "training", "batch", "ends", ".", "Updates", "the", ":", "class", ":", "`", "AveragedModel", "`", "parameters", ",", "if", "requested", "by", "`", "`", "self", ".", "should_update", "(", ")", "`", "`", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "outputs", ":", "Outputs", "from", "the", "training", "batch", ".", "batch", ":", "The", "training", "batch", ".", "batch_idx", ":", "Index", "of", "the", "training", "batch", ".", "\"", "\"", "\"", "step_idx", "=", "trainer", ".", "global_step", "-", "1", "if", "(", "trainer", ".", "global_step", ">", "self", ".", "_latest_update_step", ")", "and", "self", ".", "should_update", "(", "step_idx", "=", "step_idx", ")", ":", "assert", "self", ".", "_average_model", "is", "not", "None", "self", ".", "_average_model", ".", "update_parameters", "(", "pl_module", ")", "self", ".", "_latest_update_step", "=", "trainer", ".", "global_step"], "docstring": "Called when a training batch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            outputs: Outputs from the training batch.\r\n            batch: The training batch.\r\n            batch_idx: Index of the training batch.", "docstring_tokens": ["called", "when", "a", "training", "batch", "ends", "updates", "the", "class", "averagedmodel", "parameters", "if", "requested", "by", "self", "should_update", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "outputs", "outputs", "from", "the", "training", "batch", "batch", "the", "training", "batch", "batch_idx", "index", "of", "the", "training", "batch"], "docstring_summary": "Called when a training batch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 161, "end_line": 182, "hash": "e5bb7ed841e7bd89db3019107a4f6465", "complexity": 3, "parameters": ["trainer", "pl_module", "outputs", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_train_epoch_end", "original_string": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch", "language": "python", "code": "def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if (trainer.current_epoch > self._latest_update_epoch) and self.should_update(epoch_idx=trainer.current_epoch):\r\n            assert self._average_model is not None\r\n            self._average_model.update_parameters(pl_module)\r\n            self._latest_update_epoch = trainer.current_epoch", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "a", "training", "epoch", "ends", ".", "Updates", "the", ":", "class", ":", "`", "AveragedModel", "`", "parameters", ",", "if", "requested", "by", "`", "`", "self", ".", "should_update", "(", ")", "`", "`", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "\"", "\"", "\"", "if", "(", "trainer", ".", "current_epoch", ">", "self", ".", "_latest_update_epoch", ")", "and", "self", ".", "should_update", "(", "epoch_idx", "=", "trainer", ".", "current_epoch", ")", ":", "assert", "self", ".", "_average_model", "is", "not", "None", "self", ".", "_average_model", ".", "update_parameters", "(", "pl_module", ")", "self", ".", "_latest_update_epoch", "=", "trainer", ".", "current_epoch"], "docstring": "Called when a training epoch ends.\r\n\r\n        Updates the :class:`AveragedModel` parameters, if requested by ``self.should_update()``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.", "docstring_tokens": ["called", "when", "a", "training", "epoch", "ends", "updates", "the", "class", "averagedmodel", "parameters", "if", "requested", "by", "self", "should_update", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance"], "docstring_summary": "Called when a training epoch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 185, "end_line": 198, "hash": "4cae0ae4197dc3cdc17fcddad454350b", "complexity": 3, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_train_end", "original_string": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)", "language": "python", "code": "def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        self._copy_average_to_current(pl_module)", "code_tokens": ["def", "on_train_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "training", "ends", ".", "Transfers", "parameters", "from", "the", ":", "class", ":", "`", "AveragedModel", "`", "to", "the", "current", "model", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "\"", "\"", "\"", "assert", "self", ".", "_average_model", "is", "not", "None", "self", ".", "_copy_average_to_current", "(", "pl_module", ")"], "docstring": "Called when training ends.\r\n\r\n        Transfers parameters from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.", "docstring_tokens": ["called", "when", "training", "ends", "transfers", "parameters", "from", "the", "class", "averagedmodel", "to", "the", "current", "model", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance"], "docstring_summary": "Called when training ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 201, "end_line": 212, "hash": "119aaedf55fa4b7c199f7d22fb5145db", "complexity": 1, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_validation_epoch_start", "original_string": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "language": "python", "code": "def on_validation_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "code_tokens": ["def", "on_validation_epoch_start", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "a", "validation", "epoch", "begins", ".", "Transfers", "parameter", "values", "from", "the", ":", "class", ":", "`", "AveragedModel", "`", "to", "the", "current", "model", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "\"", "\"", "\"", "if", "self", ".", "_average_model", "is", "not", "None", ":", "self", ".", "_swap_models", "(", "pl_module", ")"], "docstring": "Called when a validation epoch begins.\r\n\r\n        Transfers parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.", "docstring_tokens": ["called", "when", "a", "validation", "epoch", "begins", "transfers", "parameter", "values", "from", "the", "class", "averagedmodel", "to", "the", "current", "model", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance"], "docstring_summary": "Called when a validation epoch begins.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 215, "end_line": 226, "hash": "6616aa45a387fdb3c54cc51e59fdea64", "complexity": 2, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_validation_epoch_end", "original_string": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "language": "python", "code": "def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        if self._average_model is not None:\r\n            self._swap_models(pl_module)", "code_tokens": ["def", "on_validation_epoch_end", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "a", "validation", "epoch", "ends", ".", "Recovers", "the", "current", "model", "parameters", "from", "the", ":", "class", ":", "`", "AveragedModel", "`", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "\"", "\"", "\"", "if", "self", ".", "_average_model", "is", "not", "None", ":", "self", ".", "_swap_models", "(", "pl_module", ")"], "docstring": "Called when a validation epoch ends.\r\n\r\n        Recovers the current model parameters from the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.", "docstring_tokens": ["called", "when", "a", "validation", "epoch", "ends", "recovers", "the", "current", "model", "parameters", "from", "the", "class", "averagedmodel", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance"], "docstring_summary": "Called when a validation epoch ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 229, "end_line": 240, "hash": "ef63d81d8385d85d9a9673a6d5303001", "complexity": 2, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "state_dict", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        return {\"latest_update_step\": self._latest_update_step}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "checkpoint", ".", "Creates", "a", "`", "`", "state_dict", "`", "`", "of", "the", "callback", "state", ".", "Returns", ":", "A", "dictionary", "containing", "the", "callback", "state", ".", "\"", "\"", "\"", "return", "{", "\"", "latest_update_step", "\"", ":", "self", ".", "_latest_update_step", "}"], "docstring": "Called when saving a checkpoint.\r\n\r\n        Creates a ``state_dict`` of the callback state.\r\n\r\n        Returns:\r\n            A dictionary containing the callback state.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "creates", "a", "state_dict", "of", "the", "callback", "state", "returns", "a", "dictionary", "containing", "the", "callback", "state"], "docstring_summary": "Called when saving a checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 243, "end_line": 252, "hash": "c695fa17314630600f897497d05eb81c", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "load_state_dict", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.\r\n\r\n        \"\"\"\r\n        self._latest_update_step = state_dict[\"latest_update_step\"]", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "loading", "a", "checkpoint", ".", "Reloads", "the", "callback", "state", "given", "a", "`", "`", "state_dict", "`", "`", ".", "Args", ":", "state_dict", ":", "A", "dictionary", "containing", "the", "callback", "state", ".", "\"", "\"", "\"", "self", ".", "_latest_update_step", "=", "state_dict", "[", "\"", "latest_update_step", "\"", "]"], "docstring": "Called when loading a checkpoint.\r\n\r\n        Reloads the callback state given a ``state_dict``.\r\n\r\n        Args:\r\n            state_dict: A dictionary containing the callback state.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "reloads", "the", "callback", "state", "given", "a", "state_dict", "args", "state_dict", "a", "dictionary", "containing", "the", "callback", "state"], "docstring_summary": "Called when loading a checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 255, "end_line": 264, "hash": "4b93d414f78323ac169f9d05f201c346", "complexity": 1, "parameters": ["state_dict", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_save_checkpoint", "original_string": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            # Truncate the \"module.\" prefix (the first 7 characters) from the names of the variables in the\r\n            # AveragedModel state.\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }", "language": "python", "code": "def on_save_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_info(\r\n                \"You're using the WeightAveraging callback, but saving a checkpoint outside the 'fit' stage. The state \"\r\n                \"of the WeightAveraging callback won't be saved in the checkpoint. If training has finished, the \"\r\n                \"average model parameters will be saved to the state_dict in the checkpoint.\"\r\n            )\r\n        else:\r\n            average_model_state = self._average_model.state_dict()\r\n            checkpoint[\"current_model_state\"] = checkpoint[\"state_dict\"]\r\n            # Truncate the \"module.\" prefix (the first 7 characters) from the names of the variables in the\r\n            # AveragedModel state.\r\n            checkpoint[\"state_dict\"] = {\r\n                name[7:]: value for name, value in average_model_state.items() if name.startswith(\"module.\")\r\n            }\r\n            checkpoint[\"averaging_state\"] = {\r\n                name: value for name, value in average_model_state.items() if not name.startswith(\"module.\")\r\n            }", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Called", "when", "saving", "a", "checkpoint", ".", "Moves", "the", "current", "model", "state", "to", "the", "key", "`", "`", "current_model_state", "`", "`", ",", "and", "places", "the", "average", "model", "state", "in", "`", "`", "state_dict", "`", "`", "instead", ".", "Any", "other", "state", "variables", "of", "the", "`", "`", "AveragedModel", "`", "`", "will", "be", "saved", "in", "`", "`", "averaging_state", "`", "`", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "checkpoint", ":", "The", "checkpoint", "dictionary", "that", "will", "be", "saved", ".", "\"", "\"", "\"", "if", "self", ".", "_average_model", "is", "None", ":", "rank_zero_info", "(", "\"", "You", "'", "re", "using", "the", "WeightAveraging", "callback", ",", "but", "saving", "a", "checkpoint", "outside", "the", "'", "fit", "'", "stage", ".", "The", "state", "\"", "\"", "of", "the", "WeightAveraging", "callback", "won", "'", "t", "be", "saved", "in", "the", "checkpoint", ".", "If", "training", "has", "finished", ",", "the", "\"", "\"", "average", "model", "parameters", "will", "be", "saved", "to", "the", "state_dict", "in", "the", "checkpoint", ".", "\"", ")", "else", ":", "average_model_state", "=", "self", ".", "_average_model", ".", "state_dict", "(", ")", "checkpoint", "[", "\"", "current_model_state", "\"", "]", "=", "checkpoint", "[", "\"", "state_dict", "\"", "]", "checkpoint", "[", "\"", "state_dict", "\"", "]", "=", "{", "name", "[", "7", ":", "]", ":", "value", "for", "name", ",", "value", "in", "average_model_state", ".", "items", "(", ")", "if", "name", ".", "startswith", "(", "\"", "module", ".", "\"", ")", "}", "checkpoint", "[", "\"", "averaging_state", "\"", "]", "=", "{", "name", ":", "value", "for", "name", ",", "value", "in", "average_model_state", ".", "items", "(", ")", "if", "not", "name", ".", "startswith", "(", "\"", "module", ".", "\"", ")", "}"], "docstring": "r\"\"\"Called when saving a checkpoint.\r\n\r\n        Moves the current model state to the key ``current_model_state``, and places the average model state in\r\n        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\r\n        ``averaging_state``.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The checkpoint dictionary that will be saved.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "called", "when", "saving", "a", "checkpoint", "moves", "the", "current", "model", "state", "to", "the", "key", "current_model_state", "and", "places", "the", "average", "model", "state", "in", "state_dict", "instead", "any", "other", "state", "variables", "of", "the", "averagedmodel", "will", "be", "saved", "in", "averaging_state", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "checkpoint", "the", "checkpoint", "dictionary", "that", "will", "be", "saved"], "docstring_summary": "r\"\"\"Called when saving a checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 267, "end_line": 298, "hash": "b40c2860dc4a2f464a4bf88729e2775f", "complexity": 6, "parameters": ["trainer", "pl_module", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "on_load_checkpoint", "original_string": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            # The current model state has already been loaded from \"state_dict\" (which contains the average model\r\n            # weights) at this point, so overwriting \"state_dict\" in the checkpoint dictionary makes no difference. We\r\n            # have to reload the model state from \"current_model_state\".\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)", "language": "python", "code": "def on_load_checkpoint(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", checkpoint: dict[str, Any]\r\n    ) -> None:\r\n        r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"\r\n        if self._average_model is None:\r\n            rank_zero_warn(\r\n                \"You're using the WeightAveraging callback, but loading a checkpoint outside the 'fit' stage. The \"\r\n                \"WeightAveraging state cannot be restored. If you're using the checkpoint for prediction or testing, \"\r\n                \"you can ignore this warning. To disable the warning, remove the WeightAveraging callback.\"\r\n            )\r\n        elif (\"current_model_state\" in checkpoint) and (\"averaging_state\" in checkpoint):\r\n            rank_zero_info(\"Found current_model_state in the checkpoint. This will be used to initialize the model.\")\r\n            average_model_state = {\"module.\" + name: value for name, value in checkpoint[\"state_dict\"].items()}\r\n            average_model_state |= checkpoint[\"averaging_state\"]\r\n            self._average_model.load_state_dict(average_model_state)\r\n            # The current model state has already been loaded from \"state_dict\" (which contains the average model\r\n            # weights) at this point, so overwriting \"state_dict\" in the checkpoint dictionary makes no difference. We\r\n            # have to reload the model state from \"current_model_state\".\r\n            pl_module.load_state_dict(checkpoint[\"current_model_state\"])\r\n        else:\r\n            rank_zero_warn(\r\n                \"The checkpoint was not created with WeightAveraging. Both the current and the average model will be \"\r\n                \"initialized with state_dict.\"\r\n            )\r\n            self._average_model.module.load_state_dict(deepcopy(checkpoint[\"state_dict\"]), strict=False)", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Called", "when", "loading", "a", "model", "checkpoint", ".", "Loads", "the", "current", "model", "and", "the", ":", "class", ":", "`", "AveragedModel", "`", "parameters", "from", "the", "checkpoint", ".", "Args", ":", "trainer", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "instance", ".", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "checkpoint", ":", "The", "full", "checkpoint", "dictionary", "that", "got", "loaded", "by", "the", "Trainer", ".", "\"", "\"", "\"", "if", "self", ".", "_average_model", "is", "None", ":", "rank_zero_warn", "(", "\"", "You", "'", "re", "using", "the", "WeightAveraging", "callback", ",", "but", "loading", "a", "checkpoint", "outside", "the", "'", "fit", "'", "stage", ".", "The", "\"", "\"", "WeightAveraging", "state", "cannot", "be", "restored", ".", "If", "you", "'", "re", "using", "the", "checkpoint", "for", "prediction", "or", "testing", ",", "\"", "\"", "you", "can", "ignore", "this", "warning", ".", "To", "disable", "the", "warning", ",", "remove", "the", "WeightAveraging", "callback", ".", "\"", ")", "elif", "(", "\"", "current_model_state", "\"", "in", "checkpoint", ")", "and", "(", "\"", "averaging_state", "\"", "in", "checkpoint", ")", ":", "rank_zero_info", "(", "\"", "Found", "current_model_state", "in", "the", "checkpoint", ".", "This", "will", "be", "used", "to", "initialize", "the", "model", ".", "\"", ")", "average_model_state", "=", "{", "\"", "module", ".", "\"", "+", "name", ":", "value", "for", "name", ",", "value", "in", "checkpoint", "[", "\"", "state_dict", "\"", "]", ".", "items", "(", ")", "}", "average_model_state", "|", "=", "checkpoint", "[", "\"", "averaging_state", "\"", "]", "self", ".", "_average_model", ".", "load_state_dict", "(", "average_model_state", ")", "pl_module", ".", "load_state_dict", "(", "checkpoint", "[", "\"", "current_model_state", "\"", "]", ")", "else", ":", "rank_zero_warn", "(", "\"", "The", "checkpoint", "was", "not", "created", "with", "WeightAveraging", ".", "Both", "the", "current", "and", "the", "average", "model", "will", "be", "\"", "\"", "initialized", "with", "state_dict", ".", "\"", ")", "self", ".", "_average_model", ".", "module", ".", "load_state_dict", "(", "deepcopy", "(", "checkpoint", "[", "\"", "state_dict", "\"", "]", ")", ",", "strict", "=", "False", ")"], "docstring": "r\"\"\"Called when loading a model checkpoint.\r\n\r\n        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\r\n\r\n        Args:\r\n            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "called", "when", "loading", "a", "model", "checkpoint", "loads", "the", "current", "model", "and", "the", "class", "averagedmodel", "parameters", "from", "the", "checkpoint", "args", "trainer", "the", "current", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "instance", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance", "checkpoint", "the", "full", "checkpoint", "dictionary", "that", "got", "loaded", "by", "the", "trainer"], "docstring_summary": "r\"\"\"Called when loading a model checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 301, "end_line": 334, "hash": "a45d3130507c11c3dba9006e6e4f7dca", "complexity": 5, "parameters": ["trainer", "pl_module", "checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "_swap_models", "original_string": "def _swap_models(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Swaps the parameter values of the current model and the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            tmp = average_param.data.clone()\r\n            average_param.data.copy_(current_param.data)\r\n            current_param.data.copy_(tmp)", "language": "python", "code": "def _swap_models(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Swaps the parameter values of the current model and the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            tmp = average_param.data.clone()\r\n            average_param.data.copy_(current_param.data)\r\n            current_param.data.copy_(tmp)", "code_tokens": ["def", "_swap_models", "(", "self", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Swaps", "the", "parameter", "values", "of", "the", "current", "model", "and", "the", ":", "class", ":", "`", "AveragedModel", "`", ".", "Args", ":", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "\"", "\"", "\"", "assert", "self", ".", "_average_model", "is", "not", "None", "average_params", "=", "itertools", ".", "chain", "(", "self", ".", "_average_model", ".", "module", ".", "parameters", "(", ")", ",", "self", ".", "_average_model", ".", "module", ".", "buffers", "(", ")", ")", "current_params", "=", "itertools", ".", "chain", "(", "pl_module", ".", "parameters", "(", ")", ",", "pl_module", ".", "buffers", "(", ")", ")", "for", "average_param", ",", "current_param", "in", "zip", "(", "average_params", ",", "current_params", ")", ":", "tmp", "=", "average_param", ".", "data", ".", "clone", "(", ")", "average_param", ".", "data", ".", "copy_", "(", "current_param", ".", "data", ")", "current_param", ".", "data", ".", "copy_", "(", "tmp", ")"], "docstring": "Swaps the parameter values of the current model and the :class:`AveragedModel`.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.", "docstring_tokens": ["swaps", "the", "parameter", "values", "of", "the", "current", "model", "and", "the", "class", "averagedmodel", "args", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance"], "docstring_summary": "Swaps the parameter values of the current model and the :class:`AveragedModel`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 336, "end_line": 349, "hash": "6d6137138e0b78621c8c821600839b13", "complexity": 2, "parameters": ["pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "func_name": "_copy_average_to_current", "original_string": "def _copy_average_to_current(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Copies the parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            current_param.data.copy_(average_param.data)", "language": "python", "code": "def _copy_average_to_current(self, pl_module: \"pl.LightningModule\") -> None:\r\n        \"\"\"Copies the parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\r\n\r\n        \"\"\"\r\n        assert self._average_model is not None\r\n        average_params = itertools.chain(self._average_model.module.parameters(), self._average_model.module.buffers())\r\n        current_params = itertools.chain(pl_module.parameters(), pl_module.buffers())\r\n        for average_param, current_param in zip(average_params, current_params):\r\n            current_param.data.copy_(average_param.data)", "code_tokens": ["def", "_copy_average_to_current", "(", "self", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Copies", "the", "parameter", "values", "from", "the", ":", "class", ":", "`", "AveragedModel", "`", "to", "the", "current", "model", ".", "Args", ":", "pl_module", ":", "The", "current", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "instance", ".", "\"", "\"", "\"", "assert", "self", ".", "_average_model", "is", "not", "None", "average_params", "=", "itertools", ".", "chain", "(", "self", ".", "_average_model", ".", "module", ".", "parameters", "(", ")", ",", "self", ".", "_average_model", ".", "module", ".", "buffers", "(", ")", ")", "current_params", "=", "itertools", ".", "chain", "(", "pl_module", ".", "parameters", "(", ")", ",", "pl_module", ".", "buffers", "(", ")", ")", "for", "average_param", ",", "current_param", "in", "zip", "(", "average_params", ",", "current_params", ")", ":", "current_param", ".", "data", ".", "copy_", "(", "average_param", ".", "data", ")"], "docstring": "Copies the parameter values from the :class:`AveragedModel` to the current model.\r\n\r\n        Args:\r\n            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.", "docstring_tokens": ["copies", "the", "parameter", "values", "from", "the", "class", "averagedmodel", "to", "the", "current", "model", "args", "pl_module", "the", "current", "class", "lightning", "pytorch", "core", "lightningmodule", "instance"], "docstring_summary": "Copies the parameter values from the :class:`AveragedModel` to the current model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\weight_averaging.py", "partition": "train", "function_type": "class_method", "class_name": "WeightAveraging", "start_line": 351, "end_line": 362, "hash": "b6acd3b4791dbf1cdbcf4089318d38dc", "complexity": 2, "parameters": ["pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "total_train_batches", "original_string": "def total_train_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of training batches, which may change from epoch to epoch.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the training\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        if self.trainer.max_epochs == -1 and self.trainer.max_steps is not None and self.trainer.max_steps > 0:\r\n            remaining_steps = self.trainer.max_steps - self.trainer.global_step\r\n            return min(self.trainer.num_training_batches, remaining_steps)\r\n        return self.trainer.num_training_batches", "language": "python", "code": "def total_train_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of training batches, which may change from epoch to epoch.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the training\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        if self.trainer.max_epochs == -1 and self.trainer.max_steps is not None and self.trainer.max_steps > 0:\r\n            remaining_steps = self.trainer.max_steps - self.trainer.global_step\r\n            return min(self.trainer.num_training_batches, remaining_steps)\r\n        return self.trainer.num_training_batches", "code_tokens": ["def", "total_train_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "\"", "\"", "\"", "The", "total", "number", "of", "training", "batches", ",", "which", "may", "change", "from", "epoch", "to", "epoch", ".", "Use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", ".", "Can", "return", "`", "`", "inf", "`", "`", "if", "the", "training", "dataloader", "is", "of", "infinite", "size", ".", "\"", "\"", "\"", "if", "self", ".", "trainer", ".", "max_epochs", "=", "=", "-", "1", "and", "self", ".", "trainer", ".", "max_steps", "is", "not", "None", "and", "self", ".", "trainer", ".", "max_steps", ">", "0", ":", "remaining_steps", "=", "self", ".", "trainer", ".", "max_steps", "-", "self", ".", "trainer", ".", "global_step", "return", "min", "(", "self", ".", "trainer", ".", "num_training_batches", ",", "remaining_steps", ")", "return", "self", ".", "trainer", ".", "num_training_batches"], "docstring": "The total number of training batches, which may change from epoch to epoch.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the training\r\n        dataloader is of infinite size.", "docstring_tokens": ["the", "total", "number", "of", "training", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", "can", "return", "inf", "if", "the", "training", "dataloader", "is", "of", "infinite", "size"], "docstring_summary": "The total number of training batches, which may change from epoch to epoch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 80, "end_line": 90, "hash": "4920880e2d255457c7dafdb5ed582481", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "total_val_batches_current_dataloader", "original_string": "def total_val_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the validation\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_sanity_val_batches if self.trainer.sanity_checking else self.trainer.num_val_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "language": "python", "code": "def total_val_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the validation\r\n        dataloader is of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_sanity_val_batches if self.trainer.sanity_checking else self.trainer.num_val_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "code_tokens": ["def", "total_val_batches_current_dataloader", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "\"", "\"", "\"", "The", "total", "number", "of", "validation", "batches", ",", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader", ".", "Use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", ".", "Can", "return", "`", "`", "inf", "`", "`", "if", "the", "validation", "dataloader", "is", "of", "infinite", "size", ".", "\"", "\"", "\"", "batches", "=", "self", ".", "trainer", ".", "num_sanity_val_batches", "if", "self", ".", "trainer", ".", "sanity_checking", "else", "self", ".", "trainer", ".", "num_val_batches", "if", "isinstance", "(", "batches", ",", "list", ")", ":", "assert", "self", ".", "_current_eval_dataloader_idx", "is", "not", "None", "return", "batches", "[", "self", ".", "_current_eval_dataloader_idx", "]", "return", "batches"], "docstring": "The total number of validation batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the validation\r\n        dataloader is of infinite size.", "docstring_tokens": ["the", "total", "number", "of", "validation", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader", "use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", "can", "return", "inf", "if", "the", "validation", "dataloader", "is", "of", "infinite", "size"], "docstring_summary": "The total number of validation batches, which may change from epoch to epoch for current dataloader.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 93, "end_line": 104, "hash": "a1229e66e9917c2d8f813c72b8b7a266", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "total_test_batches_current_dataloader", "original_string": "def total_test_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of testing batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the test dataloader is\r\n        of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_test_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "language": "python", "code": "def total_test_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of testing batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the test dataloader is\r\n        of infinite size.\r\n\r\n        \"\"\"\r\n        batches = self.trainer.num_test_batches\r\n        if isinstance(batches, list):\r\n            assert self._current_eval_dataloader_idx is not None\r\n            return batches[self._current_eval_dataloader_idx]\r\n        return batches", "code_tokens": ["def", "total_test_batches_current_dataloader", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "\"", "\"", "\"", "The", "total", "number", "of", "testing", "batches", ",", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader", ".", "Use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", ".", "Can", "return", "`", "`", "inf", "`", "`", "if", "the", "test", "dataloader", "is", "of", "infinite", "size", ".", "\"", "\"", "\"", "batches", "=", "self", ".", "trainer", ".", "num_test_batches", "if", "isinstance", "(", "batches", ",", "list", ")", ":", "assert", "self", ".", "_current_eval_dataloader_idx", "is", "not", "None", "return", "batches", "[", "self", ".", "_current_eval_dataloader_idx", "]", "return", "batches"], "docstring": "The total number of testing batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the test dataloader is\r\n        of infinite size.", "docstring_tokens": ["the", "total", "number", "of", "testing", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader", "use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", "can", "return", "inf", "if", "the", "test", "dataloader", "is", "of", "infinite", "size"], "docstring_summary": "The total number of testing batches, which may change from epoch to epoch for current dataloader.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 107, "end_line": 118, "hash": "d7e75533beb6f226e364ac3785e7ce9d", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "total_predict_batches_current_dataloader", "original_string": "def total_predict_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of prediction batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        assert self._current_eval_dataloader_idx is not None\r\n        return self.trainer.num_predict_batches[self._current_eval_dataloader_idx]", "language": "python", "code": "def total_predict_batches_current_dataloader(self) -> Union[int, float]:\r\n        \"\"\"The total number of prediction batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        assert self._current_eval_dataloader_idx is not None\r\n        return self.trainer.num_predict_batches[self._current_eval_dataloader_idx]", "code_tokens": ["def", "total_predict_batches_current_dataloader", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "\"", "\"", "\"", "The", "total", "number", "of", "prediction", "batches", ",", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader", ".", "Use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", ".", "Can", "return", "`", "`", "inf", "`", "`", "if", "the", "predict", "dataloader", "is", "of", "infinite", "size", ".", "\"", "\"", "\"", "assert", "self", ".", "_current_eval_dataloader_idx", "is", "not", "None", "return", "self", ".", "trainer", ".", "num_predict_batches", "[", "self", ".", "_current_eval_dataloader_idx", "]"], "docstring": "The total number of prediction batches, which may change from epoch to epoch for current dataloader.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.", "docstring_tokens": ["the", "total", "number", "of", "prediction", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "current", "dataloader", "use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", "can", "return", "inf", "if", "the", "predict", "dataloader", "is", "of", "infinite", "size"], "docstring_summary": "The total number of prediction batches, which may change from epoch to epoch for current dataloader.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 121, "end_line": 129, "hash": "d4218e3b483a35eea112a4fea912d7f6", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "total_val_batches", "original_string": "def total_val_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for all val dataloaders.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        if not self.trainer.fit_loop.epoch_loop._should_check_val_epoch():\r\n            return 0\r\n        return (\r\n            sum(self.trainer.num_val_batches)\r\n            if isinstance(self.trainer.num_val_batches, list)\r\n            else self.trainer.num_val_batches\r\n        )", "language": "python", "code": "def total_val_batches(self) -> Union[int, float]:\r\n        \"\"\"The total number of validation batches, which may change from epoch to epoch for all val dataloaders.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.\r\n\r\n        \"\"\"\r\n        if not self.trainer.fit_loop.epoch_loop._should_check_val_epoch():\r\n            return 0\r\n        return (\r\n            sum(self.trainer.num_val_batches)\r\n            if isinstance(self.trainer.num_val_batches, list)\r\n            else self.trainer.num_val_batches\r\n        )", "code_tokens": ["def", "total_val_batches", "(", "self", ")", "-", ">", "Union", "[", "int", ",", "float", "]", ":", "\"", "\"", "\"", "The", "total", "number", "of", "validation", "batches", ",", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "all", "val", "dataloaders", ".", "Use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", ".", "Can", "return", "`", "`", "inf", "`", "`", "if", "the", "predict", "dataloader", "is", "of", "infinite", "size", ".", "\"", "\"", "\"", "if", "not", "self", ".", "trainer", ".", "fit_loop", ".", "epoch_loop", ".", "_should_check_val_epoch", "(", ")", ":", "return", "0", "return", "(", "sum", "(", "self", ".", "trainer", ".", "num_val_batches", ")", "if", "isinstance", "(", "self", ".", "trainer", ".", "num_val_batches", ",", "list", ")", "else", "self", ".", "trainer", ".", "num_val_batches", ")"], "docstring": "The total number of validation batches, which may change from epoch to epoch for all val dataloaders.\r\n\r\n        Use this to set the total number of iterations in the progress bar. Can return ``inf`` if the predict dataloader\r\n        is of infinite size.", "docstring_tokens": ["the", "total", "number", "of", "validation", "batches", "which", "may", "change", "from", "epoch", "to", "epoch", "for", "all", "val", "dataloaders", "use", "this", "to", "set", "the", "total", "number", "of", "iterations", "in", "the", "progress", "bar", "can", "return", "inf", "if", "the", "predict", "dataloader", "is", "of", "infinite", "size"], "docstring_summary": "The total number of validation batches, which may change from epoch to epoch for all val dataloaders.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 132, "end_line": 145, "hash": "47cb5bf561b5fd731dcb13daa608adab", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "enable", "original_string": "def enable(self) -> None:\r\n        \"\"\"You should provide a way to enable the progress bar.\r\n\r\n        The :class:`~lightning.pytorch.trainer.trainer.Trainer` will call this in e.g. pre-training\r\n        routines like the :ref:`learning rate finder <advanced/training_tricks:Learning Rate Finder>`.\r\n        to temporarily enable and disable the training progress bar.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "language": "python", "code": "def enable(self) -> None:\r\n        \"\"\"You should provide a way to enable the progress bar.\r\n\r\n        The :class:`~lightning.pytorch.trainer.trainer.Trainer` will call this in e.g. pre-training\r\n        routines like the :ref:`learning rate finder <advanced/training_tricks:Learning Rate Finder>`.\r\n        to temporarily enable and disable the training progress bar.\r\n\r\n        \"\"\"\r\n        raise NotImplementedError", "code_tokens": ["def", "enable", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "You", "should", "provide", "a", "way", "to", "enable", "the", "progress", "bar", ".", "The", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "will", "call", "this", "in", "e", ".", "g", ".", "pre", "-", "training", "routines", "like", "the", ":", "ref", ":", "`", "learning", "rate", "finder", "<", "advanced", "/", "training_tricks", ":", "Learning", "Rate", "Finder", ">", "`", ".", "to", "temporarily", "enable", "and", "disable", "the", "training", "progress", "bar", ".", "\"", "\"", "\"", "raise", "NotImplementedError"], "docstring": "You should provide a way to enable the progress bar.\r\n\r\n        The :class:`~lightning.pytorch.trainer.trainer.Trainer` will call this in e.g. pre-training\r\n        routines like the :ref:`learning rate finder <advanced/training_tricks:Learning Rate Finder>`.\r\n        to temporarily enable and disable the training progress bar.", "docstring_tokens": ["you", "should", "provide", "a", "way", "to", "enable", "the", "progress", "bar", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "will", "call", "this", "in", "e", "g", "pre", "training", "routines", "like", "the", "ref", "learning", "rate", "finder", "advanced", "training_tricks", "learning", "rate", "finder", "to", "temporarily", "enable", "and", "disable", "the", "training", "progress", "bar"], "docstring_summary": "You should provide a way to enable the progress bar.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 159, "end_line": 167, "hash": "3367730608b4746b689fcfad5375732c", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "get_metrics", "original_string": "def get_metrics(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\r\n    ) -> dict[str, Union[int, str, float, dict[str, float]]]:\r\n        r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.\r\n        Implement this to override the items displayed in the progress bar.\r\n\r\n        Here is an example of how to override the defaults:\r\n\r\n        .. code-block:: python\r\n\r\n            def get_metrics(self, trainer, model):\r\n                # don't show the version number\r\n                items = super().get_metrics(trainer, model)\r\n                items.pop(\"v_num\", None)\r\n                return items\r\n\r\n        Return:\r\n            Dictionary with the items to be displayed in the progress bar.\r\n\r\n        \"\"\"\r\n        standard_metrics = get_standard_metrics(trainer)\r\n        pbar_metrics = trainer.progress_bar_metrics\r\n        duplicates = list(standard_metrics.keys() & pbar_metrics.keys())\r\n        if duplicates:\r\n            rank_zero_warn(\r\n                f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\r\n                f\" `self.log('{duplicates[0]}', ..., prog_bar=True)` will overwrite this value. \"\r\n                \" If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\",\r\n            )\r\n\r\n        return {**standard_metrics, **pbar_metrics}", "language": "python", "code": "def get_metrics(\r\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\r\n    ) -> dict[str, Union[int, str, float, dict[str, float]]]:\r\n        r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.\r\n        Implement this to override the items displayed in the progress bar.\r\n\r\n        Here is an example of how to override the defaults:\r\n\r\n        .. code-block:: python\r\n\r\n            def get_metrics(self, trainer, model):\r\n                # don't show the version number\r\n                items = super().get_metrics(trainer, model)\r\n                items.pop(\"v_num\", None)\r\n                return items\r\n\r\n        Return:\r\n            Dictionary with the items to be displayed in the progress bar.\r\n\r\n        \"\"\"\r\n        standard_metrics = get_standard_metrics(trainer)\r\n        pbar_metrics = trainer.progress_bar_metrics\r\n        duplicates = list(standard_metrics.keys() & pbar_metrics.keys())\r\n        if duplicates:\r\n            rank_zero_warn(\r\n                f\"The progress bar already tracks a metric with the name(s) '{', '.join(duplicates)}' and\"\r\n                f\" `self.log('{duplicates[0]}', ..., prog_bar=True)` will overwrite this value. \"\r\n                \" If this is undesired, change the name or override `get_metrics()` in the progress bar callback.\",\r\n            )\r\n\r\n        return {**standard_metrics, **pbar_metrics}", "code_tokens": ["def", "get_metrics", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", "pl_module", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "dict", "[", "str", ",", "Union", "[", "int", ",", "str", ",", "float", ",", "dict", "[", "str", ",", "float", "]", "]", "]", ":", "r", "\"", "\"", "\"", "Combines", "progress", "bar", "metrics", "collected", "from", "the", "trainer", "with", "standard", "metrics", "from", "get_standard_metrics", ".", "Implement", "this", "to", "override", "the", "items", "displayed", "in", "the", "progress", "bar", ".", "Here", "is", "an", "example", "of", "how", "to", "override", "the", "defaults", ":", ".", ".", "code", "-", "block", ":", ":", "python", "def", "get_metrics", "(", "self", ",", "trainer", ",", "model", ")", ":", "items", "=", "super", "(", ")", ".", "get_metrics", "(", "trainer", ",", "model", ")", "items", ".", "pop", "(", "\"", "v_num", "\"", ",", "None", ")", "return", "items", "Return", ":", "Dictionary", "with", "the", "items", "to", "be", "displayed", "in", "the", "progress", "bar", ".", "\"", "\"", "\"", "standard_metrics", "=", "get_standard_metrics", "(", "trainer", ")", "pbar_metrics", "=", "trainer", ".", "progress_bar_metrics", "duplicates", "=", "list", "(", "standard_metrics", ".", "keys", "(", ")", "&", "pbar_metrics", ".", "keys", "(", ")", ")", "if", "duplicates", ":", "rank_zero_warn", "(", "f", "\"", "The", "progress", "bar", "already", "tracks", "a", "metric", "with", "the", "name", "(", "s", ")", "'", "{", "'", ",", "'", ".", "join", "(", "duplicates", ")", "}", "'", "and", "\"", "f", "\"", "`", "self", ".", "log", "(", "'", "{", "duplicates", "[", "0", "]", "}", "'", ",", ".", ".", ".", ",", "prog_bar", "=", "True", ")", "`", "will", "overwrite", "this", "value", ".", "\"", "\"", "If", "this", "is", "undesired", ",", "change", "the", "name", "or", "override", "`", "get_metrics", "(", ")", "`", "in", "the", "progress", "bar", "callback", ".", "\"", ",", ")", "return", "{", "*", "*", "standard_metrics", ",", "*", "*", "pbar_metrics", "}"], "docstring": "r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.\r\n        Implement this to override the items displayed in the progress bar.\r\n\r\n        Here is an example of how to override the defaults:\r\n\r\n        .. code-block:: python\r\n\r\n            def get_metrics(self, trainer, model):\r\n                # don't show the version number\r\n                items = super().get_metrics(trainer, model)\r\n                items.pop(\"v_num\", None)\r\n                return items\r\n\r\n        Return:\r\n            Dictionary with the items to be displayed in the progress bar.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "combines", "progress", "bar", "metrics", "collected", "from", "the", "trainer", "with", "standard", "metrics", "from", "get_standard_metrics", "implement", "this", "to", "override", "the", "items", "displayed", "in", "the", "progress", "bar", "here", "is", "an", "example", "of", "how", "to", "override", "the", "defaults", "code", "block", "python", "def", "get_metrics", "self", "trainer", "model", "don", "t", "show", "the", "version", "number", "items", "super", "get_metrics", "trainer", "model", "items", "pop", "v_num", "none", "return", "items", "return", "dictionary", "with", "the", "items", "to", "be", "displayed", "in", "the", "progress", "bar"], "docstring_summary": "r\"\"\"Combines progress bar metrics collected from the trainer with standard metrics from get_standard_metrics.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "class_method", "class_name": "ProgressBar", "start_line": 179, "end_line": 209, "hash": "1316e5293ec2c85e3975316f1890a960", "complexity": 2, "parameters": ["trainer", "pl_module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "func_name": "get_standard_metrics", "original_string": "def get_standard_metrics(trainer: \"pl.Trainer\") -> dict[str, Union[int, str]]:\r\n    r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the\r\n    experiment when using a logger.\r\n\r\n    .. code-block::\r\n\r\n        Epoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, v_num=10]\r\n\r\n    Return:\r\n        Dictionary with the standard metrics to be displayed in the progress bar.\r\n\r\n    \"\"\"\r\n    items_dict: dict[str, Union[int, str]] = {}\r\n    if trainer.loggers:\r\n        from lightning.pytorch.loggers.utilities import _version\r\n\r\n        if (version := _version(trainer.loggers)) not in (\"\", None):\r\n            if isinstance(version, str):\r\n                # show last 4 places of long version strings\r\n                version = version[-4:]\r\n            items_dict[\"v_num\"] = version\r\n\r\n    return items_dict", "language": "python", "code": "def get_standard_metrics(trainer: \"pl.Trainer\") -> dict[str, Union[int, str]]:\r\n    r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the\r\n    experiment when using a logger.\r\n\r\n    .. code-block::\r\n\r\n        Epoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, v_num=10]\r\n\r\n    Return:\r\n        Dictionary with the standard metrics to be displayed in the progress bar.\r\n\r\n    \"\"\"\r\n    items_dict: dict[str, Union[int, str]] = {}\r\n    if trainer.loggers:\r\n        from lightning.pytorch.loggers.utilities import _version\r\n\r\n        if (version := _version(trainer.loggers)) not in (\"\", None):\r\n            if isinstance(version, str):\r\n                # show last 4 places of long version strings\r\n                version = version[-4:]\r\n            items_dict[\"v_num\"] = version\r\n\r\n    return items_dict", "code_tokens": ["def", "get_standard_metrics", "(", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "dict", "[", "str", ",", "Union", "[", "int", ",", "str", "]", "]", ":", "r", "\"", "\"", "\"", "Returns", "the", "standard", "metrics", "displayed", "in", "the", "progress", "bar", ".", "Currently", ",", "it", "only", "includes", "the", "version", "of", "the", "experiment", "when", "using", "a", "logger", ".", ".", ".", "code", "-", "block", ":", ":", "Epoch", "1", ":", "4", "%", "|", "\u258e", "|", "40", "/", "1095", "[", "00", ":", "03", "<", "01", ":", "37", ",", "10", ".", "84it", "/", "s", ",", "v_num", "=", "10", "]", "Return", ":", "Dictionary", "with", "the", "standard", "metrics", "to", "be", "displayed", "in", "the", "progress", "bar", ".", "\"", "\"", "\"", "items_dict", ":", "dict", "[", "str", ",", "Union", "[", "int", ",", "str", "]", "]", "=", "{", "}", "if", "trainer", ".", "loggers", ":", "from", "lightning", ".", "pytorch", ".", "loggers", ".", "utilities", "import", "_version", "if", "(", "version", ":", "=", "_version", "(", "trainer", ".", "loggers", ")", ")", "not", "in", "(", "\"", "\"", ",", "None", ")", ":", "if", "isinstance", "(", "version", ",", "str", ")", ":", "version", "=", "version", "[", "-", "4", ":", "]", "items_dict", "[", "\"", "v_num", "\"", "]", "=", "version", "return", "items_dict"], "docstring": "r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the\r\n    experiment when using a logger.\r\n\r\n    .. code-block::\r\n\r\n        Epoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, v_num=10]\r\n\r\n    Return:\r\n        Dictionary with the standard metrics to be displayed in the progress bar.\r\n\r\n    \"\"\"", "docstring_tokens": ["r", "returns", "the", "standard", "metrics", "displayed", "in", "the", "progress", "bar", "currently", "it", "only", "includes", "the", "version", "of", "the", "experiment", "when", "using", "a", "logger", "code", "block", "epoch", "1", "4", "40", "1095", "00", "03", "01", "37", "10", "84it", "s", "v_num", "10", "return", "dictionary", "with", "the", "standard", "metrics", "to", "be", "displayed", "in", "the", "progress", "bar"], "docstring_summary": "r\"\"\"Returns the standard metrics displayed in the progress bar. Currently, it only includes the version of the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\progress_bar.py", "partition": "train", "function_type": "function", "start_line": 212, "end_line": 234, "hash": "c93942ae149e17648cffd80945f91e78", "complexity": 4, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\rich_progress.py", "func_name": "render", "original_string": "def render(self, task: \"Task\") -> _RichProgressBar:\r\n            \"\"\"Gets a progress bar widget for a task.\"\"\"\r\n            assert task.total is not None\r\n            assert task.remaining is not None\r\n            return _RichProgressBar(\r\n                total=max(0, task.total),\r\n                completed=max(0, task.completed),\r\n                width=None if self.bar_width is None else max(1, self.bar_width),\r\n                pulse=not task.started or not math.isfinite(task.remaining),\r\n                animation_time=task.get_time(),\r\n                style=self.style,\r\n                complete_style=self.complete_style,\r\n                finished_style=self.finished_style,\r\n                pulse_style=self.pulse_style,\r\n            )", "language": "python", "code": "def render(self, task: \"Task\") -> _RichProgressBar:\r\n            \"\"\"Gets a progress bar widget for a task.\"\"\"\r\n            assert task.total is not None\r\n            assert task.remaining is not None\r\n            return _RichProgressBar(\r\n                total=max(0, task.total),\r\n                completed=max(0, task.completed),\r\n                width=None if self.bar_width is None else max(1, self.bar_width),\r\n                pulse=not task.started or not math.isfinite(task.remaining),\r\n                animation_time=task.get_time(),\r\n                style=self.style,\r\n                complete_style=self.complete_style,\r\n                finished_style=self.finished_style,\r\n                pulse_style=self.pulse_style,\r\n            )", "code_tokens": ["def", "render", "(", "self", ",", "task", ":", "\"", "Task", "\"", ")", "-", ">", "_RichProgressBar", ":", "\"", "\"", "\"", "Gets", "a", "progress", "bar", "widget", "for", "a", "task", ".", "\"", "\"", "\"", "assert", "task", ".", "total", "is", "not", "None", "assert", "task", ".", "remaining", "is", "not", "None", "return", "_RichProgressBar", "(", "total", "=", "max", "(", "0", ",", "task", ".", "total", ")", ",", "completed", "=", "max", "(", "0", ",", "task", ".", "completed", ")", ",", "width", "=", "None", "if", "self", ".", "bar_width", "is", "None", "else", "max", "(", "1", ",", "self", ".", "bar_width", ")", ",", "pulse", "=", "not", "task", ".", "started", "or", "not", "math", ".", "isfinite", "(", "task", ".", "remaining", ")", ",", "animation_time", "=", "task", ".", "get_time", "(", ")", ",", "style", "=", "self", ".", "style", ",", "complete_style", "=", "self", ".", "complete_style", ",", "finished_style", "=", "self", ".", "finished_style", ",", "pulse_style", "=", "self", ".", "pulse_style", ",", ")"], "docstring": "Gets a progress bar widget for a task.", "docstring_tokens": ["gets", "a", "progress", "bar", "widget", "for", "a", "task"], "docstring_summary": "Gets a progress bar widget for a task.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\rich_progress.py", "partition": "train", "function_type": "class_method", "class_name": "CustomBarColumn", "start_line": 40, "end_line": 54, "hash": "3ed2023e1a277746e22850200663eaa5", "complexity": 3, "parameters": ["task"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "__init__", "original_string": "def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from\r\n        flickering.\"\"\"\r\n        # this just to make the make docs happy, otherwise it pulls docs which has some issues...\r\n        super().__init__(*args, **kwargs)", "language": "python", "code": "def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from\r\n        flickering.\"\"\"\r\n        # this just to make the make docs happy, otherwise it pulls docs which has some issues...\r\n        super().__init__(*args, **kwargs)", "code_tokens": ["def", "__init__", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Custom", "tqdm", "progressbar", "where", "we", "append", "0", "to", "floating", "points", "/", "strings", "to", "prevent", "the", "progress", "bar", "from", "flickering", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from\r\n        flickering.", "docstring_tokens": ["custom", "tqdm", "progressbar", "where", "we", "append", "0", "to", "floating", "points", "strings", "to", "prevent", "the", "progress", "bar", "from", "flickering"], "docstring_summary": "Custom tqdm progressbar where we append 0 to floating points/strings to prevent the progress bar from", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "Tqdm", "start_line": 39, "end_line": 43, "hash": "1c7c324f57c8b14a77cb04dd1b012b2e", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "format_num", "original_string": "def format_num(n: Union[int, float, str]) -> str:\r\n        \"\"\"Add additional padding to the formatted numbers.\"\"\"\r\n        should_be_padded = isinstance(n, (float, str))\r\n        if not isinstance(n, str):\r\n            n = _tqdm.format_num(n)\r\n            assert isinstance(n, str)\r\n        if should_be_padded and \"e\" not in n:\r\n            if \".\" not in n and len(n) < _PAD_SIZE:\r\n                try:\r\n                    _ = float(n)\r\n                except ValueError:\r\n                    return n\r\n                n += \".\"\r\n            n += \"0\" * (_PAD_SIZE - len(n))\r\n        return n", "language": "python", "code": "def format_num(n: Union[int, float, str]) -> str:\r\n        \"\"\"Add additional padding to the formatted numbers.\"\"\"\r\n        should_be_padded = isinstance(n, (float, str))\r\n        if not isinstance(n, str):\r\n            n = _tqdm.format_num(n)\r\n            assert isinstance(n, str)\r\n        if should_be_padded and \"e\" not in n:\r\n            if \".\" not in n and len(n) < _PAD_SIZE:\r\n                try:\r\n                    _ = float(n)\r\n                except ValueError:\r\n                    return n\r\n                n += \".\"\r\n            n += \"0\" * (_PAD_SIZE - len(n))\r\n        return n", "code_tokens": ["def", "format_num", "(", "n", ":", "Union", "[", "int", ",", "float", ",", "str", "]", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Add", "additional", "padding", "to", "the", "formatted", "numbers", ".", "\"", "\"", "\"", "should_be_padded", "=", "isinstance", "(", "n", ",", "(", "float", ",", "str", ")", ")", "if", "not", "isinstance", "(", "n", ",", "str", ")", ":", "n", "=", "_tqdm", ".", "format_num", "(", "n", ")", "assert", "isinstance", "(", "n", ",", "str", ")", "if", "should_be_padded", "and", "\"", "e", "\"", "not", "in", "n", ":", "if", "\"", ".", "\"", "not", "in", "n", "and", "len", "(", "n", ")", "<", "_PAD_SIZE", ":", "try", ":", "_", "=", "float", "(", "n", ")", "except", "ValueError", ":", "return", "n", "n", "+", "=", "\"", ".", "\"", "n", "+", "=", "\"", "0", "\"", "*", "(", "_PAD_SIZE", "-", "len", "(", "n", ")", ")", "return", "n"], "docstring": "Add additional padding to the formatted numbers.", "docstring_tokens": ["add", "additional", "padding", "to", "the", "formatted", "numbers"], "docstring_summary": "Add additional padding to the formatted numbers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "Tqdm", "start_line": 46, "end_line": 60, "hash": "686befc39aabf96060861ef6da9e0ba1", "complexity": 7, "parameters": ["n", "float", "str]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "init_sanity_tqdm", "original_string": "def init_sanity_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for the validation sanity run.\"\"\"\r\n        return Tqdm(\r\n            desc=self.sanity_check_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=False,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_sanity_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for the validation sanity run.\"\"\"\r\n        return Tqdm(\r\n            desc=self.sanity_check_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=False,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_sanity_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "\"", "\"", "\"", "Override", "this", "to", "customize", "the", "tqdm", "bar", "for", "the", "validation", "sanity", "run", ".", "\"", "\"", "\"", "return", "Tqdm", "(", "desc", "=", "self", ".", "sanity_check_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "False", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for the validation sanity run.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "the", "validation", "sanity", "run"], "docstring_summary": "Override this to customize the tqdm bar for the validation sanity run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "TQDMProgressBar", "start_line": 185, "end_line": 195, "hash": "ea42ff33641e68b8529858b17e3b3b76", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "init_train_tqdm", "original_string": "def init_train_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for training.\"\"\"\r\n        return Tqdm(\r\n            desc=self.train_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_train_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for training.\"\"\"\r\n        return Tqdm(\r\n            desc=self.train_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_train_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "\"", "\"", "\"", "Override", "this", "to", "customize", "the", "tqdm", "bar", "for", "training", ".", "\"", "\"", "\"", "return", "Tqdm", "(", "desc", "=", "self", ".", "train_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "True", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "smoothing", "=", "0", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for training.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "training"], "docstring_summary": "Override this to customize the tqdm bar for training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "TQDMProgressBar", "start_line": 197, "end_line": 208, "hash": "386f56f8fa671f8f49acbba47e98c500", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "init_predict_tqdm", "original_string": "def init_predict_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for predicting.\"\"\"\r\n        return Tqdm(\r\n            desc=self.predict_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_predict_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for predicting.\"\"\"\r\n        return Tqdm(\r\n            desc=self.predict_description,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_predict_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "\"", "\"", "\"", "Override", "this", "to", "customize", "the", "tqdm", "bar", "for", "predicting", ".", "\"", "\"", "\"", "return", "Tqdm", "(", "desc", "=", "self", ".", "predict_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "True", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "smoothing", "=", "0", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for predicting.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "predicting"], "docstring_summary": "Override this to customize the tqdm bar for predicting.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "TQDMProgressBar", "start_line": 210, "end_line": 221, "hash": "13efb68039141bdbf7e2810315645af8", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "init_validation_tqdm", "original_string": "def init_validation_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for validation.\"\"\"\r\n        # The train progress bar doesn't exist in `trainer.validate()`\r\n        has_main_bar = self.trainer.state.fn != \"validate\"\r\n        return Tqdm(\r\n            desc=self.validation_description,\r\n            position=(2 * self.process_position + has_main_bar),\r\n            disable=self.is_disabled,\r\n            leave=not has_main_bar,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_validation_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for validation.\"\"\"\r\n        # The train progress bar doesn't exist in `trainer.validate()`\r\n        has_main_bar = self.trainer.state.fn != \"validate\"\r\n        return Tqdm(\r\n            desc=self.validation_description,\r\n            position=(2 * self.process_position + has_main_bar),\r\n            disable=self.is_disabled,\r\n            leave=not has_main_bar,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_validation_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "\"", "\"", "\"", "Override", "this", "to", "customize", "the", "tqdm", "bar", "for", "validation", ".", "\"", "\"", "\"", "has_main_bar", "=", "self", ".", "trainer", ".", "state", ".", "fn", "!", "=", "\"", "validate", "\"", "return", "Tqdm", "(", "desc", "=", "self", ".", "validation_description", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", "+", "has_main_bar", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "not", "has_main_bar", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for validation.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "validation"], "docstring_summary": "Override this to customize the tqdm bar for validation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "TQDMProgressBar", "start_line": 223, "end_line": 235, "hash": "fb6598e0329eac870dac7c8fab2cc256", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "init_test_tqdm", "original_string": "def init_test_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for testing.\"\"\"\r\n        return Tqdm(\r\n            desc=\"Testing\",\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "language": "python", "code": "def init_test_tqdm(self) -> Tqdm:\r\n        \"\"\"Override this to customize the tqdm bar for testing.\"\"\"\r\n        return Tqdm(\r\n            desc=\"Testing\",\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            bar_format=self.BAR_FORMAT,\r\n        )", "code_tokens": ["def", "init_test_tqdm", "(", "self", ")", "-", ">", "Tqdm", ":", "\"", "\"", "\"", "Override", "this", "to", "customize", "the", "tqdm", "bar", "for", "testing", ".", "\"", "\"", "\"", "return", "Tqdm", "(", "desc", "=", "\"", "Testing", "\"", ",", "position", "=", "(", "2", "*", "self", ".", "process_position", ")", ",", "disable", "=", "self", ".", "is_disabled", ",", "leave", "=", "True", ",", "dynamic_ncols", "=", "True", ",", "file", "=", "sys", ".", "stdout", ",", "bar_format", "=", "self", ".", "BAR_FORMAT", ",", ")"], "docstring": "Override this to customize the tqdm bar for testing.", "docstring_tokens": ["override", "this", "to", "customize", "the", "tqdm", "bar", "for", "testing"], "docstring_summary": "Override this to customize the tqdm bar for testing.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "class_method", "class_name": "TQDMProgressBar", "start_line": 237, "end_line": 247, "hash": "523c0f297858faec66a66d0702a1a2bb", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "func_name": "convert_inf", "original_string": "def convert_inf(x: Optional[Union[int, float]]) -> Optional[Union[int, float]]:\r\n    \"\"\"The tqdm doesn't support inf/nan values.\r\n\r\n    We have to convert it to None.\r\n\r\n    \"\"\"\r\n    if x is None or math.isinf(x) or math.isnan(x):\r\n        return None\r\n    return x", "language": "python", "code": "def convert_inf(x: Optional[Union[int, float]]) -> Optional[Union[int, float]]:\r\n    \"\"\"The tqdm doesn't support inf/nan values.\r\n\r\n    We have to convert it to None.\r\n\r\n    \"\"\"\r\n    if x is None or math.isinf(x) or math.isnan(x):\r\n        return None\r\n    return x", "code_tokens": ["def", "convert_inf", "(", "x", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", ")", "-", ">", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "\"", "\"", "\"", "The", "tqdm", "doesn", "'", "t", "support", "inf", "/", "nan", "values", ".", "We", "have", "to", "convert", "it", "to", "None", ".", "\"", "\"", "\"", "if", "x", "is", "None", "or", "math", ".", "isinf", "(", "x", ")", "or", "math", ".", "isnan", "(", "x", ")", ":", "return", "None", "return", "x"], "docstring": "The tqdm doesn't support inf/nan values.\r\n\r\n    We have to convert it to None.", "docstring_tokens": ["the", "tqdm", "doesn", "t", "support", "inf", "nan", "values", "we", "have", "to", "convert", "it", "to", "none"], "docstring_summary": "The tqdm doesn't support inf/nan values.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\callbacks\\progress\\tqdm_progress.py", "partition": "train", "function_type": "function", "start_line": 452, "end_line": 460, "hash": "6e24fe0ee4406b9c97d85111bdb9558d", "complexity": 4, "parameters": ["x", "float]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "from_datasets", "original_string": "def from_datasets(\r\n        cls,\r\n        train_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        val_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        test_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        predict_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        batch_size: int = 1,\r\n        num_workers: int = 0,\r\n        **datamodule_kwargs: Any,\r\n    ) -> \"LightningDataModule\":\r\n        r\"\"\"Create an instance from torch.utils.data.Dataset.\r\n\r\n        Args:\r\n            train_dataset: Optional dataset or iterable of datasets to be used for train_dataloader()\r\n            val_dataset: Optional dataset or iterable of datasets to be used for val_dataloader()\r\n            test_dataset: Optional dataset or iterable of datasets to be used for test_dataloader()\r\n            predict_dataset: Optional dataset or iterable of datasets to be used for predict_dataloader()\r\n            batch_size: Batch size to use for each dataloader. Default is 1. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            num_workers: Number of subprocesses to use for data loading. 0 means that the\r\n                data will be loaded in the main process. Number of CPUs available. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            **datamodule_kwargs: Additional parameters that get passed down to the datamodule's ``__init__``.\r\n\r\n        \"\"\"\r\n\r\n        def dataloader(ds: Dataset, shuffle: bool = False) -> DataLoader:\r\n            shuffle &= not isinstance(ds, IterableDataset)\r\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\r\n\r\n        def train_dataloader() -> TRAIN_DATALOADERS:\r\n            return apply_to_collection(train_dataset, Dataset, dataloader, shuffle=True)\r\n\r\n        def val_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(val_dataset, Dataset, dataloader)\r\n\r\n        def test_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(test_dataset, Dataset, dataloader)\r\n\r\n        def predict_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(predict_dataset, Dataset, dataloader)\r\n\r\n        candidate_kwargs = {\"batch_size\": batch_size, \"num_workers\": num_workers}\r\n        accepted_params = inspect.signature(cls.__init__).parameters\r\n        accepts_kwargs = any(param.kind == param.VAR_KEYWORD for param in accepted_params.values())\r\n        if accepts_kwargs:\r\n            special_kwargs = candidate_kwargs\r\n        else:\r\n            accepted_param_names = set(accepted_params)\r\n            accepted_param_names.discard(\"self\")\r\n            special_kwargs = {k: v for k, v in candidate_kwargs.items() if k in accepted_param_names}\r\n\r\n        datamodule = cls(**datamodule_kwargs, **special_kwargs)\r\n        if train_dataset is not None:\r\n            datamodule.train_dataloader = train_dataloader  # type: ignore[method-assign]\r\n        if val_dataset is not None:\r\n            datamodule.val_dataloader = val_dataloader  # type: ignore[method-assign]\r\n        if test_dataset is not None:\r\n            datamodule.test_dataloader = test_dataloader  # type: ignore[method-assign]\r\n        if predict_dataset is not None:\r\n            datamodule.predict_dataloader = predict_dataloader  # type: ignore[method-assign]\r\n        return datamodule", "language": "python", "code": "def from_datasets(\r\n        cls,\r\n        train_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        val_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        test_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        predict_dataset: Optional[Union[Dataset, Iterable[Dataset]]] = None,\r\n        batch_size: int = 1,\r\n        num_workers: int = 0,\r\n        **datamodule_kwargs: Any,\r\n    ) -> \"LightningDataModule\":\r\n        r\"\"\"Create an instance from torch.utils.data.Dataset.\r\n\r\n        Args:\r\n            train_dataset: Optional dataset or iterable of datasets to be used for train_dataloader()\r\n            val_dataset: Optional dataset or iterable of datasets to be used for val_dataloader()\r\n            test_dataset: Optional dataset or iterable of datasets to be used for test_dataloader()\r\n            predict_dataset: Optional dataset or iterable of datasets to be used for predict_dataloader()\r\n            batch_size: Batch size to use for each dataloader. Default is 1. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            num_workers: Number of subprocesses to use for data loading. 0 means that the\r\n                data will be loaded in the main process. Number of CPUs available. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            **datamodule_kwargs: Additional parameters that get passed down to the datamodule's ``__init__``.\r\n\r\n        \"\"\"\r\n\r\n        def dataloader(ds: Dataset, shuffle: bool = False) -> DataLoader:\r\n            shuffle &= not isinstance(ds, IterableDataset)\r\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\r\n\r\n        def train_dataloader() -> TRAIN_DATALOADERS:\r\n            return apply_to_collection(train_dataset, Dataset, dataloader, shuffle=True)\r\n\r\n        def val_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(val_dataset, Dataset, dataloader)\r\n\r\n        def test_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(test_dataset, Dataset, dataloader)\r\n\r\n        def predict_dataloader() -> EVAL_DATALOADERS:\r\n            return apply_to_collection(predict_dataset, Dataset, dataloader)\r\n\r\n        candidate_kwargs = {\"batch_size\": batch_size, \"num_workers\": num_workers}\r\n        accepted_params = inspect.signature(cls.__init__).parameters\r\n        accepts_kwargs = any(param.kind == param.VAR_KEYWORD for param in accepted_params.values())\r\n        if accepts_kwargs:\r\n            special_kwargs = candidate_kwargs\r\n        else:\r\n            accepted_param_names = set(accepted_params)\r\n            accepted_param_names.discard(\"self\")\r\n            special_kwargs = {k: v for k, v in candidate_kwargs.items() if k in accepted_param_names}\r\n\r\n        datamodule = cls(**datamodule_kwargs, **special_kwargs)\r\n        if train_dataset is not None:\r\n            datamodule.train_dataloader = train_dataloader  # type: ignore[method-assign]\r\n        if val_dataset is not None:\r\n            datamodule.val_dataloader = val_dataloader  # type: ignore[method-assign]\r\n        if test_dataset is not None:\r\n            datamodule.test_dataloader = test_dataloader  # type: ignore[method-assign]\r\n        if predict_dataset is not None:\r\n            datamodule.predict_dataloader = predict_dataloader  # type: ignore[method-assign]\r\n        return datamodule", "code_tokens": ["def", "from_datasets", "(", "cls", ",", "train_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "val_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "test_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "predict_dataset", ":", "Optional", "[", "Union", "[", "Dataset", ",", "Iterable", "[", "Dataset", "]", "]", "]", "=", "None", ",", "batch_size", ":", "int", "=", "1", ",", "num_workers", ":", "int", "=", "0", ",", "*", "*", "datamodule_kwargs", ":", "Any", ",", ")", "-", ">", "\"", "LightningDataModule", "\"", ":", "r", "\"", "\"", "\"", "Create", "an", "instance", "from", "torch", ".", "utils", ".", "data", ".", "Dataset", ".", "Args", ":", "train_dataset", ":", "Optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "train_dataloader", "(", ")", "val_dataset", ":", "Optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "val_dataloader", "(", ")", "test_dataset", ":", "Optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "test_dataloader", "(", ")", "predict_dataset", ":", "Optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "predict_dataloader", "(", ")", "batch_size", ":", "Batch", "size", "to", "use", "for", "each", "dataloader", ".", "Default", "is", "1", ".", "This", "parameter", "gets", "forwarded", "to", "the", "`", "`", "__init__", "`", "`", "if", "the", "datamodule", "has", "such", "a", "name", "defined", "in", "its", "signature", ".", "num_workers", ":", "Number", "of", "subprocesses", "to", "use", "for", "data", "loading", ".", "0", "means", "that", "the", "data", "will", "be", "loaded", "in", "the", "main", "process", ".", "Number", "of", "CPUs", "available", ".", "This", "parameter", "gets", "forwarded", "to", "the", "`", "`", "__init__", "`", "`", "if", "the", "datamodule", "has", "such", "a", "name", "defined", "in", "its", "signature", ".", "*", "*", "datamodule_kwargs", ":", "Additional", "parameters", "that", "get", "passed", "down", "to", "the", "datamodule", "'", "s", "`", "`", "__init__", "`", "`", ".", "\"", "\"", "\"", "def", "dataloader", "(", "ds", ":", "Dataset", ",", "shuffle", ":", "bool", "=", "False", ")", "-", ">", "DataLoader", ":", "shuffle", "&", "=", "not", "isinstance", "(", "ds", ",", "IterableDataset", ")", "return", "DataLoader", "(", "ds", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "shuffle", ",", "num_workers", "=", "num_workers", ",", "pin_memory", "=", "True", ")", "def", "train_dataloader", "(", ")", "-", ">", "TRAIN_DATALOADERS", ":", "return", "apply_to_collection", "(", "train_dataset", ",", "Dataset", ",", "dataloader", ",", "shuffle", "=", "True", ")", "def", "val_dataloader", "(", ")", "-", ">", "EVAL_DATALOADERS", ":", "return", "apply_to_collection", "(", "val_dataset", ",", "Dataset", ",", "dataloader", ")", "def", "test_dataloader", "(", ")", "-", ">", "EVAL_DATALOADERS", ":", "return", "apply_to_collection", "(", "test_dataset", ",", "Dataset", ",", "dataloader", ")", "def", "predict_dataloader", "(", ")", "-", ">", "EVAL_DATALOADERS", ":", "return", "apply_to_collection", "(", "predict_dataset", ",", "Dataset", ",", "dataloader", ")", "candidate_kwargs", "=", "{", "\"", "batch_size", "\"", ":", "batch_size", ",", "\"", "num_workers", "\"", ":", "num_workers", "}", "accepted_params", "=", "inspect", ".", "signature", "(", "cls", ".", "__init__", ")", ".", "parameters", "accepts_kwargs", "=", "any", "(", "param", ".", "kind", "=", "=", "param", ".", "VAR_KEYWORD", "for", "param", "in", "accepted_params", ".", "values", "(", ")", ")", "if", "accepts_kwargs", ":", "special_kwargs", "=", "candidate_kwargs", "else", ":", "accepted_param_names", "=", "set", "(", "accepted_params", ")", "accepted_param_names", ".", "discard", "(", "\"", "self", "\"", ")", "special_kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "candidate_kwargs", ".", "items", "(", ")", "if", "k", "in", "accepted_param_names", "}", "datamodule", "=", "cls", "(", "*", "*", "datamodule_kwargs", ",", "*", "*", "special_kwargs", ")", "if", "train_dataset", "is", "not", "None", ":", "datamodule", ".", "train_dataloader", "=", "train_dataloader", "if", "val_dataset", "is", "not", "None", ":", "datamodule", ".", "val_dataloader", "=", "val_dataloader", "if", "test_dataset", "is", "not", "None", ":", "datamodule", ".", "test_dataloader", "=", "test_dataloader", "if", "predict_dataset", "is", "not", "None", ":", "datamodule", ".", "predict_dataloader", "=", "predict_dataloader", "return", "datamodule"], "docstring": "r\"\"\"Create an instance from torch.utils.data.Dataset.\r\n\r\n        Args:\r\n            train_dataset: Optional dataset or iterable of datasets to be used for train_dataloader()\r\n            val_dataset: Optional dataset or iterable of datasets to be used for val_dataloader()\r\n            test_dataset: Optional dataset or iterable of datasets to be used for test_dataloader()\r\n            predict_dataset: Optional dataset or iterable of datasets to be used for predict_dataloader()\r\n            batch_size: Batch size to use for each dataloader. Default is 1. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            num_workers: Number of subprocesses to use for data loading. 0 means that the\r\n                data will be loaded in the main process. Number of CPUs available. This parameter gets forwarded to the\r\n                ``__init__`` if the datamodule has such a name defined in its signature.\r\n            **datamodule_kwargs: Additional parameters that get passed down to the datamodule's ``__init__``.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "create", "an", "instance", "from", "torch", "utils", "data", "dataset", "args", "train_dataset", "optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "train_dataloader", "val_dataset", "optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "val_dataloader", "test_dataset", "optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "test_dataloader", "predict_dataset", "optional", "dataset", "or", "iterable", "of", "datasets", "to", "be", "used", "for", "predict_dataloader", "batch_size", "batch", "size", "to", "use", "for", "each", "dataloader", "default", "is", "1", "this", "parameter", "gets", "forwarded", "to", "the", "__init__", "if", "the", "datamodule", "has", "such", "a", "name", "defined", "in", "its", "signature", "num_workers", "number", "of", "subprocesses", "to", "use", "for", "data", "loading", "0", "means", "that", "the", "data", "will", "be", "loaded", "in", "the", "main", "process", "number", "of", "cpus", "available", "this", "parameter", "gets", "forwarded", "to", "the", "__init__", "if", "the", "datamodule", "has", "such", "a", "name", "defined", "in", "its", "signature", "datamodule_kwargs", "additional", "parameters", "that", "get", "passed", "down", "to", "the", "datamodule", "s", "__init__"], "docstring_summary": "r\"\"\"Create an instance from torch.utils.data.Dataset.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "LightningDataModule", "start_line": 88, "end_line": 149, "hash": "ee937748f159959684e4566a29dd57a6", "complexity": 9, "parameters": ["train_dataset", "Iterable[Dataset]]]", "val_dataset", "Iterable[Dataset]]]", "test_dataset", "Iterable[Dataset]]]", "predict_dataset", "Iterable[Dataset]]]", "batch_size", "num_workers", "**datamodule_kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "state_dict", "original_string": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\r\n\r\n        Returns:\r\n            A dictionary containing datamodule state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\r\n\r\n        Returns:\r\n            A dictionary containing datamodule state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "checkpoint", ",", "implement", "to", "generate", "and", "save", "datamodule", "state", ".", "Returns", ":", "A", "dictionary", "containing", "datamodule", "state", ".", "\"", "\"", "\"", "return", "{", "}"], "docstring": "Called when saving a checkpoint, implement to generate and save datamodule state.\r\n\r\n        Returns:\r\n            A dictionary containing datamodule state.", "docstring_tokens": ["called", "when", "saving", "a", "checkpoint", "implement", "to", "generate", "and", "save", "datamodule", "state", "returns", "a", "dictionary", "containing", "datamodule", "state"], "docstring_summary": "Called when saving a checkpoint, implement to generate and save datamodule state.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "LightningDataModule", "start_line": 151, "end_line": 158, "hash": "e4c72cc7b1a679036140fdf25d77374b", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "load_state_dict", "original_string": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.\r\n\r\n        Args:\r\n            state_dict: the datamodule state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def load_state_dict(self, state_dict: dict[str, Any]) -> None:\r\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.\r\n\r\n        Args:\r\n            state_dict: the datamodule state returned by ``state_dict``.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "loading", "a", "checkpoint", ",", "implement", "to", "reload", "datamodule", "state", "given", "datamodule", "state_dict", ".", "Args", ":", "state_dict", ":", "the", "datamodule", "state", "returned", "by", "`", "`", "state_dict", "`", "`", ".", "\"", "\"", "\"", "pass"], "docstring": "Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.\r\n\r\n        Args:\r\n            state_dict: the datamodule state returned by ``state_dict``.", "docstring_tokens": ["called", "when", "loading", "a", "checkpoint", "implement", "to", "reload", "datamodule", "state", "given", "datamodule", "state_dict", "args", "state_dict", "the", "datamodule", "state", "returned", "by", "state_dict"], "docstring_summary": "Called when loading a checkpoint, implement to reload datamodule state given datamodule state_dict.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "LightningDataModule", "start_line": 160, "end_line": 167, "hash": "b38c77795a8c31a5291d12fea60dce9d", "complexity": 1, "parameters": ["state_dict", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "load_from_checkpoint", "original_string": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the\r\n        arguments passed to ``__init__``  in the checkpoint under ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningDataModule` for use.\r\n\r\n                If your datamodule's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your datamodule to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            \\**kwargs: Any extra keyword args needed to init the datamodule. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningDataModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You must use your :class:`LightningDataModule`\r\n            **class** to call it instead of the :class:`LightningDataModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Example::\r\n\r\n            # load weights without mapping ...\r\n            datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            # or load weights and hyperparameters from separate files.\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            # override some of the params with new values\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                PATH,\r\n                batch_size=32,\r\n                num_workers=10,\r\n            )\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location=map_location,\r\n            hparams_file=hparams_file,\r\n            strict=None,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "language": "python", "code": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the\r\n        arguments passed to ``__init__``  in the checkpoint under ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningDataModule` for use.\r\n\r\n                If your datamodule's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your datamodule to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            \\**kwargs: Any extra keyword args needed to init the datamodule. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningDataModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You must use your :class:`LightningDataModule`\r\n            **class** to call it instead of the :class:`LightningDataModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Example::\r\n\r\n            # load weights without mapping ...\r\n            datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            # or load weights and hyperparameters from separate files.\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            # override some of the params with new values\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                PATH,\r\n                batch_size=32,\r\n                num_workers=10,\r\n            )\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location=map_location,\r\n            hparams_file=hparams_file,\r\n            strict=None,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "code_tokens": ["def", "load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ":", "Union", "[", "_PATH", ",", "IO", "]", ",", "map_location", ":", "_MAP_LOCATION_TYPE", "=", "None", ",", "hparams_file", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Self", ":", "r", "\"", "\"", "\"", "Primary", "way", "of", "loading", "a", "datamodule", "from", "a", "checkpoint", ".", "When", "Lightning", "saves", "a", "checkpoint", "it", "stores", "the", "arguments", "passed", "to", "`", "`", "__init__", "`", "`", "in", "the", "checkpoint", "under", "`", "`", "\"", "datamodule_hyper_parameters", "\"", "`", "`", ".", "Any", "arguments", "specified", "through", "\\", "*", "\\", "*", "kwargs", "will", "override", "args", "stored", "in", "`", "`", "\"", "datamodule_hyper_parameters", "\"", "`", "`", ".", "Args", ":", "checkpoint_path", ":", "Path", "to", "checkpoint", ".", "This", "can", "also", "be", "a", "URL", ",", "or", "file", "-", "like", "object", "map_location", ":", "If", "your", "checkpoint", "saved", "a", "GPU", "model", "and", "you", "now", "load", "on", "CPUs", "or", "a", "different", "number", "of", "GPUs", ",", "use", "this", "to", "map", "to", "the", "new", "setup", ".", "The", "behaviour", "is", "the", "same", "as", "in", ":", "func", ":", "`", "torch", ".", "load", "`", ".", "hparams_file", ":", "Optional", "path", "to", "a", "`", "`", ".", "yaml", "`", "`", "or", "`", "`", ".", "csv", "`", "`", "file", "with", "hierarchical", "structure", "as", "in", "this", "example", ":", ":", "dataloader", ":", "batch_size", ":", "32", "You", "most", "likely", "won", "'", "t", "need", "this", "since", "Lightning", "will", "always", "save", "the", "hyperparameters", "to", "the", "checkpoint", ".", "However", ",", "if", "your", "checkpoint", "weights", "don", "'", "t", "have", "the", "hyperparameters", "saved", ",", "use", "this", "method", "to", "pass", "in", "a", "`", "`", ".", "yaml", "`", "`", "file", "with", "the", "hparams", "you", "'", "d", "like", "to", "use", ".", "These", "will", "be", "converted", "into", "a", ":", "class", ":", "`", "~", "dict", "`", "and", "passed", "into", "your", ":", "class", ":", "`", "LightningDataModule", "`", "for", "use", ".", "If", "your", "datamodule", "'", "s", "`", "`", "hparams", "`", "`", "argument", "is", ":", "class", ":", "`", "~", "argparse", ".", "Namespace", "`", "and", "`", "`", ".", "yaml", "`", "`", "file", "has", "hierarchical", "structure", ",", "you", "need", "to", "refactor", "your", "datamodule", "to", "treat", "`", "`", "hparams", "`", "`", "as", ":", "class", ":", "`", "~", "dict", "`", ".", "\\", "*", "*", "kwargs", ":", "Any", "extra", "keyword", "args", "needed", "to", "init", "the", "datamodule", ".", "Can", "also", "be", "used", "to", "override", "saved", "hyperparameter", "values", ".", "Return", ":", ":", "class", ":", "`", "LightningDataModule", "`", "instance", "with", "loaded", "weights", "and", "hyperparameters", "(", "if", "available", ")", ".", "Note", ":", "`", "`", "load_from_checkpoint", "`", "`", "is", "a", "*", "*", "class", "*", "*", "method", ".", "You", "must", "use", "your", ":", "class", ":", "`", "LightningDataModule", "`", "*", "*", "class", "*", "*", "to", "call", "it", "instead", "of", "the", ":", "class", ":", "`", "LightningDataModule", "`", "instance", ",", "or", "a", "`", "`", "TypeError", "`", "`", "will", "be", "raised", ".", "Example", ":", ":", "datamodule", "=", "MyLightningDataModule", ".", "load_from_checkpoint", "(", "'", "path", "/", "to", "/", "checkpoint", ".", "ckpt", "'", ")", "datamodule", "=", "MyLightningDataModule", ".", "load_from_checkpoint", "(", "'", "path", "/", "to", "/", "checkpoint", ".", "ckpt", "'", ",", "hparams_file", "=", "'", "/", "path", "/", "to", "/", "hparams_file", ".", "yaml", "'", ")", "datamodule", "=", "MyLightningDataModule", ".", "load_from_checkpoint", "(", "PATH", ",", "batch_size", "=", "32", ",", "num_workers", "=", "10", ",", ")", "\"", "\"", "\"", "loaded", "=", "_load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ",", "map_location", "=", "map_location", ",", "hparams_file", "=", "hparams_file", ",", "strict", "=", "None", ",", "*", "*", "kwargs", ",", ")", "return", "cast", "(", "Self", ",", "loaded", ")"], "docstring": "r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the\r\n        arguments passed to ``__init__``  in the checkpoint under ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"datamodule_hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningDataModule` for use.\r\n\r\n                If your datamodule's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your datamodule to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            \\**kwargs: Any extra keyword args needed to init the datamodule. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningDataModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You must use your :class:`LightningDataModule`\r\n            **class** to call it instead of the :class:`LightningDataModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Example::\r\n\r\n            # load weights without mapping ...\r\n            datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            # or load weights and hyperparameters from separate files.\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            # override some of the params with new values\r\n            datamodule = MyLightningDataModule.load_from_checkpoint(\r\n                PATH,\r\n                batch_size=32,\r\n                num_workers=10,\r\n            )\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "primary", "way", "of", "loading", "a", "datamodule", "from", "a", "checkpoint", "when", "lightning", "saves", "a", "checkpoint", "it", "stores", "the", "arguments", "passed", "to", "__init__", "in", "the", "checkpoint", "under", "datamodule_hyper_parameters", "any", "arguments", "specified", "through", "kwargs", "will", "override", "args", "stored", "in", "datamodule_hyper_parameters", "args", "checkpoint_path", "path", "to", "checkpoint", "this", "can", "also", "be", "a", "url", "or", "file", "like", "object", "map_location", "if", "your", "checkpoint", "saved", "a", "gpu", "model", "and", "you", "now", "load", "on", "cpus", "or", "a", "different", "number", "of", "gpus", "use", "this", "to", "map", "to", "the", "new", "setup", "the", "behaviour", "is", "the", "same", "as", "in", "func", "torch", "load", "hparams_file", "optional", "path", "to", "a", "yaml", "or", "csv", "file", "with", "hierarchical", "structure", "as", "in", "this", "example", "dataloader", "batch_size", "32", "you", "most", "likely", "won", "t", "need", "this", "since", "lightning", "will", "always", "save", "the", "hyperparameters", "to", "the", "checkpoint", "however", "if", "your", "checkpoint", "weights", "don", "t", "have", "the", "hyperparameters", "saved", "use", "this", "method", "to", "pass", "in", "a", "yaml", "file", "with", "the", "hparams", "you", "d", "like", "to", "use", "these", "will", "be", "converted", "into", "a", "class", "dict", "and", "passed", "into", "your", "class", "lightningdatamodule", "for", "use", "if", "your", "datamodule", "s", "hparams", "argument", "is", "class", "argparse", "namespace", "and", "yaml", "file", "has", "hierarchical", "structure", "you", "need", "to", "refactor", "your", "datamodule", "to", "treat", "hparams", "as", "class", "dict", "kwargs", "any", "extra", "keyword", "args", "needed", "to", "init", "the", "datamodule", "can", "also", "be", "used", "to", "override", "saved", "hyperparameter", "values", "return", "class", "lightningdatamodule", "instance", "with", "loaded", "weights", "and", "hyperparameters", "if", "available", "note", "load_from_checkpoint", "is", "a", "class", "method", "you", "must", "use", "your", "class", "lightningdatamodule", "class", "to", "call", "it", "instead", "of", "the", "class", "lightningdatamodule", "instance", "or", "a", "typeerror", "will", "be", "raised", "example", "load", "weights", "without", "mapping", "datamodule", "mylightningdatamodule", "load_from_checkpoint", "path", "to", "checkpoint", "ckpt", "or", "load", "weights", "and", "hyperparameters", "from", "separate", "files", "datamodule", "mylightningdatamodule", "load_from_checkpoint", "path", "to", "checkpoint", "ckpt", "hparams_file", "path", "to", "hparams_file", "yaml", "override", "some", "of", "the", "params", "with", "new", "values", "datamodule", "mylightningdatamodule", "load_from_checkpoint", "path", "batch_size", "32", "num_workers", "10"], "docstring_summary": "r\"\"\"Primary way of loading a datamodule from a checkpoint. When Lightning saves a checkpoint it stores the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "LightningDataModule", "start_line": 174, "end_line": 246, "hash": "c0652af4489e402abe5f62016ec5da6f", "complexity": 1, "parameters": ["checkpoint_path", "IO]", "map_location", "hparams_file", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\datamodule.py", "func_name": "__str__", "original_string": "def __str__(self) -> str:\r\n        \"\"\"Return a string representation of the datasets that are set up.\r\n\r\n        Returns:\r\n            A string representation of the datasets that are setup.\r\n\r\n        \"\"\"\r\n\r\n        class dataset_info:\r\n            def __init__(self, available: bool, length: str) -> None:\r\n                self.available = available\r\n                self.length = length\r\n\r\n        def retrieve_dataset_info(loader: DataLoader) -> dataset_info:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            dataset = loader.dataset\r\n            size: str = str(len(dataset)) if isinstance(dataset, Sized) else \"NA\"\r\n\r\n            return dataset_info(True, size)\r\n\r\n        def loader_info(\r\n            loader: Union[DataLoader, Iterable[DataLoader]],\r\n        ) -> Union[dataset_info, Iterable[dataset_info]]:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            return apply_to_collection(loader, DataLoader, retrieve_dataset_info)\r\n\r\n        def extract_loader_info(methods: list[tuple[str, str]]) -> dict:\r\n            \"\"\"Helper function to extract information for each dataloader method.\"\"\"\r\n            info: dict[str, Union[dataset_info, Iterable[dataset_info]]] = {}\r\n            for loader_name, func_name in methods:\r\n                loader_method = getattr(self, func_name, None)\r\n\r\n                try:\r\n                    loader = loader_method()  # type: ignore\r\n                    info[loader_name] = loader_info(loader)\r\n                except Exception:\r\n                    info[loader_name] = dataset_info(False, \"\")\r\n\r\n            return info\r\n\r\n        def format_loader_info(info: dict[str, Union[dataset_info, Iterable[dataset_info]]]) -> str:\r\n            \"\"\"Helper function to format loader information.\"\"\"\r\n            output = []\r\n            for loader_name, loader_info in info.items():\r\n                # Single dataset\r\n                if isinstance(loader_info, dataset_info):\r\n                    loader_info_formatted = \"None\" if not loader_info.available else f\"size={loader_info.length}\"\r\n                # Iterable of datasets\r\n                else:\r\n                    loader_info_formatted = \" ; \".join(\r\n                        \"None\" if not loader_info_i.available else f\"{i}. size={loader_info_i.length}\"\r\n                        for i, loader_info_i in enumerate(loader_info, start=1)\r\n                    )\r\n\r\n                output.append(f\"{{{loader_name}: {loader_info_formatted}}}\")\r\n\r\n            return os.linesep.join(output)\r\n\r\n        # Available dataloader methods\r\n        datamodule_loader_methods: list[tuple[str, str]] = [\r\n            (\"Train dataloader\", \"train_dataloader\"),\r\n            (\"Validation dataloader\", \"val_dataloader\"),\r\n            (\"Test dataloader\", \"test_dataloader\"),\r\n            (\"Predict dataloader\", \"predict_dataloader\"),\r\n        ]\r\n\r\n        # Retrieve information for each dataloader method\r\n        dataloader_info = extract_loader_info(datamodule_loader_methods)\r\n        # Format the information\r\n        dataloader_str = format_loader_info(dataloader_info)\r\n        return dataloader_str", "language": "python", "code": "def __str__(self) -> str:\r\n        \"\"\"Return a string representation of the datasets that are set up.\r\n\r\n        Returns:\r\n            A string representation of the datasets that are setup.\r\n\r\n        \"\"\"\r\n\r\n        class dataset_info:\r\n            def __init__(self, available: bool, length: str) -> None:\r\n                self.available = available\r\n                self.length = length\r\n\r\n        def retrieve_dataset_info(loader: DataLoader) -> dataset_info:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            dataset = loader.dataset\r\n            size: str = str(len(dataset)) if isinstance(dataset, Sized) else \"NA\"\r\n\r\n            return dataset_info(True, size)\r\n\r\n        def loader_info(\r\n            loader: Union[DataLoader, Iterable[DataLoader]],\r\n        ) -> Union[dataset_info, Iterable[dataset_info]]:\r\n            \"\"\"Helper function to compute dataset information.\"\"\"\r\n            return apply_to_collection(loader, DataLoader, retrieve_dataset_info)\r\n\r\n        def extract_loader_info(methods: list[tuple[str, str]]) -> dict:\r\n            \"\"\"Helper function to extract information for each dataloader method.\"\"\"\r\n            info: dict[str, Union[dataset_info, Iterable[dataset_info]]] = {}\r\n            for loader_name, func_name in methods:\r\n                loader_method = getattr(self, func_name, None)\r\n\r\n                try:\r\n                    loader = loader_method()  # type: ignore\r\n                    info[loader_name] = loader_info(loader)\r\n                except Exception:\r\n                    info[loader_name] = dataset_info(False, \"\")\r\n\r\n            return info\r\n\r\n        def format_loader_info(info: dict[str, Union[dataset_info, Iterable[dataset_info]]]) -> str:\r\n            \"\"\"Helper function to format loader information.\"\"\"\r\n            output = []\r\n            for loader_name, loader_info in info.items():\r\n                # Single dataset\r\n                if isinstance(loader_info, dataset_info):\r\n                    loader_info_formatted = \"None\" if not loader_info.available else f\"size={loader_info.length}\"\r\n                # Iterable of datasets\r\n                else:\r\n                    loader_info_formatted = \" ; \".join(\r\n                        \"None\" if not loader_info_i.available else f\"{i}. size={loader_info_i.length}\"\r\n                        for i, loader_info_i in enumerate(loader_info, start=1)\r\n                    )\r\n\r\n                output.append(f\"{{{loader_name}: {loader_info_formatted}}}\")\r\n\r\n            return os.linesep.join(output)\r\n\r\n        # Available dataloader methods\r\n        datamodule_loader_methods: list[tuple[str, str]] = [\r\n            (\"Train dataloader\", \"train_dataloader\"),\r\n            (\"Validation dataloader\", \"val_dataloader\"),\r\n            (\"Test dataloader\", \"test_dataloader\"),\r\n            (\"Predict dataloader\", \"predict_dataloader\"),\r\n        ]\r\n\r\n        # Retrieve information for each dataloader method\r\n        dataloader_info = extract_loader_info(datamodule_loader_methods)\r\n        # Format the information\r\n        dataloader_str = format_loader_info(dataloader_info)\r\n        return dataloader_str", "code_tokens": ["def", "__str__", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Return", "a", "string", "representation", "of", "the", "datasets", "that", "are", "set", "up", ".", "Returns", ":", "A", "string", "representation", "of", "the", "datasets", "that", "are", "setup", ".", "\"", "\"", "\"", "class", "dataset_info", ":", "def", "__init__", "(", "self", ",", "available", ":", "bool", ",", "length", ":", "str", ")", "-", ">", "None", ":", "self", ".", "available", "=", "available", "self", ".", "length", "=", "length", "def", "retrieve_dataset_info", "(", "loader", ":", "DataLoader", ")", "-", ">", "dataset_info", ":", "\"", "\"", "\"", "Helper", "function", "to", "compute", "dataset", "information", ".", "\"", "\"", "\"", "dataset", "=", "loader", ".", "dataset", "size", ":", "str", "=", "str", "(", "len", "(", "dataset", ")", ")", "if", "isinstance", "(", "dataset", ",", "Sized", ")", "else", "\"", "NA", "\"", "return", "dataset_info", "(", "True", ",", "size", ")", "def", "loader_info", "(", "loader", ":", "Union", "[", "DataLoader", ",", "Iterable", "[", "DataLoader", "]", "]", ",", ")", "-", ">", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", ":", "\"", "\"", "\"", "Helper", "function", "to", "compute", "dataset", "information", ".", "\"", "\"", "\"", "return", "apply_to_collection", "(", "loader", ",", "DataLoader", ",", "retrieve_dataset_info", ")", "def", "extract_loader_info", "(", "methods", ":", "list", "[", "tuple", "[", "str", ",", "str", "]", "]", ")", "-", ">", "dict", ":", "\"", "\"", "\"", "Helper", "function", "to", "extract", "information", "for", "each", "dataloader", "method", ".", "\"", "\"", "\"", "info", ":", "dict", "[", "str", ",", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", "]", "=", "{", "}", "for", "loader_name", ",", "func_name", "in", "methods", ":", "loader_method", "=", "getattr", "(", "self", ",", "func_name", ",", "None", ")", "try", ":", "loader", "=", "loader_method", "(", ")", "info", "[", "loader_name", "]", "=", "loader_info", "(", "loader", ")", "except", "Exception", ":", "info", "[", "loader_name", "]", "=", "dataset_info", "(", "False", ",", "\"", "\"", ")", "return", "info", "def", "format_loader_info", "(", "info", ":", "dict", "[", "str", ",", "Union", "[", "dataset_info", ",", "Iterable", "[", "dataset_info", "]", "]", "]", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Helper", "function", "to", "format", "loader", "information", ".", "\"", "\"", "\"", "output", "=", "[", "]", "for", "loader_name", ",", "loader_info", "in", "info", ".", "items", "(", ")", ":", "if", "isinstance", "(", "loader_info", ",", "dataset_info", ")", ":", "loader_info_formatted", "=", "\"", "None", "\"", "if", "not", "loader_info", ".", "available", "else", "f", "\"", "size", "=", "{", "loader_info", ".", "length", "}", "\"", "else", ":", "loader_info_formatted", "=", "\"", ";", "\"", ".", "join", "(", "\"", "None", "\"", "if", "not", "loader_info_i", ".", "available", "else", "f", "\"", "{", "i", "}", ".", "size", "=", "{", "loader_info_i", ".", "length", "}", "\"", "for", "i", ",", "loader_info_i", "in", "enumerate", "(", "loader_info", ",", "start", "=", "1", ")", ")", "output", ".", "append", "(", "f", "\"", "{", "{", "{", "loader_name", "}", ":", "{", "loader_info_formatted", "}", "}", "}", "\"", ")", "return", "os", ".", "linesep", ".", "join", "(", "output", ")", "datamodule_loader_methods", ":", "list", "[", "tuple", "[", "str", ",", "str", "]", "]", "=", "[", "(", "\"", "Train", "dataloader", "\"", ",", "\"", "train_dataloader", "\"", ")", ",", "(", "\"", "Validation", "dataloader", "\"", ",", "\"", "val_dataloader", "\"", ")", ",", "(", "\"", "Test", "dataloader", "\"", ",", "\"", "test_dataloader", "\"", ")", ",", "(", "\"", "Predict", "dataloader", "\"", ",", "\"", "predict_dataloader", "\"", ")", ",", "]", "dataloader_info", "=", "extract_loader_info", "(", "datamodule_loader_methods", ")", "dataloader_str", "=", "format_loader_info", "(", "dataloader_info", ")", "return", "dataloader_str"], "docstring": "Return a string representation of the datasets that are set up.\r\n\r\n        Returns:\r\n            A string representation of the datasets that are setup.", "docstring_tokens": ["return", "a", "string", "representation", "of", "the", "datasets", "that", "are", "set", "up", "returns", "a", "string", "representation", "of", "the", "datasets", "that", "are", "setup"], "docstring_summary": "Return a string representation of the datasets that are set up.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "LightningDataModule", "start_line": 248, "end_line": 318, "hash": "047cf5ce625db5024825a2af92c2f7c0", "complexity": 9, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_fit_start", "original_string": "def on_fit_start(self) -> None:\r\n        \"\"\"Called at the very beginning of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "language": "python", "code": "def on_fit_start(self) -> None:\r\n        \"\"\"Called at the very beginning of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_fit_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "at", "the", "very", "beginning", "of", "fit", ".", "If", "on", "DDP", "it", "is", "called", "on", "every", "process", "\"", "\"", "\""], "docstring": "Called at the very beginning of fit.\r\n\r\n        If on DDP it is called on every process", "docstring_tokens": ["called", "at", "the", "very", "beginning", "of", "fit", "if", "on", "ddp", "it", "is", "called", "on", "every", "process"], "docstring_summary": "Called at the very beginning of fit.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 29, "end_line": 34, "hash": "4763b5eb3af96753fa4af32bdcd1e738", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_fit_end", "original_string": "def on_fit_end(self) -> None:\r\n        \"\"\"Called at the very end of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "language": "python", "code": "def on_fit_end(self) -> None:\r\n        \"\"\"Called at the very end of fit.\r\n\r\n        If on DDP it is called on every process\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_fit_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "at", "the", "very", "end", "of", "fit", ".", "If", "on", "DDP", "it", "is", "called", "on", "every", "process", "\"", "\"", "\""], "docstring": "Called at the very end of fit.\r\n\r\n        If on DDP it is called on every process", "docstring_tokens": ["called", "at", "the", "very", "end", "of", "fit", "if", "on", "ddp", "it", "is", "called", "on", "every", "process"], "docstring_summary": "Called at the very end of fit.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 36, "end_line": 41, "hash": "281c3952e91e61ae61dce94c47bbb362", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_train_batch_start", "original_string": "def on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:\r\n        \"\"\"Called in the training loop before anything happens for that batch.\r\n\r\n        If you return -1 here, you will skip training for the rest of the current epoch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:\r\n        \"\"\"Called in the training loop before anything happens for that batch.\r\n\r\n        If you return -1 here, you will skip training for the rest of the current epoch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "Optional", "[", "int", "]", ":", "\"", "\"", "\"", "Called", "in", "the", "training", "loop", "before", "anything", "happens", "for", "that", "batch", ".", "If", "you", "return", "-", "1", "here", ",", "you", "will", "skip", "training", "for", "the", "rest", "of", "the", "current", "epoch", ".", "Args", ":", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "training", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "\"", "\"", "\""], "docstring": "Called in the training loop before anything happens for that batch.\r\n\r\n        If you return -1 here, you will skip training for the rest of the current epoch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch", "docstring_tokens": ["called", "in", "the", "training", "loop", "before", "anything", "happens", "for", "that", "batch", "if", "you", "return", "1", "here", "you", "will", "skip", "training", "for", "the", "rest", "of", "the", "current", "epoch", "args", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "training", "dataloader", "batch_idx", "the", "index", "of", "the", "batch"], "docstring_summary": "Called in the training loop before anything happens for that batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 67, "end_line": 76, "hash": "95778c4c20814e15894bfbc9ca874943", "complexity": 1, "parameters": ["batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_train_batch_end", "original_string": "def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Called in the training loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of training_step(x)\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int) -> None:\r\n        \"\"\"Called in the training loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of training_step(x)\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_batch_end", "(", "self", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "training", "loop", "after", "the", "batch", ".", "Args", ":", "outputs", ":", "The", "outputs", "of", "training_step", "(", "x", ")", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "training", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "Note", ":", "The", "value", "`", "`", "outputs", "[", "\"", "loss", "\"", "]", "`", "`", "here", "will", "be", "the", "normalized", "value", "w", ".", "r", ".", "t", "`", "`", "accumulate_grad_batches", "`", "`", "of", "the", "loss", "returned", "from", "`", "`", "training_step", "`", "`", ".", "\"", "\"", "\""], "docstring": "Called in the training loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of training_step(x)\r\n            batch: The batched data as it is returned by the training DataLoader.\r\n            batch_idx: the index of the batch\r\n\r\n        Note:\r\n            The value ``outputs[\"loss\"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the\r\n            loss returned from ``training_step``.", "docstring_tokens": ["called", "in", "the", "training", "loop", "after", "the", "batch", "args", "outputs", "the", "outputs", "of", "training_step", "x", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "training", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "note", "the", "value", "outputs", "loss", "here", "will", "be", "the", "normalized", "value", "w", "r", "t", "accumulate_grad_batches", "of", "the", "loss", "returned", "from", "training_step"], "docstring_summary": "Called in the training loop after the batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 78, "end_line": 90, "hash": "2e135cbe533bd8ab4cfdbcef94dfd641", "complexity": 1, "parameters": ["outputs", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_validation_batch_start", "original_string": "def on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the validation loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the validation loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_validation_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "validation", "loop", "before", "anything", "happens", "for", "that", "batch", ".", "Args", ":", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "validation", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "\"", "\"", "\""], "docstring": "Called in the validation loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader", "docstring_tokens": ["called", "in", "the", "validation", "loop", "before", "anything", "happens", "for", "that", "batch", "args", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "validation", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader"], "docstring_summary": "Called in the validation loop before anything happens for that batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 92, "end_line": 100, "hash": "f3e45a175ce39b90ef9ba3424c47ec81", "complexity": 1, "parameters": ["batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_validation_batch_end", "original_string": "def on_validation_batch_end(\r\n        self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0\r\n    ) -> None:\r\n        \"\"\"Called in the validation loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of validation_step(x)\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_validation_batch_end(\r\n        self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0\r\n    ) -> None:\r\n        \"\"\"Called in the validation loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of validation_step(x)\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_validation_batch_end", "(", "self", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "validation", "loop", "after", "the", "batch", ".", "Args", ":", "outputs", ":", "The", "outputs", "of", "validation_step", "(", "x", ")", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "validation", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "\"", "\"", "\""], "docstring": "Called in the validation loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of validation_step(x)\r\n            batch: The batched data as it is returned by the validation DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader", "docstring_tokens": ["called", "in", "the", "validation", "loop", "after", "the", "batch", "args", "outputs", "the", "outputs", "of", "validation_step", "x", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "validation", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader"], "docstring_summary": "Called in the validation loop after the batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 102, "end_line": 113, "hash": "d4d2ea264c6ca7b5062cd410f81d20be", "complexity": 1, "parameters": ["outputs", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_test_batch_start", "original_string": "def on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_test_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "test", "loop", "before", "anything", "happens", "for", "that", "batch", ".", "Args", ":", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "test", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "\"", "\"", "\""], "docstring": "Called in the test loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader", "docstring_tokens": ["called", "in", "the", "test", "loop", "before", "anything", "happens", "for", "that", "batch", "args", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "test", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader"], "docstring_summary": "Called in the test loop before anything happens for that batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 115, "end_line": 123, "hash": "63d9727569ca6aa67958ed15cff5d857", "complexity": 1, "parameters": ["batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_test_batch_end", "original_string": "def on_test_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of test_step(x)\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_test_batch_end(self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the test loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of test_step(x)\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_test_batch_end", "(", "self", ",", "outputs", ":", "STEP_OUTPUT", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "test", "loop", "after", "the", "batch", ".", "Args", ":", "outputs", ":", "The", "outputs", "of", "test_step", "(", "x", ")", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "test", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "\"", "\"", "\""], "docstring": "Called in the test loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of test_step(x)\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader", "docstring_tokens": ["called", "in", "the", "test", "loop", "after", "the", "batch", "args", "outputs", "the", "outputs", "of", "test_step", "x", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "test", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader"], "docstring_summary": "Called in the test loop after the batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 125, "end_line": 134, "hash": "6e8dc41e2a15ee64ca2b83f6c207480e", "complexity": 1, "parameters": ["outputs", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_predict_batch_start", "original_string": "def on_predict_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_predict_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_predict_batch_start", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "predict", "loop", "before", "anything", "happens", "for", "that", "batch", ".", "Args", ":", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "test", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "\"", "\"", "\""], "docstring": "Called in the predict loop before anything happens for that batch.\r\n\r\n        Args:\r\n            batch: The batched data as it is returned by the test DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader", "docstring_tokens": ["called", "in", "the", "predict", "loop", "before", "anything", "happens", "for", "that", "batch", "args", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "test", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader"], "docstring_summary": "Called in the predict loop before anything happens for that batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 136, "end_line": 144, "hash": "b2238b876d2ddf2e3a8700c69e0f7d0a", "complexity": 1, "parameters": ["batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_predict_batch_end", "original_string": "def on_predict_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of predict_step(x)\r\n            batch: The batched data as it is returned by the prediction DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "language": "python", "code": "def on_predict_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None:\r\n        \"\"\"Called in the predict loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of predict_step(x)\r\n            batch: The batched data as it is returned by the prediction DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_predict_batch_end", "(", "self", ",", "outputs", ":", "Optional", "[", "Any", "]", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "predict", "loop", "after", "the", "batch", ".", "Args", ":", "outputs", ":", "The", "outputs", "of", "predict_step", "(", "x", ")", "batch", ":", "The", "batched", "data", "as", "it", "is", "returned", "by", "the", "prediction", "DataLoader", ".", "batch_idx", ":", "the", "index", "of", "the", "batch", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "\"", "\"", "\""], "docstring": "Called in the predict loop after the batch.\r\n\r\n        Args:\r\n            outputs: The outputs of predict_step(x)\r\n            batch: The batched data as it is returned by the prediction DataLoader.\r\n            batch_idx: the index of the batch\r\n            dataloader_idx: the index of the dataloader", "docstring_tokens": ["called", "in", "the", "predict", "loop", "after", "the", "batch", "args", "outputs", "the", "outputs", "of", "predict_step", "x", "batch", "the", "batched", "data", "as", "it", "is", "returned", "by", "the", "prediction", "dataloader", "batch_idx", "the", "index", "of", "the", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader"], "docstring_summary": "Called in the predict loop after the batch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 146, "end_line": 155, "hash": "3eb63a936fd61618f55f2547f1ad2486", "complexity": 1, "parameters": ["outputs", "batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_validation_model_eval", "original_string": "def on_validation_model_eval(self) -> None:\r\n        \"\"\"Called when the validation loop starts.\r\n\r\n        The validation loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "language": "python", "code": "def on_validation_model_eval(self) -> None:\r\n        \"\"\"Called when the validation loop starts.\r\n\r\n        The validation loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "code_tokens": ["def", "on_validation_model_eval", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "validation", "loop", "starts", ".", "The", "validation", "loop", "by", "default", "calls", "`", "`", ".", "eval", "(", ")", "`", "`", "on", "the", "LightningModule", "before", "it", "starts", ".", "Override", "this", "hook", "to", "change", "the", "behavior", ".", "See", "also", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "on_validation_model_train", "`", ".", "\"", "\"", "\"", "self", ".", "trainer", ".", "model", ".", "eval", "(", ")"], "docstring": "Called when the validation loop starts.\r\n\r\n        The validation loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_train`.", "docstring_tokens": ["called", "when", "the", "validation", "loop", "starts", "the", "validation", "loop", "by", "default", "calls", "eval", "on", "the", "lightningmodule", "before", "it", "starts", "override", "this", "hook", "to", "change", "the", "behavior", "see", "also", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "on_validation_model_train"], "docstring_summary": "Called when the validation loop starts.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 161, "end_line": 168, "hash": "815a456fe1f5f74d5adb4ac381a9ee7d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_validation_model_train", "original_string": "def on_validation_model_train(self) -> None:\r\n        \"\"\"Called when the validation loop ends.\r\n\r\n        The validation loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting validation. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_eval`.\r\n\r\n        \"\"\"\r\n        # The loop won't call this hook unless it is overridden. The line below is here in case the user calls super().\r\n        self.trainer.model.train()", "language": "python", "code": "def on_validation_model_train(self) -> None:\r\n        \"\"\"Called when the validation loop ends.\r\n\r\n        The validation loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting validation. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_eval`.\r\n\r\n        \"\"\"\r\n        # The loop won't call this hook unless it is overridden. The line below is here in case the user calls super().\r\n        self.trainer.model.train()", "code_tokens": ["def", "on_validation_model_train", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "validation", "loop", "ends", ".", "The", "validation", "loop", "by", "default", "restores", "the", "`", "training", "`", "mode", "of", "the", "LightningModule", "to", "what", "it", "was", "before", "starting", "validation", ".", "Override", "this", "hook", "to", "change", "the", "behavior", ".", "See", "also", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "on_validation_model_eval", "`", ".", "\"", "\"", "\"", "self", ".", "trainer", ".", "model", ".", "train", "(", ")"], "docstring": "Called when the validation loop ends.\r\n\r\n        The validation loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting validation. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_eval`.", "docstring_tokens": ["called", "when", "the", "validation", "loop", "ends", "the", "validation", "loop", "by", "default", "restores", "the", "training", "mode", "of", "the", "lightningmodule", "to", "what", "it", "was", "before", "starting", "validation", "override", "this", "hook", "to", "change", "the", "behavior", "see", "also", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "on_validation_model_eval"], "docstring_summary": "Called when the validation loop ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 170, "end_line": 179, "hash": "832948dd9c60ba5f4fbfa444a2b00083", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_test_model_eval", "original_string": "def on_test_model_eval(self) -> None:\r\n        \"\"\"Called when the test loop starts.\r\n\r\n        The test loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "language": "python", "code": "def on_test_model_eval(self) -> None:\r\n        \"\"\"Called when the test loop starts.\r\n\r\n        The test loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_train`.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "code_tokens": ["def", "on_test_model_eval", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "test", "loop", "starts", ".", "The", "test", "loop", "by", "default", "calls", "`", "`", ".", "eval", "(", ")", "`", "`", "on", "the", "LightningModule", "before", "it", "starts", ".", "Override", "this", "hook", "to", "change", "the", "behavior", ".", "See", "also", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "on_test_model_train", "`", ".", "\"", "\"", "\"", "self", ".", "trainer", ".", "model", ".", "eval", "(", ")"], "docstring": "Called when the test loop starts.\r\n\r\n        The test loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_train`.", "docstring_tokens": ["called", "when", "the", "test", "loop", "starts", "the", "test", "loop", "by", "default", "calls", "eval", "on", "the", "lightningmodule", "before", "it", "starts", "override", "this", "hook", "to", "change", "the", "behavior", "see", "also", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "on_test_model_train"], "docstring_summary": "Called when the test loop starts.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 181, "end_line": 188, "hash": "11d26134716daf7c16adb81edf4c776d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_test_model_train", "original_string": "def on_test_model_train(self) -> None:\r\n        \"\"\"Called when the test loop ends.\r\n\r\n        The test loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting testing. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_eval`.\r\n\r\n        \"\"\"\r\n        # The loop won't call this hook unless it is overridden. The line below is here in case the user calls super().\r\n        self.trainer.model.train()", "language": "python", "code": "def on_test_model_train(self) -> None:\r\n        \"\"\"Called when the test loop ends.\r\n\r\n        The test loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting testing. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_eval`.\r\n\r\n        \"\"\"\r\n        # The loop won't call this hook unless it is overridden. The line below is here in case the user calls super().\r\n        self.trainer.model.train()", "code_tokens": ["def", "on_test_model_train", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "test", "loop", "ends", ".", "The", "test", "loop", "by", "default", "restores", "the", "`", "training", "`", "mode", "of", "the", "LightningModule", "to", "what", "it", "was", "before", "starting", "testing", ".", "Override", "this", "hook", "to", "change", "the", "behavior", ".", "See", "also", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "on_test_model_eval", "`", ".", "\"", "\"", "\"", "self", ".", "trainer", ".", "model", ".", "train", "(", ")"], "docstring": "Called when the test loop ends.\r\n\r\n        The test loop by default restores the `training` mode of the LightningModule to what it was before\r\n        starting testing. Override this hook to change the behavior. See also\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_eval`.", "docstring_tokens": ["called", "when", "the", "test", "loop", "ends", "the", "test", "loop", "by", "default", "restores", "the", "training", "mode", "of", "the", "lightningmodule", "to", "what", "it", "was", "before", "starting", "testing", "override", "this", "hook", "to", "change", "the", "behavior", "see", "also", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "on_test_model_eval"], "docstring_summary": "Called when the test loop ends.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 190, "end_line": 199, "hash": "c87b07cd2c1fc1b190748feca551e5dc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_predict_model_eval", "original_string": "def on_predict_model_eval(self) -> None:\r\n        \"\"\"Called when the predict loop starts.\r\n\r\n        The predict loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "language": "python", "code": "def on_predict_model_eval(self) -> None:\r\n        \"\"\"Called when the predict loop starts.\r\n\r\n        The predict loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior.\r\n\r\n        \"\"\"\r\n        self.trainer.model.eval()", "code_tokens": ["def", "on_predict_model_eval", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "when", "the", "predict", "loop", "starts", ".", "The", "predict", "loop", "by", "default", "calls", "`", "`", ".", "eval", "(", ")", "`", "`", "on", "the", "LightningModule", "before", "it", "starts", ".", "Override", "this", "hook", "to", "change", "the", "behavior", ".", "\"", "\"", "\"", "self", ".", "trainer", ".", "model", ".", "eval", "(", ")"], "docstring": "Called when the predict loop starts.\r\n\r\n        The predict loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\r\n        to change the behavior.", "docstring_tokens": ["called", "when", "the", "predict", "loop", "starts", "the", "predict", "loop", "by", "default", "calls", "eval", "on", "the", "lightningmodule", "before", "it", "starts", "override", "this", "hook", "to", "change", "the", "behavior"], "docstring_summary": "Called when the predict loop starts.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 201, "end_line": 208, "hash": "6c4bdc02dd07b2b106dbd2e33247bb5d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_train_epoch_end", "original_string": "def on_train_epoch_end(self) -> None:\r\n        \"\"\"Called in the training loop at the very end of the epoch.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`~lightning.pytorch.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n                def on_train_epoch_end(self):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(self.training_step_outputs).mean()\r\n                    self.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    self.training_step_outputs.clear()\r\n\r\n        \"\"\"", "language": "python", "code": "def on_train_epoch_end(self) -> None:\r\n        \"\"\"Called in the training loop at the very end of the epoch.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`~lightning.pytorch.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n                def on_train_epoch_end(self):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(self.training_step_outputs).mean()\r\n                    self.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    self.training_step_outputs.clear()\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_train_epoch_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "in", "the", "training", "loop", "at", "the", "very", "end", "of", "the", "epoch", ".", "To", "access", "all", "batch", "outputs", "at", "the", "end", "of", "the", "epoch", ",", "you", "can", "cache", "step", "outputs", "as", "an", "attribute", "of", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "LightningModule", "`", "and", "access", "them", "in", "this", "hook", ":", ".", ".", "code", "-", "block", ":", ":", "python", "class", "MyLightningModule", "(", "L", ".", "LightningModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "training_step_outputs", "=", "[", "]", "def", "training_step", "(", "self", ")", ":", "loss", "=", ".", ".", ".", "self", ".", "training_step_outputs", ".", "append", "(", "loss", ")", "return", "loss", "def", "on_train_epoch_end", "(", "self", ")", ":", "epoch_mean", "=", "torch", ".", "stack", "(", "self", ".", "training_step_outputs", ")", ".", "mean", "(", ")", "self", ".", "log", "(", "\"", "training_epoch_mean", "\"", ",", "epoch_mean", ")", "self", ".", "training_step_outputs", ".", "clear", "(", ")", "\"", "\"", "\""], "docstring": "Called in the training loop at the very end of the epoch.\r\n\r\n        To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\r\n        :class:`~lightning.pytorch.LightningModule` and access them in this hook:\r\n\r\n        .. code-block:: python\r\n\r\n            class MyLightningModule(L.LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.training_step_outputs = []\r\n\r\n                def training_step(self):\r\n                    loss = ...\r\n                    self.training_step_outputs.append(loss)\r\n                    return loss\r\n\r\n                def on_train_epoch_end(self):\r\n                    # do something with all training_step outputs, for example:\r\n                    epoch_mean = torch.stack(self.training_step_outputs).mean()\r\n                    self.log(\"training_epoch_mean\", epoch_mean)\r\n                    # free up the memory\r\n                    self.training_step_outputs.clear()", "docstring_tokens": ["called", "in", "the", "training", "loop", "at", "the", "very", "end", "of", "the", "epoch", "to", "access", "all", "batch", "outputs", "at", "the", "end", "of", "the", "epoch", "you", "can", "cache", "step", "outputs", "as", "an", "attribute", "of", "the", "class", "lightning", "pytorch", "lightningmodule", "and", "access", "them", "in", "this", "hook", "code", "block", "python", "class", "mylightningmodule", "l", "lightningmodule", "def", "__init__", "self", "super", "__init__", "self", "training_step_outputs", "def", "training_step", "self", "loss", "self", "training_step_outputs", "append", "loss", "return", "loss", "def", "on_train_epoch_end", "self", "do", "something", "with", "all", "training_step", "outputs", "for", "example", "epoch_mean", "torch", "stack", "self", "training_step_outputs", "mean", "self", "log", "training_epoch_mean", "epoch_mean", "free", "up", "the", "memory", "self", "training_step_outputs", "clear"], "docstring_summary": "Called in the training loop at the very end of the epoch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 213, "end_line": 238, "hash": "a3a58a30b4a6940a6eb3eb165866366a", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_before_zero_grad", "original_string": "def on_before_zero_grad(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called after ``training_step()`` and before ``optimizer.zero_grad()``.\r\n\r\n        Called in the training loop after taking an optimizer step and before zeroing grads.\r\n        Good place to inspect weight information with weights updated.\r\n\r\n        This is where it is called::\r\n\r\n            for optimizer in optimizers:\r\n                out = training_step(...)\r\n\r\n                model.on_before_zero_grad(optimizer) # < ---- called here\r\n                optimizer.zero_grad()\r\n\r\n                backward()\r\n\r\n        Args:\r\n            optimizer: The optimizer for which grads should be zeroed.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_before_zero_grad(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called after ``training_step()`` and before ``optimizer.zero_grad()``.\r\n\r\n        Called in the training loop after taking an optimizer step and before zeroing grads.\r\n        Good place to inspect weight information with weights updated.\r\n\r\n        This is where it is called::\r\n\r\n            for optimizer in optimizers:\r\n                out = training_step(...)\r\n\r\n                model.on_before_zero_grad(optimizer) # < ---- called here\r\n                optimizer.zero_grad()\r\n\r\n                backward()\r\n\r\n        Args:\r\n            optimizer: The optimizer for which grads should be zeroed.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_before_zero_grad", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "after", "`", "`", "training_step", "(", ")", "`", "`", "and", "before", "`", "`", "optimizer", ".", "zero_grad", "(", ")", "`", "`", ".", "Called", "in", "the", "training", "loop", "after", "taking", "an", "optimizer", "step", "and", "before", "zeroing", "grads", ".", "Good", "place", "to", "inspect", "weight", "information", "with", "weights", "updated", ".", "This", "is", "where", "it", "is", "called", ":", ":", "for", "optimizer", "in", "optimizers", ":", "out", "=", "training_step", "(", ".", ".", ".", ")", "model", ".", "on_before_zero_grad", "(", "optimizer", ")", "optimizer", ".", "zero_grad", "(", ")", "backward", "(", ")", "Args", ":", "optimizer", ":", "The", "optimizer", "for", "which", "grads", "should", "be", "zeroed", ".", "\"", "\"", "\""], "docstring": "Called after ``training_step()`` and before ``optimizer.zero_grad()``.\r\n\r\n        Called in the training loop after taking an optimizer step and before zeroing grads.\r\n        Good place to inspect weight information with weights updated.\r\n\r\n        This is where it is called::\r\n\r\n            for optimizer in optimizers:\r\n                out = training_step(...)\r\n\r\n                model.on_before_zero_grad(optimizer) # < ---- called here\r\n                optimizer.zero_grad()\r\n\r\n                backward()\r\n\r\n        Args:\r\n            optimizer: The optimizer for which grads should be zeroed.", "docstring_tokens": ["called", "after", "training_step", "and", "before", "optimizer", "zero_grad", "called", "in", "the", "training", "loop", "after", "taking", "an", "optimizer", "step", "and", "before", "zeroing", "grads", "good", "place", "to", "inspect", "weight", "information", "with", "weights", "updated", "this", "is", "where", "it", "is", "called", "for", "optimizer", "in", "optimizers", "out", "training_step", "model", "on_before_zero_grad", "optimizer", "called", "here", "optimizer", "zero_grad", "backward", "args", "optimizer", "the", "optimizer", "for", "which", "grads", "should", "be", "zeroed"], "docstring_summary": "Called after ``training_step()`` and before ``optimizer.zero_grad()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 258, "end_line": 277, "hash": "b35efbd5fe998bca03e06ebdfd309611", "complexity": 1, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_before_backward", "original_string": "def on_before_backward(self, loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\r\n\r\n        Args:\r\n            loss: Loss divided by number of batches for gradient accumulation and scaled if using AMP.\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def on_before_backward(self, loss: Tensor) -> None:\r\n        \"\"\"Called before ``loss.backward()``.\r\n\r\n        Args:\r\n            loss: Loss divided by number of batches for gradient accumulation and scaled if using AMP.\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "on_before_backward", "(", "self", ",", "loss", ":", "Tensor", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "before", "`", "`", "loss", ".", "backward", "(", ")", "`", "`", ".", "Args", ":", "loss", ":", "Loss", "divided", "by", "number", "of", "batches", "for", "gradient", "accumulation", "and", "scaled", "if", "using", "AMP", ".", "\"", "\"", "\"", "pass"], "docstring": "Called before ``loss.backward()``.\r\n\r\n        Args:\r\n            loss: Loss divided by number of batches for gradient accumulation and scaled if using AMP.", "docstring_tokens": ["called", "before", "loss", "backward", "args", "loss", "loss", "divided", "by", "number", "of", "batches", "for", "gradient", "accumulation", "and", "scaled", "if", "using", "amp"], "docstring_summary": "Called before ``loss.backward()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 279, "end_line": 286, "hash": "f2e1aa91eb4114126be6721737ed63b7", "complexity": 1, "parameters": ["loss"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_after_backward", "original_string": "def on_after_backward(self) -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\r\n\r\n        Note:\r\n            If using native AMP, the gradients will not be unscaled at this point.\r\n            Use the ``on_before_optimizer_step`` if you need the unscaled gradients.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_after_backward(self) -> None:\r\n        \"\"\"Called after ``loss.backward()`` and before optimizers are stepped.\r\n\r\n        Note:\r\n            If using native AMP, the gradients will not be unscaled at this point.\r\n            Use the ``on_before_optimizer_step`` if you need the unscaled gradients.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_after_backward", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "after", "`", "`", "loss", ".", "backward", "(", ")", "`", "`", "and", "before", "optimizers", "are", "stepped", ".", "Note", ":", "If", "using", "native", "AMP", ",", "the", "gradients", "will", "not", "be", "unscaled", "at", "this", "point", ".", "Use", "the", "`", "`", "on_before_optimizer_step", "`", "`", "if", "you", "need", "the", "unscaled", "gradients", ".", "\"", "\"", "\""], "docstring": "Called after ``loss.backward()`` and before optimizers are stepped.\r\n\r\n        Note:\r\n            If using native AMP, the gradients will not be unscaled at this point.\r\n            Use the ``on_before_optimizer_step`` if you need the unscaled gradients.", "docstring_tokens": ["called", "after", "loss", "backward", "and", "before", "optimizers", "are", "stepped", "note", "if", "using", "native", "amp", "the", "gradients", "will", "not", "be", "unscaled", "at", "this", "point", "use", "the", "on_before_optimizer_step", "if", "you", "need", "the", "unscaled", "gradients"], "docstring_summary": "Called after ``loss.backward()`` and before optimizers are stepped.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 288, "end_line": 295, "hash": "c99f00591914254dfb80cf58cd2f30c7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_before_optimizer_step", "original_string": "def on_before_optimizer_step(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\r\n\r\n        If using gradient accumulation, the hook is called once the gradients have been accumulated.\r\n        See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`.\r\n\r\n        If using AMP, the loss will be unscaled before calling this hook.\r\n        See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__\r\n        for more information on the scaling of gradients.\r\n\r\n        If clipping gradients, the gradients will not have been clipped yet.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n\r\n        Example::\r\n\r\n            def on_before_optimizer_step(self, optimizer):\r\n                # example to inspect gradient information in tensorboard\r\n                if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\r\n                    for k, v in self.named_parameters():\r\n                        self.logger.experiment.add_histogram(\r\n                            tag=k, values=v.grad, global_step=self.trainer.global_step\r\n                        )\r\n\r\n        \"\"\"", "language": "python", "code": "def on_before_optimizer_step(self, optimizer: Optimizer) -> None:\r\n        \"\"\"Called before ``optimizer.step()``.\r\n\r\n        If using gradient accumulation, the hook is called once the gradients have been accumulated.\r\n        See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`.\r\n\r\n        If using AMP, the loss will be unscaled before calling this hook.\r\n        See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__\r\n        for more information on the scaling of gradients.\r\n\r\n        If clipping gradients, the gradients will not have been clipped yet.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n\r\n        Example::\r\n\r\n            def on_before_optimizer_step(self, optimizer):\r\n                # example to inspect gradient information in tensorboard\r\n                if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\r\n                    for k, v in self.named_parameters():\r\n                        self.logger.experiment.add_histogram(\r\n                            tag=k, values=v.grad, global_step=self.trainer.global_step\r\n                        )\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_before_optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "before", "`", "`", "optimizer", ".", "step", "(", ")", "`", "`", ".", "If", "using", "gradient", "accumulation", ",", "the", "hook", "is", "called", "once", "the", "gradients", "have", "been", "accumulated", ".", "See", ":", ":", "paramref", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "accumulate_grad_batches", "`", ".", "If", "using", "AMP", ",", "the", "loss", "will", "be", "unscaled", "before", "calling", "this", "hook", ".", "See", "these", "`", "docs", "<", "https", ":", "/", "/", "pytorch", ".", "org", "/", "docs", "/", "stable", "/", "notes", "/", "amp_examples", ".", "html", "for", "more", "information", "on", "the", "scaling", "of", "gradients", ".", "If", "clipping", "gradients", ",", "the", "gradients", "will", "not", "have", "been", "clipped", "yet", ".", "Args", ":", "optimizer", ":", "Current", "optimizer", "being", "used", ".", "Example", ":", ":", "def", "on_before_optimizer_step", "(", "self", ",", "optimizer", ")", ":", "if", "self", ".", "trainer", ".", "global_step", "%", "25", "=", "=", "0", ":", "for", "k", ",", "v", "in", "self", ".", "named_parameters", "(", ")", ":", "self", ".", "logger", ".", "experiment", ".", "add_histogram", "(", "tag", "=", "k", ",", "values", "=", "v", ".", "grad", ",", "global_step", "=", "self", ".", "trainer", ".", "global_step", ")", "\"", "\"", "\""], "docstring": "Called before ``optimizer.step()``.\r\n\r\n        If using gradient accumulation, the hook is called once the gradients have been accumulated.\r\n        See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`.\r\n\r\n        If using AMP, the loss will be unscaled before calling this hook.\r\n        See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__\r\n        for more information on the scaling of gradients.\r\n\r\n        If clipping gradients, the gradients will not have been clipped yet.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n\r\n        Example::\r\n\r\n            def on_before_optimizer_step(self, optimizer):\r\n                # example to inspect gradient information in tensorboard\r\n                if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\r\n                    for k, v in self.named_parameters():\r\n                        self.logger.experiment.add_histogram(\r\n                            tag=k, values=v.grad, global_step=self.trainer.global_step\r\n                        )", "docstring_tokens": ["called", "before", "optimizer", "step", "if", "using", "gradient", "accumulation", "the", "hook", "is", "called", "once", "the", "gradients", "have", "been", "accumulated", "see", "paramref", "lightning", "pytorch", "trainer", "trainer", "trainer", "accumulate_grad_batches", "if", "using", "amp", "the", "loss", "will", "be", "unscaled", "before", "calling", "this", "hook", "see", "these", "docs", "https", "pytorch", "org", "docs", "stable", "notes", "amp_examples", "html", "working", "with", "unscaled", "gradients", "__", "for", "more", "information", "on", "the", "scaling", "of", "gradients", "if", "clipping", "gradients", "the", "gradients", "will", "not", "have", "been", "clipped", "yet", "args", "optimizer", "current", "optimizer", "being", "used", "example", "def", "on_before_optimizer_step", "self", "optimizer", "example", "to", "inspect", "gradient", "information", "in", "tensorboard", "if", "self", "trainer", "global_step", "25", "0", "don", "t", "make", "the", "tf", "file", "huge", "for", "k", "v", "in", "self", "named_parameters", "self", "logger", "experiment", "add_histogram", "tag", "k", "values", "v", "grad", "global_step", "self", "trainer", "global_step"], "docstring_summary": "Called before ``optimizer.step()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 297, "end_line": 322, "hash": "6c5db8b82d9ec6786c25598a932e37fb", "complexity": 1, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "configure_sharded_model", "original_string": "def configure_sharded_model(self) -> None:\r\n        \"\"\"Deprecated.\r\n\r\n        Use :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` instead.\r\n\r\n        \"\"\"", "language": "python", "code": "def configure_sharded_model(self) -> None:\r\n        \"\"\"Deprecated.\r\n\r\n        Use :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` instead.\r\n\r\n        \"\"\"", "code_tokens": ["def", "configure_sharded_model", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Deprecated", ".", "Use", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "configure_model", "`", "instead", ".", "\"", "\"", "\""], "docstring": "Deprecated.\r\n\r\n        Use :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` instead.", "docstring_tokens": ["deprecated", "use", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "configure_model", "instead"], "docstring_summary": "Deprecated.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 324, "end_line": 329, "hash": "87096a1f31c45328ef67a316c985ccc7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "configure_model", "original_string": "def configure_model(self) -> None:\r\n        \"\"\"Hook to create modules in a strategy and precision aware context.\r\n\r\n        This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where we'd like to shard\r\n        the model instantly to save memory and initialization time.\r\n        For non-sharded strategies, you can choose to override this hook or to initialize your model under the\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager.\r\n\r\n        This hook is called during each of fit/val/test/predict stages in the same process, so ensure that\r\n        implementation of this hook is **idempotent**, i.e., after the first time the hook is called, subsequent calls\r\n        to it should be a no-op.\r\n\r\n        \"\"\"", "language": "python", "code": "def configure_model(self) -> None:\r\n        \"\"\"Hook to create modules in a strategy and precision aware context.\r\n\r\n        This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where we'd like to shard\r\n        the model instantly to save memory and initialization time.\r\n        For non-sharded strategies, you can choose to override this hook or to initialize your model under the\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager.\r\n\r\n        This hook is called during each of fit/val/test/predict stages in the same process, so ensure that\r\n        implementation of this hook is **idempotent**, i.e., after the first time the hook is called, subsequent calls\r\n        to it should be a no-op.\r\n\r\n        \"\"\"", "code_tokens": ["def", "configure_model", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Hook", "to", "create", "modules", "in", "a", "strategy", "and", "precision", "aware", "context", ".", "This", "is", "particularly", "useful", "for", "when", "using", "sharded", "strategies", "(", "FSDP", "and", "DeepSpeed", ")", ",", "where", "we", "'", "d", "like", "to", "shard", "the", "model", "instantly", "to", "save", "memory", "and", "initialization", "time", ".", "For", "non", "-", "sharded", "strategies", ",", "you", "can", "choose", "to", "override", "this", "hook", "or", "to", "initialize", "your", "model", "under", "the", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "init_module", "`", "context", "manager", ".", "This", "hook", "is", "called", "during", "each", "of", "fit", "/", "val", "/", "test", "/", "predict", "stages", "in", "the", "same", "process", ",", "so", "ensure", "that", "implementation", "of", "this", "hook", "is", "*", "*", "idempotent", "*", "*", ",", "i", ".", "e", ".", ",", "after", "the", "first", "time", "the", "hook", "is", "called", ",", "subsequent", "calls", "to", "it", "should", "be", "a", "no", "-", "op", ".", "\"", "\"", "\""], "docstring": "Hook to create modules in a strategy and precision aware context.\r\n\r\n        This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where we'd like to shard\r\n        the model instantly to save memory and initialization time.\r\n        For non-sharded strategies, you can choose to override this hook or to initialize your model under the\r\n        :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager.\r\n\r\n        This hook is called during each of fit/val/test/predict stages in the same process, so ensure that\r\n        implementation of this hook is **idempotent**, i.e., after the first time the hook is called, subsequent calls\r\n        to it should be a no-op.", "docstring_tokens": ["hook", "to", "create", "modules", "in", "a", "strategy", "and", "precision", "aware", "context", "this", "is", "particularly", "useful", "for", "when", "using", "sharded", "strategies", "fsdp", "and", "deepspeed", "where", "we", "d", "like", "to", "shard", "the", "model", "instantly", "to", "save", "memory", "and", "initialization", "time", "for", "non", "sharded", "strategies", "you", "can", "choose", "to", "override", "this", "hook", "or", "to", "initialize", "your", "model", "under", "the", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "init_module", "context", "manager", "this", "hook", "is", "called", "during", "each", "of", "fit", "val", "test", "predict", "stages", "in", "the", "same", "process", "so", "ensure", "that", "implementation", "of", "this", "hook", "is", "idempotent", "i", "e", "after", "the", "first", "time", "the", "hook", "is", "called", "subsequent", "calls", "to", "it", "should", "be", "a", "no", "op"], "docstring_summary": "Hook to create modules in a strategy and precision aware context.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "ModelHooks", "start_line": 331, "end_line": 343, "hash": "1dd66b6ec14b3b5a8d4684bd904421a9", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "__init__", "original_string": "def __init__(self) -> None:\r\n        \"\"\"\r\n        Attributes:\r\n            prepare_data_per_node:\r\n                If True, each LOCAL_RANK=0 will call prepare data.\r\n                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\r\n            allow_zero_length_dataloader_with_multiple_devices:\r\n                If True, dataloader with zero length within local rank is allowed.\r\n                Default value is False.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.prepare_data_per_node: bool = True\r\n        self.allow_zero_length_dataloader_with_multiple_devices: bool = False", "language": "python", "code": "def __init__(self) -> None:\r\n        \"\"\"\r\n        Attributes:\r\n            prepare_data_per_node:\r\n                If True, each LOCAL_RANK=0 will call prepare data.\r\n                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\r\n            allow_zero_length_dataloader_with_multiple_devices:\r\n                If True, dataloader with zero length within local rank is allowed.\r\n                Default value is False.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.prepare_data_per_node: bool = True\r\n        self.allow_zero_length_dataloader_with_multiple_devices: bool = False", "code_tokens": ["def", "__init__", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Attributes", ":", "prepare_data_per_node", ":", "If", "True", ",", "each", "LOCAL_RANK", "=", "0", "will", "call", "prepare", "data", ".", "Otherwise", "only", "NODE_RANK", "=", "0", ",", "LOCAL_RANK", "=", "0", "will", "prepare", "data", ".", "allow_zero_length_dataloader_with_multiple_devices", ":", "If", "True", ",", "dataloader", "with", "zero", "length", "within", "local", "rank", "is", "allowed", ".", "Default", "value", "is", "False", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "prepare_data_per_node", ":", "bool", "=", "True", "self", ".", "allow_zero_length_dataloader_with_multiple_devices", ":", "bool", "=", "False"], "docstring": "Attributes:\r\n            prepare_data_per_node:\r\n                If True, each LOCAL_RANK=0 will call prepare data.\r\n                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\r\n            allow_zero_length_dataloader_with_multiple_devices:\r\n                If True, dataloader with zero length within local rank is allowed.\r\n                Default value is False.", "docstring_tokens": ["attributes", "prepare_data_per_node", "if", "true", "each", "local_rank", "0", "will", "call", "prepare", "data", "otherwise", "only", "node_rank", "0", "local_rank", "0", "will", "prepare", "data", "allow_zero_length_dataloader_with_multiple_devices", "if", "true", "dataloader", "with", "zero", "length", "within", "local", "rank", "is", "allowed", "default", "value", "is", "false"], "docstring_summary": "Attributes:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 349, "end_line": 361, "hash": "654c0a4012120f11a2bbb8bec8b86b9d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "prepare_data", "original_string": "def prepare_data(self) -> None:\r\n        \"\"\"Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\r\n        settings) will result in corrupted data. Lightning ensures this method is called only within a single process,\r\n        so you can safely add your downloading logic within.\r\n\r\n        .. warning:: DO NOT set state to the model (use ``setup`` instead)\r\n            since this is NOT called on every device\r\n\r\n        Example::\r\n\r\n            def prepare_data(self):\r\n                # good\r\n                download_data()\r\n                tokenize()\r\n                etc()\r\n\r\n                # bad\r\n                self.split = data_split\r\n                self.some_state = some_other_state()\r\n\r\n        In a distributed environment, ``prepare_data`` can be called in two ways\r\n        (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)\r\n\r\n        1. Once per node. This is the default and is only called on LOCAL_RANK=0.\r\n        2. Once in total. Only called on GLOBAL_RANK=0.\r\n\r\n        Example::\r\n\r\n            # DEFAULT\r\n            # called once per node on LOCAL_RANK=0 of that node\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = True\r\n\r\n\r\n            # call on GLOBAL_RANK=0 (great for shared file systems)\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = False\r\n\r\n        This is called before requesting the dataloaders:\r\n\r\n        .. code-block:: python\r\n\r\n            model.prepare_data()\r\n            initialize_distributed()\r\n            model.setup(stage)\r\n            model.train_dataloader()\r\n            model.val_dataloader()\r\n            model.test_dataloader()\r\n            model.predict_dataloader()\r\n\r\n        \"\"\"", "language": "python", "code": "def prepare_data(self) -> None:\r\n        \"\"\"Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\r\n        settings) will result in corrupted data. Lightning ensures this method is called only within a single process,\r\n        so you can safely add your downloading logic within.\r\n\r\n        .. warning:: DO NOT set state to the model (use ``setup`` instead)\r\n            since this is NOT called on every device\r\n\r\n        Example::\r\n\r\n            def prepare_data(self):\r\n                # good\r\n                download_data()\r\n                tokenize()\r\n                etc()\r\n\r\n                # bad\r\n                self.split = data_split\r\n                self.some_state = some_other_state()\r\n\r\n        In a distributed environment, ``prepare_data`` can be called in two ways\r\n        (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)\r\n\r\n        1. Once per node. This is the default and is only called on LOCAL_RANK=0.\r\n        2. Once in total. Only called on GLOBAL_RANK=0.\r\n\r\n        Example::\r\n\r\n            # DEFAULT\r\n            # called once per node on LOCAL_RANK=0 of that node\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = True\r\n\r\n\r\n            # call on GLOBAL_RANK=0 (great for shared file systems)\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = False\r\n\r\n        This is called before requesting the dataloaders:\r\n\r\n        .. code-block:: python\r\n\r\n            model.prepare_data()\r\n            initialize_distributed()\r\n            model.setup(stage)\r\n            model.train_dataloader()\r\n            model.val_dataloader()\r\n            model.test_dataloader()\r\n            model.predict_dataloader()\r\n\r\n        \"\"\"", "code_tokens": ["def", "prepare_data", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Use", "this", "to", "download", "and", "prepare", "data", ".", "Downloading", "and", "saving", "data", "with", "multiple", "processes", "(", "distributed", "settings", ")", "will", "result", "in", "corrupted", "data", ".", "Lightning", "ensures", "this", "method", "is", "called", "only", "within", "a", "single", "process", ",", "so", "you", "can", "safely", "add", "your", "downloading", "logic", "within", ".", ".", ".", "warning", ":", ":", "DO", "NOT", "set", "state", "to", "the", "model", "(", "use", "`", "`", "setup", "`", "`", "instead", ")", "since", "this", "is", "NOT", "called", "on", "every", "device", "Example", ":", ":", "def", "prepare_data", "(", "self", ")", ":", "download_data", "(", ")", "tokenize", "(", ")", "etc", "(", ")", "self", ".", "split", "=", "data_split", "self", ".", "some_state", "=", "some_other_state", "(", ")", "In", "a", "distributed", "environment", ",", "`", "`", "prepare_data", "`", "`", "can", "be", "called", "in", "two", "ways", "(", "using", ":", "ref", ":", "`", "prepare_data_per_node", "<", "common", "/", "lightning_module", ":", "prepare_data_per_node", ">", "`", ")", "1", ".", "Once", "per", "node", ".", "This", "is", "the", "default", "and", "is", "only", "called", "on", "LOCAL_RANK", "=", "0", ".", "2", ".", "Once", "in", "total", ".", "Only", "called", "on", "GLOBAL_RANK", "=", "0", ".", "Example", ":", ":", "class", "LitDataModule", "(", "LightningDataModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "prepare_data_per_node", "=", "True", "class", "LitDataModule", "(", "LightningDataModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "prepare_data_per_node", "=", "False", "This", "is", "called", "before", "requesting", "the", "dataloaders", ":", ".", ".", "code", "-", "block", ":", ":", "python", "model", ".", "prepare_data", "(", ")", "initialize_distributed", "(", ")", "model", ".", "setup", "(", "stage", ")", "model", ".", "train_dataloader", "(", ")", "model", ".", "val_dataloader", "(", ")", "model", ".", "test_dataloader", "(", ")", "model", ".", "predict_dataloader", "(", ")", "\"", "\"", "\""], "docstring": "Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\r\n        settings) will result in corrupted data. Lightning ensures this method is called only within a single process,\r\n        so you can safely add your downloading logic within.\r\n\r\n        .. warning:: DO NOT set state to the model (use ``setup`` instead)\r\n            since this is NOT called on every device\r\n\r\n        Example::\r\n\r\n            def prepare_data(self):\r\n                # good\r\n                download_data()\r\n                tokenize()\r\n                etc()\r\n\r\n                # bad\r\n                self.split = data_split\r\n                self.some_state = some_other_state()\r\n\r\n        In a distributed environment, ``prepare_data`` can be called in two ways\r\n        (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)\r\n\r\n        1. Once per node. This is the default and is only called on LOCAL_RANK=0.\r\n        2. Once in total. Only called on GLOBAL_RANK=0.\r\n\r\n        Example::\r\n\r\n            # DEFAULT\r\n            # called once per node on LOCAL_RANK=0 of that node\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = True\r\n\r\n\r\n            # call on GLOBAL_RANK=0 (great for shared file systems)\r\n            class LitDataModule(LightningDataModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.prepare_data_per_node = False\r\n\r\n        This is called before requesting the dataloaders:\r\n\r\n        .. code-block:: python\r\n\r\n            model.prepare_data()\r\n            initialize_distributed()\r\n            model.setup(stage)\r\n            model.train_dataloader()\r\n            model.val_dataloader()\r\n            model.test_dataloader()\r\n            model.predict_dataloader()", "docstring_tokens": ["use", "this", "to", "download", "and", "prepare", "data", "downloading", "and", "saving", "data", "with", "multiple", "processes", "distributed", "settings", "will", "result", "in", "corrupted", "data", "lightning", "ensures", "this", "method", "is", "called", "only", "within", "a", "single", "process", "so", "you", "can", "safely", "add", "your", "downloading", "logic", "within", "warning", "do", "not", "set", "state", "to", "the", "model", "use", "setup", "instead", "since", "this", "is", "not", "called", "on", "every", "device", "example", "def", "prepare_data", "self", "good", "download_data", "tokenize", "etc", "bad", "self", "split", "data_split", "self", "some_state", "some_other_state", "in", "a", "distributed", "environment", "prepare_data", "can", "be", "called", "in", "two", "ways", "using", "ref", "prepare_data_per_node", "common", "lightning_module", "prepare_data_per_node", "1", "once", "per", "node", "this", "is", "the", "default", "and", "is", "only", "called", "on", "local_rank", "0", "2", "once", "in", "total", "only", "called", "on", "global_rank", "0", "example", "default", "called", "once", "per", "node", "on", "local_rank", "0", "of", "that", "node", "class", "litdatamodule", "lightningdatamodule", "def", "__init__", "self", "super", "__init__", "self", "prepare_data_per_node", "true", "call", "on", "global_rank", "0", "great", "for", "shared", "file", "systems", "class", "litdatamodule", "lightningdatamodule", "def", "__init__", "self", "super", "__init__", "self", "prepare_data_per_node", "false", "this", "is", "called", "before", "requesting", "the", "dataloaders", "code", "block", "python", "model", "prepare_data", "initialize_distributed", "model", "setup", "stage", "model", "train_dataloader", "model", "val_dataloader", "model", "test_dataloader", "model", "predict_dataloader"], "docstring_summary": "Use this to download and prepare data. Downloading and saving data with multiple processes (distributed", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 363, "end_line": 417, "hash": "c76862a17e100c43db87ec83c218b8f5", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "setup", "original_string": "def setup(self, stage: str) -> None:\r\n        \"\"\"Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\r\n        need to build models dynamically or adjust something about them. This hook is called on every process when\r\n        using DDP.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        Example::\r\n\r\n            class LitModel(...):\r\n                def __init__(self):\r\n                    self.l1 = None\r\n\r\n                def prepare_data(self):\r\n                    download_data()\r\n                    tokenize()\r\n\r\n                    # don't do this\r\n                    self.something = else\r\n\r\n                def setup(self, stage):\r\n                    data = load_data(...)\r\n                    self.l1 = nn.Linear(28, data.num_classes)\r\n\r\n        \"\"\"", "language": "python", "code": "def setup(self, stage: str) -> None:\r\n        \"\"\"Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\r\n        need to build models dynamically or adjust something about them. This hook is called on every process when\r\n        using DDP.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        Example::\r\n\r\n            class LitModel(...):\r\n                def __init__(self):\r\n                    self.l1 = None\r\n\r\n                def prepare_data(self):\r\n                    download_data()\r\n                    tokenize()\r\n\r\n                    # don't do this\r\n                    self.something = else\r\n\r\n                def setup(self, stage):\r\n                    data = load_data(...)\r\n                    self.l1 = nn.Linear(28, data.num_classes)\r\n\r\n        \"\"\"", "code_tokens": ["def", "setup", "(", "self", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "at", "the", "beginning", "of", "fit", "(", "train", "+", "validate", ")", ",", "validate", ",", "test", ",", "or", "predict", ".", "This", "is", "a", "good", "hook", "when", "you", "need", "to", "build", "models", "dynamically", "or", "adjust", "something", "about", "them", ".", "This", "hook", "is", "called", "on", "every", "process", "when", "using", "DDP", ".", "Args", ":", "stage", ":", "either", "`", "`", "'", "fit", "'", "`", "`", ",", "`", "`", "'", "validate", "'", "`", "`", ",", "`", "`", "'", "test", "'", "`", "`", ",", "or", "`", "`", "'", "predict", "'", "`", "`", "Example", ":", ":", "class", "LitModel", "(", ".", ".", ".", ")", ":", "def", "__init__", "(", "self", ")", ":", "self", ".", "l1", "=", "None", "def", "prepare_data", "(", "self", ")", ":", "download_data", "(", ")", "tokenize", "(", ")", "self", ".", "something", "=", "else", "def", "setup", "(", "self", ",", "stage", ")", ":", "data", "=", "load_data", "(", ".", ".", ".", ")", "self", ".", "l1", "=", "nn", ".", "Linear", "(", "28", ",", "data", ".", "num_classes", ")", "\"", "\"", "\""], "docstring": "Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\r\n        need to build models dynamically or adjust something about them. This hook is called on every process when\r\n        using DDP.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        Example::\r\n\r\n            class LitModel(...):\r\n                def __init__(self):\r\n                    self.l1 = None\r\n\r\n                def prepare_data(self):\r\n                    download_data()\r\n                    tokenize()\r\n\r\n                    # don't do this\r\n                    self.something = else\r\n\r\n                def setup(self, stage):\r\n                    data = load_data(...)\r\n                    self.l1 = nn.Linear(28, data.num_classes)", "docstring_tokens": ["called", "at", "the", "beginning", "of", "fit", "train", "validate", "validate", "test", "or", "predict", "this", "is", "a", "good", "hook", "when", "you", "need", "to", "build", "models", "dynamically", "or", "adjust", "something", "about", "them", "this", "hook", "is", "called", "on", "every", "process", "when", "using", "ddp", "args", "stage", "either", "fit", "validate", "test", "or", "predict", "example", "class", "litmodel", "def", "__init__", "self", "self", "l1", "none", "def", "prepare_data", "self", "download_data", "tokenize", "don", "t", "do", "this", "self", "something", "else", "def", "setup", "self", "stage", "data", "load_data", "self", "l1", "nn", "linear", "28", "data", "num_classes"], "docstring_summary": "Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 419, "end_line": 444, "hash": "1eabcbfe27222a347c2bb9a4ff8457ee", "complexity": 1, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "teardown", "original_string": "def teardown(self, stage: str) -> None:\r\n        \"\"\"Called at the end of fit (train + validate), validate, test, or predict.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        \"\"\"", "language": "python", "code": "def teardown(self, stage: str) -> None:\r\n        \"\"\"Called at the end of fit (train + validate), validate, test, or predict.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\r\n\r\n        \"\"\"", "code_tokens": ["def", "teardown", "(", "self", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "at", "the", "end", "of", "fit", "(", "train", "+", "validate", ")", ",", "validate", ",", "test", ",", "or", "predict", ".", "Args", ":", "stage", ":", "either", "`", "`", "'", "fit", "'", "`", "`", ",", "`", "`", "'", "validate", "'", "`", "`", ",", "`", "`", "'", "test", "'", "`", "`", ",", "or", "`", "`", "'", "predict", "'", "`", "`", "\"", "\"", "\""], "docstring": "Called at the end of fit (train + validate), validate, test, or predict.\r\n\r\n        Args:\r\n            stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``", "docstring_tokens": ["called", "at", "the", "end", "of", "fit", "train", "validate", "validate", "test", "or", "predict", "args", "stage", "either", "fit", "validate", "test", "or", "predict"], "docstring_summary": "Called at the end of fit (train + validate), validate, test, or predict.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 446, "end_line": 452, "hash": "61b3c7b4ec7968f6fa858ad3c5297f92", "complexity": 1, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "train_dataloader", "original_string": "def train_dataloader(self) -> TRAIN_DATALOADERS:\r\n        \"\"\"An iterable or collection of iterables specifying training samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def train_dataloader(self) -> TRAIN_DATALOADERS:\r\n        \"\"\"An iterable or collection of iterables specifying training samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "train_dataloader", "(", "self", ")", "-", ">", "TRAIN_DATALOADERS", ":", "\"", "\"", "\"", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "training", "samples", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "The", "dataloader", "you", "return", "will", "not", "be", "reloaded", "unless", "you", "set", ":", "paramref", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "reload_dataloaders_every_n_epochs", "`", "to", "a", "positive", "integer", ".", "For", "data", "processing", "use", "the", "following", "pattern", ":", "-", "download", "in", ":", "meth", ":", "`", "prepare_data", "`", "-", "process", "and", "split", "in", ":", "meth", ":", "`", "setup", "`", "However", ",", "the", "above", "are", "only", "necessary", "for", "distributed", "processing", ".", ".", ".", "warning", ":", ":", "do", "not", "assign", "state", "in", "prepare_data", "-", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "fit", "`", "-", ":", "meth", ":", "`", "prepare_data", "`", "-", ":", "meth", ":", "`", "setup", "`", "Note", ":", "Lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", ".", "There", "is", "no", "need", "to", "set", "it", "yourself", ".", "\"", "\"", "\"", "raise", "MisconfigurationException", "(", "\"", "`", "train_dataloader", "`", "must", "be", "implemented", "to", "be", "used", "with", "the", "Lightning", "Trainer", "\"", ")"], "docstring": "An iterable or collection of iterables specifying training samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.", "docstring_tokens": ["an", "iterable", "or", "collection", "of", "iterables", "specifying", "training", "samples", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "the", "dataloader", "you", "return", "will", "not", "be", "reloaded", "unless", "you", "set", "paramref", "lightning", "pytorch", "trainer", "trainer", "trainer", "reload_dataloaders_every_n_epochs", "to", "a", "positive", "integer", "for", "data", "processing", "use", "the", "following", "pattern", "download", "in", "meth", "prepare_data", "process", "and", "split", "in", "meth", "setup", "however", "the", "above", "are", "only", "necessary", "for", "distributed", "processing", "warning", "do", "not", "assign", "state", "in", "prepare_data", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "fit", "meth", "prepare_data", "meth", "setup", "note", "lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", "there", "is", "no", "need", "to", "set", "it", "yourself"], "docstring_summary": "An iterable or collection of iterables specifying training samples.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 454, "end_line": 481, "hash": "ced59f62de34ccaf8473fa83c532fc31", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "test_dataloader", "original_string": "def test_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying test samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a test dataset and a :meth:`test_step`, you don't need to implement\r\n            this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`test_dataloader` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def test_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying test samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a test dataset and a :meth:`test_step`, you don't need to implement\r\n            this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`test_dataloader` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "test_dataloader", "(", "self", ")", "-", ">", "EVAL_DATALOADERS", ":", "r", "\"", "\"", "\"", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "test", "samples", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "For", "data", "processing", "use", "the", "following", "pattern", ":", "-", "download", "in", ":", "meth", ":", "`", "prepare_data", "`", "-", "process", "and", "split", "in", ":", "meth", ":", "`", "setup", "`", "However", ",", "the", "above", "are", "only", "necessary", "for", "distributed", "processing", ".", ".", ".", "warning", ":", ":", "do", "not", "assign", "state", "in", "prepare_data", "-", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "test", "`", "-", ":", "meth", ":", "`", "prepare_data", "`", "-", ":", "meth", ":", "`", "setup", "`", "Note", ":", "Lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", ".", "There", "is", "no", "need", "to", "set", "it", "yourself", ".", "Note", ":", "If", "you", "don", "'", "t", "need", "a", "test", "dataset", "and", "a", ":", "meth", ":", "`", "test_step", "`", ",", "you", "don", "'", "t", "need", "to", "implement", "this", "method", ".", "\"", "\"", "\"", "raise", "MisconfigurationException", "(", "\"", "`", "test_dataloader", "`", "must", "be", "implemented", "to", "be", "used", "with", "the", "Lightning", "Trainer", "\"", ")"], "docstring": "r\"\"\"An iterable or collection of iterables specifying test samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        For data processing use the following pattern:\r\n\r\n            - download in :meth:`prepare_data`\r\n            - process and split in :meth:`setup`\r\n\r\n        However, the above are only necessary for distributed processing.\r\n\r\n        .. warning:: do not assign state in prepare_data\r\n\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware.\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a test dataset and a :meth:`test_step`, you don't need to implement\r\n            this method.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "test", "samples", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "for", "data", "processing", "use", "the", "following", "pattern", "download", "in", "meth", "prepare_data", "process", "and", "split", "in", "meth", "setup", "however", "the", "above", "are", "only", "necessary", "for", "distributed", "processing", "warning", "do", "not", "assign", "state", "in", "prepare_data", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "test", "meth", "prepare_data", "meth", "setup", "note", "lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", "there", "is", "no", "need", "to", "set", "it", "yourself", "note", "if", "you", "don", "t", "need", "a", "test", "dataset", "and", "a", "meth", "test_step", "you", "don", "t", "need", "to", "implement", "this", "method"], "docstring_summary": "r\"\"\"An iterable or collection of iterables specifying test samples.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 483, "end_line": 511, "hash": "1b576449ac59a0de027045b9d98b9547", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "val_dataloader", "original_string": "def val_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying validation samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a validation dataset and a :meth:`validation_step`, you don't need to\r\n            implement this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def val_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying validation samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a validation dataset and a :meth:`validation_step`, you don't need to\r\n            implement this method.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "val_dataloader", "(", "self", ")", "-", ">", "EVAL_DATALOADERS", ":", "r", "\"", "\"", "\"", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "The", "dataloader", "you", "return", "will", "not", "be", "reloaded", "unless", "you", "set", ":", "paramref", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "reload_dataloaders_every_n_epochs", "`", "to", "a", "positive", "integer", ".", "It", "'", "s", "recommended", "that", "all", "data", "downloads", "and", "preparation", "happen", "in", ":", "meth", ":", "`", "prepare_data", "`", ".", "-", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "fit", "`", "-", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "validate", "`", "-", ":", "meth", ":", "`", "prepare_data", "`", "-", ":", "meth", ":", "`", "setup", "`", "Note", ":", "Lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", "There", "is", "no", "need", "to", "set", "it", "yourself", ".", "Note", ":", "If", "you", "don", "'", "t", "need", "a", "validation", "dataset", "and", "a", ":", "meth", ":", "`", "validation_step", "`", ",", "you", "don", "'", "t", "need", "to", "implement", "this", "method", ".", "\"", "\"", "\"", "raise", "MisconfigurationException", "(", "\"", "`", "val_dataloader", "`", "must", "be", "implemented", "to", "be", "used", "with", "the", "Lightning", "Trainer", "\"", ")"], "docstring": "r\"\"\"An iterable or collection of iterables specifying validation samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        The dataloader you return will not be reloaded unless you set\r\n        :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\r\n        a positive integer.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Note:\r\n            If you don't need a validation dataset and a :meth:`validation_step`, you don't need to\r\n            implement this method.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "validation", "samples", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "the", "dataloader", "you", "return", "will", "not", "be", "reloaded", "unless", "you", "set", "paramref", "lightning", "pytorch", "trainer", "trainer", "trainer", "reload_dataloaders_every_n_epochs", "to", "a", "positive", "integer", "it", "s", "recommended", "that", "all", "data", "downloads", "and", "preparation", "happen", "in", "meth", "prepare_data", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "fit", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "validate", "meth", "prepare_data", "meth", "setup", "note", "lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", "there", "is", "no", "need", "to", "set", "it", "yourself", "note", "if", "you", "don", "t", "need", "a", "validation", "dataset", "and", "a", "meth", "validation_step", "you", "don", "t", "need", "to", "implement", "this", "method"], "docstring_summary": "r\"\"\"An iterable or collection of iterables specifying validation samples.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 513, "end_line": 538, "hash": "8733c440b755f510382b28ca2c9e0c70", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "predict_dataloader", "original_string": "def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying prediction samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Return:\r\n            A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\r\n            \"`predict_dataloader` must be implemented to be used with the Lightning Trainer\"\r\n        )", "language": "python", "code": "def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n        r\"\"\"An iterable or collection of iterables specifying prediction samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Return:\r\n            A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.\r\n\r\n        \"\"\"\r\n        raise MisconfigurationException(\r\n            \"`predict_dataloader` must be implemented to be used with the Lightning Trainer\"\r\n        )", "code_tokens": ["def", "predict_dataloader", "(", "self", ")", "-", ">", "EVAL_DATALOADERS", ":", "r", "\"", "\"", "\"", "An", "iterable", "or", "collection", "of", "iterables", "specifying", "prediction", "samples", ".", "For", "more", "information", "about", "multiple", "dataloaders", ",", "see", "this", ":", "ref", ":", "`", "section", "<", "multiple", "-", "dataloaders", ">", "`", ".", "It", "'", "s", "recommended", "that", "all", "data", "downloads", "and", "preparation", "happen", "in", ":", "meth", ":", "`", "prepare_data", "`", ".", "-", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "predict", "`", "-", ":", "meth", ":", "`", "prepare_data", "`", "-", ":", "meth", ":", "`", "setup", "`", "Note", ":", "Lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", "There", "is", "no", "need", "to", "set", "it", "yourself", ".", "Return", ":", "A", ":", "class", ":", "`", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "or", "a", "sequence", "of", "them", "specifying", "prediction", "samples", ".", "\"", "\"", "\"", "raise", "MisconfigurationException", "(", "\"", "`", "predict_dataloader", "`", "must", "be", "implemented", "to", "be", "used", "with", "the", "Lightning", "Trainer", "\"", ")"], "docstring": "r\"\"\"An iterable or collection of iterables specifying prediction samples.\r\n\r\n        For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\r\n\r\n        It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\r\n\r\n        - :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`\r\n        - :meth:`prepare_data`\r\n        - :meth:`setup`\r\n\r\n        Note:\r\n            Lightning tries to add the correct sampler for distributed and arbitrary hardware\r\n            There is no need to set it yourself.\r\n\r\n        Return:\r\n            A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "an", "iterable", "or", "collection", "of", "iterables", "specifying", "prediction", "samples", "for", "more", "information", "about", "multiple", "dataloaders", "see", "this", "ref", "section", "multiple", "dataloaders", "it", "s", "recommended", "that", "all", "data", "downloads", "and", "preparation", "happen", "in", "meth", "prepare_data", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "predict", "meth", "prepare_data", "meth", "setup", "note", "lightning", "tries", "to", "add", "the", "correct", "sampler", "for", "distributed", "and", "arbitrary", "hardware", "there", "is", "no", "need", "to", "set", "it", "yourself", "return", "a", "class", "torch", "utils", "data", "dataloader", "or", "a", "sequence", "of", "them", "specifying", "prediction", "samples"], "docstring_summary": "r\"\"\"An iterable or collection of iterables specifying prediction samples.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 540, "end_line": 561, "hash": "eadea5cb9c5d3bc7ee82991513612a86", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "transfer_batch_to_device", "original_string": "def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\r\n        \"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\r\n        structure.\r\n\r\n        The data types listed below (and any arbitrary nesting of them) are supported out of the box:\r\n\r\n        - :class:`torch.Tensor` or anything that implements `.to(...)`\r\n        - :class:`list`\r\n        - :class:`dict`\r\n        - :class:`tuple`\r\n\r\n        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\r\n\r\n        Note:\r\n            This hook should only transfer the data and not modify it, nor should it move the data to\r\n            any other device than the one passed in as argument (unless you know what you are doing).\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be transferred to a new device.\r\n            device: The target device as defined in PyTorch.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A reference to the data on the new device.\r\n\r\n        Example::\r\n\r\n            def transfer_batch_to_device(self, batch, device, dataloader_idx):\r\n                if isinstance(batch, CustomBatch):\r\n                    # move all tensors in your custom data structure to the device\r\n                    batch.samples = batch.samples.to(device)\r\n                    batch.targets = batch.targets.to(device)\r\n                elif dataloader_idx == 0:\r\n                    # skip device transfer for the first dataloader or anything you wish\r\n                    pass\r\n                else:\r\n                    batch = super().transfer_batch_to_device(batch, device, dataloader_idx)\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`move_data_to_device`\r\n            - :meth:`apply_to_collection`\r\n\r\n        \"\"\"\r\n        return move_data_to_device(batch, device)", "language": "python", "code": "def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\r\n        \"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\r\n        structure.\r\n\r\n        The data types listed below (and any arbitrary nesting of them) are supported out of the box:\r\n\r\n        - :class:`torch.Tensor` or anything that implements `.to(...)`\r\n        - :class:`list`\r\n        - :class:`dict`\r\n        - :class:`tuple`\r\n\r\n        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\r\n\r\n        Note:\r\n            This hook should only transfer the data and not modify it, nor should it move the data to\r\n            any other device than the one passed in as argument (unless you know what you are doing).\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be transferred to a new device.\r\n            device: The target device as defined in PyTorch.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A reference to the data on the new device.\r\n\r\n        Example::\r\n\r\n            def transfer_batch_to_device(self, batch, device, dataloader_idx):\r\n                if isinstance(batch, CustomBatch):\r\n                    # move all tensors in your custom data structure to the device\r\n                    batch.samples = batch.samples.to(device)\r\n                    batch.targets = batch.targets.to(device)\r\n                elif dataloader_idx == 0:\r\n                    # skip device transfer for the first dataloader or anything you wish\r\n                    pass\r\n                else:\r\n                    batch = super().transfer_batch_to_device(batch, device, dataloader_idx)\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`move_data_to_device`\r\n            - :meth:`apply_to_collection`\r\n\r\n        \"\"\"\r\n        return move_data_to_device(batch, device)", "code_tokens": ["def", "transfer_batch_to_device", "(", "self", ",", "batch", ":", "Any", ",", "device", ":", "torch", ".", "device", ",", "dataloader_idx", ":", "int", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Override", "this", "hook", "if", "your", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", "returns", "tensors", "wrapped", "in", "a", "custom", "data", "structure", ".", "The", "data", "types", "listed", "below", "(", "and", "any", "arbitrary", "nesting", "of", "them", ")", "are", "supported", "out", "of", "the", "box", ":", "-", ":", "class", ":", "`", "torch", ".", "Tensor", "`", "or", "anything", "that", "implements", "`", ".", "to", "(", ".", ".", ".", ")", "`", "-", ":", "class", ":", "`", "list", "`", "-", ":", "class", ":", "`", "dict", "`", "-", ":", "class", ":", "`", "tuple", "`", "For", "anything", "else", ",", "you", "need", "to", "define", "how", "the", "data", "is", "moved", "to", "the", "target", "device", "(", "CPU", ",", "GPU", ",", "TPU", ",", ".", ".", ".", ")", ".", "Note", ":", "This", "hook", "should", "only", "transfer", "the", "data", "and", "not", "modify", "it", ",", "nor", "should", "it", "move", "the", "data", "to", "any", "other", "device", "than", "the", "one", "passed", "in", "as", "argument", "(", "unless", "you", "know", "what", "you", "are", "doing", ")", ".", "To", "check", "the", "current", "state", "of", "execution", "of", "this", "hook", "you", "can", "use", "`", "`", "self", ".", "trainer", ".", "training", "/", "testing", "/", "validating", "/", "predicting", "`", "`", "so", "that", "you", "can", "add", "different", "logic", "as", "per", "your", "requirement", ".", "Args", ":", "batch", ":", "A", "batch", "of", "data", "that", "needs", "to", "be", "transferred", "to", "a", "new", "device", ".", "device", ":", "The", "target", "device", "as", "defined", "in", "PyTorch", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", ".", "Returns", ":", "A", "reference", "to", "the", "data", "on", "the", "new", "device", ".", "Example", ":", ":", "def", "transfer_batch_to_device", "(", "self", ",", "batch", ",", "device", ",", "dataloader_idx", ")", ":", "if", "isinstance", "(", "batch", ",", "CustomBatch", ")", ":", "batch", ".", "samples", "=", "batch", ".", "samples", ".", "to", "(", "device", ")", "batch", ".", "targets", "=", "batch", ".", "targets", ".", "to", "(", "device", ")", "elif", "dataloader_idx", "=", "=", "0", ":", "pass", "else", ":", "batch", "=", "super", "(", ")", ".", "transfer_batch_to_device", "(", "batch", ",", "device", ",", "dataloader_idx", ")", "return", "batch", "See", "Also", ":", "-", ":", "meth", ":", "`", "move_data_to_device", "`", "-", ":", "meth", ":", "`", "apply_to_collection", "`", "\"", "\"", "\"", "return", "move_data_to_device", "(", "batch", ",", "device", ")"], "docstring": "Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\r\n        structure.\r\n\r\n        The data types listed below (and any arbitrary nesting of them) are supported out of the box:\r\n\r\n        - :class:`torch.Tensor` or anything that implements `.to(...)`\r\n        - :class:`list`\r\n        - :class:`dict`\r\n        - :class:`tuple`\r\n\r\n        For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\r\n\r\n        Note:\r\n            This hook should only transfer the data and not modify it, nor should it move the data to\r\n            any other device than the one passed in as argument (unless you know what you are doing).\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be transferred to a new device.\r\n            device: The target device as defined in PyTorch.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A reference to the data on the new device.\r\n\r\n        Example::\r\n\r\n            def transfer_batch_to_device(self, batch, device, dataloader_idx):\r\n                if isinstance(batch, CustomBatch):\r\n                    # move all tensors in your custom data structure to the device\r\n                    batch.samples = batch.samples.to(device)\r\n                    batch.targets = batch.targets.to(device)\r\n                elif dataloader_idx == 0:\r\n                    # skip device transfer for the first dataloader or anything you wish\r\n                    pass\r\n                else:\r\n                    batch = super().transfer_batch_to_device(batch, device, dataloader_idx)\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`move_data_to_device`\r\n            - :meth:`apply_to_collection`", "docstring_tokens": ["override", "this", "hook", "if", "your", "class", "torch", "utils", "data", "dataloader", "returns", "tensors", "wrapped", "in", "a", "custom", "data", "structure", "the", "data", "types", "listed", "below", "and", "any", "arbitrary", "nesting", "of", "them", "are", "supported", "out", "of", "the", "box", "class", "torch", "tensor", "or", "anything", "that", "implements", "to", "class", "list", "class", "dict", "class", "tuple", "for", "anything", "else", "you", "need", "to", "define", "how", "the", "data", "is", "moved", "to", "the", "target", "device", "cpu", "gpu", "tpu", "note", "this", "hook", "should", "only", "transfer", "the", "data", "and", "not", "modify", "it", "nor", "should", "it", "move", "the", "data", "to", "any", "other", "device", "than", "the", "one", "passed", "in", "as", "argument", "unless", "you", "know", "what", "you", "are", "doing", "to", "check", "the", "current", "state", "of", "execution", "of", "this", "hook", "you", "can", "use", "self", "trainer", "training", "testing", "validating", "predicting", "so", "that", "you", "can", "add", "different", "logic", "as", "per", "your", "requirement", "args", "batch", "a", "batch", "of", "data", "that", "needs", "to", "be", "transferred", "to", "a", "new", "device", "device", "the", "target", "device", "as", "defined", "in", "pytorch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", "returns", "a", "reference", "to", "the", "data", "on", "the", "new", "device", "example", "def", "transfer_batch_to_device", "self", "batch", "device", "dataloader_idx", "if", "isinstance", "batch", "custombatch", "move", "all", "tensors", "in", "your", "custom", "data", "structure", "to", "the", "device", "batch", "samples", "batch", "samples", "to", "device", "batch", "targets", "batch", "targets", "to", "device", "elif", "dataloader_idx", "0", "skip", "device", "transfer", "for", "the", "first", "dataloader", "or", "anything", "you", "wish", "pass", "else", "batch", "super", "transfer_batch_to_device", "batch", "device", "dataloader_idx", "return", "batch", "see", "also", "meth", "move_data_to_device", "meth", "apply_to_collection"], "docstring_summary": "Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 563, "end_line": 610, "hash": "00ca059fc36e70310a08fd73f1e2b512", "complexity": 1, "parameters": ["batch", "device", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_before_batch_transfer", "original_string": "def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch before it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_before_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_after_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "language": "python", "code": "def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch before it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_before_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_after_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "code_tokens": ["def", "on_before_batch_transfer", "(", "self", ",", "batch", ":", "Any", ",", "dataloader_idx", ":", "int", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Override", "to", "alter", "or", "apply", "batch", "augmentations", "to", "your", "batch", "before", "it", "is", "transferred", "to", "the", "device", ".", "Note", ":", "To", "check", "the", "current", "state", "of", "execution", "of", "this", "hook", "you", "can", "use", "`", "`", "self", ".", "trainer", ".", "training", "/", "testing", "/", "validating", "/", "predicting", "`", "`", "so", "that", "you", "can", "add", "different", "logic", "as", "per", "your", "requirement", ".", "Args", ":", "batch", ":", "A", "batch", "of", "data", "that", "needs", "to", "be", "altered", "or", "augmented", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", ".", "Returns", ":", "A", "batch", "of", "data", "Example", ":", ":", "def", "on_before_batch_transfer", "(", "self", ",", "batch", ",", "dataloader_idx", ")", ":", "batch", "[", "'", "x", "'", "]", "=", "transforms", "(", "batch", "[", "'", "x", "'", "]", ")", "return", "batch", "See", "Also", ":", "-", ":", "meth", ":", "`", "on_after_batch_transfer", "`", "-", ":", "meth", ":", "`", "transfer_batch_to_device", "`", "\"", "\"", "\"", "return", "batch"], "docstring": "Override to alter or apply batch augmentations to your batch before it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_before_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_after_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`", "docstring_tokens": ["override", "to", "alter", "or", "apply", "batch", "augmentations", "to", "your", "batch", "before", "it", "is", "transferred", "to", "the", "device", "note", "to", "check", "the", "current", "state", "of", "execution", "of", "this", "hook", "you", "can", "use", "self", "trainer", "training", "testing", "validating", "predicting", "so", "that", "you", "can", "add", "different", "logic", "as", "per", "your", "requirement", "args", "batch", "a", "batch", "of", "data", "that", "needs", "to", "be", "altered", "or", "augmented", "dataloader_idx", "the", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", "returns", "a", "batch", "of", "data", "example", "def", "on_before_batch_transfer", "self", "batch", "dataloader_idx", "batch", "x", "transforms", "batch", "x", "return", "batch", "see", "also", "meth", "on_after_batch_transfer", "meth", "transfer_batch_to_device"], "docstring_summary": "Override to alter or apply batch augmentations to your batch before it is transferred to the device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 612, "end_line": 638, "hash": "c3398e50226f82d1745cbec8030c9f2c", "complexity": 1, "parameters": ["batch", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_after_batch_transfer", "original_string": "def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch after it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_after_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = gpu_transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_before_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "language": "python", "code": "def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\r\n        \"\"\"Override to alter or apply batch augmentations to your batch after it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_after_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = gpu_transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_before_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`\r\n\r\n        \"\"\"\r\n        return batch", "code_tokens": ["def", "on_after_batch_transfer", "(", "self", ",", "batch", ":", "Any", ",", "dataloader_idx", ":", "int", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Override", "to", "alter", "or", "apply", "batch", "augmentations", "to", "your", "batch", "after", "it", "is", "transferred", "to", "the", "device", ".", "Note", ":", "To", "check", "the", "current", "state", "of", "execution", "of", "this", "hook", "you", "can", "use", "`", "`", "self", ".", "trainer", ".", "training", "/", "testing", "/", "validating", "/", "predicting", "`", "`", "so", "that", "you", "can", "add", "different", "logic", "as", "per", "your", "requirement", ".", "Args", ":", "batch", ":", "A", "batch", "of", "data", "that", "needs", "to", "be", "altered", "or", "augmented", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", ".", "Returns", ":", "A", "batch", "of", "data", "Example", ":", ":", "def", "on_after_batch_transfer", "(", "self", ",", "batch", ",", "dataloader_idx", ")", ":", "batch", "[", "'", "x", "'", "]", "=", "gpu_transforms", "(", "batch", "[", "'", "x", "'", "]", ")", "return", "batch", "See", "Also", ":", "-", ":", "meth", ":", "`", "on_before_batch_transfer", "`", "-", ":", "meth", ":", "`", "transfer_batch_to_device", "`", "\"", "\"", "\"", "return", "batch"], "docstring": "Override to alter or apply batch augmentations to your batch after it is transferred to the device.\r\n\r\n        Note:\r\n            To check the current state of execution of this hook you can use\r\n            ``self.trainer.training/testing/validating/predicting`` so that you can\r\n            add different logic as per your requirement.\r\n\r\n        Args:\r\n            batch: A batch of data that needs to be altered or augmented.\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        Returns:\r\n            A batch of data\r\n\r\n        Example::\r\n\r\n            def on_after_batch_transfer(self, batch, dataloader_idx):\r\n                batch['x'] = gpu_transforms(batch['x'])\r\n                return batch\r\n\r\n        See Also:\r\n            - :meth:`on_before_batch_transfer`\r\n            - :meth:`transfer_batch_to_device`", "docstring_tokens": ["override", "to", "alter", "or", "apply", "batch", "augmentations", "to", "your", "batch", "after", "it", "is", "transferred", "to", "the", "device", "note", "to", "check", "the", "current", "state", "of", "execution", "of", "this", "hook", "you", "can", "use", "self", "trainer", "training", "testing", "validating", "predicting", "so", "that", "you", "can", "add", "different", "logic", "as", "per", "your", "requirement", "args", "batch", "a", "batch", "of", "data", "that", "needs", "to", "be", "altered", "or", "augmented", "dataloader_idx", "the", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", "returns", "a", "batch", "of", "data", "example", "def", "on_after_batch_transfer", "self", "batch", "dataloader_idx", "batch", "x", "gpu_transforms", "batch", "x", "return", "batch", "see", "also", "meth", "on_before_batch_transfer", "meth", "transfer_batch_to_device"], "docstring_summary": "Override to alter or apply batch augmentations to your batch after it is transferred to the device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "DataHooks", "start_line": 640, "end_line": 666, "hash": "5103bb02059f0f26972c97fb0cf10a45", "complexity": 1, "parameters": ["batch", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_load_checkpoint", "original_string": "def on_load_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\r\n        your chance to restore this.\r\n\r\n        Args:\r\n            checkpoint: Loaded checkpoint\r\n\r\n        Example::\r\n\r\n            def on_load_checkpoint(self, checkpoint):\r\n                # 99% of the time you don't need to implement this method\r\n                self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\r\n\r\n        Note:\r\n            Lightning auto-restores global step, epoch, and train state including amp scaling.\r\n            There is no need for you to restore anything regarding training.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_load_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\r\n        your chance to restore this.\r\n\r\n        Args:\r\n            checkpoint: Loaded checkpoint\r\n\r\n        Example::\r\n\r\n            def on_load_checkpoint(self, checkpoint):\r\n                # 99% of the time you don't need to implement this method\r\n                self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\r\n\r\n        Note:\r\n            Lightning auto-restores global step, epoch, and train state including amp scaling.\r\n            There is no need for you to restore anything regarding training.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_load_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Called", "by", "Lightning", "to", "restore", "your", "model", ".", "If", "you", "saved", "something", "with", ":", "meth", ":", "`", "on_save_checkpoint", "`", "this", "is", "your", "chance", "to", "restore", "this", ".", "Args", ":", "checkpoint", ":", "Loaded", "checkpoint", "Example", ":", ":", "def", "on_load_checkpoint", "(", "self", ",", "checkpoint", ")", ":", "self", ".", "something_cool_i_want_to_save", "=", "checkpoint", "[", "'", "something_cool_i_want_to_save", "'", "]", "Note", ":", "Lightning", "auto", "-", "restores", "global", "step", ",", "epoch", ",", "and", "train", "state", "including", "amp", "scaling", ".", "There", "is", "no", "need", "for", "you", "to", "restore", "anything", "regarding", "training", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\r\n        your chance to restore this.\r\n\r\n        Args:\r\n            checkpoint: Loaded checkpoint\r\n\r\n        Example::\r\n\r\n            def on_load_checkpoint(self, checkpoint):\r\n                # 99% of the time you don't need to implement this method\r\n                self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\r\n\r\n        Note:\r\n            Lightning auto-restores global step, epoch, and train state including amp scaling.\r\n            There is no need for you to restore anything regarding training.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "called", "by", "lightning", "to", "restore", "your", "model", "if", "you", "saved", "something", "with", "meth", "on_save_checkpoint", "this", "is", "your", "chance", "to", "restore", "this", "args", "checkpoint", "loaded", "checkpoint", "example", "def", "on_load_checkpoint", "self", "checkpoint", "99", "of", "the", "time", "you", "don", "t", "need", "to", "implement", "this", "method", "self", "something_cool_i_want_to_save", "checkpoint", "something_cool_i_want_to_save", "note", "lightning", "auto", "restores", "global", "step", "epoch", "and", "train", "state", "including", "amp", "scaling", "there", "is", "no", "need", "for", "you", "to", "restore", "anything", "regarding", "training"], "docstring_summary": "r\"\"\"Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "CheckpointHooks", "start_line": 672, "end_line": 689, "hash": "6a05a99414e72b846fb5384a2e0e1358", "complexity": 1, "parameters": ["checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\hooks.py", "func_name": "on_save_checkpoint", "original_string": "def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\r\n        save.\r\n\r\n        Args:\r\n            checkpoint: The full checkpoint dictionary before it gets dumped to a file.\r\n                Implementations of this hook can insert additional data into this dictionary.\r\n\r\n        Example::\r\n\r\n            def on_save_checkpoint(self, checkpoint):\r\n                # 99% of use cases you don't need to implement this method\r\n                checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\r\n\r\n        Note:\r\n            Lightning saves all aspects of training (epoch, global step, etc...)\r\n            including amp scaling.\r\n            There is no need for you to store anything about training.\r\n\r\n        \"\"\"", "language": "python", "code": "def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\r\n        r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\r\n        save.\r\n\r\n        Args:\r\n            checkpoint: The full checkpoint dictionary before it gets dumped to a file.\r\n                Implementations of this hook can insert additional data into this dictionary.\r\n\r\n        Example::\r\n\r\n            def on_save_checkpoint(self, checkpoint):\r\n                # 99% of use cases you don't need to implement this method\r\n                checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\r\n\r\n        Note:\r\n            Lightning saves all aspects of training (epoch, global step, etc...)\r\n            including amp scaling.\r\n            There is no need for you to store anything about training.\r\n\r\n        \"\"\"", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Called", "by", "Lightning", "when", "saving", "a", "checkpoint", "to", "give", "you", "a", "chance", "to", "store", "anything", "else", "you", "might", "want", "to", "save", ".", "Args", ":", "checkpoint", ":", "The", "full", "checkpoint", "dictionary", "before", "it", "gets", "dumped", "to", "a", "file", ".", "Implementations", "of", "this", "hook", "can", "insert", "additional", "data", "into", "this", "dictionary", ".", "Example", ":", ":", "def", "on_save_checkpoint", "(", "self", ",", "checkpoint", ")", ":", "checkpoint", "[", "'", "something_cool_i_want_to_save", "'", "]", "=", "my_cool_pickable_object", "Note", ":", "Lightning", "saves", "all", "aspects", "of", "training", "(", "epoch", ",", "global", "step", ",", "etc", ".", ".", ".", ")", "including", "amp", "scaling", ".", "There", "is", "no", "need", "for", "you", "to", "store", "anything", "about", "training", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\r\n        save.\r\n\r\n        Args:\r\n            checkpoint: The full checkpoint dictionary before it gets dumped to a file.\r\n                Implementations of this hook can insert additional data into this dictionary.\r\n\r\n        Example::\r\n\r\n            def on_save_checkpoint(self, checkpoint):\r\n                # 99% of use cases you don't need to implement this method\r\n                checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\r\n\r\n        Note:\r\n            Lightning saves all aspects of training (epoch, global step, etc...)\r\n            including amp scaling.\r\n            There is no need for you to store anything about training.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "called", "by", "lightning", "when", "saving", "a", "checkpoint", "to", "give", "you", "a", "chance", "to", "store", "anything", "else", "you", "might", "want", "to", "save", "args", "checkpoint", "the", "full", "checkpoint", "dictionary", "before", "it", "gets", "dumped", "to", "a", "file", "implementations", "of", "this", "hook", "can", "insert", "additional", "data", "into", "this", "dictionary", "example", "def", "on_save_checkpoint", "self", "checkpoint", "99", "of", "use", "cases", "you", "don", "t", "need", "to", "implement", "this", "method", "checkpoint", "something_cool_i_want_to_save", "my_cool_pickable_object", "note", "lightning", "saves", "all", "aspects", "of", "training", "epoch", "global", "step", "etc", "including", "amp", "scaling", "there", "is", "no", "need", "for", "you", "to", "store", "anything", "about", "training"], "docstring_summary": "r\"\"\"Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\hooks.py", "partition": "train", "function_type": "class_method", "class_name": "CheckpointHooks", "start_line": 691, "end_line": 710, "hash": "ea2b6e4493f6cf78b1c07a845ac0a69f", "complexity": 1, "parameters": ["checkpoint", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "optimizers", "original_string": "def optimizers(self, use_pl_optimizer: bool = True) -> MODULE_OPTIMIZERS:\r\n        \"\"\"Returns the optimizer(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Args:\r\n            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a\r\n                :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` for automatic handling of precision,\r\n                profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the\r\n                ``step`` method and custom optimizers that don't have this method are not supported.\r\n\r\n        Returns:\r\n            A single optimizer, or a list of optimizers in case multiple ones are present.\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            opts: MODULE_OPTIMIZERS = self._fabric_optimizers\r\n        elif use_pl_optimizer:\r\n            opts = self.trainer.strategy._lightning_optimizers\r\n        else:\r\n            opts = self.trainer.optimizers\r\n\r\n        # single optimizer\r\n        if (\r\n            isinstance(opts, list)\r\n            and len(opts) == 1\r\n            and isinstance(opts[0], (Optimizer, LightningOptimizer, _FabricOptimizer))\r\n        ):\r\n            return opts[0]\r\n        # multiple opts\r\n        return opts", "language": "python", "code": "def optimizers(self, use_pl_optimizer: bool = True) -> MODULE_OPTIMIZERS:\r\n        \"\"\"Returns the optimizer(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Args:\r\n            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a\r\n                :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` for automatic handling of precision,\r\n                profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the\r\n                ``step`` method and custom optimizers that don't have this method are not supported.\r\n\r\n        Returns:\r\n            A single optimizer, or a list of optimizers in case multiple ones are present.\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            opts: MODULE_OPTIMIZERS = self._fabric_optimizers\r\n        elif use_pl_optimizer:\r\n            opts = self.trainer.strategy._lightning_optimizers\r\n        else:\r\n            opts = self.trainer.optimizers\r\n\r\n        # single optimizer\r\n        if (\r\n            isinstance(opts, list)\r\n            and len(opts) == 1\r\n            and isinstance(opts[0], (Optimizer, LightningOptimizer, _FabricOptimizer))\r\n        ):\r\n            return opts[0]\r\n        # multiple opts\r\n        return opts", "code_tokens": ["def", "optimizers", "(", "self", ",", "use_pl_optimizer", ":", "bool", "=", "True", ")", "-", ">", "MODULE_OPTIMIZERS", ":", "\"", "\"", "\"", "Returns", "the", "optimizer", "(", "s", ")", "that", "are", "being", "used", "during", "training", ".", "Useful", "for", "manual", "optimization", ".", "Args", ":", "use_pl_optimizer", ":", "If", "`", "`", "True", "`", "`", ",", "will", "wrap", "the", "optimizer", "(", "s", ")", "in", "a", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "optimizer", ".", "LightningOptimizer", "`", "for", "automatic", "handling", "of", "precision", ",", "profiling", ",", "and", "counting", "of", "step", "calls", "for", "proper", "logging", "and", "checkpointing", ".", "It", "specifically", "wraps", "the", "`", "`", "step", "`", "`", "method", "and", "custom", "optimizers", "that", "don", "'", "t", "have", "this", "method", "are", "not", "supported", ".", "Returns", ":", "A", "single", "optimizer", ",", "or", "a", "list", "of", "optimizers", "in", "case", "multiple", "ones", "are", "present", ".", "\"", "\"", "\"", "if", "self", ".", "_fabric", ":", "opts", ":", "MODULE_OPTIMIZERS", "=", "self", ".", "_fabric_optimizers", "elif", "use_pl_optimizer", ":", "opts", "=", "self", ".", "trainer", ".", "strategy", ".", "_lightning_optimizers", "else", ":", "opts", "=", "self", ".", "trainer", ".", "optimizers", "if", "(", "isinstance", "(", "opts", ",", "list", ")", "and", "len", "(", "opts", ")", "=", "=", "1", "and", "isinstance", "(", "opts", "[", "0", "]", ",", "(", "Optimizer", ",", "LightningOptimizer", ",", "_FabricOptimizer", ")", ")", ")", ":", "return", "opts", "[", "0", "]", "return", "opts"], "docstring": "Returns the optimizer(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Args:\r\n            use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a\r\n                :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` for automatic handling of precision,\r\n                profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the\r\n                ``step`` method and custom optimizers that don't have this method are not supported.\r\n\r\n        Returns:\r\n            A single optimizer, or a list of optimizers in case multiple ones are present.", "docstring_tokens": ["returns", "the", "optimizer", "s", "that", "are", "being", "used", "during", "training", "useful", "for", "manual", "optimization", "args", "use_pl_optimizer", "if", "true", "will", "wrap", "the", "optimizer", "s", "in", "a", "class", "lightning", "pytorch", "core", "optimizer", "lightningoptimizer", "for", "automatic", "handling", "of", "precision", "profiling", "and", "counting", "of", "step", "calls", "for", "proper", "logging", "and", "checkpointing", "it", "specifically", "wraps", "the", "step", "method", "and", "custom", "optimizers", "that", "don", "t", "have", "this", "method", "are", "not", "supported", "returns", "a", "single", "optimizer", "or", "a", "list", "of", "optimizers", "in", "case", "multiple", "ones", "are", "present"], "docstring_summary": "Returns the optimizer(s) that are being used during training. Useful for manual optimization.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 167, "end_line": 195, "hash": "a546ad5d2cba0586a429e006a471a602", "complexity": 6, "parameters": ["use_pl_optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "lr_schedulers", "original_string": "def lr_schedulers(self) -> Union[None, list[LRSchedulerPLType], LRSchedulerPLType]:\r\n        \"\"\"Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Returns:\r\n            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no\r\n            schedulers were returned in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        if not self.trainer.lr_scheduler_configs:\r\n            return None\r\n\r\n        # ignore other keys \"interval\", \"frequency\", etc.\r\n        lr_schedulers: list[LRSchedulerPLType] = [config.scheduler for config in self.trainer.lr_scheduler_configs]\r\n\r\n        # single scheduler\r\n        if len(lr_schedulers) == 1:\r\n            return lr_schedulers[0]\r\n\r\n        # multiple schedulers\r\n        return lr_schedulers", "language": "python", "code": "def lr_schedulers(self) -> Union[None, list[LRSchedulerPLType], LRSchedulerPLType]:\r\n        \"\"\"Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Returns:\r\n            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no\r\n            schedulers were returned in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.\r\n\r\n        \"\"\"\r\n        if not self.trainer.lr_scheduler_configs:\r\n            return None\r\n\r\n        # ignore other keys \"interval\", \"frequency\", etc.\r\n        lr_schedulers: list[LRSchedulerPLType] = [config.scheduler for config in self.trainer.lr_scheduler_configs]\r\n\r\n        # single scheduler\r\n        if len(lr_schedulers) == 1:\r\n            return lr_schedulers[0]\r\n\r\n        # multiple schedulers\r\n        return lr_schedulers", "code_tokens": ["def", "lr_schedulers", "(", "self", ")", "-", ">", "Union", "[", "None", ",", "list", "[", "LRSchedulerPLType", "]", ",", "LRSchedulerPLType", "]", ":", "\"", "\"", "\"", "Returns", "the", "learning", "rate", "scheduler", "(", "s", ")", "that", "are", "being", "used", "during", "training", ".", "Useful", "for", "manual", "optimization", ".", "Returns", ":", "A", "single", "scheduler", ",", "or", "a", "list", "of", "schedulers", "in", "case", "multiple", "ones", "are", "present", ",", "or", "`", "`", "None", "`", "`", "if", "no", "schedulers", "were", "returned", "in", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "configure_optimizers", "`", ".", "\"", "\"", "\"", "if", "not", "self", ".", "trainer", ".", "lr_scheduler_configs", ":", "return", "None", "lr_schedulers", ":", "list", "[", "LRSchedulerPLType", "]", "=", "[", "config", ".", "scheduler", "for", "config", "in", "self", ".", "trainer", ".", "lr_scheduler_configs", "]", "if", "len", "(", "lr_schedulers", ")", "=", "=", "1", ":", "return", "lr_schedulers", "[", "0", "]", "return", "lr_schedulers"], "docstring": "Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\r\n\r\n        Returns:\r\n            A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no\r\n            schedulers were returned in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.", "docstring_tokens": ["returns", "the", "learning", "rate", "scheduler", "s", "that", "are", "being", "used", "during", "training", "useful", "for", "manual", "optimization", "returns", "a", "single", "scheduler", "or", "a", "list", "of", "schedulers", "in", "case", "multiple", "ones", "are", "present", "or", "none", "if", "no", "schedulers", "were", "returned", "in", "meth", "lightning", "pytorch", "core", "lightningmodule", "configure_optimizers"], "docstring_summary": "Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 197, "end_line": 216, "hash": "2baf5e45861a327a2ad23aece23ffccb", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "example_input_array", "original_string": "def example_input_array(self) -> Optional[Union[Tensor, tuple, dict]]:\r\n        \"\"\"The example input array is a specification of what the module can consume in the :meth:`forward` method. The\r\n        return type is interpreted as follows:\r\n\r\n        -   Single tensor: It is assumed the model takes a single argument, i.e.,\r\n            ``model.forward(model.example_input_array)``\r\n        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\r\n            ``model.forward(*model.example_input_array)``\r\n        -   Dict: The input array represents named keyword arguments, i.e.,\r\n            ``model.forward(**model.example_input_array)``\r\n\r\n        \"\"\"\r\n        return self._example_input_array", "language": "python", "code": "def example_input_array(self) -> Optional[Union[Tensor, tuple, dict]]:\r\n        \"\"\"The example input array is a specification of what the module can consume in the :meth:`forward` method. The\r\n        return type is interpreted as follows:\r\n\r\n        -   Single tensor: It is assumed the model takes a single argument, i.e.,\r\n            ``model.forward(model.example_input_array)``\r\n        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\r\n            ``model.forward(*model.example_input_array)``\r\n        -   Dict: The input array represents named keyword arguments, i.e.,\r\n            ``model.forward(**model.example_input_array)``\r\n\r\n        \"\"\"\r\n        return self._example_input_array", "code_tokens": ["def", "example_input_array", "(", "self", ")", "-", ">", "Optional", "[", "Union", "[", "Tensor", ",", "tuple", ",", "dict", "]", "]", ":", "\"", "\"", "\"", "The", "example", "input", "array", "is", "a", "specification", "of", "what", "the", "module", "can", "consume", "in", "the", ":", "meth", ":", "`", "forward", "`", "method", ".", "The", "return", "type", "is", "interpreted", "as", "follows", ":", "-", "Single", "tensor", ":", "It", "is", "assumed", "the", "model", "takes", "a", "single", "argument", ",", "i", ".", "e", ".", ",", "`", "`", "model", ".", "forward", "(", "model", ".", "example_input_array", ")", "`", "`", "-", "Tuple", ":", "The", "input", "array", "should", "be", "interpreted", "as", "a", "sequence", "of", "positional", "arguments", ",", "i", ".", "e", ".", ",", "`", "`", "model", ".", "forward", "(", "*", "model", ".", "example_input_array", ")", "`", "`", "-", "Dict", ":", "The", "input", "array", "represents", "named", "keyword", "arguments", ",", "i", ".", "e", ".", ",", "`", "`", "model", ".", "forward", "(", "*", "*", "model", ".", "example_input_array", ")", "`", "`", "\"", "\"", "\"", "return", "self", ".", "_example_input_array"], "docstring": "The example input array is a specification of what the module can consume in the :meth:`forward` method. The\r\n        return type is interpreted as follows:\r\n\r\n        -   Single tensor: It is assumed the model takes a single argument, i.e.,\r\n            ``model.forward(model.example_input_array)``\r\n        -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\r\n            ``model.forward(*model.example_input_array)``\r\n        -   Dict: The input array represents named keyword arguments, i.e.,\r\n            ``model.forward(**model.example_input_array)``", "docstring_tokens": ["the", "example", "input", "array", "is", "a", "specification", "of", "what", "the", "module", "can", "consume", "in", "the", "meth", "forward", "method", "the", "return", "type", "is", "interpreted", "as", "follows", "single", "tensor", "it", "is", "assumed", "the", "model", "takes", "a", "single", "argument", "i", "e", "model", "forward", "model", "example_input_array", "tuple", "the", "input", "array", "should", "be", "interpreted", "as", "a", "sequence", "of", "positional", "arguments", "i", "e", "model", "forward", "model", "example_input_array", "dict", "the", "input", "array", "represents", "named", "keyword", "arguments", "i", "e", "model", "forward", "model", "example_input_array"], "docstring_summary": "The example input array is a specification of what the module can consume in the :meth:`forward` method. The", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 247, "end_line": 259, "hash": "d96e26a3570f55db19351836f20cb912", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "global_step", "original_string": "def global_step(self) -> int:\r\n        \"\"\"Total training batches seen across all epochs.\r\n\r\n        If no Trainer is attached, this property is 0.\r\n\r\n        \"\"\"\r\n        return self.trainer.global_step if self._trainer else 0", "language": "python", "code": "def global_step(self) -> int:\r\n        \"\"\"Total training batches seen across all epochs.\r\n\r\n        If no Trainer is attached, this property is 0.\r\n\r\n        \"\"\"\r\n        return self.trainer.global_step if self._trainer else 0", "code_tokens": ["def", "global_step", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Total", "training", "batches", "seen", "across", "all", "epochs", ".", "If", "no", "Trainer", "is", "attached", ",", "this", "property", "is", "0", ".", "\"", "\"", "\"", "return", "self", ".", "trainer", ".", "global_step", "if", "self", ".", "_trainer", "else", "0"], "docstring": "Total training batches seen across all epochs.\r\n\r\n        If no Trainer is attached, this property is 0.", "docstring_tokens": ["total", "training", "batches", "seen", "across", "all", "epochs", "if", "no", "trainer", "is", "attached", "this", "property", "is", "0"], "docstring_summary": "Total training batches seen across all epochs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 271, "end_line": 277, "hash": "2ee5a555cd0b24ec001c1ed3e671fe64", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "on_gpu", "original_string": "def on_gpu(self) -> bool:\r\n        \"\"\"Returns ``True`` if this model is currently located on a GPU.\r\n\r\n        Useful to set flags around the LightningModule for different CPU vs GPU behavior.\r\n\r\n        \"\"\"\r\n        return self.device.type == \"cuda\"", "language": "python", "code": "def on_gpu(self) -> bool:\r\n        \"\"\"Returns ``True`` if this model is currently located on a GPU.\r\n\r\n        Useful to set flags around the LightningModule for different CPU vs GPU behavior.\r\n\r\n        \"\"\"\r\n        return self.device.type == \"cuda\"", "code_tokens": ["def", "on_gpu", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Returns", "`", "`", "True", "`", "`", "if", "this", "model", "is", "currently", "located", "on", "a", "GPU", ".", "Useful", "to", "set", "flags", "around", "the", "LightningModule", "for", "different", "CPU", "vs", "GPU", "behavior", ".", "\"", "\"", "\"", "return", "self", ".", "device", ".", "type", "=", "=", "\"", "cuda", "\""], "docstring": "Returns ``True`` if this model is currently located on a GPU.\r\n\r\n        Useful to set flags around the LightningModule for different CPU vs GPU behavior.", "docstring_tokens": ["returns", "true", "if", "this", "model", "is", "currently", "located", "on", "a", "gpu", "useful", "to", "set", "flags", "around", "the", "lightningmodule", "for", "different", "cpu", "vs", "gpu", "behavior"], "docstring_summary": "Returns ``True`` if this model is currently located on a GPU.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 290, "end_line": 296, "hash": "0dc7196dc24df2a0a21ca50e360f2a2e", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "strict_loading", "original_string": "def strict_loading(self) -> bool:\r\n        \"\"\"Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.\"\"\"\r\n        # We use None as the default internally to determine whether the user has set a value\r\n        return self._strict_loading in (None, True)", "language": "python", "code": "def strict_loading(self) -> bool:\r\n        \"\"\"Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.\"\"\"\r\n        # We use None as the default internally to determine whether the user has set a value\r\n        return self._strict_loading in (None, True)", "code_tokens": ["def", "strict_loading", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Determines", "how", "Lightning", "loads", "this", "model", "using", "`", ".", "load_state_dict", "(", ".", ".", ".", ",", "strict", "=", "model", ".", "strict_loading", ")", "`", ".", "\"", "\"", "\"", "return", "self", ".", "_strict_loading", "in", "(", "None", ",", "True", ")"], "docstring": "Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.", "docstring_tokens": ["determines", "how", "lightning", "loads", "this", "model", "using", "load_state_dict", "strict", "model", "strict_loading"], "docstring_summary": "Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 308, "end_line": 311, "hash": "2b7824a14bafb48a2113dbc6bf8794ab", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "logger", "original_string": "def logger(self) -> Optional[Union[Logger, FabricLogger]]:\r\n        \"\"\"Reference to the logger object in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.logger\r\n        return self._trainer.logger if self._trainer is not None else None", "language": "python", "code": "def logger(self) -> Optional[Union[Logger, FabricLogger]]:\r\n        \"\"\"Reference to the logger object in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.logger\r\n        return self._trainer.logger if self._trainer is not None else None", "code_tokens": ["def", "logger", "(", "self", ")", "-", ">", "Optional", "[", "Union", "[", "Logger", ",", "FabricLogger", "]", "]", ":", "\"", "\"", "\"", "Reference", "to", "the", "logger", "object", "in", "the", "Trainer", ".", "\"", "\"", "\"", "if", "self", ".", "_fabric", "is", "not", "None", ":", "return", "self", ".", "_fabric", ".", "logger", "return", "self", ".", "_trainer", ".", "logger", "if", "self", ".", "_trainer", "is", "not", "None", "else", "None"], "docstring": "Reference to the logger object in the Trainer.", "docstring_tokens": ["reference", "to", "the", "logger", "object", "in", "the", "trainer"], "docstring_summary": "Reference to the logger object in the Trainer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 318, "end_line": 322, "hash": "487c60504dc3b414668ec3082cfb1712", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "loggers", "original_string": "def loggers(self) -> Union[list[Logger], list[FabricLogger]]:\r\n        \"\"\"Reference to the list of loggers in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.loggers\r\n        if self._trainer is not None:\r\n            return self._trainer.loggers\r\n        return []", "language": "python", "code": "def loggers(self) -> Union[list[Logger], list[FabricLogger]]:\r\n        \"\"\"Reference to the list of loggers in the Trainer.\"\"\"\r\n        if self._fabric is not None:\r\n            return self._fabric.loggers\r\n        if self._trainer is not None:\r\n            return self._trainer.loggers\r\n        return []", "code_tokens": ["def", "loggers", "(", "self", ")", "-", ">", "Union", "[", "list", "[", "Logger", "]", ",", "list", "[", "FabricLogger", "]", "]", ":", "\"", "\"", "\"", "Reference", "to", "the", "list", "of", "loggers", "in", "the", "Trainer", ".", "\"", "\"", "\"", "if", "self", ".", "_fabric", "is", "not", "None", ":", "return", "self", ".", "_fabric", ".", "loggers", "if", "self", ".", "_trainer", "is", "not", "None", ":", "return", "self", ".", "_trainer", ".", "loggers", "return", "[", "]"], "docstring": "Reference to the list of loggers in the Trainer.", "docstring_tokens": ["reference", "to", "the", "list", "of", "loggers", "in", "the", "trainer"], "docstring_summary": "Reference to the list of loggers in the Trainer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 325, "end_line": 331, "hash": "e1bfcd83b0dab10cd82995c6964a91b5", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "device_mesh", "original_string": "def device_mesh(self) -> Optional[\"DeviceMesh\"]:\r\n        \"\"\"Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule.\"\"\"\r\n        return self._device_mesh", "language": "python", "code": "def device_mesh(self) -> Optional[\"DeviceMesh\"]:\r\n        \"\"\"Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule.\"\"\"\r\n        return self._device_mesh", "code_tokens": ["def", "device_mesh", "(", "self", ")", "-", ">", "Optional", "[", "\"", "DeviceMesh", "\"", "]", ":", "\"", "\"", "\"", "Strategies", "like", "`", "`", "ModelParallelStrategy", "`", "`", "will", "create", "a", "device", "mesh", "that", "can", "be", "accessed", "in", "the", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "configure_model", "`", "hook", "to", "parallelize", "the", "LightningModule", ".", "\"", "\"", "\"", "return", "self", ".", "_device_mesh"], "docstring": "Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the\r\n        :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` hook to parallelize the LightningModule.", "docstring_tokens": ["strategies", "like", "modelparallelstrategy", "will", "create", "a", "device", "mesh", "that", "can", "be", "accessed", "in", "the", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "configure_model", "hook", "to", "parallelize", "the", "lightningmodule"], "docstring_summary": "Strategies like ``ModelParallelStrategy`` will create a device mesh that can be accessed in the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 334, "end_line": 337, "hash": "ee98b41e394a8070f0a7672aaa52be0b", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "print", "original_string": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.\r\n\r\n        Args:\r\n            *args: The thing to print. The same as for Python's built-in print function.\r\n            **kwargs: The same as for Python's built-in print function.\r\n\r\n        Example::\r\n\r\n            def forward(self, x):\r\n                self.print(x, 'in forward')\r\n\r\n        \"\"\"\r\n        if self.trainer.is_global_zero:\r\n            progress_bar = self.trainer.progress_bar_callback\r\n            if progress_bar is not None and progress_bar.is_enabled:\r\n                progress_bar.print(*args, **kwargs)\r\n            else:\r\n                print(*args, **kwargs)", "language": "python", "code": "def print(self, *args: Any, **kwargs: Any) -> None:\r\n        r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.\r\n\r\n        Args:\r\n            *args: The thing to print. The same as for Python's built-in print function.\r\n            **kwargs: The same as for Python's built-in print function.\r\n\r\n        Example::\r\n\r\n            def forward(self, x):\r\n                self.print(x, 'in forward')\r\n\r\n        \"\"\"\r\n        if self.trainer.is_global_zero:\r\n            progress_bar = self.trainer.progress_bar_callback\r\n            if progress_bar is not None and progress_bar.is_enabled:\r\n                progress_bar.print(*args, **kwargs)\r\n            else:\r\n                print(*args, **kwargs)", "code_tokens": ["def", "print", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Prints", "only", "from", "process", "0", ".", "Use", "this", "in", "any", "distributed", "mode", "to", "log", "only", "once", ".", "Args", ":", "*", "args", ":", "The", "thing", "to", "print", ".", "The", "same", "as", "for", "Python", "'", "s", "built", "-", "in", "print", "function", ".", "*", "*", "kwargs", ":", "The", "same", "as", "for", "Python", "'", "s", "built", "-", "in", "print", "function", ".", "Example", ":", ":", "def", "forward", "(", "self", ",", "x", ")", ":", "self", ".", "print", "(", "x", ",", "'", "in", "forward", "'", ")", "\"", "\"", "\"", "if", "self", ".", "trainer", ".", "is_global_zero", ":", "progress_bar", "=", "self", ".", "trainer", ".", "progress_bar_callback", "if", "progress_bar", "is", "not", "None", "and", "progress_bar", ".", "is_enabled", ":", "progress_bar", ".", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "print", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.\r\n\r\n        Args:\r\n            *args: The thing to print. The same as for Python's built-in print function.\r\n            **kwargs: The same as for Python's built-in print function.\r\n\r\n        Example::\r\n\r\n            def forward(self, x):\r\n                self.print(x, 'in forward')\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "prints", "only", "from", "process", "0", "use", "this", "in", "any", "distributed", "mode", "to", "log", "only", "once", "args", "args", "the", "thing", "to", "print", "the", "same", "as", "for", "python", "s", "built", "in", "print", "function", "kwargs", "the", "same", "as", "for", "python", "s", "built", "in", "print", "function", "example", "def", "forward", "self", "x", "self", "print", "x", "in", "forward"], "docstring_summary": "r\"\"\"Prints only from process 0. Use this in any distributed mode to log only once.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 365, "end_line": 383, "hash": "8557830636e5d82555a0d8efcbc6b8f2", "complexity": 4, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "log_dict", "original_string": "def log_dict(\r\n        self,\r\n        dictionary: Union[Mapping[str, _METRIC], MetricCollection],\r\n        prog_bar: bool = False,\r\n        logger: Optional[bool] = None,\r\n        on_step: Optional[bool] = None,\r\n        on_epoch: Optional[bool] = None,\r\n        reduce_fx: Union[str, Callable[[Any], Any]] = \"mean\",\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"Log a dictionary of values at once.\r\n\r\n        Example::\r\n\r\n            values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}\r\n            self.log_dict(values)\r\n\r\n        Args:\r\n            dictionary: key value pairs.\r\n                Keys must be identical across all processes if using DDP or any other distributed strategy.\r\n                The values can be a ``float``, ``Tensor``, ``Metric``, or ``MetricCollection``.\r\n            prog_bar: if ``True`` logs to the progress base.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step.\r\n                ``None`` auto-logs for training_step but not validation/test_step.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics.\r\n                ``None`` auto-logs for val/test step but not ``training_step``.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto-detach the graph\r\n            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the ddp group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple). If ``False``, user needs to give unique names for\r\n                each dataloader to not mix values.\r\n            batch_size: Current batch size. This will be directly inferred from the loaded batch,\r\n                but some data structures might need to explicitly provide it.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\r\n\r\n        \"\"\"\r\n        if self._fabric is not None:\r\n            return self._log_dict_through_fabric(dictionary=dictionary, logger=logger)\r\n\r\n        kwargs: dict[str, bool] = {}\r\n\r\n        if isinstance(dictionary, MetricCollection):\r\n            kwargs[\"keep_base\"] = False\r\n            if _TORCHMETRICS_GREATER_EQUAL_0_9_1 and dictionary._enable_compute_groups:\r\n                kwargs[\"copy_state\"] = False\r\n\r\n        for k, v in dictionary.items(**kwargs):\r\n            self.log(\r\n                name=k,\r\n                value=v,\r\n                prog_bar=prog_bar,\r\n                logger=logger,\r\n                on_step=on_step,\r\n                on_epoch=on_epoch,\r\n                reduce_fx=reduce_fx,\r\n                enable_graph=enable_graph,\r\n                sync_dist=sync_dist,\r\n                sync_dist_group=sync_dist_group,\r\n                add_dataloader_idx=add_dataloader_idx,\r\n                batch_size=batch_size,\r\n                rank_zero_only=rank_zero_only,\r\n            )\r\n        return None", "language": "python", "code": "def log_dict(\r\n        self,\r\n        dictionary: Union[Mapping[str, _METRIC], MetricCollection],\r\n        prog_bar: bool = False,\r\n        logger: Optional[bool] = None,\r\n        on_step: Optional[bool] = None,\r\n        on_epoch: Optional[bool] = None,\r\n        reduce_fx: Union[str, Callable[[Any], Any]] = \"mean\",\r\n        enable_graph: bool = False,\r\n        sync_dist: bool = False,\r\n        sync_dist_group: Optional[Any] = None,\r\n        add_dataloader_idx: bool = True,\r\n        batch_size: Optional[int] = None,\r\n        rank_zero_only: bool = False,\r\n    ) -> None:\r\n        \"\"\"Log a dictionary of values at once.\r\n\r\n        Example::\r\n\r\n            values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}\r\n            self.log_dict(values)\r\n\r\n        Args:\r\n            dictionary: key value pairs.\r\n                Keys must be identical across all processes if using DDP or any other distributed strategy.\r\n                The values can be a ``float``, ``Tensor``, ``Metric``, or ``MetricCollection``.\r\n            prog_bar: if ``True`` logs to the progress base.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step.\r\n                ``None`` auto-logs for training_step but not validation/test_step.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics.\r\n                ``None`` auto-logs for val/test step but not ``training_step``.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto-detach the graph\r\n            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the ddp group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple). If ``False``, user needs to give unique names for\r\n                each dataloader to not mix values.\r\n            batch_size: Current batch size. This will be directly inferred from the loaded batch,\r\n                but some data structures might need to explicitly provide it.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\r\n\r\n        \"\"\"\r\n        if self._fabric is not None:\r\n            return self._log_dict_through_fabric(dictionary=dictionary, logger=logger)\r\n\r\n        kwargs: dict[str, bool] = {}\r\n\r\n        if isinstance(dictionary, MetricCollection):\r\n            kwargs[\"keep_base\"] = False\r\n            if _TORCHMETRICS_GREATER_EQUAL_0_9_1 and dictionary._enable_compute_groups:\r\n                kwargs[\"copy_state\"] = False\r\n\r\n        for k, v in dictionary.items(**kwargs):\r\n            self.log(\r\n                name=k,\r\n                value=v,\r\n                prog_bar=prog_bar,\r\n                logger=logger,\r\n                on_step=on_step,\r\n                on_epoch=on_epoch,\r\n                reduce_fx=reduce_fx,\r\n                enable_graph=enable_graph,\r\n                sync_dist=sync_dist,\r\n                sync_dist_group=sync_dist_group,\r\n                add_dataloader_idx=add_dataloader_idx,\r\n                batch_size=batch_size,\r\n                rank_zero_only=rank_zero_only,\r\n            )\r\n        return None", "code_tokens": ["def", "log_dict", "(", "self", ",", "dictionary", ":", "Union", "[", "Mapping", "[", "str", ",", "_METRIC", "]", ",", "MetricCollection", "]", ",", "prog_bar", ":", "bool", "=", "False", ",", "logger", ":", "Optional", "[", "bool", "]", "=", "None", ",", "on_step", ":", "Optional", "[", "bool", "]", "=", "None", ",", "on_epoch", ":", "Optional", "[", "bool", "]", "=", "None", ",", "reduce_fx", ":", "Union", "[", "str", ",", "Callable", "[", "[", "Any", "]", ",", "Any", "]", "]", "=", "\"", "mean", "\"", ",", "enable_graph", ":", "bool", "=", "False", ",", "sync_dist", ":", "bool", "=", "False", ",", "sync_dist_group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "add_dataloader_idx", ":", "bool", "=", "True", ",", "batch_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "rank_zero_only", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "a", "dictionary", "of", "values", "at", "once", ".", "Example", ":", ":", "values", "=", "{", "'", "loss", "'", ":", "loss", ",", "'", "acc", "'", ":", "acc", ",", ".", ".", ".", ",", "'", "metric_n", "'", ":", "metric_n", "}", "self", ".", "log_dict", "(", "values", ")", "Args", ":", "dictionary", ":", "key", "value", "pairs", ".", "Keys", "must", "be", "identical", "across", "all", "processes", "if", "using", "DDP", "or", "any", "other", "distributed", "strategy", ".", "The", "values", "can", "be", "a", "`", "`", "float", "`", "`", ",", "`", "`", "Tensor", "`", "`", ",", "`", "`", "Metric", "`", "`", ",", "or", "`", "`", "MetricCollection", "`", "`", ".", "prog_bar", ":", "if", "`", "`", "True", "`", "`", "logs", "to", "the", "progress", "base", ".", "logger", ":", "if", "`", "`", "True", "`", "`", "logs", "to", "the", "logger", ".", "on_step", ":", "if", "`", "`", "True", "`", "`", "logs", "at", "this", "step", ".", "`", "`", "None", "`", "`", "auto", "-", "logs", "for", "training_step", "but", "not", "validation", "/", "test_step", ".", "The", "default", "value", "is", "determined", "by", "the", "hook", ".", "See", ":", "ref", ":", "`", "extensions", "/", "logging", ":", "Automatic", "Logging", "`", "for", "details", ".", "on_epoch", ":", "if", "`", "`", "True", "`", "`", "logs", "epoch", "accumulated", "metrics", ".", "`", "`", "None", "`", "`", "auto", "-", "logs", "for", "val", "/", "test", "step", "but", "not", "`", "`", "training_step", "`", "`", ".", "The", "default", "value", "is", "determined", "by", "the", "hook", ".", "See", ":", "ref", ":", "`", "extensions", "/", "logging", ":", "Automatic", "Logging", "`", "for", "details", ".", "reduce_fx", ":", "reduction", "function", "over", "step", "values", "for", "end", "of", "epoch", ".", ":", "meth", ":", "`", "torch", ".", "mean", "`", "by", "default", ".", "enable_graph", ":", "if", "`", "`", "True", "`", "`", ",", "will", "not", "auto", "-", "detach", "the", "graph", "sync_dist", ":", "if", "`", "`", "True", "`", "`", ",", "reduces", "the", "metric", "across", "GPUs", "/", "TPUs", ".", "Use", "with", "care", "as", "this", "may", "lead", "to", "a", "significant", "communication", "overhead", ".", "sync_dist_group", ":", "the", "ddp", "group", "to", "sync", "across", ".", "add_dataloader_idx", ":", "if", "`", "`", "True", "`", "`", ",", "appends", "the", "index", "of", "the", "current", "dataloader", "to", "the", "name", "(", "when", "using", "multiple", ")", ".", "If", "`", "`", "False", "`", "`", ",", "user", "needs", "to", "give", "unique", "names", "for", "each", "dataloader", "to", "not", "mix", "values", ".", "batch_size", ":", "Current", "batch", "size", ".", "This", "will", "be", "directly", "inferred", "from", "the", "loaded", "batch", ",", "but", "some", "data", "structures", "might", "need", "to", "explicitly", "provide", "it", ".", "rank_zero_only", ":", "Tells", "Lightning", "if", "you", "are", "calling", "`", "`", "self", ".", "log", "`", "`", "from", "every", "process", "(", "default", ")", "or", "only", "from", "rank", "0", ".", "If", "`", "`", "True", "`", "`", ",", "you", "won", "'", "t", "be", "able", "to", "use", "this", "metric", "as", "a", "monitor", "in", "callbacks", "(", "e", ".", "g", ".", ",", "early", "stopping", ")", ".", "Warning", ":", "Improper", "use", "can", "lead", "to", "deadlocks", "!", "See", ":", "ref", ":", "`", "Advanced", "Logging", "<", "visualize", "/", "logging_advanced", ":", "rank_zero_only", ">", "`", "for", "more", "details", ".", "\"", "\"", "\"", "if", "self", ".", "_fabric", "is", "not", "None", ":", "return", "self", ".", "_log_dict_through_fabric", "(", "dictionary", "=", "dictionary", ",", "logger", "=", "logger", ")", "kwargs", ":", "dict", "[", "str", ",", "bool", "]", "=", "{", "}", "if", "isinstance", "(", "dictionary", ",", "MetricCollection", ")", ":", "kwargs", "[", "\"", "keep_base", "\"", "]", "=", "False", "if", "_TORCHMETRICS_GREATER_EQUAL_0_9_1", "and", "dictionary", ".", "_enable_compute_groups", ":", "kwargs", "[", "\"", "copy_state", "\"", "]", "=", "False", "for", "k", ",", "v", "in", "dictionary", ".", "items", "(", "*", "*", "kwargs", ")", ":", "self", ".", "log", "(", "name", "=", "k", ",", "value", "=", "v", ",", "prog_bar", "=", "prog_bar", ",", "logger", "=", "logger", ",", "on_step", "=", "on_step", ",", "on_epoch", "=", "on_epoch", ",", "reduce_fx", "=", "reduce_fx", ",", "enable_graph", "=", "enable_graph", ",", "sync_dist", "=", "sync_dist", ",", "sync_dist_group", "=", "sync_dist_group", ",", "add_dataloader_idx", "=", "add_dataloader_idx", ",", "batch_size", "=", "batch_size", ",", "rank_zero_only", "=", "rank_zero_only", ",", ")", "return", "None"], "docstring": "Log a dictionary of values at once.\r\n\r\n        Example::\r\n\r\n            values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}\r\n            self.log_dict(values)\r\n\r\n        Args:\r\n            dictionary: key value pairs.\r\n                Keys must be identical across all processes if using DDP or any other distributed strategy.\r\n                The values can be a ``float``, ``Tensor``, ``Metric``, or ``MetricCollection``.\r\n            prog_bar: if ``True`` logs to the progress base.\r\n            logger: if ``True`` logs to the logger.\r\n            on_step: if ``True`` logs at this step.\r\n                ``None`` auto-logs for training_step but not validation/test_step.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            on_epoch: if ``True`` logs epoch accumulated metrics.\r\n                ``None`` auto-logs for val/test step but not ``training_step``.\r\n                The default value is determined by the hook.\r\n                See :ref:`extensions/logging:Automatic Logging` for details.\r\n            reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\r\n            enable_graph: if ``True``, will not auto-detach the graph\r\n            sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant\r\n                communication overhead.\r\n            sync_dist_group: the ddp group to sync across.\r\n            add_dataloader_idx: if ``True``, appends the index of the current dataloader to\r\n                the name (when using multiple). If ``False``, user needs to give unique names for\r\n                each dataloader to not mix values.\r\n            batch_size: Current batch size. This will be directly inferred from the loaded batch,\r\n                but some data structures might need to explicitly provide it.\r\n            rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\r\n                rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\r\n                (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\r\n                :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.", "docstring_tokens": ["log", "a", "dictionary", "of", "values", "at", "once", "example", "values", "loss", "loss", "acc", "acc", "metric_n", "metric_n", "self", "log_dict", "values", "args", "dictionary", "key", "value", "pairs", "keys", "must", "be", "identical", "across", "all", "processes", "if", "using", "ddp", "or", "any", "other", "distributed", "strategy", "the", "values", "can", "be", "a", "float", "tensor", "metric", "or", "metriccollection", "prog_bar", "if", "true", "logs", "to", "the", "progress", "base", "logger", "if", "true", "logs", "to", "the", "logger", "on_step", "if", "true", "logs", "at", "this", "step", "none", "auto", "logs", "for", "training_step", "but", "not", "validation", "test_step", "the", "default", "value", "is", "determined", "by", "the", "hook", "see", "ref", "extensions", "logging", "automatic", "logging", "for", "details", "on_epoch", "if", "true", "logs", "epoch", "accumulated", "metrics", "none", "auto", "logs", "for", "val", "test", "step", "but", "not", "training_step", "the", "default", "value", "is", "determined", "by", "the", "hook", "see", "ref", "extensions", "logging", "automatic", "logging", "for", "details", "reduce_fx", "reduction", "function", "over", "step", "values", "for", "end", "of", "epoch", "meth", "torch", "mean", "by", "default", "enable_graph", "if", "true", "will", "not", "auto", "detach", "the", "graph", "sync_dist", "if", "true", "reduces", "the", "metric", "across", "gpus", "tpus", "use", "with", "care", "as", "this", "may", "lead", "to", "a", "significant", "communication", "overhead", "sync_dist_group", "the", "ddp", "group", "to", "sync", "across", "add_dataloader_idx", "if", "true", "appends", "the", "index", "of", "the", "current", "dataloader", "to", "the", "name", "when", "using", "multiple", "if", "false", "user", "needs", "to", "give", "unique", "names", "for", "each", "dataloader", "to", "not", "mix", "values", "batch_size", "current", "batch", "size", "this", "will", "be", "directly", "inferred", "from", "the", "loaded", "batch", "but", "some", "data", "structures", "might", "need", "to", "explicitly", "provide", "it", "rank_zero_only", "tells", "lightning", "if", "you", "are", "calling", "self", "log", "from", "every", "process", "default", "or", "only", "from", "rank", "0", "if", "true", "you", "won", "t", "be", "able", "to", "use", "this", "metric", "as", "a", "monitor", "in", "callbacks", "e", "g", "early", "stopping", "warning", "improper", "use", "can", "lead", "to", "deadlocks", "see", "ref", "advanced", "logging", "visualize", "logging_advanced", "rank_zero_only", "for", "more", "details"], "docstring_summary": "Log a dictionary of values at once.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 551, "end_line": 629, "hash": "9607857e434efda85cc7ecfae614b2e2", "complexity": 6, "parameters": ["dictionary", "_METRIC]", "MetricCollection]", "prog_bar", "logger", "on_step", "on_epoch", "reduce_fx", "Callable[[Any]", "Any]]", "enable_graph", "sync_dist", "sync_dist_group", "add_dataloader_idx", "batch_size", "rank_zero_only"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "all_gather", "original_string": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        r\"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            sync_grads: flag that allows users to synchronize gradients for the all_gather operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        all_gather = self.trainer.strategy.all_gather\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, all_gather, group=group, sync_grads=sync_grads)", "language": "python", "code": "def all_gather(\r\n        self, data: Union[Tensor, dict, list, tuple], group: Optional[Any] = None, sync_grads: bool = False\r\n    ) -> Union[Tensor, dict, list, tuple]:\r\n        r\"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            sync_grads: flag that allows users to synchronize gradients for the all_gather operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"\r\n        group = group if group is not None else torch.distributed.group.WORLD\r\n        all_gather = self.trainer.strategy.all_gather\r\n        data = convert_to_tensors(data, device=self.device)\r\n        return apply_to_collection(data, Tensor, all_gather, group=group, sync_grads=sync_grads)", "code_tokens": ["def", "all_gather", "(", "self", ",", "data", ":", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "sync_grads", ":", "bool", "=", "False", ")", "-", ">", "Union", "[", "Tensor", ",", "dict", ",", "list", ",", "tuple", "]", ":", "r", "\"", "\"", "\"", "Gather", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes", ".", "This", "method", "needs", "to", "be", "called", "on", "all", "processes", "and", "the", "tensors", "need", "to", "have", "the", "same", "shape", "across", "all", "processes", ",", "otherwise", "your", "program", "will", "stall", "forever", ".", "Args", ":", "data", ":", "int", ",", "float", ",", "tensor", "of", "shape", "(", "batch", ",", ".", ".", ".", ")", ",", "or", "a", "(", "possibly", "nested", ")", "collection", "thereof", ".", "group", ":", "the", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "sync_grads", ":", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all_gather", "operation", "Return", ":", "A", "tensor", "of", "shape", "(", "world_size", ",", "batch", ",", ".", ".", ".", ")", ",", "or", "if", "the", "input", "was", "a", "collection", "the", "output", "will", "also", "be", "a", "collection", "with", "tensors", "of", "this", "shape", ".", "For", "the", "special", "case", "where", "world_size", "is", "1", ",", "no", "additional", "dimension", "is", "added", "to", "the", "tensor", "(", "s", ")", ".", "\"", "\"", "\"", "group", "=", "group", "if", "group", "is", "not", "None", "else", "torch", ".", "distributed", ".", "group", ".", "WORLD", "all_gather", "=", "self", ".", "trainer", ".", "strategy", ".", "all_gather", "data", "=", "convert_to_tensors", "(", "data", ",", "device", "=", "self", ".", "device", ")", "return", "apply_to_collection", "(", "data", ",", "Tensor", ",", "all_gather", ",", "group", "=", "group", ",", "sync_grads", "=", "sync_grads", ")"], "docstring": "r\"\"\"Gather tensors or collections of tensors from multiple processes.\r\n\r\n        This method needs to be called on all processes and the tensors need to have the same shape across all\r\n        processes, otherwise your program will stall forever.\r\n\r\n        Args:\r\n            data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            sync_grads: flag that allows users to synchronize gradients for the all_gather operation\r\n\r\n        Return:\r\n            A tensor of shape (world_size, batch, ...), or if the input was a collection\r\n            the output will also be a collection with tensors of this shape. For the special case where\r\n            world_size is 1, no additional dimension is added to the tensor(s).\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "gather", "tensors", "or", "collections", "of", "tensors", "from", "multiple", "processes", "this", "method", "needs", "to", "be", "called", "on", "all", "processes", "and", "the", "tensors", "need", "to", "have", "the", "same", "shape", "across", "all", "processes", "otherwise", "your", "program", "will", "stall", "forever", "args", "data", "int", "float", "tensor", "of", "shape", "batch", "or", "a", "possibly", "nested", "collection", "thereof", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "sync_grads", "flag", "that", "allows", "users", "to", "synchronize", "gradients", "for", "the", "all_gather", "operation", "return", "a", "tensor", "of", "shape", "world_size", "batch", "or", "if", "the", "input", "was", "a", "collection", "the", "output", "will", "also", "be", "a", "collection", "with", "tensors", "of", "this", "shape", "for", "the", "special", "case", "where", "world_size", "is", "1", "no", "additional", "dimension", "is", "added", "to", "the", "tensor", "s"], "docstring_summary": "r\"\"\"Gather tensors or collections of tensors from multiple processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 671, "end_line": 693, "hash": "8a6d786e7ec60e99f7141d49ec783404", "complexity": 2, "parameters": ["data", "dict", "list", "tuple]", "group", "sync_grads"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "forward", "original_string": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        r\"\"\"Same as :meth:`torch.nn.Module.forward`.\r\n\r\n        Args:\r\n            *args: Whatever you decide to pass into the forward method.\r\n            **kwargs: Keyword arguments are also possible.\r\n\r\n        Return:\r\n            Your model's output\r\n\r\n        \"\"\"\r\n        return super().forward(*args, **kwargs)", "language": "python", "code": "def forward(self, *args: Any, **kwargs: Any) -> Any:\r\n        r\"\"\"Same as :meth:`torch.nn.Module.forward`.\r\n\r\n        Args:\r\n            *args: Whatever you decide to pass into the forward method.\r\n            **kwargs: Keyword arguments are also possible.\r\n\r\n        Return:\r\n            Your model's output\r\n\r\n        \"\"\"\r\n        return super().forward(*args, **kwargs)", "code_tokens": ["def", "forward", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "r", "\"", "\"", "\"", "Same", "as", ":", "meth", ":", "`", "torch", ".", "nn", ".", "Module", ".", "forward", "`", ".", "Args", ":", "*", "args", ":", "Whatever", "you", "decide", "to", "pass", "into", "the", "forward", "method", ".", "*", "*", "kwargs", ":", "Keyword", "arguments", "are", "also", "possible", ".", "Return", ":", "Your", "model", "'", "s", "output", "\"", "\"", "\"", "return", "super", "(", ")", ".", "forward", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Same as :meth:`torch.nn.Module.forward`.\r\n\r\n        Args:\r\n            *args: Whatever you decide to pass into the forward method.\r\n            **kwargs: Keyword arguments are also possible.\r\n\r\n        Return:\r\n            Your model's output\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "same", "as", "meth", "torch", "nn", "module", "forward", "args", "args", "whatever", "you", "decide", "to", "pass", "into", "the", "forward", "method", "kwargs", "keyword", "arguments", "are", "also", "possible", "return", "your", "model", "s", "output"], "docstring_summary": "r\"\"\"Same as :meth:`torch.nn.Module.forward`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 696, "end_line": 707, "hash": "b12cd62ccf3936e7776b3500f5d11422", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "training_step", "original_string": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\r\n        logger.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of\r\n              automatic optimization.\r\n            - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for\r\n              multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\r\n              the loss is not required.\r\n\r\n        In this step you'd normally do the forward pass and calculate the loss for a batch.\r\n        You can also do fancier things like multiple forward passes or something model specific.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                x, y, z = batch\r\n                out = self.encoder(x)\r\n                loss = self.loss(out, x)\r\n                return loss\r\n\r\n        To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:\r\n\r\n        .. code-block:: python\r\n\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.automatic_optimization = False\r\n\r\n\r\n            # Multiple optimizers (e.g.: GANs)\r\n            def training_step(self, batch, batch_idx):\r\n                opt1, opt2 = self.optimizers()\r\n\r\n                # do training_step with encoder\r\n                ...\r\n                opt1.step()\r\n                # do training_step with decoder\r\n                ...\r\n                opt2.step()\r\n\r\n        Note:\r\n            When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically\r\n            normalized by ``accumulate_grad_batches`` internally.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`training_step` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def training_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\r\n        logger.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of\r\n              automatic optimization.\r\n            - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for\r\n              multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\r\n              the loss is not required.\r\n\r\n        In this step you'd normally do the forward pass and calculate the loss for a batch.\r\n        You can also do fancier things like multiple forward passes or something model specific.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                x, y, z = batch\r\n                out = self.encoder(x)\r\n                loss = self.loss(out, x)\r\n                return loss\r\n\r\n        To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:\r\n\r\n        .. code-block:: python\r\n\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.automatic_optimization = False\r\n\r\n\r\n            # Multiple optimizers (e.g.: GANs)\r\n            def training_step(self, batch, batch_idx):\r\n                opt1, opt2 = self.optimizers()\r\n\r\n                # do training_step with encoder\r\n                ...\r\n                opt1.step()\r\n                # do training_step with decoder\r\n                ...\r\n                opt2.step()\r\n\r\n        Note:\r\n            When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically\r\n            normalized by ``accumulate_grad_batches`` internally.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`training_step` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "training_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "r", "\"", "\"", "\"", "Here", "you", "compute", "and", "return", "the", "training", "loss", "and", "some", "additional", "metrics", "for", "e", ".", "g", ".", "the", "progress", "bar", "or", "logger", ".", "Args", ":", "batch", ":", "The", "output", "of", "your", "data", "iterable", ",", "normally", "a", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", ".", "batch_idx", ":", "The", "index", "of", "this", "batch", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", ".", "(", "only", "if", "multiple", "dataloaders", "used", ")", "Return", ":", "-", ":", "class", ":", "`", "~", "torch", ".", "Tensor", "`", "-", "The", "loss", "tensor", "-", "`", "`", "dict", "`", "`", "-", "A", "dictionary", "which", "can", "include", "any", "keys", ",", "but", "must", "include", "the", "key", "`", "`", "'", "loss", "'", "`", "`", "in", "the", "case", "of", "automatic", "optimization", ".", "-", "`", "`", "None", "`", "`", "-", "In", "automatic", "optimization", ",", "this", "will", "skip", "to", "the", "next", "batch", "(", "but", "is", "not", "supported", "for", "multi", "-", "GPU", ",", "TPU", ",", "or", "DeepSpeed", ")", ".", "For", "manual", "optimization", ",", "this", "has", "no", "special", "meaning", ",", "as", "returning", "the", "loss", "is", "not", "required", ".", "In", "this", "step", "you", "'", "d", "normally", "do", "the", "forward", "pass", "and", "calculate", "the", "loss", "for", "a", "batch", ".", "You", "can", "also", "do", "fancier", "things", "like", "multiple", "forward", "passes", "or", "something", "model", "specific", ".", "Example", ":", ":", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "x", ",", "y", ",", "z", "=", "batch", "out", "=", "self", ".", "encoder", "(", "x", ")", "loss", "=", "self", ".", "loss", "(", "out", ",", "x", ")", "return", "loss", "To", "use", "multiple", "optimizers", ",", "you", "can", "switch", "to", "'", "manual", "optimization", "'", "and", "control", "their", "stepping", ":", ".", ".", "code", "-", "block", ":", ":", "python", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "automatic_optimization", "=", "False", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "opt1", ",", "opt2", "=", "self", ".", "optimizers", "(", ")", ".", ".", ".", "opt1", ".", "step", "(", ")", ".", ".", ".", "opt2", ".", "step", "(", ")", "Note", ":", "When", "`", "`", "accumulate_grad_batches", "`", "`", ">", "1", ",", "the", "loss", "returned", "here", "will", "be", "automatically", "normalized", "by", "`", "`", "accumulate_grad_batches", "`", "`", "internally", ".", "\"", "\"", "\"", "rank_zero_warn", "(", "\"", "`", "training_step", "`", "must", "be", "implemented", "to", "be", "used", "with", "the", "Lightning", "Trainer", "\"", ")"], "docstring": "r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or\r\n        logger.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of\r\n              automatic optimization.\r\n            - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for\r\n              multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning\r\n              the loss is not required.\r\n\r\n        In this step you'd normally do the forward pass and calculate the loss for a batch.\r\n        You can also do fancier things like multiple forward passes or something model specific.\r\n\r\n        Example::\r\n\r\n            def training_step(self, batch, batch_idx):\r\n                x, y, z = batch\r\n                out = self.encoder(x)\r\n                loss = self.loss(out, x)\r\n                return loss\r\n\r\n        To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:\r\n\r\n        .. code-block:: python\r\n\r\n            def __init__(self):\r\n                super().__init__()\r\n                self.automatic_optimization = False\r\n\r\n\r\n            # Multiple optimizers (e.g.: GANs)\r\n            def training_step(self, batch, batch_idx):\r\n                opt1, opt2 = self.optimizers()\r\n\r\n                # do training_step with encoder\r\n                ...\r\n                opt1.step()\r\n                # do training_step with decoder\r\n                ...\r\n                opt2.step()\r\n\r\n        Note:\r\n            When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically\r\n            normalized by ``accumulate_grad_batches`` internally.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "here", "you", "compute", "and", "return", "the", "training", "loss", "and", "some", "additional", "metrics", "for", "e", "g", "the", "progress", "bar", "or", "logger", "args", "batch", "the", "output", "of", "your", "data", "iterable", "normally", "a", "class", "torch", "utils", "data", "dataloader", "batch_idx", "the", "index", "of", "this", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", "only", "if", "multiple", "dataloaders", "used", "return", "class", "torch", "tensor", "the", "loss", "tensor", "dict", "a", "dictionary", "which", "can", "include", "any", "keys", "but", "must", "include", "the", "key", "loss", "in", "the", "case", "of", "automatic", "optimization", "none", "in", "automatic", "optimization", "this", "will", "skip", "to", "the", "next", "batch", "but", "is", "not", "supported", "for", "multi", "gpu", "tpu", "or", "deepspeed", "for", "manual", "optimization", "this", "has", "no", "special", "meaning", "as", "returning", "the", "loss", "is", "not", "required", "in", "this", "step", "you", "d", "normally", "do", "the", "forward", "pass", "and", "calculate", "the", "loss", "for", "a", "batch", "you", "can", "also", "do", "fancier", "things", "like", "multiple", "forward", "passes", "or", "something", "model", "specific", "example", "def", "training_step", "self", "batch", "batch_idx", "x", "y", "z", "batch", "out", "self", "encoder", "x", "loss", "self", "loss", "out", "x", "return", "loss", "to", "use", "multiple", "optimizers", "you", "can", "switch", "to", "manual", "optimization", "and", "control", "their", "stepping", "code", "block", "python", "def", "__init__", "self", "super", "__init__", "self", "automatic_optimization", "false", "multiple", "optimizers", "e", "g", "gans", "def", "training_step", "self", "batch", "batch_idx", "opt1", "opt2", "self", "optimizers", "do", "training_step", "with", "encoder", "opt1", "step", "do", "training_step", "with", "decoder", "opt2", "step", "note", "when", "accumulate_grad_batches", "1", "the", "loss", "returned", "here", "will", "be", "automatically", "normalized", "by", "accumulate_grad_batches", "internally"], "docstring_summary": "r\"\"\"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 709, "end_line": 763, "hash": "140f38779e595e98252cbbd528ca7a6a", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "validation_step", "original_string": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or\r\n        calculate anything of interest like accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            # if you have one val dataloader:\r\n            def validation_step(self, batch, batch_idx): ...\r\n\r\n\r\n            # if you have multiple val dataloaders:\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            # CASE 1: A single validation dataset\r\n            def validation_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                # log 6 example images\r\n                # or generated text... or whatever\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs!\r\n                self.log_dict({'val_loss': loss, 'val_acc': val_acc})\r\n\r\n        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            # CASE 2: multiple validation dataloaders\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n                # dataloader_idx tells you which dataset this is.\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs separately for each dataloader\r\n                self.log_dict({f\"val_loss_{dataloader_idx}\": loss, f\"val_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to validate you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`validation_step` is called, the model has been put in eval mode\r\n            and PyTorch gradients have been disabled. At the end of validation,\r\n            the model goes back to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "language": "python", "code": "def validation_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or\r\n        calculate anything of interest like accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            # if you have one val dataloader:\r\n            def validation_step(self, batch, batch_idx): ...\r\n\r\n\r\n            # if you have multiple val dataloaders:\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            # CASE 1: A single validation dataset\r\n            def validation_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                # log 6 example images\r\n                # or generated text... or whatever\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs!\r\n                self.log_dict({'val_loss': loss, 'val_acc': val_acc})\r\n\r\n        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            # CASE 2: multiple validation dataloaders\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n                # dataloader_idx tells you which dataset this is.\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs separately for each dataloader\r\n                self.log_dict({f\"val_loss_{dataloader_idx}\": loss, f\"val_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to validate you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`validation_step` is called, the model has been put in eval mode\r\n            and PyTorch gradients have been disabled. At the end of validation,\r\n            the model goes back to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "code_tokens": ["def", "validation_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "r", "\"", "\"", "\"", "Operates", "on", "a", "single", "batch", "of", "data", "from", "the", "validation", "set", ".", "In", "this", "step", "you", "'", "d", "might", "generate", "examples", "or", "calculate", "anything", "of", "interest", "like", "accuracy", ".", "Args", ":", "batch", ":", "The", "output", "of", "your", "data", "iterable", ",", "normally", "a", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", ".", "batch_idx", ":", "The", "index", "of", "this", "batch", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", ".", "(", "only", "if", "multiple", "dataloaders", "used", ")", "Return", ":", "-", ":", "class", ":", "`", "~", "torch", ".", "Tensor", "`", "-", "The", "loss", "tensor", "-", "`", "`", "dict", "`", "`", "-", "A", "dictionary", ".", "Can", "include", "any", "keys", ",", "but", "must", "include", "the", "key", "`", "`", "'", "loss", "'", "`", "`", ".", "-", "`", "`", "None", "`", "`", "-", "Skip", "to", "the", "next", "batch", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", ".", ".", ".", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "=", "0", ")", ":", ".", ".", ".", "Examples", ":", ":", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "x", ",", "y", "=", "batch", "out", "=", "self", "(", "x", ")", "loss", "=", "self", ".", "loss", "(", "out", ",", "y", ")", "sample_imgs", "=", "x", "[", ":", "6", "]", "grid", "=", "torchvision", ".", "utils", ".", "make_grid", "(", "sample_imgs", ")", "self", ".", "logger", ".", "experiment", ".", "add_image", "(", "'", "example_images", "'", ",", "grid", ",", "0", ")", "labels_hat", "=", "torch", ".", "argmax", "(", "out", ",", "dim", "=", "1", ")", "val_acc", "=", "torch", ".", "sum", "(", "y", "=", "=", "labels_hat", ")", ".", "item", "(", ")", "/", "(", "len", "(", "y", ")", "*", "1", ".", "0", ")", "self", ".", "log_dict", "(", "{", "'", "val_loss", "'", ":", "loss", ",", "'", "val_acc", "'", ":", "val_acc", "}", ")", "If", "you", "pass", "in", "multiple", "val", "dataloaders", ",", ":", "meth", ":", "`", "validation_step", "`", "will", "have", "an", "additional", "argument", ".", "We", "recommend", "setting", "the", "default", "value", "of", "0", "so", "that", "you", "can", "quickly", "switch", "between", "single", "and", "multiple", "dataloaders", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "=", "0", ")", ":", "x", ",", "y", "=", "batch", "out", "=", "self", "(", "x", ")", "if", "dataloader_idx", "=", "=", "0", ":", "loss", "=", "self", ".", "loss0", "(", "out", ",", "y", ")", "else", ":", "loss", "=", "self", ".", "loss1", "(", "out", ",", "y", ")", "labels_hat", "=", "torch", ".", "argmax", "(", "out", ",", "dim", "=", "1", ")", "acc", "=", "torch", ".", "sum", "(", "y", "=", "=", "labels_hat", ")", ".", "item", "(", ")", "/", "(", "len", "(", "y", ")", "*", "1", ".", "0", ")", "self", ".", "log_dict", "(", "{", "f", "\"", "val_loss_", "{", "dataloader_idx", "}", "\"", ":", "loss", ",", "f", "\"", "val_acc_", "{", "dataloader_idx", "}", "\"", ":", "acc", "}", ")", "Note", ":", "If", "you", "don", "'", "t", "need", "to", "validate", "you", "don", "'", "t", "need", "to", "implement", "this", "method", ".", "Note", ":", "When", "the", ":", "meth", ":", "`", "validation_step", "`", "is", "called", ",", "the", "model", "has", "been", "put", "in", "eval", "mode", "and", "PyTorch", "gradients", "have", "been", "disabled", ".", "At", "the", "end", "of", "validation", ",", "the", "model", "goes", "back", "to", "training", "mode", "and", "gradients", "are", "enabled", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or\r\n        calculate anything of interest like accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            # if you have one val dataloader:\r\n            def validation_step(self, batch, batch_idx): ...\r\n\r\n\r\n            # if you have multiple val dataloaders:\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            # CASE 1: A single validation dataset\r\n            def validation_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                # log 6 example images\r\n                # or generated text... or whatever\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs!\r\n                self.log_dict({'val_loss': loss, 'val_acc': val_acc})\r\n\r\n        If you pass in multiple val dataloaders, :meth:`validation_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            # CASE 2: multiple validation dataloaders\r\n            def validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n                # dataloader_idx tells you which dataset this is.\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs separately for each dataloader\r\n                self.log_dict({f\"val_loss_{dataloader_idx}\": loss, f\"val_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to validate you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`validation_step` is called, the model has been put in eval mode\r\n            and PyTorch gradients have been disabled. At the end of validation,\r\n            the model goes back to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "operates", "on", "a", "single", "batch", "of", "data", "from", "the", "validation", "set", "in", "this", "step", "you", "d", "might", "generate", "examples", "or", "calculate", "anything", "of", "interest", "like", "accuracy", "args", "batch", "the", "output", "of", "your", "data", "iterable", "normally", "a", "class", "torch", "utils", "data", "dataloader", "batch_idx", "the", "index", "of", "this", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", "only", "if", "multiple", "dataloaders", "used", "return", "class", "torch", "tensor", "the", "loss", "tensor", "dict", "a", "dictionary", "can", "include", "any", "keys", "but", "must", "include", "the", "key", "loss", "none", "skip", "to", "the", "next", "batch", "code", "block", "python", "if", "you", "have", "one", "val", "dataloader", "def", "validation_step", "self", "batch", "batch_idx", "if", "you", "have", "multiple", "val", "dataloaders", "def", "validation_step", "self", "batch", "batch_idx", "dataloader_idx", "0", "examples", "case", "1", "a", "single", "validation", "dataset", "def", "validation_step", "self", "batch", "batch_idx", "x", "y", "batch", "implement", "your", "own", "out", "self", "x", "loss", "self", "loss", "out", "y", "log", "6", "example", "images", "or", "generated", "text", "or", "whatever", "sample_imgs", "x", "6", "grid", "torchvision", "utils", "make_grid", "sample_imgs", "self", "logger", "experiment", "add_image", "example_images", "grid", "0", "calculate", "acc", "labels_hat", "torch", "argmax", "out", "dim", "1", "val_acc", "torch", "sum", "y", "labels_hat", "item", "len", "y", "1", "0", "log", "the", "outputs", "self", "log_dict", "val_loss", "loss", "val_acc", "val_acc", "if", "you", "pass", "in", "multiple", "val", "dataloaders", "meth", "validation_step", "will", "have", "an", "additional", "argument", "we", "recommend", "setting", "the", "default", "value", "of", "0", "so", "that", "you", "can", "quickly", "switch", "between", "single", "and", "multiple", "dataloaders", "code", "block", "python", "case", "2", "multiple", "validation", "dataloaders", "def", "validation_step", "self", "batch", "batch_idx", "dataloader_idx", "0", "dataloader_idx", "tells", "you", "which", "dataset", "this", "is", "x", "y", "batch", "implement", "your", "own", "out", "self", "x", "if", "dataloader_idx", "0", "loss", "self", "loss0", "out", "y", "else", "loss", "self", "loss1", "out", "y", "calculate", "acc", "labels_hat", "torch", "argmax", "out", "dim", "1", "acc", "torch", "sum", "y", "labels_hat", "item", "len", "y", "1", "0", "log", "the", "outputs", "separately", "for", "each", "dataloader", "self", "log_dict", "f", "val_loss_", "dataloader_idx", "loss", "f", "val_acc_", "dataloader_idx", "acc", "note", "if", "you", "don", "t", "need", "to", "validate", "you", "don", "t", "need", "to", "implement", "this", "method", "note", "when", "the", "meth", "validation_step", "is", "called", "the", "model", "has", "been", "put", "in", "eval", "mode", "and", "pytorch", "gradients", "have", "been", "disabled", "at", "the", "end", "of", "validation", "the", "model", "goes", "back", "to", "training", "mode", "and", "gradients", "are", "enabled"], "docstring_summary": "r\"\"\"Operates on a single batch of data from the validation set. In this step you'd might generate examples or", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 765, "end_line": 845, "hash": "09e53da54e7886b1f5791f2fd23cb55f", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "test_step", "original_string": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or\r\n        calculate anything of interest such as accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            # if you have one test dataloader:\r\n            def test_step(self, batch, batch_idx): ...\r\n\r\n\r\n            # if you have multiple test dataloaders:\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            # CASE 1: A single test dataset\r\n            def test_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                # log 6 example images\r\n                # or generated text... or whatever\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs!\r\n                self.log_dict({'test_loss': loss, 'test_acc': test_acc})\r\n\r\n        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            # CASE 2: multiple test dataloaders\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0):\r\n                # dataloader_idx tells you which dataset this is.\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs separately for each dataloader\r\n                self.log_dict({f\"test_loss_{dataloader_idx}\": loss, f\"test_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to test you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`test_step` is called, the model has been put in eval mode and\r\n            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\r\n            to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "language": "python", "code": "def test_step(self, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\r\n        r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or\r\n        calculate anything of interest such as accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            # if you have one test dataloader:\r\n            def test_step(self, batch, batch_idx): ...\r\n\r\n\r\n            # if you have multiple test dataloaders:\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            # CASE 1: A single test dataset\r\n            def test_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                # log 6 example images\r\n                # or generated text... or whatever\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs!\r\n                self.log_dict({'test_loss': loss, 'test_acc': test_acc})\r\n\r\n        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            # CASE 2: multiple test dataloaders\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0):\r\n                # dataloader_idx tells you which dataset this is.\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs separately for each dataloader\r\n                self.log_dict({f\"test_loss_{dataloader_idx}\": loss, f\"test_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to test you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`test_step` is called, the model has been put in eval mode and\r\n            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\r\n            to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "code_tokens": ["def", "test_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "STEP_OUTPUT", ":", "r", "\"", "\"", "\"", "Operates", "on", "a", "single", "batch", "of", "data", "from", "the", "test", "set", ".", "In", "this", "step", "you", "'", "d", "normally", "generate", "examples", "or", "calculate", "anything", "of", "interest", "such", "as", "accuracy", ".", "Args", ":", "batch", ":", "The", "output", "of", "your", "data", "iterable", ",", "normally", "a", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", ".", "batch_idx", ":", "The", "index", "of", "this", "batch", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", ".", "(", "only", "if", "multiple", "dataloaders", "used", ")", "Return", ":", "-", ":", "class", ":", "`", "~", "torch", ".", "Tensor", "`", "-", "The", "loss", "tensor", "-", "`", "`", "dict", "`", "`", "-", "A", "dictionary", ".", "Can", "include", "any", "keys", ",", "but", "must", "include", "the", "key", "`", "`", "'", "loss", "'", "`", "`", ".", "-", "`", "`", "None", "`", "`", "-", "Skip", "to", "the", "next", "batch", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", ".", ".", ".", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "=", "0", ")", ":", ".", ".", ".", "Examples", ":", ":", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "x", ",", "y", "=", "batch", "out", "=", "self", "(", "x", ")", "loss", "=", "self", ".", "loss", "(", "out", ",", "y", ")", "sample_imgs", "=", "x", "[", ":", "6", "]", "grid", "=", "torchvision", ".", "utils", ".", "make_grid", "(", "sample_imgs", ")", "self", ".", "logger", ".", "experiment", ".", "add_image", "(", "'", "example_images", "'", ",", "grid", ",", "0", ")", "labels_hat", "=", "torch", ".", "argmax", "(", "out", ",", "dim", "=", "1", ")", "test_acc", "=", "torch", ".", "sum", "(", "y", "=", "=", "labels_hat", ")", ".", "item", "(", ")", "/", "(", "len", "(", "y", ")", "*", "1", ".", "0", ")", "self", ".", "log_dict", "(", "{", "'", "test_loss", "'", ":", "loss", ",", "'", "test_acc", "'", ":", "test_acc", "}", ")", "If", "you", "pass", "in", "multiple", "test", "dataloaders", ",", ":", "meth", ":", "`", "test_step", "`", "will", "have", "an", "additional", "argument", ".", "We", "recommend", "setting", "the", "default", "value", "of", "0", "so", "that", "you", "can", "quickly", "switch", "between", "single", "and", "multiple", "dataloaders", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "=", "0", ")", ":", "x", ",", "y", "=", "batch", "out", "=", "self", "(", "x", ")", "if", "dataloader_idx", "=", "=", "0", ":", "loss", "=", "self", ".", "loss0", "(", "out", ",", "y", ")", "else", ":", "loss", "=", "self", ".", "loss1", "(", "out", ",", "y", ")", "labels_hat", "=", "torch", ".", "argmax", "(", "out", ",", "dim", "=", "1", ")", "acc", "=", "torch", ".", "sum", "(", "y", "=", "=", "labels_hat", ")", ".", "item", "(", ")", "/", "(", "len", "(", "y", ")", "*", "1", ".", "0", ")", "self", ".", "log_dict", "(", "{", "f", "\"", "test_loss_", "{", "dataloader_idx", "}", "\"", ":", "loss", ",", "f", "\"", "test_acc_", "{", "dataloader_idx", "}", "\"", ":", "acc", "}", ")", "Note", ":", "If", "you", "don", "'", "t", "need", "to", "test", "you", "don", "'", "t", "need", "to", "implement", "this", "method", ".", "Note", ":", "When", "the", ":", "meth", ":", "`", "test_step", "`", "is", "called", ",", "the", "model", "has", "been", "put", "in", "eval", "mode", "and", "PyTorch", "gradients", "have", "been", "disabled", ".", "At", "the", "end", "of", "the", "test", "epoch", ",", "the", "model", "goes", "back", "to", "training", "mode", "and", "gradients", "are", "enabled", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or\r\n        calculate anything of interest such as accuracy.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            - :class:`~torch.Tensor` - The loss tensor\r\n            - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``.\r\n            - ``None`` - Skip to the next batch.\r\n\r\n        .. code-block:: python\r\n\r\n            # if you have one test dataloader:\r\n            def test_step(self, batch, batch_idx): ...\r\n\r\n\r\n            # if you have multiple test dataloaders:\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0): ...\r\n\r\n        Examples::\r\n\r\n            # CASE 1: A single test dataset\r\n            def test_step(self, batch, batch_idx):\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n                loss = self.loss(out, y)\r\n\r\n                # log 6 example images\r\n                # or generated text... or whatever\r\n                sample_imgs = x[:6]\r\n                grid = torchvision.utils.make_grid(sample_imgs)\r\n                self.logger.experiment.add_image('example_images', grid, 0)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs!\r\n                self.log_dict({'test_loss': loss, 'test_acc': test_acc})\r\n\r\n        If you pass in multiple test dataloaders, :meth:`test_step` will have an additional argument. We recommend\r\n        setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.\r\n\r\n        .. code-block:: python\r\n\r\n            # CASE 2: multiple test dataloaders\r\n            def test_step(self, batch, batch_idx, dataloader_idx=0):\r\n                # dataloader_idx tells you which dataset this is.\r\n                x, y = batch\r\n\r\n                # implement your own\r\n                out = self(x)\r\n\r\n                if dataloader_idx == 0:\r\n                    loss = self.loss0(out, y)\r\n                else:\r\n                    loss = self.loss1(out, y)\r\n\r\n                # calculate acc\r\n                labels_hat = torch.argmax(out, dim=1)\r\n                acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\r\n\r\n                # log the outputs separately for each dataloader\r\n                self.log_dict({f\"test_loss_{dataloader_idx}\": loss, f\"test_acc_{dataloader_idx}\": acc})\r\n\r\n        Note:\r\n            If you don't need to test you don't need to implement this method.\r\n\r\n        Note:\r\n            When the :meth:`test_step` is called, the model has been put in eval mode and\r\n            PyTorch gradients have been disabled. At the end of the test epoch, the model goes back\r\n            to training mode and gradients are enabled.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "operates", "on", "a", "single", "batch", "of", "data", "from", "the", "test", "set", "in", "this", "step", "you", "d", "normally", "generate", "examples", "or", "calculate", "anything", "of", "interest", "such", "as", "accuracy", "args", "batch", "the", "output", "of", "your", "data", "iterable", "normally", "a", "class", "torch", "utils", "data", "dataloader", "batch_idx", "the", "index", "of", "this", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", "only", "if", "multiple", "dataloaders", "used", "return", "class", "torch", "tensor", "the", "loss", "tensor", "dict", "a", "dictionary", "can", "include", "any", "keys", "but", "must", "include", "the", "key", "loss", "none", "skip", "to", "the", "next", "batch", "code", "block", "python", "if", "you", "have", "one", "test", "dataloader", "def", "test_step", "self", "batch", "batch_idx", "if", "you", "have", "multiple", "test", "dataloaders", "def", "test_step", "self", "batch", "batch_idx", "dataloader_idx", "0", "examples", "case", "1", "a", "single", "test", "dataset", "def", "test_step", "self", "batch", "batch_idx", "x", "y", "batch", "implement", "your", "own", "out", "self", "x", "loss", "self", "loss", "out", "y", "log", "6", "example", "images", "or", "generated", "text", "or", "whatever", "sample_imgs", "x", "6", "grid", "torchvision", "utils", "make_grid", "sample_imgs", "self", "logger", "experiment", "add_image", "example_images", "grid", "0", "calculate", "acc", "labels_hat", "torch", "argmax", "out", "dim", "1", "test_acc", "torch", "sum", "y", "labels_hat", "item", "len", "y", "1", "0", "log", "the", "outputs", "self", "log_dict", "test_loss", "loss", "test_acc", "test_acc", "if", "you", "pass", "in", "multiple", "test", "dataloaders", "meth", "test_step", "will", "have", "an", "additional", "argument", "we", "recommend", "setting", "the", "default", "value", "of", "0", "so", "that", "you", "can", "quickly", "switch", "between", "single", "and", "multiple", "dataloaders", "code", "block", "python", "case", "2", "multiple", "test", "dataloaders", "def", "test_step", "self", "batch", "batch_idx", "dataloader_idx", "0", "dataloader_idx", "tells", "you", "which", "dataset", "this", "is", "x", "y", "batch", "implement", "your", "own", "out", "self", "x", "if", "dataloader_idx", "0", "loss", "self", "loss0", "out", "y", "else", "loss", "self", "loss1", "out", "y", "calculate", "acc", "labels_hat", "torch", "argmax", "out", "dim", "1", "acc", "torch", "sum", "y", "labels_hat", "item", "len", "y", "1", "0", "log", "the", "outputs", "separately", "for", "each", "dataloader", "self", "log_dict", "f", "test_loss_", "dataloader_idx", "loss", "f", "test_acc_", "dataloader_idx", "acc", "note", "if", "you", "don", "t", "need", "to", "test", "you", "don", "t", "need", "to", "implement", "this", "method", "note", "when", "the", "meth", "test_step", "is", "called", "the", "model", "has", "been", "put", "in", "eval", "mode", "and", "pytorch", "gradients", "have", "been", "disabled", "at", "the", "end", "of", "the", "test", "epoch", "the", "model", "goes", "back", "to", "training", "mode", "and", "gradients", "are", "enabled"], "docstring_summary": "r\"\"\"Operates on a single batch of data from the test set. In this step you'd normally generate examples or", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 847, "end_line": 927, "hash": "2ad3d5beac3309c854a187891a20e776", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "predict_step", "original_string": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls\r\n        :meth:`~lightning.pytorch.core.LightningModule.forward`. Override to add any processing logic.\r\n\r\n        The :meth:`~lightning.pytorch.core.LightningModule.predict_step` is used\r\n        to scale inference on multi-devices.\r\n\r\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\r\n        callback to write the predictions to disk or database after each batch or on epoch end.\r\n\r\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\r\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\r\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            Predicted output (optional).\r\n\r\n        Example ::\r\n\r\n            class MyModel(LightningModule):\r\n\r\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n                    return self(batch)\r\n\r\n            dm = ...\r\n            model = MyModel()\r\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\r\n            predictions = trainer.predict(model, dm)\r\n\r\n        \"\"\"\r\n        # For backwards compatibility\r\n        batch = kwargs.get(\"batch\", args[0])\r\n        return self(batch)", "language": "python", "code": "def predict_step(self, *args: Any, **kwargs: Any) -> Any:\r\n        \"\"\"Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls\r\n        :meth:`~lightning.pytorch.core.LightningModule.forward`. Override to add any processing logic.\r\n\r\n        The :meth:`~lightning.pytorch.core.LightningModule.predict_step` is used\r\n        to scale inference on multi-devices.\r\n\r\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\r\n        callback to write the predictions to disk or database after each batch or on epoch end.\r\n\r\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\r\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\r\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            Predicted output (optional).\r\n\r\n        Example ::\r\n\r\n            class MyModel(LightningModule):\r\n\r\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n                    return self(batch)\r\n\r\n            dm = ...\r\n            model = MyModel()\r\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\r\n            predictions = trainer.predict(model, dm)\r\n\r\n        \"\"\"\r\n        # For backwards compatibility\r\n        batch = kwargs.get(\"batch\", args[0])\r\n        return self(batch)", "code_tokens": ["def", "predict_step", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Step", "function", "called", "during", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", ".", "predict", "`", ".", "By", "default", ",", "it", "calls", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "forward", "`", ".", "Override", "to", "add", "any", "processing", "logic", ".", "The", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "predict_step", "`", "is", "used", "to", "scale", "inference", "on", "multi", "-", "devices", ".", "To", "prevent", "an", "OOM", "error", ",", "it", "is", "possible", "to", "use", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "BasePredictionWriter", "`", "callback", "to", "write", "the", "predictions", "to", "disk", "or", "database", "after", "each", "batch", "or", "on", "epoch", "end", ".", "The", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "BasePredictionWriter", "`", "should", "be", "used", "while", "using", "a", "spawn", "based", "accelerator", ".", "This", "happens", "for", "`", "`", "Trainer", "(", "strategy", "=", "\"", "ddp_spawn", "\"", ")", "`", "`", "or", "training", "on", "8", "TPU", "cores", "with", "`", "`", "Trainer", "(", "accelerator", "=", "\"", "tpu", "\"", ",", "devices", "=", "8", ")", "`", "`", "as", "predictions", "won", "'", "t", "be", "returned", ".", "Args", ":", "batch", ":", "The", "output", "of", "your", "data", "iterable", ",", "normally", "a", ":", "class", ":", "`", "~", "torch", ".", "utils", ".", "data", ".", "DataLoader", "`", ".", "batch_idx", ":", "The", "index", "of", "this", "batch", ".", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", ".", "(", "only", "if", "multiple", "dataloaders", "used", ")", "Return", ":", "Predicted", "output", "(", "optional", ")", ".", "Example", ":", ":", "class", "MyModel", "(", "LightningModule", ")", ":", "def", "predict_step", "(", "self", ",", "batch", ",", "batch_idx", ",", "dataloader_idx", "=", "0", ")", ":", "return", "self", "(", "batch", ")", "dm", "=", ".", ".", ".", "model", "=", "MyModel", "(", ")", "trainer", "=", "Trainer", "(", "accelerator", "=", "\"", "gpu", "\"", ",", "devices", "=", "2", ")", "predictions", "=", "trainer", ".", "predict", "(", "model", ",", "dm", ")", "\"", "\"", "\"", "batch", "=", "kwargs", ".", "get", "(", "\"", "batch", "\"", ",", "args", "[", "0", "]", ")", "return", "self", "(", "batch", ")"], "docstring": "Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls\r\n        :meth:`~lightning.pytorch.core.LightningModule.forward`. Override to add any processing logic.\r\n\r\n        The :meth:`~lightning.pytorch.core.LightningModule.predict_step` is used\r\n        to scale inference on multi-devices.\r\n\r\n        To prevent an OOM error, it is possible to use :class:`~lightning.pytorch.callbacks.BasePredictionWriter`\r\n        callback to write the predictions to disk or database after each batch or on epoch end.\r\n\r\n        The :class:`~lightning.pytorch.callbacks.BasePredictionWriter` should be used while using a spawn\r\n        based accelerator. This happens for ``Trainer(strategy=\"ddp_spawn\")``\r\n        or training on 8 TPU cores with ``Trainer(accelerator=\"tpu\", devices=8)`` as predictions won't be returned.\r\n\r\n        Args:\r\n            batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.\r\n            batch_idx: The index of this batch.\r\n            dataloader_idx: The index of the dataloader that produced this batch.\r\n                (only if multiple dataloaders used)\r\n\r\n        Return:\r\n            Predicted output (optional).\r\n\r\n        Example ::\r\n\r\n            class MyModel(LightningModule):\r\n\r\n                def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n                    return self(batch)\r\n\r\n            dm = ...\r\n            model = MyModel()\r\n            trainer = Trainer(accelerator=\"gpu\", devices=2)\r\n            predictions = trainer.predict(model, dm)", "docstring_tokens": ["step", "function", "called", "during", "meth", "lightning", "pytorch", "trainer", "trainer", "trainer", "predict", "by", "default", "it", "calls", "meth", "lightning", "pytorch", "core", "lightningmodule", "forward", "override", "to", "add", "any", "processing", "logic", "the", "meth", "lightning", "pytorch", "core", "lightningmodule", "predict_step", "is", "used", "to", "scale", "inference", "on", "multi", "devices", "to", "prevent", "an", "oom", "error", "it", "is", "possible", "to", "use", "class", "lightning", "pytorch", "callbacks", "basepredictionwriter", "callback", "to", "write", "the", "predictions", "to", "disk", "or", "database", "after", "each", "batch", "or", "on", "epoch", "end", "the", "class", "lightning", "pytorch", "callbacks", "basepredictionwriter", "should", "be", "used", "while", "using", "a", "spawn", "based", "accelerator", "this", "happens", "for", "trainer", "strategy", "ddp_spawn", "or", "training", "on", "8", "tpu", "cores", "with", "trainer", "accelerator", "tpu", "devices", "8", "as", "predictions", "won", "t", "be", "returned", "args", "batch", "the", "output", "of", "your", "data", "iterable", "normally", "a", "class", "torch", "utils", "data", "dataloader", "batch_idx", "the", "index", "of", "this", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "that", "produced", "this", "batch", "only", "if", "multiple", "dataloaders", "used", "return", "predicted", "output", "optional", "example", "class", "mymodel", "lightningmodule", "def", "predict_step", "self", "batch", "batch_idx", "dataloader_idx", "0", "return", "self", "batch", "dm", "model", "mymodel", "trainer", "trainer", "accelerator", "gpu", "devices", "2", "predictions", "trainer", "predict", "model", "dm"], "docstring_summary": "Step function called during :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`. By default, it calls", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 929, "end_line": 967, "hash": "1cfaaafef5f56a842c2610e793274cae", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "configure_callbacks", "original_string": "def configure_callbacks(self) -> Union[Sequence[Callback], Callback]:\r\n        \"\"\"Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\r\n        called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainer's\r\n        ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already\r\n        present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will\r\n        make sure :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last.\r\n\r\n        Return:\r\n            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.\r\n\r\n        Example::\r\n\r\n            def configure_callbacks(self):\r\n                early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\r\n                checkpoint = ModelCheckpoint(monitor=\"val_loss\")\r\n                return [early_stop, checkpoint]\r\n\r\n        \"\"\"\r\n        return []", "language": "python", "code": "def configure_callbacks(self) -> Union[Sequence[Callback], Callback]:\r\n        \"\"\"Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\r\n        called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainer's\r\n        ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already\r\n        present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will\r\n        make sure :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last.\r\n\r\n        Return:\r\n            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.\r\n\r\n        Example::\r\n\r\n            def configure_callbacks(self):\r\n                early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\r\n                checkpoint = ModelCheckpoint(monitor=\"val_loss\")\r\n                return [early_stop, checkpoint]\r\n\r\n        \"\"\"\r\n        return []", "code_tokens": ["def", "configure_callbacks", "(", "self", ")", "-", ">", "Union", "[", "Sequence", "[", "Callback", "]", ",", "Callback", "]", ":", "\"", "\"", "\"", "Configure", "model", "-", "specific", "callbacks", ".", "When", "the", "model", "gets", "attached", ",", "e", ".", "g", ".", ",", "when", "`", "`", ".", "fit", "(", ")", "`", "`", "or", "`", "`", ".", "test", "(", ")", "`", "`", "gets", "called", ",", "the", "list", "or", "a", "callback", "returned", "here", "will", "be", "merged", "with", "the", "list", "of", "callbacks", "passed", "to", "the", "Trainer", "'", "s", "`", "`", "callbacks", "`", "`", "argument", ".", "If", "a", "callback", "returned", "here", "has", "the", "same", "type", "as", "one", "or", "several", "callbacks", "already", "present", "in", "the", "Trainer", "'", "s", "callbacks", "list", ",", "it", "will", "take", "priority", "and", "replace", "them", ".", "In", "addition", ",", "Lightning", "will", "make", "sure", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "callbacks", ".", "model_checkpoint", ".", "ModelCheckpoint", "`", "callbacks", "run", "last", ".", "Return", ":", "A", "callback", "or", "a", "list", "of", "callbacks", "which", "will", "extend", "the", "list", "of", "callbacks", "in", "the", "Trainer", ".", "Example", ":", ":", "def", "configure_callbacks", "(", "self", ")", ":", "early_stop", "=", "EarlyStopping", "(", "monitor", "=", "\"", "val_acc", "\"", ",", "mode", "=", "\"", "max", "\"", ")", "checkpoint", "=", "ModelCheckpoint", "(", "monitor", "=", "\"", "val_loss", "\"", ")", "return", "[", "early_stop", ",", "checkpoint", "]", "\"", "\"", "\"", "return", "[", "]"], "docstring": "Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\r\n        called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainer's\r\n        ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already\r\n        present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will\r\n        make sure :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last.\r\n\r\n        Return:\r\n            A callback or a list of callbacks which will extend the list of callbacks in the Trainer.\r\n\r\n        Example::\r\n\r\n            def configure_callbacks(self):\r\n                early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\r\n                checkpoint = ModelCheckpoint(monitor=\"val_loss\")\r\n                return [early_stop, checkpoint]", "docstring_tokens": ["configure", "model", "specific", "callbacks", "when", "the", "model", "gets", "attached", "e", "g", "when", "fit", "or", "test", "gets", "called", "the", "list", "or", "a", "callback", "returned", "here", "will", "be", "merged", "with", "the", "list", "of", "callbacks", "passed", "to", "the", "trainer", "s", "callbacks", "argument", "if", "a", "callback", "returned", "here", "has", "the", "same", "type", "as", "one", "or", "several", "callbacks", "already", "present", "in", "the", "trainer", "s", "callbacks", "list", "it", "will", "take", "priority", "and", "replace", "them", "in", "addition", "lightning", "will", "make", "sure", "class", "lightning", "pytorch", "callbacks", "model_checkpoint", "modelcheckpoint", "callbacks", "run", "last", "return", "a", "callback", "or", "a", "list", "of", "callbacks", "which", "will", "extend", "the", "list", "of", "callbacks", "in", "the", "trainer", "example", "def", "configure_callbacks", "self", "early_stop", "earlystopping", "monitor", "val_acc", "mode", "max", "checkpoint", "modelcheckpoint", "monitor", "val_loss", "return", "early_stop", "checkpoint"], "docstring_summary": "Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 969, "end_line": 987, "hash": "1d1561e320387b142dbbd160526eca3c", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "configure_optimizers", "original_string": "def configure_optimizers(self) -> OptimizerLRScheduler:\r\n        r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\r\n        But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\r\n        the manual optimization mode.\r\n\r\n        Return:\r\n            Any of these 6 options.\r\n\r\n            - **Single optimizer**.\r\n            - **List or Tuple** of optimizers.\r\n            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\r\n              (or multiple ``lr_scheduler_config``).\r\n            - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\r\n              key whose value is a single LR scheduler or ``lr_scheduler_config``.\r\n            - **None** - Fit will run without any optimizer.\r\n\r\n        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\r\n        The default configuration is shown below.\r\n\r\n        .. code-block:: python\r\n\r\n            lr_scheduler_config = {\r\n                # REQUIRED: The scheduler instance\r\n                \"scheduler\": lr_scheduler,\r\n                # The unit of the scheduler's step size, could also be 'step'.\r\n                # 'epoch' updates the scheduler on epoch end whereas 'step'\r\n                # updates it after a optimizer update.\r\n                \"interval\": \"epoch\",\r\n                # How many epochs/steps should pass between calls to\r\n                # `scheduler.step()`. 1 corresponds to updating the learning\r\n                # rate after every epoch/step.\r\n                \"frequency\": 1,\r\n                # Metric to monitor for schedulers like `ReduceLROnPlateau`\r\n                \"monitor\": \"val_loss\",\r\n                # If set to `True`, will enforce that the value specified 'monitor'\r\n                # is available when the scheduler is updated, thus stopping\r\n                # training if not found. If set to `False`, it will only produce a warning\r\n                \"strict\": True,\r\n                # If using the `LearningRateMonitor` callback to monitor the\r\n                # learning rate progress, this keyword can be used to specify\r\n                # a custom logged name\r\n                \"name\": None,\r\n            }\r\n\r\n        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\r\n        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\r\n        ``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\r\n        should be conditioned on.\r\n\r\n        .. testcode::\r\n\r\n            # The ReduceLROnPlateau scheduler requires a monitor\r\n            def configure_optimizers(self):\r\n                optimizer = Adam(...)\r\n                return {\r\n                    \"optimizer\": optimizer,\r\n                    \"lr_scheduler\": {\r\n                        \"scheduler\": ReduceLROnPlateau(optimizer, ...),\r\n                        \"monitor\": \"metric_to_track\",\r\n                        \"frequency\": \"indicates how often the metric is updated\",\r\n                        # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\r\n                        # multiple of \"trainer.check_val_every_n_epoch\".\r\n                    },\r\n                }\r\n\r\n\r\n            # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\r\n            def configure_optimizers(self):\r\n                optimizer1 = Adam(...)\r\n                optimizer2 = SGD(...)\r\n                scheduler1 = ReduceLROnPlateau(optimizer1, ...)\r\n                scheduler2 = LambdaLR(optimizer2, ...)\r\n                return (\r\n                    {\r\n                        \"optimizer\": optimizer1,\r\n                        \"lr_scheduler\": {\r\n                            \"scheduler\": scheduler1,\r\n                            \"monitor\": \"metric_to_track\",\r\n                        },\r\n                    },\r\n                    {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\r\n                )\r\n\r\n        Metrics can be made available to monitor by simply logging it using\r\n        ``self.log('metric_to_track', metric_val)`` in your :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Note:\r\n            Some things to know:\r\n\r\n            - Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\r\n            - If a learning rate scheduler is specified in ``configure_optimizers()`` with key\r\n              ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\r\n              the scheduler's ``.step()`` method automatically in case of automatic optimization.\r\n            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\r\n            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\r\n            - If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\r\n              yourself.\r\n            - If you need to control how often the optimizer steps, override the :meth:`optimizer_step` hook.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`configure_optimizers` must be implemented to be used with the Lightning Trainer\")", "language": "python", "code": "def configure_optimizers(self) -> OptimizerLRScheduler:\r\n        r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\r\n        But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\r\n        the manual optimization mode.\r\n\r\n        Return:\r\n            Any of these 6 options.\r\n\r\n            - **Single optimizer**.\r\n            - **List or Tuple** of optimizers.\r\n            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\r\n              (or multiple ``lr_scheduler_config``).\r\n            - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\r\n              key whose value is a single LR scheduler or ``lr_scheduler_config``.\r\n            - **None** - Fit will run without any optimizer.\r\n\r\n        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\r\n        The default configuration is shown below.\r\n\r\n        .. code-block:: python\r\n\r\n            lr_scheduler_config = {\r\n                # REQUIRED: The scheduler instance\r\n                \"scheduler\": lr_scheduler,\r\n                # The unit of the scheduler's step size, could also be 'step'.\r\n                # 'epoch' updates the scheduler on epoch end whereas 'step'\r\n                # updates it after a optimizer update.\r\n                \"interval\": \"epoch\",\r\n                # How many epochs/steps should pass between calls to\r\n                # `scheduler.step()`. 1 corresponds to updating the learning\r\n                # rate after every epoch/step.\r\n                \"frequency\": 1,\r\n                # Metric to monitor for schedulers like `ReduceLROnPlateau`\r\n                \"monitor\": \"val_loss\",\r\n                # If set to `True`, will enforce that the value specified 'monitor'\r\n                # is available when the scheduler is updated, thus stopping\r\n                # training if not found. If set to `False`, it will only produce a warning\r\n                \"strict\": True,\r\n                # If using the `LearningRateMonitor` callback to monitor the\r\n                # learning rate progress, this keyword can be used to specify\r\n                # a custom logged name\r\n                \"name\": None,\r\n            }\r\n\r\n        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\r\n        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\r\n        ``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\r\n        should be conditioned on.\r\n\r\n        .. testcode::\r\n\r\n            # The ReduceLROnPlateau scheduler requires a monitor\r\n            def configure_optimizers(self):\r\n                optimizer = Adam(...)\r\n                return {\r\n                    \"optimizer\": optimizer,\r\n                    \"lr_scheduler\": {\r\n                        \"scheduler\": ReduceLROnPlateau(optimizer, ...),\r\n                        \"monitor\": \"metric_to_track\",\r\n                        \"frequency\": \"indicates how often the metric is updated\",\r\n                        # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\r\n                        # multiple of \"trainer.check_val_every_n_epoch\".\r\n                    },\r\n                }\r\n\r\n\r\n            # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\r\n            def configure_optimizers(self):\r\n                optimizer1 = Adam(...)\r\n                optimizer2 = SGD(...)\r\n                scheduler1 = ReduceLROnPlateau(optimizer1, ...)\r\n                scheduler2 = LambdaLR(optimizer2, ...)\r\n                return (\r\n                    {\r\n                        \"optimizer\": optimizer1,\r\n                        \"lr_scheduler\": {\r\n                            \"scheduler\": scheduler1,\r\n                            \"monitor\": \"metric_to_track\",\r\n                        },\r\n                    },\r\n                    {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\r\n                )\r\n\r\n        Metrics can be made available to monitor by simply logging it using\r\n        ``self.log('metric_to_track', metric_val)`` in your :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Note:\r\n            Some things to know:\r\n\r\n            - Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\r\n            - If a learning rate scheduler is specified in ``configure_optimizers()`` with key\r\n              ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\r\n              the scheduler's ``.step()`` method automatically in case of automatic optimization.\r\n            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\r\n            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\r\n            - If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\r\n              yourself.\r\n            - If you need to control how often the optimizer steps, override the :meth:`optimizer_step` hook.\r\n\r\n        \"\"\"\r\n        rank_zero_warn(\"`configure_optimizers` must be implemented to be used with the Lightning Trainer\")", "code_tokens": ["def", "configure_optimizers", "(", "self", ")", "-", ">", "OptimizerLRScheduler", ":", "r", "\"", "\"", "\"", "Choose", "what", "optimizers", "and", "learning", "-", "rate", "schedulers", "to", "use", "in", "your", "optimization", ".", "Normally", "you", "'", "d", "need", "one", ".", "But", "in", "the", "case", "of", "GANs", "or", "similar", "you", "might", "have", "multiple", ".", "Optimization", "with", "multiple", "optimizers", "only", "works", "in", "the", "manual", "optimization", "mode", ".", "Return", ":", "Any", "of", "these", "6", "options", ".", "-", "*", "*", "Single", "optimizer", "*", "*", ".", "-", "*", "*", "List", "or", "Tuple", "*", "*", "of", "optimizers", ".", "-", "*", "*", "Two", "lists", "*", "*", "-", "The", "first", "list", "has", "multiple", "optimizers", ",", "and", "the", "second", "has", "multiple", "LR", "schedulers", "(", "or", "multiple", "`", "`", "lr_scheduler_config", "`", "`", ")", ".", "-", "*", "*", "Dictionary", "*", "*", ",", "with", "an", "`", "`", "\"", "optimizer", "\"", "`", "`", "key", ",", "and", "(", "optionally", ")", "a", "`", "`", "\"", "lr_scheduler", "\"", "`", "`", "key", "whose", "value", "is", "a", "single", "LR", "scheduler", "or", "`", "`", "lr_scheduler_config", "`", "`", ".", "-", "*", "*", "None", "*", "*", "-", "Fit", "will", "run", "without", "any", "optimizer", ".", "The", "`", "`", "lr_scheduler_config", "`", "`", "is", "a", "dictionary", "which", "contains", "the", "scheduler", "and", "its", "associated", "configuration", ".", "The", "default", "configuration", "is", "shown", "below", ".", ".", ".", "code", "-", "block", ":", ":", "python", "lr_scheduler_config", "=", "{", "\"", "scheduler", "\"", ":", "lr_scheduler", ",", "\"", "interval", "\"", ":", "\"", "epoch", "\"", ",", "\"", "frequency", "\"", ":", "1", ",", "\"", "monitor", "\"", ":", "\"", "val_loss", "\"", ",", "\"", "strict", "\"", ":", "True", ",", "\"", "name", "\"", ":", "None", ",", "}", "When", "there", "are", "schedulers", "in", "which", "the", "`", "`", ".", "step", "(", ")", "`", "`", "method", "is", "conditioned", "on", "a", "value", ",", "such", "as", "the", ":", "class", ":", "`", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "`", "scheduler", ",", "Lightning", "requires", "that", "the", "`", "`", "lr_scheduler_config", "`", "`", "contains", "the", "keyword", "`", "`", "\"", "monitor", "\"", "`", "`", "set", "to", "the", "metric", "name", "that", "the", "scheduler", "should", "be", "conditioned", "on", ".", ".", ".", "testcode", ":", ":", "def", "configure_optimizers", "(", "self", ")", ":", "optimizer", "=", "Adam", "(", ".", ".", ".", ")", "return", "{", "\"", "optimizer", "\"", ":", "optimizer", ",", "\"", "lr_scheduler", "\"", ":", "{", "\"", "scheduler", "\"", ":", "ReduceLROnPlateau", "(", "optimizer", ",", ".", ".", ".", ")", ",", "\"", "monitor", "\"", ":", "\"", "metric_to_track", "\"", ",", "\"", "frequency", "\"", ":", "\"", "indicates", "how", "often", "the", "metric", "is", "updated", "\"", ",", "}", ",", "}", "def", "configure_optimizers", "(", "self", ")", ":", "optimizer1", "=", "Adam", "(", ".", ".", ".", ")", "optimizer2", "=", "SGD", "(", ".", ".", ".", ")", "scheduler1", "=", "ReduceLROnPlateau", "(", "optimizer1", ",", ".", ".", ".", ")", "scheduler2", "=", "LambdaLR", "(", "optimizer2", ",", ".", ".", ".", ")", "return", "(", "{", "\"", "optimizer", "\"", ":", "optimizer1", ",", "\"", "lr_scheduler", "\"", ":", "{", "\"", "scheduler", "\"", ":", "scheduler1", ",", "\"", "monitor", "\"", ":", "\"", "metric_to_track", "\"", ",", "}", ",", "}", ",", "{", "\"", "optimizer", "\"", ":", "optimizer2", ",", "\"", "lr_scheduler", "\"", ":", "scheduler2", "}", ",", ")", "Metrics", "can", "be", "made", "available", "to", "monitor", "by", "simply", "logging", "it", "using", "`", "`", "self", ".", "log", "(", "'", "metric_to_track", "'", ",", "metric_val", ")", "`", "`", "in", "your", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", ".", "Note", ":", "Some", "things", "to", "know", ":", "-", "Lightning", "calls", "`", "`", ".", "backward", "(", ")", "`", "`", "and", "`", "`", ".", "step", "(", ")", "`", "`", "automatically", "in", "case", "of", "automatic", "optimization", ".", "-", "If", "a", "learning", "rate", "scheduler", "is", "specified", "in", "`", "`", "configure_optimizers", "(", ")", "`", "`", "with", "key", "`", "`", "\"", "interval", "\"", "`", "`", "(", "default", "\"", "epoch", "\"", ")", "in", "the", "scheduler", "configuration", ",", "Lightning", "will", "call", "the", "scheduler", "'", "s", "`", "`", ".", "step", "(", ")", "`", "`", "method", "automatically", "in", "case", "of", "automatic", "optimization", ".", "-", "If", "you", "use", "16", "-", "bit", "precision", "(", "`", "`", "precision", "=", "16", "`", "`", ")", ",", "Lightning", "will", "automatically", "handle", "the", "optimizer", ".", "-", "If", "you", "use", ":", "class", ":", "`", "torch", ".", "optim", ".", "LBFGS", "`", ",", "Lightning", "handles", "the", "closure", "function", "automatically", "for", "you", ".", "-", "If", "you", "use", "multiple", "optimizers", ",", "you", "will", "have", "to", "switch", "to", "'", "manual", "optimization", "'", "mode", "and", "step", "them", "yourself", ".", "-", "If", "you", "need", "to", "control", "how", "often", "the", "optimizer", "steps", ",", "override", "the", ":", "meth", ":", "`", "optimizer_step", "`", "hook", ".", "\"", "\"", "\"", "rank_zero_warn", "(", "\"", "`", "configure_optimizers", "`", "must", "be", "implemented", "to", "be", "used", "with", "the", "Lightning", "Trainer", "\"", ")"], "docstring": "r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\r\n        But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\r\n        the manual optimization mode.\r\n\r\n        Return:\r\n            Any of these 6 options.\r\n\r\n            - **Single optimizer**.\r\n            - **List or Tuple** of optimizers.\r\n            - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\r\n              (or multiple ``lr_scheduler_config``).\r\n            - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\r\n              key whose value is a single LR scheduler or ``lr_scheduler_config``.\r\n            - **None** - Fit will run without any optimizer.\r\n\r\n        The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\r\n        The default configuration is shown below.\r\n\r\n        .. code-block:: python\r\n\r\n            lr_scheduler_config = {\r\n                # REQUIRED: The scheduler instance\r\n                \"scheduler\": lr_scheduler,\r\n                # The unit of the scheduler's step size, could also be 'step'.\r\n                # 'epoch' updates the scheduler on epoch end whereas 'step'\r\n                # updates it after a optimizer update.\r\n                \"interval\": \"epoch\",\r\n                # How many epochs/steps should pass between calls to\r\n                # `scheduler.step()`. 1 corresponds to updating the learning\r\n                # rate after every epoch/step.\r\n                \"frequency\": 1,\r\n                # Metric to monitor for schedulers like `ReduceLROnPlateau`\r\n                \"monitor\": \"val_loss\",\r\n                # If set to `True`, will enforce that the value specified 'monitor'\r\n                # is available when the scheduler is updated, thus stopping\r\n                # training if not found. If set to `False`, it will only produce a warning\r\n                \"strict\": True,\r\n                # If using the `LearningRateMonitor` callback to monitor the\r\n                # learning rate progress, this keyword can be used to specify\r\n                # a custom logged name\r\n                \"name\": None,\r\n            }\r\n\r\n        When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\r\n        :class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\r\n        ``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\r\n        should be conditioned on.\r\n\r\n        .. testcode::\r\n\r\n            # The ReduceLROnPlateau scheduler requires a monitor\r\n            def configure_optimizers(self):\r\n                optimizer = Adam(...)\r\n                return {\r\n                    \"optimizer\": optimizer,\r\n                    \"lr_scheduler\": {\r\n                        \"scheduler\": ReduceLROnPlateau(optimizer, ...),\r\n                        \"monitor\": \"metric_to_track\",\r\n                        \"frequency\": \"indicates how often the metric is updated\",\r\n                        # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\r\n                        # multiple of \"trainer.check_val_every_n_epoch\".\r\n                    },\r\n                }\r\n\r\n\r\n            # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\r\n            def configure_optimizers(self):\r\n                optimizer1 = Adam(...)\r\n                optimizer2 = SGD(...)\r\n                scheduler1 = ReduceLROnPlateau(optimizer1, ...)\r\n                scheduler2 = LambdaLR(optimizer2, ...)\r\n                return (\r\n                    {\r\n                        \"optimizer\": optimizer1,\r\n                        \"lr_scheduler\": {\r\n                            \"scheduler\": scheduler1,\r\n                            \"monitor\": \"metric_to_track\",\r\n                        },\r\n                    },\r\n                    {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\r\n                )\r\n\r\n        Metrics can be made available to monitor by simply logging it using\r\n        ``self.log('metric_to_track', metric_val)`` in your :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Note:\r\n            Some things to know:\r\n\r\n            - Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\r\n            - If a learning rate scheduler is specified in ``configure_optimizers()`` with key\r\n              ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\r\n              the scheduler's ``.step()`` method automatically in case of automatic optimization.\r\n            - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\r\n            - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\r\n            - If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\r\n              yourself.\r\n            - If you need to control how often the optimizer steps, override the :meth:`optimizer_step` hook.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "choose", "what", "optimizers", "and", "learning", "rate", "schedulers", "to", "use", "in", "your", "optimization", "normally", "you", "d", "need", "one", "but", "in", "the", "case", "of", "gans", "or", "similar", "you", "might", "have", "multiple", "optimization", "with", "multiple", "optimizers", "only", "works", "in", "the", "manual", "optimization", "mode", "return", "any", "of", "these", "6", "options", "single", "optimizer", "list", "or", "tuple", "of", "optimizers", "two", "lists", "the", "first", "list", "has", "multiple", "optimizers", "and", "the", "second", "has", "multiple", "lr", "schedulers", "or", "multiple", "lr_scheduler_config", "dictionary", "with", "an", "optimizer", "key", "and", "optionally", "a", "lr_scheduler", "key", "whose", "value", "is", "a", "single", "lr", "scheduler", "or", "lr_scheduler_config", "none", "fit", "will", "run", "without", "any", "optimizer", "the", "lr_scheduler_config", "is", "a", "dictionary", "which", "contains", "the", "scheduler", "and", "its", "associated", "configuration", "the", "default", "configuration", "is", "shown", "below", "code", "block", "python", "lr_scheduler_config", "required", "the", "scheduler", "instance", "scheduler", "lr_scheduler", "the", "unit", "of", "the", "scheduler", "s", "step", "size", "could", "also", "be", "step", "epoch", "updates", "the", "scheduler", "on", "epoch", "end", "whereas", "step", "updates", "it", "after", "a", "optimizer", "update", "interval", "epoch", "how", "many", "epochs", "steps", "should", "pass", "between", "calls", "to", "scheduler", "step", "1", "corresponds", "to", "updating", "the", "learning", "rate", "after", "every", "epoch", "step", "frequency", "1", "metric", "to", "monitor", "for", "schedulers", "like", "reducelronplateau", "monitor", "val_loss", "if", "set", "to", "true", "will", "enforce", "that", "the", "value", "specified", "monitor", "is", "available", "when", "the", "scheduler", "is", "updated", "thus", "stopping", "training", "if", "not", "found", "if", "set", "to", "false", "it", "will", "only", "produce", "a", "warning", "strict", "true", "if", "using", "the", "learningratemonitor", "callback", "to", "monitor", "the", "learning", "rate", "progress", "this", "keyword", "can", "be", "used", "to", "specify", "a", "custom", "logged", "name", "name", "none", "when", "there", "are", "schedulers", "in", "which", "the", "step", "method", "is", "conditioned", "on", "a", "value", "such", "as", "the", "class", "torch", "optim", "lr_scheduler", "reducelronplateau", "scheduler", "lightning", "requires", "that", "the", "lr_scheduler_config", "contains", "the", "keyword", "monitor", "set", "to", "the", "metric", "name", "that", "the", "scheduler", "should", "be", "conditioned", "on", "testcode", "the", "reducelronplateau", "scheduler", "requires", "a", "monitor", "def", "configure_optimizers", "self", "optimizer", "adam", "return", "optimizer", "optimizer", "lr_scheduler", "scheduler", "reducelronplateau", "optimizer", "monitor", "metric_to_track", "frequency", "indicates", "how", "often", "the", "metric", "is", "updated", "if", "monitor", "references", "validation", "metrics", "then", "frequency", "should", "be", "set", "to", "a", "multiple", "of", "trainer", "check_val_every_n_epoch", "in", "the", "case", "of", "two", "optimizers", "only", "one", "using", "the", "reducelronplateau", "scheduler", "def", "configure_optimizers", "self", "optimizer1", "adam", "optimizer2", "sgd", "scheduler1", "reducelronplateau", "optimizer1", "scheduler2", "lambdalr", "optimizer2", "return", "optimizer", "optimizer1", "lr_scheduler", "scheduler", "scheduler1", "monitor", "metric_to_track", "optimizer", "optimizer2", "lr_scheduler", "scheduler2", "metrics", "can", "be", "made", "available", "to", "monitor", "by", "simply", "logging", "it", "using", "self", "log", "metric_to_track", "metric_val", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "note", "some", "things", "to", "know", "lightning", "calls", "backward", "and", "step", "automatically", "in", "case", "of", "automatic", "optimization", "if", "a", "learning", "rate", "scheduler", "is", "specified", "in", "configure_optimizers", "with", "key", "interval", "default", "epoch", "in", "the", "scheduler", "configuration", "lightning", "will", "call", "the", "scheduler", "s", "step", "method", "automatically", "in", "case", "of", "automatic", "optimization", "if", "you", "use", "16", "bit", "precision", "precision", "16", "lightning", "will", "automatically", "handle", "the", "optimizer", "if", "you", "use", "class", "torch", "optim", "lbfgs", "lightning", "handles", "the", "closure", "function", "automatically", "for", "you", "if", "you", "use", "multiple", "optimizers", "you", "will", "have", "to", "switch", "to", "manual", "optimization", "mode", "and", "step", "them", "yourself", "if", "you", "need", "to", "control", "how", "often", "the", "optimizer", "steps", "override", "the", "meth", "optimizer_step", "hook"], "docstring_summary": "r\"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 989, "end_line": 1089, "hash": "5e6779aeba3e5dee0df9befa787b44f1", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "manual_backward", "original_string": "def manual_backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\r\n        Lightning can ensure that all the proper scaling gets applied when using mixed precision.\r\n\r\n        See :ref:`manual optimization<common/optimization:Manual optimization>` for more examples.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                loss = ...\r\n                opt.zero_grad()\r\n                # automatically applies scaling, etc...\r\n                self.manual_backward(loss)\r\n                opt.step()\r\n\r\n        Args:\r\n            loss: The tensor on which to compute gradients. Must have a graph attached.\r\n            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            self._verify_is_manual_optimization(\"manual_backward\")\r\n            self.trainer.strategy.backward(loss, None, *args, **kwargs)", "language": "python", "code": "def manual_backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\r\n        Lightning can ensure that all the proper scaling gets applied when using mixed precision.\r\n\r\n        See :ref:`manual optimization<common/optimization:Manual optimization>` for more examples.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                loss = ...\r\n                opt.zero_grad()\r\n                # automatically applies scaling, etc...\r\n                self.manual_backward(loss)\r\n                opt.step()\r\n\r\n        Args:\r\n            loss: The tensor on which to compute gradients. Must have a graph attached.\r\n            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            self._verify_is_manual_optimization(\"manual_backward\")\r\n            self.trainer.strategy.backward(loss, None, *args, **kwargs)", "code_tokens": ["def", "manual_backward", "(", "self", ",", "loss", ":", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Call", "this", "directly", "from", "your", ":", "meth", ":", "`", "training_step", "`", "when", "doing", "optimizations", "manually", ".", "By", "using", "this", ",", "Lightning", "can", "ensure", "that", "all", "the", "proper", "scaling", "gets", "applied", "when", "using", "mixed", "precision", ".", "See", ":", "ref", ":", "`", "manual", "optimization", "<", "common", "/", "optimization", ":", "Manual", "optimization", ">", "`", "for", "more", "examples", ".", "Example", ":", ":", "def", "training_step", "(", ".", ".", ".", ")", ":", "opt", "=", "self", ".", "optimizers", "(", ")", "loss", "=", ".", ".", ".", "opt", ".", "zero_grad", "(", ")", "self", ".", "manual_backward", "(", "loss", ")", "opt", ".", "step", "(", ")", "Args", ":", "loss", ":", "The", "tensor", "on", "which", "to", "compute", "gradients", ".", "Must", "have", "a", "graph", "attached", ".", "*", "args", ":", "Additional", "positional", "arguments", "to", "be", "forwarded", "to", ":", "meth", ":", "`", "~", "torch", ".", "Tensor", ".", "backward", "`", "*", "*", "kwargs", ":", "Additional", "keyword", "arguments", "to", "be", "forwarded", "to", ":", "meth", ":", "`", "~", "torch", ".", "Tensor", ".", "backward", "`", "\"", "\"", "\"", "if", "self", ".", "_fabric", ":", "self", ".", "_fabric", ".", "backward", "(", "loss", ",", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "self", ".", "_verify_is_manual_optimization", "(", "\"", "manual_backward", "\"", ")", "self", ".", "trainer", ".", "strategy", ".", "backward", "(", "loss", ",", "None", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\r\n        Lightning can ensure that all the proper scaling gets applied when using mixed precision.\r\n\r\n        See :ref:`manual optimization<common/optimization:Manual optimization>` for more examples.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                loss = ...\r\n                opt.zero_grad()\r\n                # automatically applies scaling, etc...\r\n                self.manual_backward(loss)\r\n                opt.step()\r\n\r\n        Args:\r\n            loss: The tensor on which to compute gradients. Must have a graph attached.\r\n            *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`\r\n            **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`", "docstring_tokens": ["call", "this", "directly", "from", "your", "meth", "training_step", "when", "doing", "optimizations", "manually", "by", "using", "this", "lightning", "can", "ensure", "that", "all", "the", "proper", "scaling", "gets", "applied", "when", "using", "mixed", "precision", "see", "ref", "manual", "optimization", "common", "optimization", "manual", "optimization", "for", "more", "examples", "example", "def", "training_step", "opt", "self", "optimizers", "loss", "opt", "zero_grad", "automatically", "applies", "scaling", "etc", "self", "manual_backward", "loss", "opt", "step", "args", "loss", "the", "tensor", "on", "which", "to", "compute", "gradients", "must", "have", "a", "graph", "attached", "args", "additional", "positional", "arguments", "to", "be", "forwarded", "to", "meth", "torch", "tensor", "backward", "kwargs", "additional", "keyword", "arguments", "to", "be", "forwarded", "to", "meth", "torch", "tensor", "backward"], "docstring_summary": "Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1091, "end_line": 1117, "hash": "0c6cb9396dd87ac18577a6fa6c1b00c3", "complexity": 2, "parameters": ["loss", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "backward", "original_string": "def backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)", "language": "python", "code": "def backward(self, loss: Tensor, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()\r\n\r\n        \"\"\"\r\n        if self._fabric:\r\n            self._fabric.backward(loss, *args, **kwargs)\r\n        else:\r\n            loss.backward(*args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "loss", ":", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "to", "perform", "backward", "on", "the", "loss", "returned", "in", ":", "meth", ":", "`", "training_step", "`", ".", "Override", "this", "hook", "with", "your", "own", "implementation", "if", "you", "need", "to", ".", "Args", ":", "loss", ":", "The", "loss", "tensor", "returned", "by", ":", "meth", ":", "`", "training_step", "`", ".", "If", "gradient", "accumulation", "is", "used", ",", "the", "loss", "here", "holds", "the", "normalized", "value", "(", "scaled", "by", "1", "/", "accumulation", "steps", ")", ".", "Example", ":", ":", "def", "backward", "(", "self", ",", "loss", ")", ":", "loss", ".", "backward", "(", ")", "\"", "\"", "\"", "if", "self", ".", "_fabric", ":", "self", ".", "_fabric", ".", "backward", "(", "loss", ",", "*", "args", ",", "*", "*", "kwargs", ")", "else", ":", "loss", ".", "backward", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\r\n        implementation if you need to.\r\n\r\n        Args:\r\n            loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\r\n                holds the normalized value (scaled by 1 / accumulation steps).\r\n\r\n        Example::\r\n\r\n            def backward(self, loss):\r\n                loss.backward()", "docstring_tokens": ["called", "to", "perform", "backward", "on", "the", "loss", "returned", "in", "meth", "training_step", "override", "this", "hook", "with", "your", "own", "implementation", "if", "you", "need", "to", "args", "loss", "the", "loss", "tensor", "returned", "by", "meth", "training_step", "if", "gradient", "accumulation", "is", "used", "the", "loss", "here", "holds", "the", "normalized", "value", "scaled", "by", "1", "accumulation", "steps", "example", "def", "backward", "self", "loss", "loss", "backward"], "docstring_summary": "Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1119, "end_line": 1136, "hash": "a8e572e15f9b77146465223de79b44b6", "complexity": 2, "parameters": ["loss", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "toggle_optimizer", "original_string": "def toggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup.\r\n\r\n        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        \"\"\"\r\n        # Iterate over all optimizer parameters to preserve their `requires_grad` information\r\n        # in case these are pre-defined during `configure_optimizers`\r\n        param_requires_grad_state = {}\r\n        for opt in self.trainer.optimizers:\r\n            for group in opt.param_groups:\r\n                for param in group[\"params\"]:\r\n                    # If a param already appear in param_requires_grad_state, continue\r\n                    if param in param_requires_grad_state:\r\n                        continue\r\n                    param_requires_grad_state[param] = param.requires_grad\r\n                    param.requires_grad = False\r\n\r\n        # Then iterate over the current optimizer's parameters and set its `requires_grad`\r\n        # properties accordingly\r\n        for group in optimizer.param_groups:\r\n            for param in group[\"params\"]:\r\n                param.requires_grad = param_requires_grad_state[param]\r\n        self._param_requires_grad_state = param_requires_grad_state", "language": "python", "code": "def toggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup.\r\n\r\n        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        \"\"\"\r\n        # Iterate over all optimizer parameters to preserve their `requires_grad` information\r\n        # in case these are pre-defined during `configure_optimizers`\r\n        param_requires_grad_state = {}\r\n        for opt in self.trainer.optimizers:\r\n            for group in opt.param_groups:\r\n                for param in group[\"params\"]:\r\n                    # If a param already appear in param_requires_grad_state, continue\r\n                    if param in param_requires_grad_state:\r\n                        continue\r\n                    param_requires_grad_state[param] = param.requires_grad\r\n                    param.requires_grad = False\r\n\r\n        # Then iterate over the current optimizer's parameters and set its `requires_grad`\r\n        # properties accordingly\r\n        for group in optimizer.param_groups:\r\n            for param in group[\"params\"]:\r\n                param.requires_grad = param_requires_grad_state[param]\r\n        self._param_requires_grad_state = param_requires_grad_state", "code_tokens": ["def", "toggle_optimizer", "(", "self", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Makes", "sure", "only", "the", "gradients", "of", "the", "current", "optimizer", "'", "s", "parameters", "are", "calculated", "in", "the", "training", "step", "to", "prevent", "dangling", "gradients", "in", "multiple", "-", "optimizer", "setup", ".", "It", "works", "with", ":", "meth", ":", "`", "untoggle_optimizer", "`", "to", "make", "sure", "`", "`", "param_requires_grad_state", "`", "`", "is", "properly", "reset", ".", "Args", ":", "optimizer", ":", "The", "optimizer", "to", "toggle", ".", "\"", "\"", "\"", "param_requires_grad_state", "=", "{", "}", "for", "opt", "in", "self", ".", "trainer", ".", "optimizers", ":", "for", "group", "in", "opt", ".", "param_groups", ":", "for", "param", "in", "group", "[", "\"", "params", "\"", "]", ":", "if", "param", "in", "param_requires_grad_state", ":", "continue", "param_requires_grad_state", "[", "param", "]", "=", "param", ".", "requires_grad", "param", ".", "requires_grad", "=", "False", "for", "group", "in", "optimizer", ".", "param_groups", ":", "for", "param", "in", "group", "[", "\"", "params", "\"", "]", ":", "param", ".", "requires_grad", "=", "param_requires_grad_state", "[", "param", "]", "self", ".", "_param_requires_grad_state", "=", "param_requires_grad_state"], "docstring": "Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup.\r\n\r\n        It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.", "docstring_tokens": ["makes", "sure", "only", "the", "gradients", "of", "the", "current", "optimizer", "s", "parameters", "are", "calculated", "in", "the", "training", "step", "to", "prevent", "dangling", "gradients", "in", "multiple", "optimizer", "setup", "it", "works", "with", "meth", "untoggle_optimizer", "to", "make", "sure", "param_requires_grad_state", "is", "properly", "reset", "args", "optimizer", "the", "optimizer", "to", "toggle"], "docstring_summary": "Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1138, "end_line": 1165, "hash": "7da5ef581bb6c4c72fb8f657f4ee5412", "complexity": 7, "parameters": ["optimizer", "LightningOptimizer]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "untoggle_optimizer", "original_string": "def untoggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to untoggle.\r\n\r\n        \"\"\"\r\n        for opt in self.trainer.optimizers:\r\n            if not (opt is optimizer or (isinstance(optimizer, LightningOptimizer) and opt is optimizer.optimizer)):\r\n                for group in opt.param_groups:\r\n                    for param in group[\"params\"]:\r\n                        if param in self._param_requires_grad_state:\r\n                            param.requires_grad = self._param_requires_grad_state[param]\r\n        # save memory\r\n        self._param_requires_grad_state = {}", "language": "python", "code": "def untoggle_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> None:\r\n        \"\"\"Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to untoggle.\r\n\r\n        \"\"\"\r\n        for opt in self.trainer.optimizers:\r\n            if not (opt is optimizer or (isinstance(optimizer, LightningOptimizer) and opt is optimizer.optimizer)):\r\n                for group in opt.param_groups:\r\n                    for param in group[\"params\"]:\r\n                        if param in self._param_requires_grad_state:\r\n                            param.requires_grad = self._param_requires_grad_state[param]\r\n        # save memory\r\n        self._param_requires_grad_state = {}", "code_tokens": ["def", "untoggle_optimizer", "(", "self", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resets", "the", "state", "of", "required", "gradients", "that", "were", "toggled", "with", ":", "meth", ":", "`", "toggle_optimizer", "`", ".", "Args", ":", "optimizer", ":", "The", "optimizer", "to", "untoggle", ".", "\"", "\"", "\"", "for", "opt", "in", "self", ".", "trainer", ".", "optimizers", ":", "if", "not", "(", "opt", "is", "optimizer", "or", "(", "isinstance", "(", "optimizer", ",", "LightningOptimizer", ")", "and", "opt", "is", "optimizer", ".", "optimizer", ")", ")", ":", "for", "group", "in", "opt", ".", "param_groups", ":", "for", "param", "in", "group", "[", "\"", "params", "\"", "]", ":", "if", "param", "in", "self", ".", "_param_requires_grad_state", ":", "param", ".", "requires_grad", "=", "self", ".", "_param_requires_grad_state", "[", "param", "]", "self", ".", "_param_requires_grad_state", "=", "{", "}"], "docstring": "Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\r\n\r\n        Args:\r\n            optimizer: The optimizer to untoggle.", "docstring_tokens": ["resets", "the", "state", "of", "required", "gradients", "that", "were", "toggled", "with", "meth", "toggle_optimizer", "args", "optimizer", "the", "optimizer", "to", "untoggle"], "docstring_summary": "Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1167, "end_line": 1181, "hash": "95890b6c99df443271fc31fbbde07c2d", "complexity": 8, "parameters": ["optimizer", "LightningOptimizer]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "toggled_optimizer", "original_string": "def toggled_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> Generator:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup. Combines :meth:`toggle_optimizer` and\r\n        :meth:`untoggle_optimizer` into context manager.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                with self.toggled_optimizer(opt):\r\n                    loss = ...\r\n                    opt.zero_grad()\r\n                    self.manual_backward(loss)\r\n                    opt.step()\r\n\r\n        \"\"\"\r\n        self.toggle_optimizer(optimizer)\r\n        try:\r\n            yield\r\n        finally:\r\n            self.untoggle_optimizer(optimizer)", "language": "python", "code": "def toggled_optimizer(self, optimizer: Union[Optimizer, LightningOptimizer]) -> Generator:\r\n        \"\"\"Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup. Combines :meth:`toggle_optimizer` and\r\n        :meth:`untoggle_optimizer` into context manager.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                with self.toggled_optimizer(opt):\r\n                    loss = ...\r\n                    opt.zero_grad()\r\n                    self.manual_backward(loss)\r\n                    opt.step()\r\n\r\n        \"\"\"\r\n        self.toggle_optimizer(optimizer)\r\n        try:\r\n            yield\r\n        finally:\r\n            self.untoggle_optimizer(optimizer)", "code_tokens": ["def", "toggled_optimizer", "(", "self", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Makes", "sure", "only", "the", "gradients", "of", "the", "current", "optimizer", "'", "s", "parameters", "are", "calculated", "in", "the", "training", "step", "to", "prevent", "dangling", "gradients", "in", "multiple", "-", "optimizer", "setup", ".", "Combines", ":", "meth", ":", "`", "toggle_optimizer", "`", "and", ":", "meth", ":", "`", "untoggle_optimizer", "`", "into", "context", "manager", ".", "Args", ":", "optimizer", ":", "The", "optimizer", "to", "toggle", ".", "Example", ":", ":", "def", "training_step", "(", ".", ".", ".", ")", ":", "opt", "=", "self", ".", "optimizers", "(", ")", "with", "self", ".", "toggled_optimizer", "(", "opt", ")", ":", "loss", "=", ".", ".", ".", "opt", ".", "zero_grad", "(", ")", "self", ".", "manual_backward", "(", "loss", ")", "opt", ".", "step", "(", ")", "\"", "\"", "\"", "self", ".", "toggle_optimizer", "(", "optimizer", ")", "try", ":", "yield", "finally", ":", "self", ".", "untoggle_optimizer", "(", "optimizer", ")"], "docstring": "Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\r\n        prevent dangling gradients in multiple-optimizer setup. Combines :meth:`toggle_optimizer` and\r\n        :meth:`untoggle_optimizer` into context manager.\r\n\r\n        Args:\r\n            optimizer: The optimizer to toggle.\r\n\r\n        Example::\r\n\r\n            def training_step(...):\r\n                opt = self.optimizers()\r\n                with self.toggled_optimizer(opt):\r\n                    loss = ...\r\n                    opt.zero_grad()\r\n                    self.manual_backward(loss)\r\n                    opt.step()", "docstring_tokens": ["makes", "sure", "only", "the", "gradients", "of", "the", "current", "optimizer", "s", "parameters", "are", "calculated", "in", "the", "training", "step", "to", "prevent", "dangling", "gradients", "in", "multiple", "optimizer", "setup", "combines", "meth", "toggle_optimizer", "and", "meth", "untoggle_optimizer", "into", "context", "manager", "args", "optimizer", "the", "optimizer", "to", "toggle", "example", "def", "training_step", "opt", "self", "optimizers", "with", "self", "toggled_optimizer", "opt", "loss", "opt", "zero_grad", "self", "manual_backward", "loss", "opt", "step"], "docstring_summary": "Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1184, "end_line": 1207, "hash": "d456dd5772d718da2f9acb167678bda1", "complexity": 1, "parameters": ["optimizer", "LightningOptimizer]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "clip_gradients", "original_string": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Handles gradient clipping internally.\r\n\r\n        Note:\r\n            - Do not override this method. If you want to customize gradient clipping, consider using\r\n              :meth:`configure_gradient_clipping` method.\r\n            - For manual optimization (``self.automatic_optimization = False``), if you want to use\r\n              gradient clipping, consider calling\r\n              ``self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")``\r\n              manually in the training step.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm.\r\n\r\n        \"\"\"\r\n\r\n        if self.fabric is not None:\r\n            self.fabric.clip_gradients(\r\n                self,\r\n                optimizer,\r\n                clip_val=gradient_clip_val if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else None,\r\n                max_norm=None if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else gradient_clip_val,\r\n            )\r\n            return\r\n\r\n        if gradient_clip_val is None:\r\n            gradient_clip_val = self.trainer.gradient_clip_val or 0.0\r\n        elif self.trainer.gradient_clip_val is not None and self.trainer.gradient_clip_val != gradient_clip_val:\r\n            raise MisconfigurationException(\r\n                f\"You have set `Trainer(gradient_clip_val={self.trainer.gradient_clip_val!r})`\"\r\n                f\" and have passed `clip_gradients(gradient_clip_val={gradient_clip_val!r})`.\"\r\n                \" Please use only one of them.\"\r\n            )\r\n\r\n        if gradient_clip_algorithm is None:\r\n            gradient_clip_algorithm = self.trainer.gradient_clip_algorithm or \"norm\"\r\n        else:\r\n            gradient_clip_algorithm = gradient_clip_algorithm.lower()\r\n            if (\r\n                self.trainer.gradient_clip_algorithm is not None\r\n                and self.trainer.gradient_clip_algorithm != gradient_clip_algorithm\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"You have set `Trainer(gradient_clip_algorithm={self.trainer.gradient_clip_algorithm.value!r})`\"\r\n                    f\" and have passed `clip_gradients(gradient_clip_algorithm={gradient_clip_algorithm!r})\"\r\n                    \" Please use only one of them.\"\r\n                )\r\n\r\n        if not isinstance(gradient_clip_val, (int, float)):\r\n            raise TypeError(f\"`gradient_clip_val` should be an int or a float. Got {gradient_clip_val}.\")\r\n\r\n        if not GradClipAlgorithmType.supported_type(gradient_clip_algorithm.lower()):\r\n            raise MisconfigurationException(\r\n                f\"`gradient_clip_algorithm` {gradient_clip_algorithm} is invalid.\"\r\n                f\" Allowed algorithms: {GradClipAlgorithmType.supported_types()}.\"\r\n            )\r\n\r\n        gradient_clip_algorithm = GradClipAlgorithmType(gradient_clip_algorithm)\r\n        self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Handles gradient clipping internally.\r\n\r\n        Note:\r\n            - Do not override this method. If you want to customize gradient clipping, consider using\r\n              :meth:`configure_gradient_clipping` method.\r\n            - For manual optimization (``self.automatic_optimization = False``), if you want to use\r\n              gradient clipping, consider calling\r\n              ``self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")``\r\n              manually in the training step.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm.\r\n\r\n        \"\"\"\r\n\r\n        if self.fabric is not None:\r\n            self.fabric.clip_gradients(\r\n                self,\r\n                optimizer,\r\n                clip_val=gradient_clip_val if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else None,\r\n                max_norm=None if gradient_clip_algorithm == GradClipAlgorithmType.VALUE else gradient_clip_val,\r\n            )\r\n            return\r\n\r\n        if gradient_clip_val is None:\r\n            gradient_clip_val = self.trainer.gradient_clip_val or 0.0\r\n        elif self.trainer.gradient_clip_val is not None and self.trainer.gradient_clip_val != gradient_clip_val:\r\n            raise MisconfigurationException(\r\n                f\"You have set `Trainer(gradient_clip_val={self.trainer.gradient_clip_val!r})`\"\r\n                f\" and have passed `clip_gradients(gradient_clip_val={gradient_clip_val!r})`.\"\r\n                \" Please use only one of them.\"\r\n            )\r\n\r\n        if gradient_clip_algorithm is None:\r\n            gradient_clip_algorithm = self.trainer.gradient_clip_algorithm or \"norm\"\r\n        else:\r\n            gradient_clip_algorithm = gradient_clip_algorithm.lower()\r\n            if (\r\n                self.trainer.gradient_clip_algorithm is not None\r\n                and self.trainer.gradient_clip_algorithm != gradient_clip_algorithm\r\n            ):\r\n                raise MisconfigurationException(\r\n                    f\"You have set `Trainer(gradient_clip_algorithm={self.trainer.gradient_clip_algorithm.value!r})`\"\r\n                    f\" and have passed `clip_gradients(gradient_clip_algorithm={gradient_clip_algorithm!r})\"\r\n                    \" Please use only one of them.\"\r\n                )\r\n\r\n        if not isinstance(gradient_clip_val, (int, float)):\r\n            raise TypeError(f\"`gradient_clip_val` should be an int or a float. Got {gradient_clip_val}.\")\r\n\r\n        if not GradClipAlgorithmType.supported_type(gradient_clip_algorithm.lower()):\r\n            raise MisconfigurationException(\r\n                f\"`gradient_clip_algorithm` {gradient_clip_algorithm} is invalid.\"\r\n                f\" Allowed algorithms: {GradClipAlgorithmType.supported_types()}.\"\r\n            )\r\n\r\n        gradient_clip_algorithm = GradClipAlgorithmType(gradient_clip_algorithm)\r\n        self.trainer.precision_plugin.clip_gradients(optimizer, gradient_clip_val, gradient_clip_algorithm)", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "gradient_clip_val", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "gradient_clip_algorithm", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Handles", "gradient", "clipping", "internally", ".", "Note", ":", "-", "Do", "not", "override", "this", "method", ".", "If", "you", "want", "to", "customize", "gradient", "clipping", ",", "consider", "using", ":", "meth", ":", "`", "configure_gradient_clipping", "`", "method", ".", "-", "For", "manual", "optimization", "(", "`", "`", "self", ".", "automatic_optimization", "=", "False", "`", "`", ")", ",", "if", "you", "want", "to", "use", "gradient", "clipping", ",", "consider", "calling", "`", "`", "self", ".", "clip_gradients", "(", "opt", ",", "gradient_clip_val", "=", "0", ".", "5", ",", "gradient_clip_algorithm", "=", "\"", "norm", "\"", ")", "`", "`", "manually", "in", "the", "training", "step", ".", "Args", ":", "optimizer", ":", "Current", "optimizer", "being", "used", ".", "gradient_clip_val", ":", "The", "value", "at", "which", "to", "clip", "gradients", ".", "gradient_clip_algorithm", ":", "The", "gradient", "clipping", "algorithm", "to", "use", ".", "Pass", "`", "`", "gradient_clip_algorithm", "=", "\"", "value", "\"", "`", "`", "to", "clip", "by", "value", ",", "and", "`", "`", "gradient_clip_algorithm", "=", "\"", "norm", "\"", "`", "`", "to", "clip", "by", "norm", ".", "\"", "\"", "\"", "if", "self", ".", "fabric", "is", "not", "None", ":", "self", ".", "fabric", ".", "clip_gradients", "(", "self", ",", "optimizer", ",", "clip_val", "=", "gradient_clip_val", "if", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "VALUE", "else", "None", ",", "max_norm", "=", "None", "if", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "VALUE", "else", "gradient_clip_val", ",", ")", "return", "if", "gradient_clip_val", "is", "None", ":", "gradient_clip_val", "=", "self", ".", "trainer", ".", "gradient_clip_val", "or", "0", ".", "0", "elif", "self", ".", "trainer", ".", "gradient_clip_val", "is", "not", "None", "and", "self", ".", "trainer", ".", "gradient_clip_val", "!", "=", "gradient_clip_val", ":", "raise", "MisconfigurationException", "(", "f", "\"", "You", "have", "set", "`", "Trainer", "(", "gradient_clip_val", "=", "{", "self", ".", "trainer", ".", "gradient_clip_val", "!", "r", "}", ")", "`", "\"", "f", "\"", "and", "have", "passed", "`", "clip_gradients", "(", "gradient_clip_val", "=", "{", "gradient_clip_val", "!", "r", "}", ")", "`", ".", "\"", "\"", "Please", "use", "only", "one", "of", "them", ".", "\"", ")", "if", "gradient_clip_algorithm", "is", "None", ":", "gradient_clip_algorithm", "=", "self", ".", "trainer", ".", "gradient_clip_algorithm", "or", "\"", "norm", "\"", "else", ":", "gradient_clip_algorithm", "=", "gradient_clip_algorithm", ".", "lower", "(", ")", "if", "(", "self", ".", "trainer", ".", "gradient_clip_algorithm", "is", "not", "None", "and", "self", ".", "trainer", ".", "gradient_clip_algorithm", "!", "=", "gradient_clip_algorithm", ")", ":", "raise", "MisconfigurationException", "(", "f", "\"", "You", "have", "set", "`", "Trainer", "(", "gradient_clip_algorithm", "=", "{", "self", ".", "trainer", ".", "gradient_clip_algorithm", ".", "value", "!", "r", "}", ")", "`", "\"", "f", "\"", "and", "have", "passed", "`", "clip_gradients", "(", "gradient_clip_algorithm", "=", "{", "gradient_clip_algorithm", "!", "r", "}", ")", "\"", "\"", "Please", "use", "only", "one", "of", "them", ".", "\"", ")", "if", "not", "isinstance", "(", "gradient_clip_val", ",", "(", "int", ",", "float", ")", ")", ":", "raise", "TypeError", "(", "f", "\"", "`", "gradient_clip_val", "`", "should", "be", "an", "int", "or", "a", "float", ".", "Got", "{", "gradient_clip_val", "}", ".", "\"", ")", "if", "not", "GradClipAlgorithmType", ".", "supported_type", "(", "gradient_clip_algorithm", ".", "lower", "(", ")", ")", ":", "raise", "MisconfigurationException", "(", "f", "\"", "`", "gradient_clip_algorithm", "`", "{", "gradient_clip_algorithm", "}", "is", "invalid", ".", "\"", "f", "\"", "Allowed", "algorithms", ":", "{", "GradClipAlgorithmType", ".", "supported_types", "(", ")", "}", ".", "\"", ")", "gradient_clip_algorithm", "=", "GradClipAlgorithmType", "(", "gradient_clip_algorithm", ")", "self", ".", "trainer", ".", "precision_plugin", ".", "clip_gradients", "(", "optimizer", ",", "gradient_clip_val", ",", "gradient_clip_algorithm", ")"], "docstring": "Handles gradient clipping internally.\r\n\r\n        Note:\r\n            - Do not override this method. If you want to customize gradient clipping, consider using\r\n              :meth:`configure_gradient_clipping` method.\r\n            - For manual optimization (``self.automatic_optimization = False``), if you want to use\r\n              gradient clipping, consider calling\r\n              ``self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")``\r\n              manually in the training step.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\r\n                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm.", "docstring_tokens": ["handles", "gradient", "clipping", "internally", "note", "do", "not", "override", "this", "method", "if", "you", "want", "to", "customize", "gradient", "clipping", "consider", "using", "meth", "configure_gradient_clipping", "method", "for", "manual", "optimization", "self", "automatic_optimization", "false", "if", "you", "want", "to", "use", "gradient", "clipping", "consider", "calling", "self", "clip_gradients", "opt", "gradient_clip_val", "0", "5", "gradient_clip_algorithm", "norm", "manually", "in", "the", "training", "step", "args", "optimizer", "current", "optimizer", "being", "used", "gradient_clip_val", "the", "value", "at", "which", "to", "clip", "gradients", "gradient_clip_algorithm", "the", "gradient", "clipping", "algorithm", "to", "use", "pass", "gradient_clip_algorithm", "value", "to", "clip", "by", "value", "and", "gradient_clip_algorithm", "norm", "to", "clip", "by", "norm"], "docstring_summary": "Handles gradient clipping internally.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1209, "end_line": 1275, "hash": "81eb6d20bef793a6007f990dbbe4954c", "complexity": 14, "parameters": ["optimizer", "gradient_clip_val", "float]]", "gradient_clip_algorithm"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "configure_gradient_clipping", "original_string": "def configure_gradient_clipping(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients. By default, value passed in Trainer\r\n                will be available here.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. By default, value\r\n                passed in Trainer will be available here.\r\n\r\n        Example::\r\n\r\n            def configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\r\n                # Implement your own custom logic to clip gradients\r\n                # You can call `self.clip_gradients` with your settings:\r\n                self.clip_gradients(\r\n                    optimizer,\r\n                    gradient_clip_val=gradient_clip_val,\r\n                    gradient_clip_algorithm=gradient_clip_algorithm\r\n                )\r\n\r\n        \"\"\"\r\n        self.clip_gradients(\r\n            optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm\r\n        )", "language": "python", "code": "def configure_gradient_clipping(\r\n        self,\r\n        optimizer: Optimizer,\r\n        gradient_clip_val: Optional[Union[int, float]] = None,\r\n        gradient_clip_algorithm: Optional[str] = None,\r\n    ) -> None:\r\n        \"\"\"Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients. By default, value passed in Trainer\r\n                will be available here.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. By default, value\r\n                passed in Trainer will be available here.\r\n\r\n        Example::\r\n\r\n            def configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\r\n                # Implement your own custom logic to clip gradients\r\n                # You can call `self.clip_gradients` with your settings:\r\n                self.clip_gradients(\r\n                    optimizer,\r\n                    gradient_clip_val=gradient_clip_val,\r\n                    gradient_clip_algorithm=gradient_clip_algorithm\r\n                )\r\n\r\n        \"\"\"\r\n        self.clip_gradients(\r\n            optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm\r\n        )", "code_tokens": ["def", "configure_gradient_clipping", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "gradient_clip_val", ":", "Optional", "[", "Union", "[", "int", ",", "float", "]", "]", "=", "None", ",", "gradient_clip_algorithm", ":", "Optional", "[", "str", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Perform", "gradient", "clipping", "for", "the", "optimizer", "parameters", ".", "Called", "before", ":", "meth", ":", "`", "optimizer_step", "`", ".", "Args", ":", "optimizer", ":", "Current", "optimizer", "being", "used", ".", "gradient_clip_val", ":", "The", "value", "at", "which", "to", "clip", "gradients", ".", "By", "default", ",", "value", "passed", "in", "Trainer", "will", "be", "available", "here", ".", "gradient_clip_algorithm", ":", "The", "gradient", "clipping", "algorithm", "to", "use", ".", "By", "default", ",", "value", "passed", "in", "Trainer", "will", "be", "available", "here", ".", "Example", ":", ":", "def", "configure_gradient_clipping", "(", "self", ",", "optimizer", ",", "gradient_clip_val", ",", "gradient_clip_algorithm", ")", ":", "self", ".", "clip_gradients", "(", "optimizer", ",", "gradient_clip_val", "=", "gradient_clip_val", ",", "gradient_clip_algorithm", "=", "gradient_clip_algorithm", ")", "\"", "\"", "\"", "self", ".", "clip_gradients", "(", "optimizer", ",", "gradient_clip_val", "=", "gradient_clip_val", ",", "gradient_clip_algorithm", "=", "gradient_clip_algorithm", ")"], "docstring": "Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\r\n\r\n        Args:\r\n            optimizer: Current optimizer being used.\r\n            gradient_clip_val: The value at which to clip gradients. By default, value passed in Trainer\r\n                will be available here.\r\n            gradient_clip_algorithm: The gradient clipping algorithm to use. By default, value\r\n                passed in Trainer will be available here.\r\n\r\n        Example::\r\n\r\n            def configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\r\n                # Implement your own custom logic to clip gradients\r\n                # You can call `self.clip_gradients` with your settings:\r\n                self.clip_gradients(\r\n                    optimizer,\r\n                    gradient_clip_val=gradient_clip_val,\r\n                    gradient_clip_algorithm=gradient_clip_algorithm\r\n                )", "docstring_tokens": ["perform", "gradient", "clipping", "for", "the", "optimizer", "parameters", "called", "before", "meth", "optimizer_step", "args", "optimizer", "current", "optimizer", "being", "used", "gradient_clip_val", "the", "value", "at", "which", "to", "clip", "gradients", "by", "default", "value", "passed", "in", "trainer", "will", "be", "available", "here", "gradient_clip_algorithm", "the", "gradient", "clipping", "algorithm", "to", "use", "by", "default", "value", "passed", "in", "trainer", "will", "be", "available", "here", "example", "def", "configure_gradient_clipping", "self", "optimizer", "gradient_clip_val", "gradient_clip_algorithm", "implement", "your", "own", "custom", "logic", "to", "clip", "gradients", "you", "can", "call", "self", "clip_gradients", "with", "your", "settings", "self", "clip_gradients", "optimizer", "gradient_clip_val", "gradient_clip_val", "gradient_clip_algorithm", "gradient_clip_algorithm"], "docstring_summary": "Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1277, "end_line": 1306, "hash": "ac33e07022ffb14dfee010d43ac1ac85", "complexity": 1, "parameters": ["optimizer", "gradient_clip_val", "float]]", "gradient_clip_algorithm"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "lr_scheduler_step", "original_string": "def lr_scheduler_step(self, scheduler: LRSchedulerTypeUnion, metric: Optional[Any]) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        each scheduler. By default, Lightning calls ``step()`` and as shown in the example for each scheduler based on\r\n        its ``interval``.\r\n\r\n        Args:\r\n            scheduler: Learning rate scheduler.\r\n            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.\r\n\r\n        Examples::\r\n\r\n            # DEFAULT\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                if metric is None:\r\n                    scheduler.step()\r\n                else:\r\n                    scheduler.step(metric)\r\n\r\n            # Alternative way to update schedulers if it requires an epoch value\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                scheduler.step(epoch=self.current_epoch)\r\n\r\n        \"\"\"\r\n        if metric is None:\r\n            scheduler.step()  # type: ignore[call-arg]\r\n        else:\r\n            scheduler.step(metric)", "language": "python", "code": "def lr_scheduler_step(self, scheduler: LRSchedulerTypeUnion, metric: Optional[Any]) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        each scheduler. By default, Lightning calls ``step()`` and as shown in the example for each scheduler based on\r\n        its ``interval``.\r\n\r\n        Args:\r\n            scheduler: Learning rate scheduler.\r\n            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.\r\n\r\n        Examples::\r\n\r\n            # DEFAULT\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                if metric is None:\r\n                    scheduler.step()\r\n                else:\r\n                    scheduler.step(metric)\r\n\r\n            # Alternative way to update schedulers if it requires an epoch value\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                scheduler.step(epoch=self.current_epoch)\r\n\r\n        \"\"\"\r\n        if metric is None:\r\n            scheduler.step()  # type: ignore[call-arg]\r\n        else:\r\n            scheduler.step(metric)", "code_tokens": ["def", "lr_scheduler_step", "(", "self", ",", "scheduler", ":", "LRSchedulerTypeUnion", ",", "metric", ":", "Optional", "[", "Any", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Override", "this", "method", "to", "adjust", "the", "default", "way", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "calls", "each", "scheduler", ".", "By", "default", ",", "Lightning", "calls", "`", "`", "step", "(", ")", "`", "`", "and", "as", "shown", "in", "the", "example", "for", "each", "scheduler", "based", "on", "its", "`", "`", "interval", "`", "`", ".", "Args", ":", "scheduler", ":", "Learning", "rate", "scheduler", ".", "metric", ":", "Value", "of", "the", "monitor", "used", "for", "schedulers", "like", "`", "`", "ReduceLROnPlateau", "`", "`", ".", "Examples", ":", ":", "def", "lr_scheduler_step", "(", "self", ",", "scheduler", ",", "metric", ")", ":", "if", "metric", "is", "None", ":", "scheduler", ".", "step", "(", ")", "else", ":", "scheduler", ".", "step", "(", "metric", ")", "def", "lr_scheduler_step", "(", "self", ",", "scheduler", ",", "metric", ")", ":", "scheduler", ".", "step", "(", "epoch", "=", "self", ".", "current_epoch", ")", "\"", "\"", "\"", "if", "metric", "is", "None", ":", "scheduler", ".", "step", "(", ")", "else", ":", "scheduler", ".", "step", "(", "metric", ")"], "docstring": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        each scheduler. By default, Lightning calls ``step()`` and as shown in the example for each scheduler based on\r\n        its ``interval``.\r\n\r\n        Args:\r\n            scheduler: Learning rate scheduler.\r\n            metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.\r\n\r\n        Examples::\r\n\r\n            # DEFAULT\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                if metric is None:\r\n                    scheduler.step()\r\n                else:\r\n                    scheduler.step(metric)\r\n\r\n            # Alternative way to update schedulers if it requires an epoch value\r\n            def lr_scheduler_step(self, scheduler, metric):\r\n                scheduler.step(epoch=self.current_epoch)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "override", "this", "method", "to", "adjust", "the", "default", "way", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "calls", "each", "scheduler", "by", "default", "lightning", "calls", "step", "and", "as", "shown", "in", "the", "example", "for", "each", "scheduler", "based", "on", "its", "interval", "args", "scheduler", "learning", "rate", "scheduler", "metric", "value", "of", "the", "monitor", "used", "for", "schedulers", "like", "reducelronplateau", "examples", "default", "def", "lr_scheduler_step", "self", "scheduler", "metric", "if", "metric", "is", "none", "scheduler", "step", "else", "scheduler", "step", "metric", "alternative", "way", "to", "update", "schedulers", "if", "it", "requires", "an", "epoch", "value", "def", "lr_scheduler_step", "self", "scheduler", "metric", "scheduler", "step", "epoch", "self", "current_epoch"], "docstring_summary": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1308, "end_line": 1334, "hash": "dfe8c46d4f12b4a3e8063f209df2acd0", "complexity": 2, "parameters": ["scheduler", "metric"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(\r\n        self,\r\n        epoch: int,\r\n        batch_idx: int,\r\n        optimizer: Union[Optimizer, LightningOptimizer],\r\n        optimizer_closure: Optional[Callable[[], Any]] = None,\r\n    ) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        the optimizer.\r\n\r\n        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example.\r\n        This method (and ``zero_grad()``) won't be called during the accumulation phase when\r\n        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n            optimizer_closure: The optimizer closure. This closure must be executed as it includes the\r\n                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.\r\n\r\n        Examples::\r\n\r\n            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\r\n                # Add your custom logic to run directly before `optimizer.step()`\r\n\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n                # Add your custom logic to run directly after `optimizer.step()`\r\n\r\n        \"\"\"\r\n        optimizer.step(closure=optimizer_closure)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        epoch: int,\r\n        batch_idx: int,\r\n        optimizer: Union[Optimizer, LightningOptimizer],\r\n        optimizer_closure: Optional[Callable[[], Any]] = None,\r\n    ) -> None:\r\n        r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        the optimizer.\r\n\r\n        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example.\r\n        This method (and ``zero_grad()``) won't be called during the accumulation phase when\r\n        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n            optimizer_closure: The optimizer closure. This closure must be executed as it includes the\r\n                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.\r\n\r\n        Examples::\r\n\r\n            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\r\n                # Add your custom logic to run directly before `optimizer.step()`\r\n\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n                # Add your custom logic to run directly after `optimizer.step()`\r\n\r\n        \"\"\"\r\n        optimizer.step(closure=optimizer_closure)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "epoch", ":", "int", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "Union", "[", "Optimizer", ",", "LightningOptimizer", "]", ",", "optimizer_closure", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "Any", "]", "]", "=", "None", ",", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Override", "this", "method", "to", "adjust", "the", "default", "way", "the", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "trainer", ".", "trainer", ".", "Trainer", "`", "calls", "the", "optimizer", ".", "By", "default", ",", "Lightning", "calls", "`", "`", "step", "(", ")", "`", "`", "and", "`", "`", "zero_grad", "(", ")", "`", "`", "as", "shown", "in", "the", "example", ".", "This", "method", "(", "and", "`", "`", "zero_grad", "(", ")", "`", "`", ")", "won", "'", "t", "be", "called", "during", "the", "accumulation", "phase", "when", "`", "`", "Trainer", "(", "accumulate_grad_batches", "!", "=", "1", ")", "`", "`", ".", "Overriding", "this", "hook", "has", "no", "benefit", "with", "manual", "optimization", ".", "Args", ":", "epoch", ":", "Current", "epoch", "batch_idx", ":", "Index", "of", "current", "batch", "optimizer", ":", "A", "PyTorch", "optimizer", "optimizer_closure", ":", "The", "optimizer", "closure", ".", "This", "closure", "must", "be", "executed", "as", "it", "includes", "the", "calls", "to", "`", "`", "training_step", "(", ")", "`", "`", ",", "`", "`", "optimizer", ".", "zero_grad", "(", ")", "`", "`", ",", "and", "`", "`", "backward", "(", ")", "`", "`", ".", "Examples", ":", ":", "def", "optimizer_step", "(", "self", ",", "epoch", ",", "batch_idx", ",", "optimizer", ",", "optimizer_closure", ")", ":", "optimizer", ".", "step", "(", "closure", "=", "optimizer_closure", ")", "\"", "\"", "\"", "optimizer", ".", "step", "(", "closure", "=", "optimizer_closure", ")"], "docstring": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\r\n        the optimizer.\r\n\r\n        By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example.\r\n        This method (and ``zero_grad()``) won't be called during the accumulation phase when\r\n        ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n            optimizer_closure: The optimizer closure. This closure must be executed as it includes the\r\n                calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.\r\n\r\n        Examples::\r\n\r\n            def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\r\n                # Add your custom logic to run directly before `optimizer.step()`\r\n\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n                # Add your custom logic to run directly after `optimizer.step()`\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "override", "this", "method", "to", "adjust", "the", "default", "way", "the", "class", "lightning", "pytorch", "trainer", "trainer", "trainer", "calls", "the", "optimizer", "by", "default", "lightning", "calls", "step", "and", "zero_grad", "as", "shown", "in", "the", "example", "this", "method", "and", "zero_grad", "won", "t", "be", "called", "during", "the", "accumulation", "phase", "when", "trainer", "accumulate_grad_batches", "1", "overriding", "this", "hook", "has", "no", "benefit", "with", "manual", "optimization", "args", "epoch", "current", "epoch", "batch_idx", "index", "of", "current", "batch", "optimizer", "a", "pytorch", "optimizer", "optimizer_closure", "the", "optimizer", "closure", "this", "closure", "must", "be", "executed", "as", "it", "includes", "the", "calls", "to", "training_step", "optimizer", "zero_grad", "and", "backward", "examples", "def", "optimizer_step", "self", "epoch", "batch_idx", "optimizer", "optimizer_closure", "add", "your", "custom", "logic", "to", "run", "directly", "before", "optimizer", "step", "optimizer", "step", "closure", "optimizer_closure", "add", "your", "custom", "logic", "to", "run", "directly", "after", "optimizer", "step"], "docstring_summary": "r\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1336, "end_line": 1367, "hash": "2aed5c88e42ee12bfab742c70d2dbc75", "complexity": 1, "parameters": ["epoch", "batch_idx", "optimizer", "LightningOptimizer]", "optimizer_closure", "Any]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "optimizer_zero_grad", "original_string": "def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n\r\n        Examples::\r\n\r\n            # DEFAULT\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad()\r\n\r\n            # Set gradients to `None` instead of zero to improve performance (not required on `torch>=2.0.0`).\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad(set_to_none=True)\r\n\r\n        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.\r\n\r\n        \"\"\"\r\n        optimizer.zero_grad()", "language": "python", "code": "def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer) -> None:\r\n        \"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n\r\n        Examples::\r\n\r\n            # DEFAULT\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad()\r\n\r\n            # Set gradients to `None` instead of zero to improve performance (not required on `torch>=2.0.0`).\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad(set_to_none=True)\r\n\r\n        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.\r\n\r\n        \"\"\"\r\n        optimizer.zero_grad()", "code_tokens": ["def", "optimizer_zero_grad", "(", "self", ",", "epoch", ":", "int", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Override", "this", "method", "to", "change", "the", "default", "behaviour", "of", "`", "`", "optimizer", ".", "zero_grad", "(", ")", "`", "`", ".", "Args", ":", "epoch", ":", "Current", "epoch", "batch_idx", ":", "Index", "of", "current", "batch", "optimizer", ":", "A", "PyTorch", "optimizer", "Examples", ":", ":", "def", "optimizer_zero_grad", "(", "self", ",", "epoch", ",", "batch_idx", ",", "optimizer", ")", ":", "optimizer", ".", "zero_grad", "(", ")", "def", "optimizer_zero_grad", "(", "self", ",", "epoch", ",", "batch_idx", ",", "optimizer", ")", ":", "optimizer", ".", "zero_grad", "(", "set_to_none", "=", "True", ")", "See", ":", "meth", ":", "`", "torch", ".", "optim", ".", "Optimizer", ".", "zero_grad", "`", "for", "the", "explanation", "of", "the", "above", "example", ".", "\"", "\"", "\"", "optimizer", ".", "zero_grad", "(", ")"], "docstring": "Override this method to change the default behaviour of ``optimizer.zero_grad()``.\r\n\r\n        Args:\r\n            epoch: Current epoch\r\n            batch_idx: Index of current batch\r\n            optimizer: A PyTorch optimizer\r\n\r\n        Examples::\r\n\r\n            # DEFAULT\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad()\r\n\r\n            # Set gradients to `None` instead of zero to improve performance (not required on `torch>=2.0.0`).\r\n            def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\r\n                optimizer.zero_grad(set_to_none=True)\r\n\r\n        See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.", "docstring_tokens": ["override", "this", "method", "to", "change", "the", "default", "behaviour", "of", "optimizer", "zero_grad", "args", "epoch", "current", "epoch", "batch_idx", "index", "of", "current", "batch", "optimizer", "a", "pytorch", "optimizer", "examples", "default", "def", "optimizer_zero_grad", "self", "epoch", "batch_idx", "optimizer", "optimizer", "zero_grad", "set", "gradients", "to", "none", "instead", "of", "zero", "to", "improve", "performance", "not", "required", "on", "torch", "2", "0", "0", "def", "optimizer_zero_grad", "self", "epoch", "batch_idx", "optimizer", "optimizer", "zero_grad", "set_to_none", "true", "see", "meth", "torch", "optim", "optimizer", "zero_grad", "for", "the", "explanation", "of", "the", "above", "example"], "docstring_summary": "Override this method to change the default behaviour of ``optimizer.zero_grad()``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1369, "end_line": 1390, "hash": "93cfc12c7851ae2785298dbca3e370a2", "complexity": 1, "parameters": ["epoch", "batch_idx", "optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "freeze", "original_string": "def freeze(self) -> None:\r\n        r\"\"\"Freeze all params for inference.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule(...)\r\n            model.freeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = False\r\n\r\n        self.eval()", "language": "python", "code": "def freeze(self) -> None:\r\n        r\"\"\"Freeze all params for inference.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule(...)\r\n            model.freeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = False\r\n\r\n        self.eval()", "code_tokens": ["def", "freeze", "(", "self", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Freeze", "all", "params", "for", "inference", ".", "Example", ":", ":", "model", "=", "MyLightningModule", "(", ".", ".", ".", ")", "model", ".", "freeze", "(", ")", "\"", "\"", "\"", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "param", ".", "requires_grad", "=", "False", "self", ".", "eval", "(", ")"], "docstring": "r\"\"\"Freeze all params for inference.\r\n\r\n        Example::\r\n\r\n            model = MyLightningModule(...)\r\n            model.freeze()\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "freeze", "all", "params", "for", "inference", "example", "model", "mylightningmodule", "model", "freeze"], "docstring_summary": "r\"\"\"Freeze all params for inference.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1392, "end_line": 1404, "hash": "f0f369223cf38935e8de4213c8fc68a0", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "unfreeze", "original_string": "def unfreeze(self) -> None:\r\n        \"\"\"Unfreeze all parameters for training.\r\n\r\n        .. code-block:: python\r\n\r\n            model = MyLightningModule(...)\r\n            model.unfreeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = True\r\n\r\n        self.train()", "language": "python", "code": "def unfreeze(self) -> None:\r\n        \"\"\"Unfreeze all parameters for training.\r\n\r\n        .. code-block:: python\r\n\r\n            model = MyLightningModule(...)\r\n            model.unfreeze()\r\n\r\n        \"\"\"\r\n        for param in self.parameters():\r\n            param.requires_grad = True\r\n\r\n        self.train()", "code_tokens": ["def", "unfreeze", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Unfreeze", "all", "parameters", "for", "training", ".", ".", ".", "code", "-", "block", ":", ":", "python", "model", "=", "MyLightningModule", "(", ".", ".", ".", ")", "model", ".", "unfreeze", "(", ")", "\"", "\"", "\"", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "param", ".", "requires_grad", "=", "True", "self", ".", "train", "(", ")"], "docstring": "Unfreeze all parameters for training.\r\n\r\n        .. code-block:: python\r\n\r\n            model = MyLightningModule(...)\r\n            model.unfreeze()", "docstring_tokens": ["unfreeze", "all", "parameters", "for", "training", "code", "block", "python", "model", "mylightningmodule", "model", "unfreeze"], "docstring_summary": "Unfreeze all parameters for training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1406, "end_line": 1418, "hash": "dd7cdb9ebe79d8f654de64a429c7bea4", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "to_onnx", "original_string": "def to_onnx(\r\n        self,\r\n        file_path: Union[str, Path, BytesIO, None] = None,\r\n        input_sample: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Optional[\"ONNXProgram\"]:\r\n        \"\"\"Saves the model in ONNX format.\r\n\r\n        Args:\r\n            file_path: The path of the file the onnx model should be saved to. Default: None (no file saved).\r\n            input_sample: An input for tracing. Default: None (Use self.example_input_array)\r\n\r\n            **kwargs: Will be passed to torch.onnx.export function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            model.to_onnx(\"export.onnx\", input_sample, export_params=True)\r\n\r\n        \"\"\"\r\n        if not _ONNX_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"`{type(self).__name__}.to_onnx()` requires `onnx` to be installed.\")\r\n\r\n        if kwargs.get(\"dynamo\", False) and not (_ONNXSCRIPT_AVAILABLE and _TORCH_GREATER_EQUAL_2_5):\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_onnx(dynamo=True)` \"\r\n                \"requires `onnxscript` and `torch>=2.5.0` to be installed.\"\r\n            )\r\n\r\n        mode = self.training\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to ONNX since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        file_path = str(file_path) if isinstance(file_path, Path) else file_path\r\n        # PyTorch (2.5) declares file_path to be str | PathLike[Any] | None, but\r\n        #               BytesIO does work, too.\r\n        ret = torch.onnx.export(self, input_sample, file_path, **kwargs)  # type: ignore\r\n        self.train(mode)\r\n        return ret", "language": "python", "code": "def to_onnx(\r\n        self,\r\n        file_path: Union[str, Path, BytesIO, None] = None,\r\n        input_sample: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Optional[\"ONNXProgram\"]:\r\n        \"\"\"Saves the model in ONNX format.\r\n\r\n        Args:\r\n            file_path: The path of the file the onnx model should be saved to. Default: None (no file saved).\r\n            input_sample: An input for tracing. Default: None (Use self.example_input_array)\r\n\r\n            **kwargs: Will be passed to torch.onnx.export function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            model.to_onnx(\"export.onnx\", input_sample, export_params=True)\r\n\r\n        \"\"\"\r\n        if not _ONNX_AVAILABLE:\r\n            raise ModuleNotFoundError(f\"`{type(self).__name__}.to_onnx()` requires `onnx` to be installed.\")\r\n\r\n        if kwargs.get(\"dynamo\", False) and not (_ONNXSCRIPT_AVAILABLE and _TORCH_GREATER_EQUAL_2_5):\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_onnx(dynamo=True)` \"\r\n                \"requires `onnxscript` and `torch>=2.5.0` to be installed.\"\r\n            )\r\n\r\n        mode = self.training\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to ONNX since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        file_path = str(file_path) if isinstance(file_path, Path) else file_path\r\n        # PyTorch (2.5) declares file_path to be str | PathLike[Any] | None, but\r\n        #               BytesIO does work, too.\r\n        ret = torch.onnx.export(self, input_sample, file_path, **kwargs)  # type: ignore\r\n        self.train(mode)\r\n        return ret", "code_tokens": ["def", "to_onnx", "(", "self", ",", "file_path", ":", "Union", "[", "str", ",", "Path", ",", "BytesIO", ",", "None", "]", "=", "None", ",", "input_sample", ":", "Optional", "[", "Any", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Optional", "[", "\"", "ONNXProgram", "\"", "]", ":", "\"", "\"", "\"", "Saves", "the", "model", "in", "ONNX", "format", ".", "Args", ":", "file_path", ":", "The", "path", "of", "the", "file", "the", "onnx", "model", "should", "be", "saved", "to", ".", "Default", ":", "None", "(", "no", "file", "saved", ")", ".", "input_sample", ":", "An", "input", "for", "tracing", ".", "Default", ":", "None", "(", "Use", "self", ".", "example_input_array", ")", "*", "*", "kwargs", ":", "Will", "be", "passed", "to", "torch", ".", "onnx", ".", "export", "function", ".", "Example", ":", ":", "class", "SimpleModel", "(", "LightningModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "l1", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "64", ",", "out_features", "=", "4", ")", "def", "forward", "(", "self", ",", "x", ")", ":", "return", "torch", ".", "relu", "(", "self", ".", "l1", "(", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "model", "=", "SimpleModel", "(", ")", "input_sample", "=", "torch", ".", "randn", "(", "1", ",", "64", ")", "model", ".", "to_onnx", "(", "\"", "export", ".", "onnx", "\"", ",", "input_sample", ",", "export_params", "=", "True", ")", "\"", "\"", "\"", "if", "not", "_ONNX_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "to_onnx", "(", ")", "`", "requires", "`", "onnx", "`", "to", "be", "installed", ".", "\"", ")", "if", "kwargs", ".", "get", "(", "\"", "dynamo", "\"", ",", "False", ")", "and", "not", "(", "_ONNXSCRIPT_AVAILABLE", "and", "_TORCH_GREATER_EQUAL_2_5", ")", ":", "raise", "ModuleNotFoundError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "to_onnx", "(", "dynamo", "=", "True", ")", "`", "\"", "\"", "requires", "`", "onnxscript", "`", "and", "`", "torch", ">", "=", "2", ".", "5", ".", "0", "`", "to", "be", "installed", ".", "\"", ")", "mode", "=", "self", ".", "training", "if", "input_sample", "is", "None", ":", "if", "self", ".", "example_input_array", "is", "None", ":", "raise", "ValueError", "(", "\"", "Could", "not", "export", "to", "ONNX", "since", "neither", "`", "input_sample", "`", "nor", "\"", "\"", "`", "model", ".", "example_input_array", "`", "attribute", "is", "set", ".", "\"", ")", "input_sample", "=", "self", ".", "example_input_array", "input_sample", "=", "self", ".", "_on_before_batch_transfer", "(", "input_sample", ")", "input_sample", "=", "self", ".", "_apply_batch_transfer_handler", "(", "input_sample", ")", "file_path", "=", "str", "(", "file_path", ")", "if", "isinstance", "(", "file_path", ",", "Path", ")", "else", "file_path", "ret", "=", "torch", ".", "onnx", ".", "export", "(", "self", ",", "input_sample", ",", "file_path", ",", "*", "*", "kwargs", ")", "self", ".", "train", "(", "mode", ")", "return", "ret"], "docstring": "Saves the model in ONNX format.\r\n\r\n        Args:\r\n            file_path: The path of the file the onnx model should be saved to. Default: None (no file saved).\r\n            input_sample: An input for tracing. Default: None (Use self.example_input_array)\r\n\r\n            **kwargs: Will be passed to torch.onnx.export function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            model.to_onnx(\"export.onnx\", input_sample, export_params=True)", "docstring_tokens": ["saves", "the", "model", "in", "onnx", "format", "args", "file_path", "the", "path", "of", "the", "file", "the", "onnx", "model", "should", "be", "saved", "to", "default", "none", "no", "file", "saved", "input_sample", "an", "input", "for", "tracing", "default", "none", "use", "self", "example_input_array", "kwargs", "will", "be", "passed", "to", "torch", "onnx", "export", "function", "example", "class", "simplemodel", "lightningmodule", "def", "__init__", "self", "super", "__init__", "self", "l1", "torch", "nn", "linear", "in_features", "64", "out_features", "4", "def", "forward", "self", "x", "return", "torch", "relu", "self", "l1", "x", "view", "x", "size", "0", "1", "model", "simplemodel", "input_sample", "torch", "randn", "1", "64", "model", "to_onnx", "export", "onnx", "input_sample", "export_params", "true"], "docstring_summary": "Saves the model in ONNX format.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1428, "end_line": 1484, "hash": "552e9a286ee80b82b0b2daaf0d5a76fb", "complexity": 8, "parameters": ["file_path", "Path", "BytesIO", "None]", "input_sample", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "to_torchscript", "original_string": "def to_torchscript(\r\n        self,\r\n        file_path: Optional[Union[str, Path]] = None,\r\n        method: Optional[str] = \"script\",\r\n        example_inputs: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Union[ScriptModule, dict[str, ScriptModule]]:\r\n        \"\"\"By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\r\n        please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is\r\n        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are\r\n        scripted you should override this method. In case you want to return multiple modules, we recommend using a\r\n        dictionary.\r\n\r\n        Args:\r\n            file_path: Path where to save the torchscript. Default: None (no file saved).\r\n            method: Whether to use TorchScript's script or trace method. Default: 'script'\r\n            example_inputs: An input to be used to do tracing when method is set to 'trace'.\r\n              Default: None (uses :attr:`example_input_array`)\r\n            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or\r\n              :func:`torch.jit.trace` function.\r\n\r\n        Note:\r\n            - Requires the implementation of the\r\n              :meth:`~lightning.pytorch.core.LightningModule.forward` method.\r\n            - The exported script will be set to evaluation mode.\r\n            - It is recommended that you install the latest supported version of PyTorch\r\n              to use this feature without limitations. See also the :mod:`torch.jit`\r\n              documentation for supported features.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n            model = SimpleModel()\r\n            model.to_torchscript(file_path=\"model.pt\")\r\n\r\n            torch.jit.save(model.to_torchscript(\r\n                file_path=\"model_trace.pt\", method='trace', example_inputs=torch.randn(1, 64))\r\n            )\r\n\r\n        Return:\r\n            This LightningModule as a torchscript, regardless of whether `file_path` is\r\n            defined or not.\r\n\r\n        \"\"\"\r\n        mode = self.training\r\n\r\n        if method == \"script\":\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.script(self.eval(), **kwargs)\r\n        elif method == \"trace\":\r\n            # if no example inputs are provided, try to see if model has example_input_array set\r\n            if example_inputs is None:\r\n                if self.example_input_array is None:\r\n                    raise ValueError(\r\n                        \"Choosing method=`trace` requires either `example_inputs`\"\r\n                        \" or `model.example_input_array` to be defined.\"\r\n                    )\r\n                example_inputs = self.example_input_array\r\n\r\n            if kwargs.get(\"check_inputs\") is not None:\r\n                kwargs[\"check_inputs\"] = self._on_before_batch_transfer(kwargs[\"check_inputs\"])\r\n                kwargs[\"check_inputs\"] = self._apply_batch_transfer_handler(kwargs[\"check_inputs\"])\r\n\r\n            # automatically send example inputs to the right device and use trace\r\n            example_inputs = self._on_before_batch_transfer(example_inputs)\r\n            example_inputs = self._apply_batch_transfer_handler(example_inputs)\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.trace(func=self.eval(), example_inputs=example_inputs, **kwargs)\r\n        else:\r\n            raise ValueError(f\"The 'method' parameter only supports 'script' or 'trace', but value given was: {method}\")\r\n\r\n        self.train(mode)\r\n\r\n        if file_path is not None:\r\n            fs = get_filesystem(file_path)\r\n            with fs.open(file_path, \"wb\") as f:\r\n                torch.jit.save(torchscript_module, f)\r\n\r\n        return torchscript_module", "language": "python", "code": "def to_torchscript(\r\n        self,\r\n        file_path: Optional[Union[str, Path]] = None,\r\n        method: Optional[str] = \"script\",\r\n        example_inputs: Optional[Any] = None,\r\n        **kwargs: Any,\r\n    ) -> Union[ScriptModule, dict[str, ScriptModule]]:\r\n        \"\"\"By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\r\n        please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is\r\n        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are\r\n        scripted you should override this method. In case you want to return multiple modules, we recommend using a\r\n        dictionary.\r\n\r\n        Args:\r\n            file_path: Path where to save the torchscript. Default: None (no file saved).\r\n            method: Whether to use TorchScript's script or trace method. Default: 'script'\r\n            example_inputs: An input to be used to do tracing when method is set to 'trace'.\r\n              Default: None (uses :attr:`example_input_array`)\r\n            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or\r\n              :func:`torch.jit.trace` function.\r\n\r\n        Note:\r\n            - Requires the implementation of the\r\n              :meth:`~lightning.pytorch.core.LightningModule.forward` method.\r\n            - The exported script will be set to evaluation mode.\r\n            - It is recommended that you install the latest supported version of PyTorch\r\n              to use this feature without limitations. See also the :mod:`torch.jit`\r\n              documentation for supported features.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n            model = SimpleModel()\r\n            model.to_torchscript(file_path=\"model.pt\")\r\n\r\n            torch.jit.save(model.to_torchscript(\r\n                file_path=\"model_trace.pt\", method='trace', example_inputs=torch.randn(1, 64))\r\n            )\r\n\r\n        Return:\r\n            This LightningModule as a torchscript, regardless of whether `file_path` is\r\n            defined or not.\r\n\r\n        \"\"\"\r\n        mode = self.training\r\n\r\n        if method == \"script\":\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.script(self.eval(), **kwargs)\r\n        elif method == \"trace\":\r\n            # if no example inputs are provided, try to see if model has example_input_array set\r\n            if example_inputs is None:\r\n                if self.example_input_array is None:\r\n                    raise ValueError(\r\n                        \"Choosing method=`trace` requires either `example_inputs`\"\r\n                        \" or `model.example_input_array` to be defined.\"\r\n                    )\r\n                example_inputs = self.example_input_array\r\n\r\n            if kwargs.get(\"check_inputs\") is not None:\r\n                kwargs[\"check_inputs\"] = self._on_before_batch_transfer(kwargs[\"check_inputs\"])\r\n                kwargs[\"check_inputs\"] = self._apply_batch_transfer_handler(kwargs[\"check_inputs\"])\r\n\r\n            # automatically send example inputs to the right device and use trace\r\n            example_inputs = self._on_before_batch_transfer(example_inputs)\r\n            example_inputs = self._apply_batch_transfer_handler(example_inputs)\r\n            with _jit_is_scripting():\r\n                torchscript_module = torch.jit.trace(func=self.eval(), example_inputs=example_inputs, **kwargs)\r\n        else:\r\n            raise ValueError(f\"The 'method' parameter only supports 'script' or 'trace', but value given was: {method}\")\r\n\r\n        self.train(mode)\r\n\r\n        if file_path is not None:\r\n            fs = get_filesystem(file_path)\r\n            with fs.open(file_path, \"wb\") as f:\r\n                torch.jit.save(torchscript_module, f)\r\n\r\n        return torchscript_module", "code_tokens": ["def", "to_torchscript", "(", "self", ",", "file_path", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "method", ":", "Optional", "[", "str", "]", "=", "\"", "script", "\"", ",", "example_inputs", ":", "Optional", "[", "Any", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Union", "[", "ScriptModule", ",", "dict", "[", "str", ",", "ScriptModule", "]", "]", ":", "\"", "\"", "\"", "By", "default", "compiles", "the", "whole", "model", "to", "a", ":", "class", ":", "`", "~", "torch", ".", "jit", ".", "ScriptModule", "`", ".", "If", "you", "want", "to", "use", "tracing", ",", "please", "provided", "the", "argument", "`", "`", "method", "=", "'", "trace", "'", "`", "`", "and", "make", "sure", "that", "either", "the", "`", "example_inputs", "`", "argument", "is", "provided", ",", "or", "the", "model", "has", ":", "attr", ":", "`", "example_input_array", "`", "set", ".", "If", "you", "would", "like", "to", "customize", "the", "modules", "that", "are", "scripted", "you", "should", "override", "this", "method", ".", "In", "case", "you", "want", "to", "return", "multiple", "modules", ",", "we", "recommend", "using", "a", "dictionary", ".", "Args", ":", "file_path", ":", "Path", "where", "to", "save", "the", "torchscript", ".", "Default", ":", "None", "(", "no", "file", "saved", ")", ".", "method", ":", "Whether", "to", "use", "TorchScript", "'", "s", "script", "or", "trace", "method", ".", "Default", ":", "'", "script", "'", "example_inputs", ":", "An", "input", "to", "be", "used", "to", "do", "tracing", "when", "method", "is", "set", "to", "'", "trace", "'", ".", "Default", ":", "None", "(", "uses", ":", "attr", ":", "`", "example_input_array", "`", ")", "*", "*", "kwargs", ":", "Additional", "arguments", "that", "will", "be", "passed", "to", "the", ":", "func", ":", "`", "torch", ".", "jit", ".", "script", "`", "or", ":", "func", ":", "`", "torch", ".", "jit", ".", "trace", "`", "function", ".", "Note", ":", "-", "Requires", "the", "implementation", "of", "the", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", ".", "forward", "`", "method", ".", "-", "The", "exported", "script", "will", "be", "set", "to", "evaluation", "mode", ".", "-", "It", "is", "recommended", "that", "you", "install", "the", "latest", "supported", "version", "of", "PyTorch", "to", "use", "this", "feature", "without", "limitations", ".", "See", "also", "the", ":", "mod", ":", "`", "torch", ".", "jit", "`", "documentation", "for", "supported", "features", ".", "Example", ":", ":", "class", "SimpleModel", "(", "LightningModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "l1", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "64", ",", "out_features", "=", "4", ")", "def", "forward", "(", "self", ",", "x", ")", ":", "return", "torch", ".", "relu", "(", "self", ".", "l1", "(", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", ")", "model", "=", "SimpleModel", "(", ")", "model", ".", "to_torchscript", "(", "file_path", "=", "\"", "model", ".", "pt", "\"", ")", "torch", ".", "jit", ".", "save", "(", "model", ".", "to_torchscript", "(", "file_path", "=", "\"", "model_trace", ".", "pt", "\"", ",", "method", "=", "'", "trace", "'", ",", "example_inputs", "=", "torch", ".", "randn", "(", "1", ",", "64", ")", ")", ")", "Return", ":", "This", "LightningModule", "as", "a", "torchscript", ",", "regardless", "of", "whether", "`", "file_path", "`", "is", "defined", "or", "not", ".", "\"", "\"", "\"", "mode", "=", "self", ".", "training", "if", "method", "=", "=", "\"", "script", "\"", ":", "with", "_jit_is_scripting", "(", ")", ":", "torchscript_module", "=", "torch", ".", "jit", ".", "script", "(", "self", ".", "eval", "(", ")", ",", "*", "*", "kwargs", ")", "elif", "method", "=", "=", "\"", "trace", "\"", ":", "if", "example_inputs", "is", "None", ":", "if", "self", ".", "example_input_array", "is", "None", ":", "raise", "ValueError", "(", "\"", "Choosing", "method", "=", "`", "trace", "`", "requires", "either", "`", "example_inputs", "`", "\"", "\"", "or", "`", "model", ".", "example_input_array", "`", "to", "be", "defined", ".", "\"", ")", "example_inputs", "=", "self", ".", "example_input_array", "if", "kwargs", ".", "get", "(", "\"", "check_inputs", "\"", ")", "is", "not", "None", ":", "kwargs", "[", "\"", "check_inputs", "\"", "]", "=", "self", ".", "_on_before_batch_transfer", "(", "kwargs", "[", "\"", "check_inputs", "\"", "]", ")", "kwargs", "[", "\"", "check_inputs", "\"", "]", "=", "self", ".", "_apply_batch_transfer_handler", "(", "kwargs", "[", "\"", "check_inputs", "\"", "]", ")", "example_inputs", "=", "self", ".", "_on_before_batch_transfer", "(", "example_inputs", ")", "example_inputs", "=", "self", ".", "_apply_batch_transfer_handler", "(", "example_inputs", ")", "with", "_jit_is_scripting", "(", ")", ":", "torchscript_module", "=", "torch", ".", "jit", ".", "trace", "(", "func", "=", "self", ".", "eval", "(", ")", ",", "example_inputs", "=", "example_inputs", ",", "*", "*", "kwargs", ")", "else", ":", "raise", "ValueError", "(", "f", "\"", "The", "'", "method", "'", "parameter", "only", "supports", "'", "script", "'", "or", "'", "trace", "'", ",", "but", "value", "given", "was", ":", "{", "method", "}", "\"", ")", "self", ".", "train", "(", "mode", ")", "if", "file_path", "is", "not", "None", ":", "fs", "=", "get_filesystem", "(", "file_path", ")", "with", "fs", ".", "open", "(", "file_path", ",", "\"", "wb", "\"", ")", "as", "f", ":", "torch", ".", "jit", ".", "save", "(", "torchscript_module", ",", "f", ")", "return", "torchscript_module"], "docstring": "By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\r\n        please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is\r\n        provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are\r\n        scripted you should override this method. In case you want to return multiple modules, we recommend using a\r\n        dictionary.\r\n\r\n        Args:\r\n            file_path: Path where to save the torchscript. Default: None (no file saved).\r\n            method: Whether to use TorchScript's script or trace method. Default: 'script'\r\n            example_inputs: An input to be used to do tracing when method is set to 'trace'.\r\n              Default: None (uses :attr:`example_input_array`)\r\n            **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or\r\n              :func:`torch.jit.trace` function.\r\n\r\n        Note:\r\n            - Requires the implementation of the\r\n              :meth:`~lightning.pytorch.core.LightningModule.forward` method.\r\n            - The exported script will be set to evaluation mode.\r\n            - It is recommended that you install the latest supported version of PyTorch\r\n              to use this feature without limitations. See also the :mod:`torch.jit`\r\n              documentation for supported features.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n            model = SimpleModel()\r\n            model.to_torchscript(file_path=\"model.pt\")\r\n\r\n            torch.jit.save(model.to_torchscript(\r\n                file_path=\"model_trace.pt\", method='trace', example_inputs=torch.randn(1, 64))\r\n            )\r\n\r\n        Return:\r\n            This LightningModule as a torchscript, regardless of whether `file_path` is\r\n            defined or not.", "docstring_tokens": ["by", "default", "compiles", "the", "whole", "model", "to", "a", "class", "torch", "jit", "scriptmodule", "if", "you", "want", "to", "use", "tracing", "please", "provided", "the", "argument", "method", "trace", "and", "make", "sure", "that", "either", "the", "example_inputs", "argument", "is", "provided", "or", "the", "model", "has", "attr", "example_input_array", "set", "if", "you", "would", "like", "to", "customize", "the", "modules", "that", "are", "scripted", "you", "should", "override", "this", "method", "in", "case", "you", "want", "to", "return", "multiple", "modules", "we", "recommend", "using", "a", "dictionary", "args", "file_path", "path", "where", "to", "save", "the", "torchscript", "default", "none", "no", "file", "saved", "method", "whether", "to", "use", "torchscript", "s", "script", "or", "trace", "method", "default", "script", "example_inputs", "an", "input", "to", "be", "used", "to", "do", "tracing", "when", "method", "is", "set", "to", "trace", "default", "none", "uses", "attr", "example_input_array", "kwargs", "additional", "arguments", "that", "will", "be", "passed", "to", "the", "func", "torch", "jit", "script", "or", "func", "torch", "jit", "trace", "function", "note", "requires", "the", "implementation", "of", "the", "meth", "lightning", "pytorch", "core", "lightningmodule", "forward", "method", "the", "exported", "script", "will", "be", "set", "to", "evaluation", "mode", "it", "is", "recommended", "that", "you", "install", "the", "latest", "supported", "version", "of", "pytorch", "to", "use", "this", "feature", "without", "limitations", "see", "also", "the", "mod", "torch", "jit", "documentation", "for", "supported", "features", "example", "class", "simplemodel", "lightningmodule", "def", "__init__", "self", "super", "__init__", "self", "l1", "torch", "nn", "linear", "in_features", "64", "out_features", "4", "def", "forward", "self", "x", "return", "torch", "relu", "self", "l1", "x", "view", "x", "size", "0", "1", "model", "simplemodel", "model", "to_torchscript", "file_path", "model", "pt", "torch", "jit", "save", "model", "to_torchscript", "file_path", "model_trace", "pt", "method", "trace", "example_inputs", "torch", "randn", "1", "64", "return", "this", "lightningmodule", "as", "a", "torchscript", "regardless", "of", "whether", "file_path", "is", "defined", "or", "not"], "docstring_summary": "By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1487, "end_line": 1572, "hash": "71e4d07518fa6f9d0b77c2d32447be57", "complexity": 10, "parameters": ["file_path", "Path]]", "method", "example_inputs", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "to_tensorrt", "original_string": "def to_tensorrt(\r\n        self,\r\n        file_path: Optional[Union[str, Path, BytesIO]] = None,\r\n        input_sample: Optional[Any] = None,\r\n        ir: Literal[\"default\", \"dynamo\", \"ts\"] = \"default\",\r\n        output_format: Literal[\"exported_program\", \"torchscript\"] = \"exported_program\",\r\n        retrace: bool = False,\r\n        default_device: Union[str, torch.device] = \"cuda\",\r\n        **compile_kwargs: Any,\r\n    ) -> Union[ScriptModule, torch.fx.GraphModule]:\r\n        \"\"\"Export the model to ScriptModule or GraphModule using TensorRT compile backend.\r\n\r\n        Args:\r\n            file_path: Path where to save the tensorrt model. Default: None (no file saved).\r\n            input_sample: inputs to be used during `torch_tensorrt.compile`.\r\n                Default: None (Use :attr:`example_input_array`).\r\n            ir: The IR mode to use for TensorRT compilation. Default: \"default\".\r\n            output_format: The format of the output model. Default: \"exported_program\".\r\n            retrace: Whether to retrace the model. Default: False.\r\n            default_device: The device to use for the model when the current model is not in CUDA. Default: \"cuda\".\r\n            **compile_kwargs: Additional arguments that will be passed to the TensorRT compile function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            exported_program = model.to_tensorrt(\r\n                file_path=\"export.ep\",\r\n                inputs=input_sample,\r\n            )\r\n\r\n        \"\"\"\r\n        if not _TORCH_GREATER_EQUAL_2_2:\r\n            raise MisconfigurationException(\r\n                f\"TensorRT export requires PyTorch 2.2 or higher. Current version is {torch.__version__}.\"\r\n            )\r\n\r\n        if not _TORCH_TRT_AVAILABLE:\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_tensorrt` requires `torch_tensorrt` to be installed. \"\r\n            )\r\n\r\n        mode = self.training\r\n        device = self.device\r\n        if self.device.type != \"cuda\":\r\n            default_device = torch.device(default_device) if isinstance(default_device, str) else default_device\r\n\r\n            if not torch.cuda.is_available() or default_device.type != \"cuda\":\r\n                raise MisconfigurationException(\r\n                    f\"TensorRT only supports CUDA devices. The current device is {self.device}.\"\r\n                    f\" Please set the `default_device` argument to a CUDA device.\"\r\n                )\r\n\r\n            self.to(default_device)\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to TensorRT since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        import torch_tensorrt\r\n\r\n        input_sample = copy.deepcopy((input_sample,) if isinstance(input_sample, torch.Tensor) else input_sample)\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        with _jit_is_scripting() if ir == \"ts\" else nullcontext():\r\n            trt_obj = torch_tensorrt.compile(\r\n                module=self.eval(),\r\n                ir=ir,\r\n                inputs=input_sample,\r\n                **compile_kwargs,\r\n            )\r\n        self.train(mode)\r\n        self.to(device)\r\n\r\n        if file_path is not None:\r\n            if ir == \"ts\":\r\n                if output_format != \"torchscript\":\r\n                    raise ValueError(\r\n                        \"TensorRT with IR mode 'ts' only supports output format 'torchscript'.\"\r\n                        f\" The current output format is {output_format}.\"\r\n                    )\r\n                assert isinstance(trt_obj, (torch.jit.ScriptModule, torch.jit.ScriptFunction)), (\r\n                    f\"Expected TensorRT object to be a ScriptModule, but got {type(trt_obj)}.\"\r\n                )\r\n                # Because of https://github.com/pytorch/TensorRT/issues/3775,\r\n                # we'll need to take special care for the ScriptModule\r\n                torch.jit.save(trt_obj, file_path)\r\n            else:\r\n                torch_tensorrt.save(\r\n                    trt_obj,\r\n                    file_path,\r\n                    inputs=input_sample,\r\n                    output_format=output_format,\r\n                    retrace=retrace,\r\n                )\r\n        return trt_obj", "language": "python", "code": "def to_tensorrt(\r\n        self,\r\n        file_path: Optional[Union[str, Path, BytesIO]] = None,\r\n        input_sample: Optional[Any] = None,\r\n        ir: Literal[\"default\", \"dynamo\", \"ts\"] = \"default\",\r\n        output_format: Literal[\"exported_program\", \"torchscript\"] = \"exported_program\",\r\n        retrace: bool = False,\r\n        default_device: Union[str, torch.device] = \"cuda\",\r\n        **compile_kwargs: Any,\r\n    ) -> Union[ScriptModule, torch.fx.GraphModule]:\r\n        \"\"\"Export the model to ScriptModule or GraphModule using TensorRT compile backend.\r\n\r\n        Args:\r\n            file_path: Path where to save the tensorrt model. Default: None (no file saved).\r\n            input_sample: inputs to be used during `torch_tensorrt.compile`.\r\n                Default: None (Use :attr:`example_input_array`).\r\n            ir: The IR mode to use for TensorRT compilation. Default: \"default\".\r\n            output_format: The format of the output model. Default: \"exported_program\".\r\n            retrace: Whether to retrace the model. Default: False.\r\n            default_device: The device to use for the model when the current model is not in CUDA. Default: \"cuda\".\r\n            **compile_kwargs: Additional arguments that will be passed to the TensorRT compile function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            exported_program = model.to_tensorrt(\r\n                file_path=\"export.ep\",\r\n                inputs=input_sample,\r\n            )\r\n\r\n        \"\"\"\r\n        if not _TORCH_GREATER_EQUAL_2_2:\r\n            raise MisconfigurationException(\r\n                f\"TensorRT export requires PyTorch 2.2 or higher. Current version is {torch.__version__}.\"\r\n            )\r\n\r\n        if not _TORCH_TRT_AVAILABLE:\r\n            raise ModuleNotFoundError(\r\n                f\"`{type(self).__name__}.to_tensorrt` requires `torch_tensorrt` to be installed. \"\r\n            )\r\n\r\n        mode = self.training\r\n        device = self.device\r\n        if self.device.type != \"cuda\":\r\n            default_device = torch.device(default_device) if isinstance(default_device, str) else default_device\r\n\r\n            if not torch.cuda.is_available() or default_device.type != \"cuda\":\r\n                raise MisconfigurationException(\r\n                    f\"TensorRT only supports CUDA devices. The current device is {self.device}.\"\r\n                    f\" Please set the `default_device` argument to a CUDA device.\"\r\n                )\r\n\r\n            self.to(default_device)\r\n\r\n        if input_sample is None:\r\n            if self.example_input_array is None:\r\n                raise ValueError(\r\n                    \"Could not export to TensorRT since neither `input_sample` nor\"\r\n                    \" `model.example_input_array` attribute is set.\"\r\n                )\r\n            input_sample = self.example_input_array\r\n\r\n        import torch_tensorrt\r\n\r\n        input_sample = copy.deepcopy((input_sample,) if isinstance(input_sample, torch.Tensor) else input_sample)\r\n        input_sample = self._on_before_batch_transfer(input_sample)\r\n        input_sample = self._apply_batch_transfer_handler(input_sample)\r\n\r\n        with _jit_is_scripting() if ir == \"ts\" else nullcontext():\r\n            trt_obj = torch_tensorrt.compile(\r\n                module=self.eval(),\r\n                ir=ir,\r\n                inputs=input_sample,\r\n                **compile_kwargs,\r\n            )\r\n        self.train(mode)\r\n        self.to(device)\r\n\r\n        if file_path is not None:\r\n            if ir == \"ts\":\r\n                if output_format != \"torchscript\":\r\n                    raise ValueError(\r\n                        \"TensorRT with IR mode 'ts' only supports output format 'torchscript'.\"\r\n                        f\" The current output format is {output_format}.\"\r\n                    )\r\n                assert isinstance(trt_obj, (torch.jit.ScriptModule, torch.jit.ScriptFunction)), (\r\n                    f\"Expected TensorRT object to be a ScriptModule, but got {type(trt_obj)}.\"\r\n                )\r\n                # Because of https://github.com/pytorch/TensorRT/issues/3775,\r\n                # we'll need to take special care for the ScriptModule\r\n                torch.jit.save(trt_obj, file_path)\r\n            else:\r\n                torch_tensorrt.save(\r\n                    trt_obj,\r\n                    file_path,\r\n                    inputs=input_sample,\r\n                    output_format=output_format,\r\n                    retrace=retrace,\r\n                )\r\n        return trt_obj", "code_tokens": ["def", "to_tensorrt", "(", "self", ",", "file_path", ":", "Optional", "[", "Union", "[", "str", ",", "Path", ",", "BytesIO", "]", "]", "=", "None", ",", "input_sample", ":", "Optional", "[", "Any", "]", "=", "None", ",", "ir", ":", "Literal", "[", "\"", "default", "\"", ",", "\"", "dynamo", "\"", ",", "\"", "ts", "\"", "]", "=", "\"", "default", "\"", ",", "output_format", ":", "Literal", "[", "\"", "exported_program", "\"", ",", "\"", "torchscript", "\"", "]", "=", "\"", "exported_program", "\"", ",", "retrace", ":", "bool", "=", "False", ",", "default_device", ":", "Union", "[", "str", ",", "torch", ".", "device", "]", "=", "\"", "cuda", "\"", ",", "*", "*", "compile_kwargs", ":", "Any", ",", ")", "-", ">", "Union", "[", "ScriptModule", ",", "torch", ".", "fx", ".", "GraphModule", "]", ":", "\"", "\"", "\"", "Export", "the", "model", "to", "ScriptModule", "or", "GraphModule", "using", "TensorRT", "compile", "backend", ".", "Args", ":", "file_path", ":", "Path", "where", "to", "save", "the", "tensorrt", "model", ".", "Default", ":", "None", "(", "no", "file", "saved", ")", ".", "input_sample", ":", "inputs", "to", "be", "used", "during", "`", "torch_tensorrt", ".", "compile", "`", ".", "Default", ":", "None", "(", "Use", ":", "attr", ":", "`", "example_input_array", "`", ")", ".", "ir", ":", "The", "IR", "mode", "to", "use", "for", "TensorRT", "compilation", ".", "Default", ":", "\"", "default", "\"", ".", "output_format", ":", "The", "format", "of", "the", "output", "model", ".", "Default", ":", "\"", "exported_program", "\"", ".", "retrace", ":", "Whether", "to", "retrace", "the", "model", ".", "Default", ":", "False", ".", "default_device", ":", "The", "device", "to", "use", "for", "the", "model", "when", "the", "current", "model", "is", "not", "in", "CUDA", ".", "Default", ":", "\"", "cuda", "\"", ".", "*", "*", "compile_kwargs", ":", "Additional", "arguments", "that", "will", "be", "passed", "to", "the", "TensorRT", "compile", "function", ".", "Example", ":", ":", "class", "SimpleModel", "(", "LightningModule", ")", ":", "def", "__init__", "(", "self", ")", ":", "super", "(", ")", ".", "__init__", "(", ")", "self", ".", "l1", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "64", ",", "out_features", "=", "4", ")", "def", "forward", "(", "self", ",", "x", ")", ":", "return", "torch", ".", "relu", "(", "self", ".", "l1", "(", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "model", "=", "SimpleModel", "(", ")", "input_sample", "=", "torch", ".", "randn", "(", "1", ",", "64", ")", "exported_program", "=", "model", ".", "to_tensorrt", "(", "file_path", "=", "\"", "export", ".", "ep", "\"", ",", "inputs", "=", "input_sample", ",", ")", "\"", "\"", "\"", "if", "not", "_TORCH_GREATER_EQUAL_2_2", ":", "raise", "MisconfigurationException", "(", "f", "\"", "TensorRT", "export", "requires", "PyTorch", "2", ".", "2", "or", "higher", ".", "Current", "version", "is", "{", "torch", ".", "__version__", "}", ".", "\"", ")", "if", "not", "_TORCH_TRT_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "to_tensorrt", "`", "requires", "`", "torch_tensorrt", "`", "to", "be", "installed", ".", "\"", ")", "mode", "=", "self", ".", "training", "device", "=", "self", ".", "device", "if", "self", ".", "device", ".", "type", "!", "=", "\"", "cuda", "\"", ":", "default_device", "=", "torch", ".", "device", "(", "default_device", ")", "if", "isinstance", "(", "default_device", ",", "str", ")", "else", "default_device", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "or", "default_device", ".", "type", "!", "=", "\"", "cuda", "\"", ":", "raise", "MisconfigurationException", "(", "f", "\"", "TensorRT", "only", "supports", "CUDA", "devices", ".", "The", "current", "device", "is", "{", "self", ".", "device", "}", ".", "\"", "f", "\"", "Please", "set", "the", "`", "default_device", "`", "argument", "to", "a", "CUDA", "device", ".", "\"", ")", "self", ".", "to", "(", "default_device", ")", "if", "input_sample", "is", "None", ":", "if", "self", ".", "example_input_array", "is", "None", ":", "raise", "ValueError", "(", "\"", "Could", "not", "export", "to", "TensorRT", "since", "neither", "`", "input_sample", "`", "nor", "\"", "\"", "`", "model", ".", "example_input_array", "`", "attribute", "is", "set", ".", "\"", ")", "input_sample", "=", "self", ".", "example_input_array", "import", "torch_tensorrt", "input_sample", "=", "copy", ".", "deepcopy", "(", "(", "input_sample", ",", ")", "if", "isinstance", "(", "input_sample", ",", "torch", ".", "Tensor", ")", "else", "input_sample", ")", "input_sample", "=", "self", ".", "_on_before_batch_transfer", "(", "input_sample", ")", "input_sample", "=", "self", ".", "_apply_batch_transfer_handler", "(", "input_sample", ")", "with", "_jit_is_scripting", "(", ")", "if", "ir", "=", "=", "\"", "ts", "\"", "else", "nullcontext", "(", ")", ":", "trt_obj", "=", "torch_tensorrt", ".", "compile", "(", "module", "=", "self", ".", "eval", "(", ")", ",", "ir", "=", "ir", ",", "inputs", "=", "input_sample", ",", "*", "*", "compile_kwargs", ",", ")", "self", ".", "train", "(", "mode", ")", "self", ".", "to", "(", "device", ")", "if", "file_path", "is", "not", "None", ":", "if", "ir", "=", "=", "\"", "ts", "\"", ":", "if", "output_format", "!", "=", "\"", "torchscript", "\"", ":", "raise", "ValueError", "(", "\"", "TensorRT", "with", "IR", "mode", "'", "ts", "'", "only", "supports", "output", "format", "'", "torchscript", "'", ".", "\"", "f", "\"", "The", "current", "output", "format", "is", "{", "output_format", "}", ".", "\"", ")", "assert", "isinstance", "(", "trt_obj", ",", "(", "torch", ".", "jit", ".", "ScriptModule", ",", "torch", ".", "jit", ".", "ScriptFunction", ")", ")", ",", "(", "f", "\"", "Expected", "TensorRT", "object", "to", "be", "a", "ScriptModule", ",", "but", "got", "{", "type", "(", "trt_obj", ")", "}", ".", "\"", ")", "torch", ".", "jit", ".", "save", "(", "trt_obj", ",", "file_path", ")", "else", ":", "torch_tensorrt", ".", "save", "(", "trt_obj", ",", "file_path", ",", "inputs", "=", "input_sample", ",", "output_format", "=", "output_format", ",", "retrace", "=", "retrace", ",", ")", "return", "trt_obj"], "docstring": "Export the model to ScriptModule or GraphModule using TensorRT compile backend.\r\n\r\n        Args:\r\n            file_path: Path where to save the tensorrt model. Default: None (no file saved).\r\n            input_sample: inputs to be used during `torch_tensorrt.compile`.\r\n                Default: None (Use :attr:`example_input_array`).\r\n            ir: The IR mode to use for TensorRT compilation. Default: \"default\".\r\n            output_format: The format of the output model. Default: \"exported_program\".\r\n            retrace: Whether to retrace the model. Default: False.\r\n            default_device: The device to use for the model when the current model is not in CUDA. Default: \"cuda\".\r\n            **compile_kwargs: Additional arguments that will be passed to the TensorRT compile function.\r\n\r\n        Example::\r\n\r\n            class SimpleModel(LightningModule):\r\n                def __init__(self):\r\n                    super().__init__()\r\n                    self.l1 = torch.nn.Linear(in_features=64, out_features=4)\r\n\r\n                def forward(self, x):\r\n                    return torch.relu(self.l1(x.view(x.size(0), -1)\r\n\r\n            model = SimpleModel()\r\n            input_sample = torch.randn(1, 64)\r\n            exported_program = model.to_tensorrt(\r\n                file_path=\"export.ep\",\r\n                inputs=input_sample,\r\n            )", "docstring_tokens": ["export", "the", "model", "to", "scriptmodule", "or", "graphmodule", "using", "tensorrt", "compile", "backend", "args", "file_path", "path", "where", "to", "save", "the", "tensorrt", "model", "default", "none", "no", "file", "saved", "input_sample", "inputs", "to", "be", "used", "during", "torch_tensorrt", "compile", "default", "none", "use", "attr", "example_input_array", "ir", "the", "ir", "mode", "to", "use", "for", "tensorrt", "compilation", "default", "default", "output_format", "the", "format", "of", "the", "output", "model", "default", "exported_program", "retrace", "whether", "to", "retrace", "the", "model", "default", "false", "default_device", "the", "device", "to", "use", "for", "the", "model", "when", "the", "current", "model", "is", "not", "in", "cuda", "default", "cuda", "compile_kwargs", "additional", "arguments", "that", "will", "be", "passed", "to", "the", "tensorrt", "compile", "function", "example", "class", "simplemodel", "lightningmodule", "def", "__init__", "self", "super", "__init__", "self", "l1", "torch", "nn", "linear", "in_features", "64", "out_features", "4", "def", "forward", "self", "x", "return", "torch", "relu", "self", "l1", "x", "view", "x", "size", "0", "1", "model", "simplemodel", "input_sample", "torch", "randn", "1", "64", "exported_program", "model", "to_tensorrt", "file_path", "export", "ep", "inputs", "input_sample"], "docstring_summary": "Export the model to ScriptModule or GraphModule using TensorRT compile backend.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1575, "end_line": 1683, "hash": "8d3b5567fac7dd7431dd0d3e71c0fe1b", "complexity": 15, "parameters": ["file_path", "Path", "BytesIO]]", "input_sample", "ir", "\"dynamo\"", "\"ts\"]", "output_format", "\"torchscript\"]", "retrace", "default_device", "torch.device]", "**compile_kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "load_from_checkpoint", "original_string": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        strict: Optional[bool] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\r\n        passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    drop_prob: 0.2\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningModule` for use.\r\n\r\n                If your model's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your model to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict. Defaults to ``True`` unless ``LightningModule.strict_loading`` is\r\n                set, in which case it defaults to the value of ``LightningModule.strict_loading``.\r\n            \\**kwargs: Any extra keyword args needed to init the model. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You should use your :class:`LightningModule`\r\n            **class** to call it instead of the :class:`LightningModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Note:\r\n            To ensure all layers can be loaded from the checkpoint, this function will call\r\n            :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` directly after instantiating the\r\n            model if this hook is overridden in your LightningModule. However, note that ``load_from_checkpoint`` does\r\n            not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this\r\n            case, consider loading through the Trainer via ``.fit(ckpt_path=...)``.\r\n\r\n        Example::\r\n\r\n            # load weights without mapping ...\r\n            model = MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            # or load weights mapping all weights from GPU 1 to GPU 0 ...\r\n            map_location = {'cuda:1':'cuda:0'}\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                map_location=map_location\r\n            )\r\n\r\n            # or load weights and hyperparameters from separate files.\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            # override some of the params with new values\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                PATH,\r\n                num_layers=128,\r\n                pretrained_ckpt_path=NEW_PATH,\r\n            )\r\n\r\n            # predict\r\n            pretrained_model.eval()\r\n            pretrained_model.freeze()\r\n            y_hat = pretrained_model(x)\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location,\r\n            hparams_file,\r\n            strict,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "language": "python", "code": "def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: Union[_PATH, IO],\r\n        map_location: _MAP_LOCATION_TYPE = None,\r\n        hparams_file: Optional[_PATH] = None,\r\n        strict: Optional[bool] = None,\r\n        **kwargs: Any,\r\n    ) -> Self:\r\n        r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\r\n        passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    drop_prob: 0.2\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningModule` for use.\r\n\r\n                If your model's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your model to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict. Defaults to ``True`` unless ``LightningModule.strict_loading`` is\r\n                set, in which case it defaults to the value of ``LightningModule.strict_loading``.\r\n            \\**kwargs: Any extra keyword args needed to init the model. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You should use your :class:`LightningModule`\r\n            **class** to call it instead of the :class:`LightningModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Note:\r\n            To ensure all layers can be loaded from the checkpoint, this function will call\r\n            :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` directly after instantiating the\r\n            model if this hook is overridden in your LightningModule. However, note that ``load_from_checkpoint`` does\r\n            not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this\r\n            case, consider loading through the Trainer via ``.fit(ckpt_path=...)``.\r\n\r\n        Example::\r\n\r\n            # load weights without mapping ...\r\n            model = MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            # or load weights mapping all weights from GPU 1 to GPU 0 ...\r\n            map_location = {'cuda:1':'cuda:0'}\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                map_location=map_location\r\n            )\r\n\r\n            # or load weights and hyperparameters from separate files.\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            # override some of the params with new values\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                PATH,\r\n                num_layers=128,\r\n                pretrained_ckpt_path=NEW_PATH,\r\n            )\r\n\r\n            # predict\r\n            pretrained_model.eval()\r\n            pretrained_model.freeze()\r\n            y_hat = pretrained_model(x)\r\n\r\n        \"\"\"\r\n        loaded = _load_from_checkpoint(\r\n            cls,\r\n            checkpoint_path,\r\n            map_location,\r\n            hparams_file,\r\n            strict,\r\n            **kwargs,\r\n        )\r\n        return cast(Self, loaded)", "code_tokens": ["def", "load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ":", "Union", "[", "_PATH", ",", "IO", "]", ",", "map_location", ":", "_MAP_LOCATION_TYPE", "=", "None", ",", "hparams_file", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "strict", ":", "Optional", "[", "bool", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Self", ":", "r", "\"", "\"", "\"", "Primary", "way", "of", "loading", "a", "model", "from", "a", "checkpoint", ".", "When", "Lightning", "saves", "a", "checkpoint", "it", "stores", "the", "arguments", "passed", "to", "`", "`", "__init__", "`", "`", "in", "the", "checkpoint", "under", "`", "`", "\"", "hyper_parameters", "\"", "`", "`", ".", "Any", "arguments", "specified", "through", "\\", "*", "\\", "*", "kwargs", "will", "override", "args", "stored", "in", "`", "`", "\"", "hyper_parameters", "\"", "`", "`", ".", "Args", ":", "checkpoint_path", ":", "Path", "to", "checkpoint", ".", "This", "can", "also", "be", "a", "URL", ",", "or", "file", "-", "like", "object", "map_location", ":", "If", "your", "checkpoint", "saved", "a", "GPU", "model", "and", "you", "now", "load", "on", "CPUs", "or", "a", "different", "number", "of", "GPUs", ",", "use", "this", "to", "map", "to", "the", "new", "setup", ".", "The", "behaviour", "is", "the", "same", "as", "in", ":", "func", ":", "`", "torch", ".", "load", "`", ".", "hparams_file", ":", "Optional", "path", "to", "a", "`", "`", ".", "yaml", "`", "`", "or", "`", "`", ".", "csv", "`", "`", "file", "with", "hierarchical", "structure", "as", "in", "this", "example", ":", ":", "drop_prob", ":", "0", ".", "2", "dataloader", ":", "batch_size", ":", "32", "You", "most", "likely", "won", "'", "t", "need", "this", "since", "Lightning", "will", "always", "save", "the", "hyperparameters", "to", "the", "checkpoint", ".", "However", ",", "if", "your", "checkpoint", "weights", "don", "'", "t", "have", "the", "hyperparameters", "saved", ",", "use", "this", "method", "to", "pass", "in", "a", "`", "`", ".", "yaml", "`", "`", "file", "with", "the", "hparams", "you", "'", "d", "like", "to", "use", ".", "These", "will", "be", "converted", "into", "a", ":", "class", ":", "`", "~", "dict", "`", "and", "passed", "into", "your", ":", "class", ":", "`", "LightningModule", "`", "for", "use", ".", "If", "your", "model", "'", "s", "`", "`", "hparams", "`", "`", "argument", "is", ":", "class", ":", "`", "~", "argparse", ".", "Namespace", "`", "and", "`", "`", ".", "yaml", "`", "`", "file", "has", "hierarchical", "structure", ",", "you", "need", "to", "refactor", "your", "model", "to", "treat", "`", "`", "hparams", "`", "`", "as", ":", "class", ":", "`", "~", "dict", "`", ".", "strict", ":", "Whether", "to", "strictly", "enforce", "that", "the", "keys", "in", ":", "attr", ":", "`", "checkpoint_path", "`", "match", "the", "keys", "returned", "by", "this", "module", "'", "s", "state", "dict", ".", "Defaults", "to", "`", "`", "True", "`", "`", "unless", "`", "`", "LightningModule", ".", "strict_loading", "`", "`", "is", "set", ",", "in", "which", "case", "it", "defaults", "to", "the", "value", "of", "`", "`", "LightningModule", ".", "strict_loading", "`", "`", ".", "\\", "*", "*", "kwargs", ":", "Any", "extra", "keyword", "args", "needed", "to", "init", "the", "model", ".", "Can", "also", "be", "used", "to", "override", "saved", "hyperparameter", "values", ".", "Return", ":", ":", "class", ":", "`", "LightningModule", "`", "instance", "with", "loaded", "weights", "and", "hyperparameters", "(", "if", "available", ")", ".", "Note", ":", "`", "`", "load_from_checkpoint", "`", "`", "is", "a", "*", "*", "class", "*", "*", "method", ".", "You", "should", "use", "your", ":", "class", ":", "`", "LightningModule", "`", "*", "*", "class", "*", "*", "to", "call", "it", "instead", "of", "the", ":", "class", ":", "`", "LightningModule", "`", "instance", ",", "or", "a", "`", "`", "TypeError", "`", "`", "will", "be", "raised", ".", "Note", ":", "To", "ensure", "all", "layers", "can", "be", "loaded", "from", "the", "checkpoint", ",", "this", "function", "will", "call", ":", "meth", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "hooks", ".", "ModelHooks", ".", "configure_model", "`", "directly", "after", "instantiating", "the", "model", "if", "this", "hook", "is", "overridden", "in", "your", "LightningModule", ".", "However", ",", "note", "that", "`", "`", "load_from_checkpoint", "`", "`", "does", "not", "support", "loading", "sharded", "checkpoints", ",", "and", "you", "may", "run", "out", "of", "memory", "if", "the", "model", "is", "too", "large", ".", "In", "this", "case", ",", "consider", "loading", "through", "the", "Trainer", "via", "`", "`", ".", "fit", "(", "ckpt_path", "=", ".", ".", ".", ")", "`", "`", ".", "Example", ":", ":", "model", "=", "MyLightningModule", ".", "load_from_checkpoint", "(", "'", "path", "/", "to", "/", "checkpoint", ".", "ckpt", "'", ")", "map_location", "=", "{", "'", "cuda", ":", "1", "'", ":", "'", "cuda", ":", "0", "'", "}", "model", "=", "MyLightningModule", ".", "load_from_checkpoint", "(", "'", "path", "/", "to", "/", "checkpoint", ".", "ckpt", "'", ",", "map_location", "=", "map_location", ")", "model", "=", "MyLightningModule", ".", "load_from_checkpoint", "(", "'", "path", "/", "to", "/", "checkpoint", ".", "ckpt", "'", ",", "hparams_file", "=", "'", "/", "path", "/", "to", "/", "hparams_file", ".", "yaml", "'", ")", "model", "=", "MyLightningModule", ".", "load_from_checkpoint", "(", "PATH", ",", "num_layers", "=", "128", ",", "pretrained_ckpt_path", "=", "NEW_PATH", ",", ")", "pretrained_model", ".", "eval", "(", ")", "pretrained_model", ".", "freeze", "(", ")", "y_hat", "=", "pretrained_model", "(", "x", ")", "\"", "\"", "\"", "loaded", "=", "_load_from_checkpoint", "(", "cls", ",", "checkpoint_path", ",", "map_location", ",", "hparams_file", ",", "strict", ",", "*", "*", "kwargs", ",", ")", "return", "cast", "(", "Self", ",", "loaded", ")"], "docstring": "r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\r\n        passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\r\n\r\n        Any arguments specified through \\*\\*kwargs will override args stored in ``\"hyper_parameters\"``.\r\n\r\n        Args:\r\n            checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\r\n            map_location:\r\n                If your checkpoint saved a GPU model and you now load on CPUs\r\n                or a different number of GPUs, use this to map to the new setup.\r\n                The behaviour is the same as in :func:`torch.load`.\r\n            hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\r\n                as in this example::\r\n\r\n                    drop_prob: 0.2\r\n                    dataloader:\r\n                        batch_size: 32\r\n\r\n                You most likely won't need this since Lightning will always save the hyperparameters\r\n                to the checkpoint.\r\n                However, if your checkpoint weights don't have the hyperparameters saved,\r\n                use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\r\n                These will be converted into a :class:`~dict` and passed into your\r\n                :class:`LightningModule` for use.\r\n\r\n                If your model's ``hparams`` argument is :class:`~argparse.Namespace`\r\n                and ``.yaml`` file has hierarchical structure, you need to refactor your model to treat\r\n                ``hparams`` as :class:`~dict`.\r\n            strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\r\n                returned by this module's state dict. Defaults to ``True`` unless ``LightningModule.strict_loading`` is\r\n                set, in which case it defaults to the value of ``LightningModule.strict_loading``.\r\n            \\**kwargs: Any extra keyword args needed to init the model. Can also be used to override saved\r\n                hyperparameter values.\r\n\r\n        Return:\r\n            :class:`LightningModule` instance with loaded weights and hyperparameters (if available).\r\n\r\n        Note:\r\n            ``load_from_checkpoint`` is a **class** method. You should use your :class:`LightningModule`\r\n            **class** to call it instead of the :class:`LightningModule` instance, or a\r\n            ``TypeError`` will be raised.\r\n\r\n        Note:\r\n            To ensure all layers can be loaded from the checkpoint, this function will call\r\n            :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` directly after instantiating the\r\n            model if this hook is overridden in your LightningModule. However, note that ``load_from_checkpoint`` does\r\n            not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this\r\n            case, consider loading through the Trainer via ``.fit(ckpt_path=...)``.\r\n\r\n        Example::\r\n\r\n            # load weights without mapping ...\r\n            model = MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n\r\n            # or load weights mapping all weights from GPU 1 to GPU 0 ...\r\n            map_location = {'cuda:1':'cuda:0'}\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                map_location=map_location\r\n            )\r\n\r\n            # or load weights and hyperparameters from separate files.\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                'path/to/checkpoint.ckpt',\r\n                hparams_file='/path/to/hparams_file.yaml'\r\n            )\r\n\r\n            # override some of the params with new values\r\n            model = MyLightningModule.load_from_checkpoint(\r\n                PATH,\r\n                num_layers=128,\r\n                pretrained_ckpt_path=NEW_PATH,\r\n            )\r\n\r\n            # predict\r\n            pretrained_model.eval()\r\n            pretrained_model.freeze()\r\n            y_hat = pretrained_model(x)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "primary", "way", "of", "loading", "a", "model", "from", "a", "checkpoint", "when", "lightning", "saves", "a", "checkpoint", "it", "stores", "the", "arguments", "passed", "to", "__init__", "in", "the", "checkpoint", "under", "hyper_parameters", "any", "arguments", "specified", "through", "kwargs", "will", "override", "args", "stored", "in", "hyper_parameters", "args", "checkpoint_path", "path", "to", "checkpoint", "this", "can", "also", "be", "a", "url", "or", "file", "like", "object", "map_location", "if", "your", "checkpoint", "saved", "a", "gpu", "model", "and", "you", "now", "load", "on", "cpus", "or", "a", "different", "number", "of", "gpus", "use", "this", "to", "map", "to", "the", "new", "setup", "the", "behaviour", "is", "the", "same", "as", "in", "func", "torch", "load", "hparams_file", "optional", "path", "to", "a", "yaml", "or", "csv", "file", "with", "hierarchical", "structure", "as", "in", "this", "example", "drop_prob", "0", "2", "dataloader", "batch_size", "32", "you", "most", "likely", "won", "t", "need", "this", "since", "lightning", "will", "always", "save", "the", "hyperparameters", "to", "the", "checkpoint", "however", "if", "your", "checkpoint", "weights", "don", "t", "have", "the", "hyperparameters", "saved", "use", "this", "method", "to", "pass", "in", "a", "yaml", "file", "with", "the", "hparams", "you", "d", "like", "to", "use", "these", "will", "be", "converted", "into", "a", "class", "dict", "and", "passed", "into", "your", "class", "lightningmodule", "for", "use", "if", "your", "model", "s", "hparams", "argument", "is", "class", "argparse", "namespace", "and", "yaml", "file", "has", "hierarchical", "structure", "you", "need", "to", "refactor", "your", "model", "to", "treat", "hparams", "as", "class", "dict", "strict", "whether", "to", "strictly", "enforce", "that", "the", "keys", "in", "attr", "checkpoint_path", "match", "the", "keys", "returned", "by", "this", "module", "s", "state", "dict", "defaults", "to", "true", "unless", "lightningmodule", "strict_loading", "is", "set", "in", "which", "case", "it", "defaults", "to", "the", "value", "of", "lightningmodule", "strict_loading", "kwargs", "any", "extra", "keyword", "args", "needed", "to", "init", "the", "model", "can", "also", "be", "used", "to", "override", "saved", "hyperparameter", "values", "return", "class", "lightningmodule", "instance", "with", "loaded", "weights", "and", "hyperparameters", "if", "available", "note", "load_from_checkpoint", "is", "a", "class", "method", "you", "should", "use", "your", "class", "lightningmodule", "class", "to", "call", "it", "instead", "of", "the", "class", "lightningmodule", "instance", "or", "a", "typeerror", "will", "be", "raised", "note", "to", "ensure", "all", "layers", "can", "be", "loaded", "from", "the", "checkpoint", "this", "function", "will", "call", "meth", "lightning", "pytorch", "core", "hooks", "modelhooks", "configure_model", "directly", "after", "instantiating", "the", "model", "if", "this", "hook", "is", "overridden", "in", "your", "lightningmodule", "however", "note", "that", "load_from_checkpoint", "does", "not", "support", "loading", "sharded", "checkpoints", "and", "you", "may", "run", "out", "of", "memory", "if", "the", "model", "is", "too", "large", "in", "this", "case", "consider", "loading", "through", "the", "trainer", "via", "fit", "ckpt_path", "example", "load", "weights", "without", "mapping", "model", "mylightningmodule", "load_from_checkpoint", "path", "to", "checkpoint", "ckpt", "or", "load", "weights", "mapping", "all", "weights", "from", "gpu", "1", "to", "gpu", "0", "map_location", "cuda", "1", "cuda", "0", "model", "mylightningmodule", "load_from_checkpoint", "path", "to", "checkpoint", "ckpt", "map_location", "map_location", "or", "load", "weights", "and", "hyperparameters", "from", "separate", "files", "model", "mylightningmodule", "load_from_checkpoint", "path", "to", "checkpoint", "ckpt", "hparams_file", "path", "to", "hparams_file", "yaml", "override", "some", "of", "the", "params", "with", "new", "values", "model", "mylightningmodule", "load_from_checkpoint", "path", "num_layers", "128", "pretrained_ckpt_path", "new_path", "predict", "pretrained_model", "eval", "pretrained_model", "freeze", "y_hat", "pretrained_model", "x"], "docstring_summary": "r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "class_method", "class_name": "LightningModule", "start_line": 1686, "end_line": 1782, "hash": "e08c8b7977c47874af76048239532527", "complexity": 1, "parameters": ["checkpoint_path", "IO]", "map_location", "hparams_file", "strict", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\module.py", "func_name": "_jit_is_scripting", "original_string": "def _jit_is_scripting() -> Generator:\r\n    \"\"\"Workaround for https://github.com/pytorch/pytorch/issues/67146.\"\"\"\r\n    LightningModule._jit_is_scripting = True\r\n    try:\r\n        yield\r\n    finally:\r\n        LightningModule._jit_is_scripting = False", "language": "python", "code": "def _jit_is_scripting() -> Generator:\r\n    \"\"\"Workaround for https://github.com/pytorch/pytorch/issues/67146.\"\"\"\r\n    LightningModule._jit_is_scripting = True\r\n    try:\r\n        yield\r\n    finally:\r\n        LightningModule._jit_is_scripting = False", "code_tokens": ["def", "_jit_is_scripting", "(", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Workaround", "for", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "issues", "/", "67146", ".", "\"", "\"", "\"", "LightningModule", ".", "_jit_is_scripting", "=", "True", "try", ":", "yield", "finally", ":", "LightningModule", ".", "_jit_is_scripting", "=", "False"], "docstring": "Workaround for https://github.com/pytorch/pytorch/issues/67146.", "docstring_tokens": ["workaround", "for", "https", "github", "com", "pytorch", "pytorch", "issues", "67146"], "docstring_summary": "Workaround for https://github.com/pytorch/pytorch/issues/67146.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\module.py", "partition": "train", "function_type": "function", "start_line": 1792, "end_line": 1798, "hash": "83db83ca4bf21bc73719fdfaccc31e8f", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "toggle_model", "original_string": "def toggle_model(self, sync_grad: bool = True) -> Generator[None, None, None]:\r\n        \"\"\"This function is just a helper for advanced users.\r\n\r\n        Considering the current optimizer as A and all other optimizers as B.\r\n        Toggling means all parameters from B exclusive to A will have ``requires_grad`` set to False.\r\n\r\n        When performing gradient accumulation, there is no need to perform grad synchronization\r\n        during the accumulation phase.\r\n        Setting `sync_grad` to False will block this synchronization and improve performance.\r\n\r\n        \"\"\"\r\n        # local import here to avoid circular import\r\n        from lightning.pytorch.loops.utilities import _block_parallel_sync_behavior\r\n\r\n        assert self._strategy is not None\r\n        lightning_module = self._strategy.lightning_module\r\n        assert lightning_module is not None\r\n        with _block_parallel_sync_behavior(self._strategy, block=(not sync_grad)):\r\n            lightning_module.toggle_optimizer(self)\r\n            yield\r\n            lightning_module.untoggle_optimizer(self)", "language": "python", "code": "def toggle_model(self, sync_grad: bool = True) -> Generator[None, None, None]:\r\n        \"\"\"This function is just a helper for advanced users.\r\n\r\n        Considering the current optimizer as A and all other optimizers as B.\r\n        Toggling means all parameters from B exclusive to A will have ``requires_grad`` set to False.\r\n\r\n        When performing gradient accumulation, there is no need to perform grad synchronization\r\n        during the accumulation phase.\r\n        Setting `sync_grad` to False will block this synchronization and improve performance.\r\n\r\n        \"\"\"\r\n        # local import here to avoid circular import\r\n        from lightning.pytorch.loops.utilities import _block_parallel_sync_behavior\r\n\r\n        assert self._strategy is not None\r\n        lightning_module = self._strategy.lightning_module\r\n        assert lightning_module is not None\r\n        with _block_parallel_sync_behavior(self._strategy, block=(not sync_grad)):\r\n            lightning_module.toggle_optimizer(self)\r\n            yield\r\n            lightning_module.untoggle_optimizer(self)", "code_tokens": ["def", "toggle_model", "(", "self", ",", "sync_grad", ":", "bool", "=", "True", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "This", "function", "is", "just", "a", "helper", "for", "advanced", "users", ".", "Considering", "the", "current", "optimizer", "as", "A", "and", "all", "other", "optimizers", "as", "B", ".", "Toggling", "means", "all", "parameters", "from", "B", "exclusive", "to", "A", "will", "have", "`", "`", "requires_grad", "`", "`", "set", "to", "False", ".", "When", "performing", "gradient", "accumulation", ",", "there", "is", "no", "need", "to", "perform", "grad", "synchronization", "during", "the", "accumulation", "phase", ".", "Setting", "`", "sync_grad", "`", "to", "False", "will", "block", "this", "synchronization", "and", "improve", "performance", ".", "\"", "\"", "\"", "from", "lightning", ".", "pytorch", ".", "loops", ".", "utilities", "import", "_block_parallel_sync_behavior", "assert", "self", ".", "_strategy", "is", "not", "None", "lightning_module", "=", "self", ".", "_strategy", ".", "lightning_module", "assert", "lightning_module", "is", "not", "None", "with", "_block_parallel_sync_behavior", "(", "self", ".", "_strategy", ",", "block", "=", "(", "not", "sync_grad", ")", ")", ":", "lightning_module", ".", "toggle_optimizer", "(", "self", ")", "yield", "lightning_module", ".", "untoggle_optimizer", "(", "self", ")"], "docstring": "This function is just a helper for advanced users.\r\n\r\n        Considering the current optimizer as A and all other optimizers as B.\r\n        Toggling means all parameters from B exclusive to A will have ``requires_grad`` set to False.\r\n\r\n        When performing gradient accumulation, there is no need to perform grad synchronization\r\n        during the accumulation phase.\r\n        Setting `sync_grad` to False will block this synchronization and improve performance.", "docstring_tokens": ["this", "function", "is", "just", "a", "helper", "for", "advanced", "users", "considering", "the", "current", "optimizer", "as", "a", "and", "all", "other", "optimizers", "as", "b", "toggling", "means", "all", "parameters", "from", "b", "exclusive", "to", "a", "will", "have", "requires_grad", "set", "to", "false", "when", "performing", "gradient", "accumulation", "there", "is", "no", "need", "to", "perform", "grad", "synchronization", "during", "the", "accumulation", "phase", "setting", "sync_grad", "to", "false", "will", "block", "this", "synchronization", "and", "improve", "performance"], "docstring_summary": "This function is just a helper for advanced users.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\optimizer.py", "partition": "train", "function_type": "class_method", "class_name": "LightningOptimizer", "start_line": 62, "end_line": 82, "hash": "e4cf35f43a0d6fd70c9c0194db0c486f", "complexity": 2, "parameters": ["sync_grad"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "step", "original_string": "def step(self, closure: Optional[Callable[[], Any]] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Performs a single optimization step (parameter update).\r\n\r\n        Args:\r\n            closure: An optional optimizer closure.\r\n            kwargs: Any additional arguments to the ``optimizer.step()`` call.\r\n\r\n        Returns:\r\n            The output from the step call, which is generally the output of the closure execution.\r\n\r\n        Example::\r\n\r\n            # Scenario for a GAN using manual optimization\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n\r\n                # compute generator loss\r\n                loss_gen = self.compute_generator_loss(...)\r\n                # zero_grad needs to be called before backward\r\n                opt_gen.zero_grad()\r\n                self.manual_backward(loss_gen)\r\n                opt_gen.step()\r\n\r\n                # compute discriminator loss\r\n                loss_dis = self.compute_discriminator_loss(...)\r\n\r\n                # zero_grad needs to be called before backward\r\n                opt_dis.zero_grad()\r\n                self.manual_backward(loss_dis)\r\n                opt_dis.step()\r\n\r\n\r\n            # A more advanced example\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n                accumulated_grad_batches = batch_idx % 2 == 0\r\n\r\n                # compute generator loss\r\n                def closure_gen():\r\n                    loss_gen = self.compute_generator_loss(...)\r\n                    self.manual_backward(loss_gen)\r\n                    if accumulated_grad_batches:\r\n                        opt_gen.zero_grad()\r\n\r\n                with opt_gen.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_gen.step(closure=closure_gen)\r\n\r\n                def closure_dis():\r\n                    loss_dis = self.compute_discriminator_loss(...)\r\n                    self.manual_backward(loss_dis)\r\n                    if accumulated_grad_batches:\r\n                        opt_dis.zero_grad()\r\n\r\n                with opt_dis.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_dis.step(closure=closure_dis)\r\n\r\n        \"\"\"\r\n        self._on_before_step()\r\n\r\n        if closure is None:\r\n            closure = do_nothing_closure\r\n        elif not callable(closure):\r\n            raise MisconfigurationException(\"When `optimizer.step(closure)` is called, the closure should be callable\")\r\n\r\n        assert self._strategy is not None\r\n        step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\r\n\r\n        self._on_after_step()\r\n\r\n        return step_output", "language": "python", "code": "def step(self, closure: Optional[Callable[[], Any]] = None, **kwargs: Any) -> Any:\r\n        \"\"\"Performs a single optimization step (parameter update).\r\n\r\n        Args:\r\n            closure: An optional optimizer closure.\r\n            kwargs: Any additional arguments to the ``optimizer.step()`` call.\r\n\r\n        Returns:\r\n            The output from the step call, which is generally the output of the closure execution.\r\n\r\n        Example::\r\n\r\n            # Scenario for a GAN using manual optimization\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n\r\n                # compute generator loss\r\n                loss_gen = self.compute_generator_loss(...)\r\n                # zero_grad needs to be called before backward\r\n                opt_gen.zero_grad()\r\n                self.manual_backward(loss_gen)\r\n                opt_gen.step()\r\n\r\n                # compute discriminator loss\r\n                loss_dis = self.compute_discriminator_loss(...)\r\n\r\n                # zero_grad needs to be called before backward\r\n                opt_dis.zero_grad()\r\n                self.manual_backward(loss_dis)\r\n                opt_dis.step()\r\n\r\n\r\n            # A more advanced example\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n                accumulated_grad_batches = batch_idx % 2 == 0\r\n\r\n                # compute generator loss\r\n                def closure_gen():\r\n                    loss_gen = self.compute_generator_loss(...)\r\n                    self.manual_backward(loss_gen)\r\n                    if accumulated_grad_batches:\r\n                        opt_gen.zero_grad()\r\n\r\n                with opt_gen.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_gen.step(closure=closure_gen)\r\n\r\n                def closure_dis():\r\n                    loss_dis = self.compute_discriminator_loss(...)\r\n                    self.manual_backward(loss_dis)\r\n                    if accumulated_grad_batches:\r\n                        opt_dis.zero_grad()\r\n\r\n                with opt_dis.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_dis.step(closure=closure_dis)\r\n\r\n        \"\"\"\r\n        self._on_before_step()\r\n\r\n        if closure is None:\r\n            closure = do_nothing_closure\r\n        elif not callable(closure):\r\n            raise MisconfigurationException(\"When `optimizer.step(closure)` is called, the closure should be callable\")\r\n\r\n        assert self._strategy is not None\r\n        step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\r\n\r\n        self._on_after_step()\r\n\r\n        return step_output", "code_tokens": ["def", "step", "(", "self", ",", "closure", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "Any", "]", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Performs", "a", "single", "optimization", "step", "(", "parameter", "update", ")", ".", "Args", ":", "closure", ":", "An", "optional", "optimizer", "closure", ".", "kwargs", ":", "Any", "additional", "arguments", "to", "the", "`", "`", "optimizer", ".", "step", "(", ")", "`", "`", "call", ".", "Returns", ":", "The", "output", "from", "the", "step", "call", ",", "which", "is", "generally", "the", "output", "of", "the", "closure", "execution", ".", "Example", ":", ":", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "opt_gen", ",", "opt_dis", "=", "self", ".", "optimizers", "(", ")", ".", ".", ".", "loss_gen", "=", "self", ".", "compute_generator_loss", "(", ".", ".", ".", ")", "opt_gen", ".", "zero_grad", "(", ")", "self", ".", "manual_backward", "(", "loss_gen", ")", "opt_gen", ".", "step", "(", ")", "loss_dis", "=", "self", ".", "compute_discriminator_loss", "(", ".", ".", ".", ")", "opt_dis", ".", "zero_grad", "(", ")", "self", ".", "manual_backward", "(", "loss_dis", ")", "opt_dis", ".", "step", "(", ")", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "opt_gen", ",", "opt_dis", "=", "self", ".", "optimizers", "(", ")", ".", ".", ".", "accumulated_grad_batches", "=", "batch_idx", "%", "2", "=", "=", "0", "def", "closure_gen", "(", ")", ":", "loss_gen", "=", "self", ".", "compute_generator_loss", "(", ".", ".", ".", ")", "self", ".", "manual_backward", "(", "loss_gen", ")", "if", "accumulated_grad_batches", ":", "opt_gen", ".", "zero_grad", "(", ")", "with", "opt_gen", ".", "toggle_model", "(", "sync_grad", "=", "accumulated_grad_batches", ")", ":", "opt_gen", ".", "step", "(", "closure", "=", "closure_gen", ")", "def", "closure_dis", "(", ")", ":", "loss_dis", "=", "self", ".", "compute_discriminator_loss", "(", ".", ".", ".", ")", "self", ".", "manual_backward", "(", "loss_dis", ")", "if", "accumulated_grad_batches", ":", "opt_dis", ".", "zero_grad", "(", ")", "with", "opt_dis", ".", "toggle_model", "(", "sync_grad", "=", "accumulated_grad_batches", ")", ":", "opt_dis", ".", "step", "(", "closure", "=", "closure_dis", ")", "\"", "\"", "\"", "self", ".", "_on_before_step", "(", ")", "if", "closure", "is", "None", ":", "closure", "=", "do_nothing_closure", "elif", "not", "callable", "(", "closure", ")", ":", "raise", "MisconfigurationException", "(", "\"", "When", "`", "optimizer", ".", "step", "(", "closure", ")", "`", "is", "called", ",", "the", "closure", "should", "be", "callable", "\"", ")", "assert", "self", ".", "_strategy", "is", "not", "None", "step_output", "=", "self", ".", "_strategy", ".", "optimizer_step", "(", "self", ".", "_optimizer", ",", "closure", ",", "*", "*", "kwargs", ")", "self", ".", "_on_after_step", "(", ")", "return", "step_output"], "docstring": "Performs a single optimization step (parameter update).\r\n\r\n        Args:\r\n            closure: An optional optimizer closure.\r\n            kwargs: Any additional arguments to the ``optimizer.step()`` call.\r\n\r\n        Returns:\r\n            The output from the step call, which is generally the output of the closure execution.\r\n\r\n        Example::\r\n\r\n            # Scenario for a GAN using manual optimization\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n\r\n                # compute generator loss\r\n                loss_gen = self.compute_generator_loss(...)\r\n                # zero_grad needs to be called before backward\r\n                opt_gen.zero_grad()\r\n                self.manual_backward(loss_gen)\r\n                opt_gen.step()\r\n\r\n                # compute discriminator loss\r\n                loss_dis = self.compute_discriminator_loss(...)\r\n\r\n                # zero_grad needs to be called before backward\r\n                opt_dis.zero_grad()\r\n                self.manual_backward(loss_dis)\r\n                opt_dis.step()\r\n\r\n\r\n            # A more advanced example\r\n            def training_step(self, batch, batch_idx):\r\n                opt_gen, opt_dis = self.optimizers()\r\n\r\n                ...\r\n                accumulated_grad_batches = batch_idx % 2 == 0\r\n\r\n                # compute generator loss\r\n                def closure_gen():\r\n                    loss_gen = self.compute_generator_loss(...)\r\n                    self.manual_backward(loss_gen)\r\n                    if accumulated_grad_batches:\r\n                        opt_gen.zero_grad()\r\n\r\n                with opt_gen.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_gen.step(closure=closure_gen)\r\n\r\n                def closure_dis():\r\n                    loss_dis = self.compute_discriminator_loss(...)\r\n                    self.manual_backward(loss_dis)\r\n                    if accumulated_grad_batches:\r\n                        opt_dis.zero_grad()\r\n\r\n                with opt_dis.toggle_model(sync_grad=accumulated_grad_batches):\r\n                    opt_dis.step(closure=closure_dis)", "docstring_tokens": ["performs", "a", "single", "optimization", "step", "parameter", "update", "args", "closure", "an", "optional", "optimizer", "closure", "kwargs", "any", "additional", "arguments", "to", "the", "optimizer", "step", "call", "returns", "the", "output", "from", "the", "step", "call", "which", "is", "generally", "the", "output", "of", "the", "closure", "execution", "example", "scenario", "for", "a", "gan", "using", "manual", "optimization", "def", "training_step", "self", "batch", "batch_idx", "opt_gen", "opt_dis", "self", "optimizers", "compute", "generator", "loss", "loss_gen", "self", "compute_generator_loss", "zero_grad", "needs", "to", "be", "called", "before", "backward", "opt_gen", "zero_grad", "self", "manual_backward", "loss_gen", "opt_gen", "step", "compute", "discriminator", "loss", "loss_dis", "self", "compute_discriminator_loss", "zero_grad", "needs", "to", "be", "called", "before", "backward", "opt_dis", "zero_grad", "self", "manual_backward", "loss_dis", "opt_dis", "step", "a", "more", "advanced", "example", "def", "training_step", "self", "batch", "batch_idx", "opt_gen", "opt_dis", "self", "optimizers", "accumulated_grad_batches", "batch_idx", "2", "0", "compute", "generator", "loss", "def", "closure_gen", "loss_gen", "self", "compute_generator_loss", "self", "manual_backward", "loss_gen", "if", "accumulated_grad_batches", "opt_gen", "zero_grad", "with", "opt_gen", "toggle_model", "sync_grad", "accumulated_grad_batches", "opt_gen", "step", "closure", "closure_gen", "def", "closure_dis", "loss_dis", "self", "compute_discriminator_loss", "self", "manual_backward", "loss_dis", "if", "accumulated_grad_batches", "opt_dis", "zero_grad", "with", "opt_dis", "toggle_model", "sync_grad", "accumulated_grad_batches", "opt_dis", "step", "closure", "closure_dis"], "docstring_summary": "Performs a single optimization step (parameter update).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\optimizer.py", "partition": "train", "function_type": "class_method", "class_name": "LightningOptimizer", "start_line": 84, "end_line": 157, "hash": "b310941974a4aee1126d6a8d1f1e4155", "complexity": 3, "parameters": ["closure", "Any]]", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "_init_optimizers_and_lr_schedulers", "original_string": "def _init_optimizers_and_lr_schedulers(\r\n    model: \"pl.LightningModule\",\r\n) -> tuple[list[Optimizer], list[LRSchedulerConfig]]:\r\n    \"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\r\n    from lightning.pytorch.trainer import call\r\n\r\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\r\n\r\n    if optim_conf is None:\r\n        rank_zero_warn(\r\n            \"`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\",\r\n        )\r\n        optim_conf = _MockOptimizer()\r\n\r\n    optimizers, lr_schedulers, monitor = _configure_optimizers(optim_conf)\r\n    lr_scheduler_configs = (\r\n        _configure_schedulers_automatic_opt(lr_schedulers, monitor)\r\n        if model.automatic_optimization\r\n        else _configure_schedulers_manual_opt(lr_schedulers)\r\n    )\r\n    _validate_multiple_optimizers_support(optimizers, model)\r\n    _validate_optimizers_attached(optimizers, lr_scheduler_configs)\r\n    _validate_scheduler_api(lr_scheduler_configs, model)\r\n    return optimizers, lr_scheduler_configs", "language": "python", "code": "def _init_optimizers_and_lr_schedulers(\r\n    model: \"pl.LightningModule\",\r\n) -> tuple[list[Optimizer], list[LRSchedulerConfig]]:\r\n    \"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\r\n    from lightning.pytorch.trainer import call\r\n\r\n    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\r\n\r\n    if optim_conf is None:\r\n        rank_zero_warn(\r\n            \"`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\",\r\n        )\r\n        optim_conf = _MockOptimizer()\r\n\r\n    optimizers, lr_schedulers, monitor = _configure_optimizers(optim_conf)\r\n    lr_scheduler_configs = (\r\n        _configure_schedulers_automatic_opt(lr_schedulers, monitor)\r\n        if model.automatic_optimization\r\n        else _configure_schedulers_manual_opt(lr_schedulers)\r\n    )\r\n    _validate_multiple_optimizers_support(optimizers, model)\r\n    _validate_optimizers_attached(optimizers, lr_scheduler_configs)\r\n    _validate_scheduler_api(lr_scheduler_configs, model)\r\n    return optimizers, lr_scheduler_configs", "code_tokens": ["def", "_init_optimizers_and_lr_schedulers", "(", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", ")", "-", ">", "tuple", "[", "list", "[", "Optimizer", "]", ",", "list", "[", "LRSchedulerConfig", "]", "]", ":", "\"", "\"", "\"", "Calls", "`", "LightningModule", ".", "configure_optimizers", "`", "and", "parses", "and", "validates", "the", "output", ".", "\"", "\"", "\"", "from", "lightning", ".", "pytorch", ".", "trainer", "import", "call", "optim_conf", "=", "call", ".", "_call_lightning_module_hook", "(", "model", ".", "trainer", ",", "\"", "configure_optimizers", "\"", ",", "pl_module", "=", "model", ")", "if", "optim_conf", "is", "None", ":", "rank_zero_warn", "(", "\"", "`", "LightningModule", ".", "configure_optimizers", "`", "returned", "`", "None", "`", ",", "this", "fit", "will", "run", "with", "no", "optimizer", "\"", ",", ")", "optim_conf", "=", "_MockOptimizer", "(", ")", "optimizers", ",", "lr_schedulers", ",", "monitor", "=", "_configure_optimizers", "(", "optim_conf", ")", "lr_scheduler_configs", "=", "(", "_configure_schedulers_automatic_opt", "(", "lr_schedulers", ",", "monitor", ")", "if", "model", ".", "automatic_optimization", "else", "_configure_schedulers_manual_opt", "(", "lr_schedulers", ")", ")", "_validate_multiple_optimizers_support", "(", "optimizers", ",", "model", ")", "_validate_optimizers_attached", "(", "optimizers", ",", "lr_scheduler_configs", ")", "_validate_scheduler_api", "(", "lr_scheduler_configs", ",", "model", ")", "return", "optimizers", ",", "lr_scheduler_configs"], "docstring": "Calls `LightningModule.configure_optimizers` and parses and validates the output.", "docstring_tokens": ["calls", "lightningmodule", "configure_optimizers", "and", "parses", "and", "validates", "the", "output"], "docstring_summary": "Calls `LightningModule.configure_optimizers` and parses and validates the output.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\optimizer.py", "partition": "train", "function_type": "function", "start_line": 173, "end_line": 196, "hash": "01ba49c75b7602420e47cadbb2f4e83d", "complexity": 3, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\optimizer.py", "func_name": "_configure_schedulers_manual_opt", "original_string": "def _configure_schedulers_manual_opt(schedulers: list) -> list[LRSchedulerConfig]:\r\n    \"\"\"Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual\r\n    optimization.\"\"\"\r\n    lr_scheduler_configs = []\r\n    for scheduler in schedulers:\r\n        if isinstance(scheduler, dict):\r\n            # interval is not in this list even though the user needs to manually call the scheduler because\r\n            # the `LearningRateMonitor` callback needs to check its value to know when to log the learning rate\r\n            invalid_keys = {\"reduce_on_plateau\", \"monitor\", \"strict\"}\r\n            keys_to_warn = [k for k in scheduler if k in invalid_keys]\r\n\r\n            if keys_to_warn:\r\n                rank_zero_warn(\r\n                    f\"The lr scheduler dict contains the key(s) {keys_to_warn}, but the keys will be ignored.\"\r\n                    \" You need to call `lr_scheduler.step()` manually in manual optimization.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n            config = LRSchedulerConfig(**{key: scheduler[key] for key in scheduler if key not in invalid_keys})\r\n        else:\r\n            config = LRSchedulerConfig(scheduler)\r\n        lr_scheduler_configs.append(config)\r\n    return lr_scheduler_configs", "language": "python", "code": "def _configure_schedulers_manual_opt(schedulers: list) -> list[LRSchedulerConfig]:\r\n    \"\"\"Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual\r\n    optimization.\"\"\"\r\n    lr_scheduler_configs = []\r\n    for scheduler in schedulers:\r\n        if isinstance(scheduler, dict):\r\n            # interval is not in this list even though the user needs to manually call the scheduler because\r\n            # the `LearningRateMonitor` callback needs to check its value to know when to log the learning rate\r\n            invalid_keys = {\"reduce_on_plateau\", \"monitor\", \"strict\"}\r\n            keys_to_warn = [k for k in scheduler if k in invalid_keys]\r\n\r\n            if keys_to_warn:\r\n                rank_zero_warn(\r\n                    f\"The lr scheduler dict contains the key(s) {keys_to_warn}, but the keys will be ignored.\"\r\n                    \" You need to call `lr_scheduler.step()` manually in manual optimization.\",\r\n                    category=RuntimeWarning,\r\n                )\r\n\r\n            config = LRSchedulerConfig(**{key: scheduler[key] for key in scheduler if key not in invalid_keys})\r\n        else:\r\n            config = LRSchedulerConfig(scheduler)\r\n        lr_scheduler_configs.append(config)\r\n    return lr_scheduler_configs", "code_tokens": ["def", "_configure_schedulers_manual_opt", "(", "schedulers", ":", "list", ")", "-", ">", "list", "[", "LRSchedulerConfig", "]", ":", "\"", "\"", "\"", "Convert", "each", "scheduler", "into", "`", "LRSchedulerConfig", "`", "structure", "with", "relevant", "information", ",", "when", "using", "manual", "optimization", ".", "\"", "\"", "\"", "lr_scheduler_configs", "=", "[", "]", "for", "scheduler", "in", "schedulers", ":", "if", "isinstance", "(", "scheduler", ",", "dict", ")", ":", "invalid_keys", "=", "{", "\"", "reduce_on_plateau", "\"", ",", "\"", "monitor", "\"", ",", "\"", "strict", "\"", "}", "keys_to_warn", "=", "[", "k", "for", "k", "in", "scheduler", "if", "k", "in", "invalid_keys", "]", "if", "keys_to_warn", ":", "rank_zero_warn", "(", "f", "\"", "The", "lr", "scheduler", "dict", "contains", "the", "key", "(", "s", ")", "{", "keys_to_warn", "}", ",", "but", "the", "keys", "will", "be", "ignored", ".", "\"", "\"", "You", "need", "to", "call", "`", "lr_scheduler", ".", "step", "(", ")", "`", "manually", "in", "manual", "optimization", ".", "\"", ",", "category", "=", "RuntimeWarning", ",", ")", "config", "=", "LRSchedulerConfig", "(", "*", "*", "{", "key", ":", "scheduler", "[", "key", "]", "for", "key", "in", "scheduler", "if", "key", "not", "in", "invalid_keys", "}", ")", "else", ":", "config", "=", "LRSchedulerConfig", "(", "scheduler", ")", "lr_scheduler_configs", ".", "append", "(", "config", ")", "return", "lr_scheduler_configs"], "docstring": "Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual\r\n    optimization.", "docstring_tokens": ["convert", "each", "scheduler", "into", "lrschedulerconfig", "structure", "with", "relevant", "information", "when", "using", "manual", "optimization"], "docstring_summary": "Convert each scheduler into `LRSchedulerConfig` structure with relevant information, when using manual", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\optimizer.py", "partition": "train", "function_type": "function", "start_line": 304, "end_line": 326, "hash": "401ab58c4a6a4717f8b3cc19d6c72b84", "complexity": 8, "parameters": ["schedulers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "_convert_loaded_hparams", "original_string": "def _convert_loaded_hparams(\r\n    model_args: dict[str, Any], hparams_type: Optional[Union[Callable, str]] = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert hparams according given type in callable or string (past) format.\"\"\"\r\n    # if not hparams type define\r\n    if not hparams_type:\r\n        return model_args\r\n    # if past checkpoint loaded, convert str to callable\r\n    if isinstance(hparams_type, str):\r\n        hparams_type = AttributeDict\r\n    # convert hparams\r\n    return hparams_type(model_args)", "language": "python", "code": "def _convert_loaded_hparams(\r\n    model_args: dict[str, Any], hparams_type: Optional[Union[Callable, str]] = None\r\n) -> dict[str, Any]:\r\n    \"\"\"Convert hparams according given type in callable or string (past) format.\"\"\"\r\n    # if not hparams type define\r\n    if not hparams_type:\r\n        return model_args\r\n    # if past checkpoint loaded, convert str to callable\r\n    if isinstance(hparams_type, str):\r\n        hparams_type = AttributeDict\r\n    # convert hparams\r\n    return hparams_type(model_args)", "code_tokens": ["def", "_convert_loaded_hparams", "(", "model_args", ":", "dict", "[", "str", ",", "Any", "]", ",", "hparams_type", ":", "Optional", "[", "Union", "[", "Callable", ",", "str", "]", "]", "=", "None", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Convert", "hparams", "according", "given", "type", "in", "callable", "or", "string", "(", "past", ")", "format", ".", "\"", "\"", "\"", "if", "not", "hparams_type", ":", "return", "model_args", "if", "isinstance", "(", "hparams_type", ",", "str", ")", ":", "hparams_type", "=", "AttributeDict", "return", "hparams_type", "(", "model_args", ")"], "docstring": "Convert hparams according given type in callable or string (past) format.", "docstring_tokens": ["convert", "hparams", "according", "given", "type", "in", "callable", "or", "string", "past", "format"], "docstring_summary": "Convert hparams according given type in callable or string (past) format.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\saving.py", "partition": "train", "function_type": "function", "start_line": 201, "end_line": 212, "hash": "ac6dc6ddf4b657d0e577acf077bf53da", "complexity": 3, "parameters": ["model_args", "Any]", "hparams_type", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "update_hparams", "original_string": "def update_hparams(hparams: dict, updates: dict) -> None:\r\n    \"\"\"Overrides hparams with new values.\r\n\r\n    >>> hparams = {'c': 4}\r\n    >>> update_hparams(hparams, {'a': {'b': 2}, 'c': 1})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (2, 1)\r\n    >>> update_hparams(hparams, {'a': {'b': 4}, 'c': 7})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (4, 7)\r\n\r\n    Args:\r\n        hparams: the original params and also target object\r\n        updates: new params to be used as update\r\n\r\n    \"\"\"\r\n    for k, v in updates.items():\r\n        # if missing, add the key\r\n        if k not in hparams:\r\n            hparams[k] = v\r\n            continue\r\n\r\n        # recurse if dictionary\r\n        if isinstance(v, dict):\r\n            update_hparams(hparams[k], updates[k])\r\n        else:\r\n            # update the value\r\n            hparams.update({k: v})", "language": "python", "code": "def update_hparams(hparams: dict, updates: dict) -> None:\r\n    \"\"\"Overrides hparams with new values.\r\n\r\n    >>> hparams = {'c': 4}\r\n    >>> update_hparams(hparams, {'a': {'b': 2}, 'c': 1})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (2, 1)\r\n    >>> update_hparams(hparams, {'a': {'b': 4}, 'c': 7})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (4, 7)\r\n\r\n    Args:\r\n        hparams: the original params and also target object\r\n        updates: new params to be used as update\r\n\r\n    \"\"\"\r\n    for k, v in updates.items():\r\n        # if missing, add the key\r\n        if k not in hparams:\r\n            hparams[k] = v\r\n            continue\r\n\r\n        # recurse if dictionary\r\n        if isinstance(v, dict):\r\n            update_hparams(hparams[k], updates[k])\r\n        else:\r\n            # update the value\r\n            hparams.update({k: v})", "code_tokens": ["def", "update_hparams", "(", "hparams", ":", "dict", ",", "updates", ":", "dict", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Overrides", "hparams", "with", "new", "values", ".", ">", ">", ">", "hparams", "=", "{", "'", "c", "'", ":", "4", "}", ">", ">", ">", "update_hparams", "(", "hparams", ",", "{", "'", "a", "'", ":", "{", "'", "b", "'", ":", "2", "}", ",", "'", "c", "'", ":", "1", "}", ")", ">", ">", ">", "hparams", "[", "'", "a", "'", "]", "[", "'", "b", "'", "]", ",", "hparams", "[", "'", "c", "'", "]", "(", "2", ",", "1", ")", ">", ">", ">", "update_hparams", "(", "hparams", ",", "{", "'", "a", "'", ":", "{", "'", "b", "'", ":", "4", "}", ",", "'", "c", "'", ":", "7", "}", ")", ">", ">", ">", "hparams", "[", "'", "a", "'", "]", "[", "'", "b", "'", "]", ",", "hparams", "[", "'", "c", "'", "]", "(", "4", ",", "7", ")", "Args", ":", "hparams", ":", "the", "original", "params", "and", "also", "target", "object", "updates", ":", "new", "params", "to", "be", "used", "as", "update", "\"", "\"", "\"", "for", "k", ",", "v", "in", "updates", ".", "items", "(", ")", ":", "if", "k", "not", "in", "hparams", ":", "hparams", "[", "k", "]", "=", "v", "continue", "if", "isinstance", "(", "v", ",", "dict", ")", ":", "update_hparams", "(", "hparams", "[", "k", "]", ",", "updates", "[", "k", "]", ")", "else", ":", "hparams", ".", "update", "(", "{", "k", ":", "v", "}", ")"], "docstring": "Overrides hparams with new values.\r\n\r\n    >>> hparams = {'c': 4}\r\n    >>> update_hparams(hparams, {'a': {'b': 2}, 'c': 1})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (2, 1)\r\n    >>> update_hparams(hparams, {'a': {'b': 4}, 'c': 7})\r\n    >>> hparams['a']['b'], hparams['c']\r\n    (4, 7)\r\n\r\n    Args:\r\n        hparams: the original params and also target object\r\n        updates: new params to be used as update", "docstring_tokens": ["overrides", "hparams", "with", "new", "values", "hparams", "c", "4", "update_hparams", "hparams", "a", "b", "2", "c", "1", "hparams", "a", "b", "hparams", "c", "2", "1", "update_hparams", "hparams", "a", "b", "4", "c", "7", "hparams", "a", "b", "hparams", "c", "4", "7", "args", "hparams", "the", "original", "params", "and", "also", "target", "object", "updates", "new", "params", "to", "be", "used", "as", "update"], "docstring_summary": "Overrides hparams with new values.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\saving.py", "partition": "train", "function_type": "function", "start_line": 215, "end_line": 242, "hash": "3df88218b42cc5de44c26ccf0c34e467", "complexity": 4, "parameters": ["hparams", "updates"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "load_hparams_from_tags_csv", "original_string": "def load_hparams_from_tags_csv(tags_csv: _PATH) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_csv = os.path.join('.', 'testing-hparams.csv')\r\n    >>> save_hparams_to_tags_csv(path_csv, hparams)\r\n    >>> hparams_new = load_hparams_from_tags_csv(path_csv)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_csv)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(tags_csv)\r\n    if not fs.exists(tags_csv):\r\n        rank_zero_warn(f\"Missing Tags: {tags_csv}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(tags_csv, \"r\", newline=\"\") as fp:\r\n        csv_reader = csv.reader(fp, delimiter=\",\")\r\n        return {row[0]: convert(row[1]) for row in list(csv_reader)[1:]}", "language": "python", "code": "def load_hparams_from_tags_csv(tags_csv: _PATH) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_csv = os.path.join('.', 'testing-hparams.csv')\r\n    >>> save_hparams_to_tags_csv(path_csv, hparams)\r\n    >>> hparams_new = load_hparams_from_tags_csv(path_csv)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_csv)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(tags_csv)\r\n    if not fs.exists(tags_csv):\r\n        rank_zero_warn(f\"Missing Tags: {tags_csv}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(tags_csv, \"r\", newline=\"\") as fp:\r\n        csv_reader = csv.reader(fp, delimiter=\",\")\r\n        return {row[0]: convert(row[1]) for row in list(csv_reader)[1:]}", "code_tokens": ["def", "load_hparams_from_tags_csv", "(", "tags_csv", ":", "_PATH", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "hparams", "from", "a", "file", ".", ">", ">", ">", "hparams", "=", "Namespace", "(", "batch_size", "=", "32", ",", "learning_rate", "=", "0", ".", "001", ",", "data_root", "=", "'", ".", "/", "any", "/", "path", "/", "here", "'", ")", ">", ">", ">", "path_csv", "=", "os", ".", "path", ".", "join", "(", "'", ".", "'", ",", "'", "testing", "-", "hparams", ".", "csv", "'", ")", ">", ">", ">", "save_hparams_to_tags_csv", "(", "path_csv", ",", "hparams", ")", ">", ">", ">", "hparams_new", "=", "load_hparams_from_tags_csv", "(", "path_csv", ")", ">", ">", ">", "vars", "(", "hparams", ")", "=", "=", "hparams_new", "True", ">", ">", ">", "os", ".", "remove", "(", "path_csv", ")", "\"", "\"", "\"", "fs", "=", "get_filesystem", "(", "tags_csv", ")", "if", "not", "fs", ".", "exists", "(", "tags_csv", ")", ":", "rank_zero_warn", "(", "f", "\"", "Missing", "Tags", ":", "{", "tags_csv", "}", ".", "\"", ",", "category", "=", "RuntimeWarning", ")", "return", "{", "}", "with", "fs", ".", "open", "(", "tags_csv", ",", "\"", "r", "\"", ",", "newline", "=", "\"", "\"", ")", "as", "fp", ":", "csv_reader", "=", "csv", ".", "reader", "(", "fp", ",", "delimiter", "=", "\"", ",", "\"", ")", "return", "{", "row", "[", "0", "]", ":", "convert", "(", "row", "[", "1", "]", ")", "for", "row", "in", "list", "(", "csv_reader", ")", "[", "1", ":", "]", "}"], "docstring": "Load hparams from a file.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_csv = os.path.join('.', 'testing-hparams.csv')\r\n    >>> save_hparams_to_tags_csv(path_csv, hparams)\r\n    >>> hparams_new = load_hparams_from_tags_csv(path_csv)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_csv)", "docstring_tokens": ["load", "hparams", "from", "a", "file", "hparams", "namespace", "batch_size", "32", "learning_rate", "0", "001", "data_root", "any", "path", "here", "path_csv", "os", "path", "join", "testing", "hparams", "csv", "save_hparams_to_tags_csv", "path_csv", "hparams", "hparams_new", "load_hparams_from_tags_csv", "path_csv", "vars", "hparams", "hparams_new", "true", "os", "remove", "path_csv"], "docstring_summary": "Load hparams from a file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\saving.py", "partition": "train", "function_type": "function", "start_line": 245, "end_line": 264, "hash": "671eeceadd451358fa20a04e5ba7e429", "complexity": 4, "parameters": ["tags_csv"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "load_hparams_from_yaml", "original_string": "def load_hparams_from_yaml(config_yaml: _PATH, use_omegaconf: bool = True) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n        Args:\r\n            config_yaml: Path to config yaml file\r\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n                the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_yaml = './testing-hparams.yaml'\r\n    >>> save_hparams_to_yaml(path_yaml, hparams)\r\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_yaml)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not fs.exists(config_yaml):\r\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(config_yaml, \"r\") as fp:\r\n        hparams = yaml.full_load(fp)\r\n\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        with contextlib.suppress(UnsupportedValueType, ValidationError):\r\n            return OmegaConf.create(hparams)\r\n    return hparams", "language": "python", "code": "def load_hparams_from_yaml(config_yaml: _PATH, use_omegaconf: bool = True) -> dict[str, Any]:\r\n    \"\"\"Load hparams from a file.\r\n\r\n        Args:\r\n            config_yaml: Path to config yaml file\r\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n                the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_yaml = './testing-hparams.yaml'\r\n    >>> save_hparams_to_yaml(path_yaml, hparams)\r\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_yaml)\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not fs.exists(config_yaml):\r\n        rank_zero_warn(f\"Missing Tags: {config_yaml}.\", category=RuntimeWarning)\r\n        return {}\r\n\r\n    with fs.open(config_yaml, \"r\") as fp:\r\n        hparams = yaml.full_load(fp)\r\n\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        with contextlib.suppress(UnsupportedValueType, ValidationError):\r\n            return OmegaConf.create(hparams)\r\n    return hparams", "code_tokens": ["def", "load_hparams_from_yaml", "(", "config_yaml", ":", "_PATH", ",", "use_omegaconf", ":", "bool", "=", "True", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Load", "hparams", "from", "a", "file", ".", "Args", ":", "config_yaml", ":", "Path", "to", "config", "yaml", "file", "use_omegaconf", ":", "If", "omegaconf", "is", "available", "and", "`", "`", "use_omegaconf", "=", "True", "`", "`", ",", "the", "hparams", "will", "be", "converted", "to", "`", "`", "DictConfig", "`", "`", "if", "possible", ".", ">", ">", ">", "hparams", "=", "Namespace", "(", "batch_size", "=", "32", ",", "learning_rate", "=", "0", ".", "001", ",", "data_root", "=", "'", ".", "/", "any", "/", "path", "/", "here", "'", ")", ">", ">", ">", "path_yaml", "=", "'", ".", "/", "testing", "-", "hparams", ".", "yaml", "'", ">", ">", ">", "save_hparams_to_yaml", "(", "path_yaml", ",", "hparams", ")", ">", ">", ">", "hparams_new", "=", "load_hparams_from_yaml", "(", "path_yaml", ")", ">", ">", ">", "vars", "(", "hparams", ")", "=", "=", "hparams_new", "True", ">", ">", ">", "os", ".", "remove", "(", "path_yaml", ")", "\"", "\"", "\"", "fs", "=", "get_filesystem", "(", "config_yaml", ")", "if", "not", "fs", ".", "exists", "(", "config_yaml", ")", ":", "rank_zero_warn", "(", "f", "\"", "Missing", "Tags", ":", "{", "config_yaml", "}", ".", "\"", ",", "category", "=", "RuntimeWarning", ")", "return", "{", "}", "with", "fs", ".", "open", "(", "config_yaml", ",", "\"", "r", "\"", ")", "as", "fp", ":", "hparams", "=", "yaml", ".", "full_load", "(", "fp", ")", "if", "_OMEGACONF_AVAILABLE", "and", "use_omegaconf", ":", "from", "omegaconf", "import", "OmegaConf", "from", "omegaconf", ".", "errors", "import", "UnsupportedValueType", ",", "ValidationError", "with", "contextlib", ".", "suppress", "(", "UnsupportedValueType", ",", "ValidationError", ")", ":", "return", "OmegaConf", ".", "create", "(", "hparams", ")", "return", "hparams"], "docstring": "Load hparams from a file.\r\n\r\n        Args:\r\n            config_yaml: Path to config yaml file\r\n            use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n                the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    >>> hparams = Namespace(batch_size=32, learning_rate=0.001, data_root='./any/path/here')\r\n    >>> path_yaml = './testing-hparams.yaml'\r\n    >>> save_hparams_to_yaml(path_yaml, hparams)\r\n    >>> hparams_new = load_hparams_from_yaml(path_yaml)\r\n    >>> vars(hparams) == hparams_new\r\n    True\r\n    >>> os.remove(path_yaml)", "docstring_tokens": ["load", "hparams", "from", "a", "file", "args", "config_yaml", "path", "to", "config", "yaml", "file", "use_omegaconf", "if", "omegaconf", "is", "available", "and", "use_omegaconf", "true", "the", "hparams", "will", "be", "converted", "to", "dictconfig", "if", "possible", "hparams", "namespace", "batch_size", "32", "learning_rate", "0", "001", "data_root", "any", "path", "here", "path_yaml", "testing", "hparams", "yaml", "save_hparams_to_yaml", "path_yaml", "hparams", "hparams_new", "load_hparams_from_yaml", "path_yaml", "vars", "hparams", "hparams_new", "true", "os", "remove", "path_yaml"], "docstring_summary": "Load hparams from a file.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\saving.py", "partition": "train", "function_type": "function", "start_line": 283, "end_line": 314, "hash": "7ff6d34b29693d84c3157b96a891803c", "complexity": 6, "parameters": ["config_yaml", "use_omegaconf"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\saving.py", "func_name": "save_hparams_to_yaml", "original_string": "def save_hparams_to_yaml(config_yaml: _PATH, hparams: Union[dict, Namespace], use_omegaconf: bool = True) -> None:\r\n    \"\"\"\r\n    Args:\r\n        config_yaml: path to new YAML file\r\n        hparams: parameters to be saved\r\n        use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n            the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not _is_dir(fs, os.path.dirname(config_yaml)):\r\n        raise RuntimeError(f\"Missing folder: {os.path.dirname(config_yaml)}.\")\r\n\r\n    # convert Namespace or AD to dict\r\n    if isinstance(hparams, Namespace):\r\n        hparams = vars(hparams)\r\n    elif isinstance(hparams, AttributeDict):\r\n        hparams = dict(hparams)\r\n\r\n    # saving with OmegaConf objects\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.dictconfig import DictConfig\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        # deepcopy: hparams from user shouldn't be resolved\r\n        hparams = deepcopy(hparams)\r\n        hparams = apply_to_collection(hparams, DictConfig, OmegaConf.to_container, resolve=True)\r\n        with fs.open(config_yaml, \"w\", encoding=\"utf-8\") as fp:\r\n            try:\r\n                OmegaConf.save(hparams, fp)\r\n                return\r\n            except (UnsupportedValueType, ValidationError):\r\n                pass\r\n\r\n    if not isinstance(hparams, dict):\r\n        raise TypeError(\"hparams must be dictionary\")\r\n\r\n    hparams_allowed = {}\r\n    # drop parameters which contain some strange datatypes as fsspec\r\n    for k, v in hparams.items():\r\n        try:\r\n            v = v.name if isinstance(v, Enum) else v\r\n            yaml.dump(v)\r\n        except (TypeError, ValueError):\r\n            warn(f\"Skipping '{k}' parameter because it is not possible to safely dump to YAML.\")\r\n            hparams[k] = type(v).__name__\r\n        else:\r\n            hparams_allowed[k] = v\r\n\r\n    # saving the standard way\r\n    with fs.open(config_yaml, \"w\", newline=\"\") as fp:\r\n        yaml.dump(hparams_allowed, fp)", "language": "python", "code": "def save_hparams_to_yaml(config_yaml: _PATH, hparams: Union[dict, Namespace], use_omegaconf: bool = True) -> None:\r\n    \"\"\"\r\n    Args:\r\n        config_yaml: path to new YAML file\r\n        hparams: parameters to be saved\r\n        use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n            the hparams will be converted to ``DictConfig`` if possible.\r\n\r\n    \"\"\"\r\n    fs = get_filesystem(config_yaml)\r\n    if not _is_dir(fs, os.path.dirname(config_yaml)):\r\n        raise RuntimeError(f\"Missing folder: {os.path.dirname(config_yaml)}.\")\r\n\r\n    # convert Namespace or AD to dict\r\n    if isinstance(hparams, Namespace):\r\n        hparams = vars(hparams)\r\n    elif isinstance(hparams, AttributeDict):\r\n        hparams = dict(hparams)\r\n\r\n    # saving with OmegaConf objects\r\n    if _OMEGACONF_AVAILABLE and use_omegaconf:\r\n        from omegaconf import OmegaConf\r\n        from omegaconf.dictconfig import DictConfig\r\n        from omegaconf.errors import UnsupportedValueType, ValidationError\r\n\r\n        # deepcopy: hparams from user shouldn't be resolved\r\n        hparams = deepcopy(hparams)\r\n        hparams = apply_to_collection(hparams, DictConfig, OmegaConf.to_container, resolve=True)\r\n        with fs.open(config_yaml, \"w\", encoding=\"utf-8\") as fp:\r\n            try:\r\n                OmegaConf.save(hparams, fp)\r\n                return\r\n            except (UnsupportedValueType, ValidationError):\r\n                pass\r\n\r\n    if not isinstance(hparams, dict):\r\n        raise TypeError(\"hparams must be dictionary\")\r\n\r\n    hparams_allowed = {}\r\n    # drop parameters which contain some strange datatypes as fsspec\r\n    for k, v in hparams.items():\r\n        try:\r\n            v = v.name if isinstance(v, Enum) else v\r\n            yaml.dump(v)\r\n        except (TypeError, ValueError):\r\n            warn(f\"Skipping '{k}' parameter because it is not possible to safely dump to YAML.\")\r\n            hparams[k] = type(v).__name__\r\n        else:\r\n            hparams_allowed[k] = v\r\n\r\n    # saving the standard way\r\n    with fs.open(config_yaml, \"w\", newline=\"\") as fp:\r\n        yaml.dump(hparams_allowed, fp)", "code_tokens": ["def", "save_hparams_to_yaml", "(", "config_yaml", ":", "_PATH", ",", "hparams", ":", "Union", "[", "dict", ",", "Namespace", "]", ",", "use_omegaconf", ":", "bool", "=", "True", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "config_yaml", ":", "path", "to", "new", "YAML", "file", "hparams", ":", "parameters", "to", "be", "saved", "use_omegaconf", ":", "If", "omegaconf", "is", "available", "and", "`", "`", "use_omegaconf", "=", "True", "`", "`", ",", "the", "hparams", "will", "be", "converted", "to", "`", "`", "DictConfig", "`", "`", "if", "possible", ".", "\"", "\"", "\"", "fs", "=", "get_filesystem", "(", "config_yaml", ")", "if", "not", "_is_dir", "(", "fs", ",", "os", ".", "path", ".", "dirname", "(", "config_yaml", ")", ")", ":", "raise", "RuntimeError", "(", "f", "\"", "Missing", "folder", ":", "{", "os", ".", "path", ".", "dirname", "(", "config_yaml", ")", "}", ".", "\"", ")", "if", "isinstance", "(", "hparams", ",", "Namespace", ")", ":", "hparams", "=", "vars", "(", "hparams", ")", "elif", "isinstance", "(", "hparams", ",", "AttributeDict", ")", ":", "hparams", "=", "dict", "(", "hparams", ")", "if", "_OMEGACONF_AVAILABLE", "and", "use_omegaconf", ":", "from", "omegaconf", "import", "OmegaConf", "from", "omegaconf", ".", "dictconfig", "import", "DictConfig", "from", "omegaconf", ".", "errors", "import", "UnsupportedValueType", ",", "ValidationError", "hparams", "=", "deepcopy", "(", "hparams", ")", "hparams", "=", "apply_to_collection", "(", "hparams", ",", "DictConfig", ",", "OmegaConf", ".", "to_container", ",", "resolve", "=", "True", ")", "with", "fs", ".", "open", "(", "config_yaml", ",", "\"", "w", "\"", ",", "encoding", "=", "\"", "utf", "-", "8", "\"", ")", "as", "fp", ":", "try", ":", "OmegaConf", ".", "save", "(", "hparams", ",", "fp", ")", "return", "except", "(", "UnsupportedValueType", ",", "ValidationError", ")", ":", "pass", "if", "not", "isinstance", "(", "hparams", ",", "dict", ")", ":", "raise", "TypeError", "(", "\"", "hparams", "must", "be", "dictionary", "\"", ")", "hparams_allowed", "=", "{", "}", "for", "k", ",", "v", "in", "hparams", ".", "items", "(", ")", ":", "try", ":", "v", "=", "v", ".", "name", "if", "isinstance", "(", "v", ",", "Enum", ")", "else", "v", "yaml", ".", "dump", "(", "v", ")", "except", "(", "TypeError", ",", "ValueError", ")", ":", "warn", "(", "f", "\"", "Skipping", "'", "{", "k", "}", "'", "parameter", "because", "it", "is", "not", "possible", "to", "safely", "dump", "to", "YAML", ".", "\"", ")", "hparams", "[", "k", "]", "=", "type", "(", "v", ")", ".", "__name__", "else", ":", "hparams_allowed", "[", "k", "]", "=", "v", "with", "fs", ".", "open", "(", "config_yaml", ",", "\"", "w", "\"", ",", "newline", "=", "\"", "\"", ")", "as", "fp", ":", "yaml", ".", "dump", "(", "hparams_allowed", ",", "fp", ")"], "docstring": "Args:\r\n        config_yaml: path to new YAML file\r\n        hparams: parameters to be saved\r\n        use_omegaconf: If omegaconf is available and ``use_omegaconf=True``,\r\n            the hparams will be converted to ``DictConfig`` if possible.", "docstring_tokens": ["args", "config_yaml", "path", "to", "new", "yaml", "file", "hparams", "parameters", "to", "be", "saved", "use_omegaconf", "if", "omegaconf", "is", "available", "and", "use_omegaconf", "true", "the", "hparams", "will", "be", "converted", "to", "dictconfig", "if", "possible"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\saving.py", "partition": "train", "function_type": "function", "start_line": 317, "end_line": 369, "hash": "0130de2205e89f257439afcbd6afdbd2", "complexity": 13, "parameters": ["config_yaml", "hparams", "Namespace]", "use_omegaconf"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "func_name": "save_hyperparameters", "original_string": "def save_hyperparameters(\r\n        self,\r\n        *args: Any,\r\n        ignore: Optional[Union[Sequence[str], str]] = None,\r\n        frame: Optional[types.FrameType] = None,\r\n        logger: bool = True,\r\n    ) -> None:\r\n        \"\"\"Save arguments to ``hparams`` attribute.\r\n\r\n        Args:\r\n            args: single object of `dict`, `NameSpace` or `OmegaConf`\r\n                or string names or arguments from class ``__init__``\r\n            ignore: an argument name or a list of argument names from\r\n                class ``__init__`` to be ignored\r\n            frame: a frame object. Default is None\r\n            logger: Whether to send the hyperparameters to the logger. Default: True\r\n\r\n        Example::\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # manually assign arguments\r\n            ...         self.save_hyperparameters('arg1', 'arg3')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class AutomaticArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # equivalent automatic\r\n            ...         self.save_hyperparameters()\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = AutomaticArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg2\": abc\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class SingleArgModel(HyperparametersMixin):\r\n            ...     def __init__(self, params):\r\n            ...         super().__init__()\r\n            ...         # manually assign single argument\r\n            ...         self.save_hyperparameters(params)\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))\r\n            >>> model.hparams\r\n            \"p1\": 1\r\n            \"p2\": abc\r\n            \"p3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # pass argument(s) to ignore as a string or in a list\r\n            ...         self.save_hyperparameters(ignore='arg2')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n        \"\"\"\r\n        self._log_hyperparams = logger\r\n        given_hparams = _given_hyperparameters.get()\r\n        # the frame needs to be created in this file.\r\n        if given_hparams is None and not frame:\r\n            current_frame = inspect.currentframe()\r\n            if current_frame:\r\n                frame = current_frame.f_back\r\n        save_hyperparameters(self, *args, ignore=ignore, frame=frame, given_hparams=given_hparams)", "language": "python", "code": "def save_hyperparameters(\r\n        self,\r\n        *args: Any,\r\n        ignore: Optional[Union[Sequence[str], str]] = None,\r\n        frame: Optional[types.FrameType] = None,\r\n        logger: bool = True,\r\n    ) -> None:\r\n        \"\"\"Save arguments to ``hparams`` attribute.\r\n\r\n        Args:\r\n            args: single object of `dict`, `NameSpace` or `OmegaConf`\r\n                or string names or arguments from class ``__init__``\r\n            ignore: an argument name or a list of argument names from\r\n                class ``__init__`` to be ignored\r\n            frame: a frame object. Default is None\r\n            logger: Whether to send the hyperparameters to the logger. Default: True\r\n\r\n        Example::\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # manually assign arguments\r\n            ...         self.save_hyperparameters('arg1', 'arg3')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class AutomaticArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # equivalent automatic\r\n            ...         self.save_hyperparameters()\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = AutomaticArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg2\": abc\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class SingleArgModel(HyperparametersMixin):\r\n            ...     def __init__(self, params):\r\n            ...         super().__init__()\r\n            ...         # manually assign single argument\r\n            ...         self.save_hyperparameters(params)\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))\r\n            >>> model.hparams\r\n            \"p1\": 1\r\n            \"p2\": abc\r\n            \"p3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # pass argument(s) to ignore as a string or in a list\r\n            ...         self.save_hyperparameters(ignore='arg2')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n        \"\"\"\r\n        self._log_hyperparams = logger\r\n        given_hparams = _given_hyperparameters.get()\r\n        # the frame needs to be created in this file.\r\n        if given_hparams is None and not frame:\r\n            current_frame = inspect.currentframe()\r\n            if current_frame:\r\n                frame = current_frame.f_back\r\n        save_hyperparameters(self, *args, ignore=ignore, frame=frame, given_hparams=given_hparams)", "code_tokens": ["def", "save_hyperparameters", "(", "self", ",", "*", "args", ":", "Any", ",", "ignore", ":", "Optional", "[", "Union", "[", "Sequence", "[", "str", "]", ",", "str", "]", "]", "=", "None", ",", "frame", ":", "Optional", "[", "types", ".", "FrameType", "]", "=", "None", ",", "logger", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "arguments", "to", "`", "`", "hparams", "`", "`", "attribute", ".", "Args", ":", "args", ":", "single", "object", "of", "`", "dict", "`", ",", "`", "NameSpace", "`", "or", "`", "OmegaConf", "`", "or", "string", "names", "or", "arguments", "from", "class", "`", "`", "__init__", "`", "`", "ignore", ":", "an", "argument", "name", "or", "a", "list", "of", "argument", "names", "from", "class", "`", "`", "__init__", "`", "`", "to", "be", "ignored", "frame", ":", "a", "frame", "object", ".", "Default", "is", "None", "logger", ":", "Whether", "to", "send", "the", "hyperparameters", "to", "the", "logger", ".", "Default", ":", "True", "Example", ":", ":", ">", ">", ">", "from", "lightning", ".", "pytorch", ".", "core", ".", "mixins", "import", "HyperparametersMixin", ">", ">", ">", "class", "ManuallyArgsModel", "(", "HyperparametersMixin", ")", ":", ".", ".", ".", "def", "__init__", "(", "self", ",", "arg1", ",", "arg2", ",", "arg3", ")", ":", ".", ".", ".", "super", "(", ")", ".", "__init__", "(", ")", ".", ".", ".", ".", ".", ".", "self", ".", "save_hyperparameters", "(", "'", "arg1", "'", ",", "'", "arg3", "'", ")", ".", ".", ".", "def", "forward", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", ".", ".", ".", ".", ".", ".", ">", ">", ">", "model", "=", "ManuallyArgsModel", "(", "1", ",", "'", "abc", "'", ",", "3", ".", "14", ")", ">", ">", ">", "model", ".", "hparams", "\"", "arg1", "\"", ":", "1", "\"", "arg3", "\"", ":", "3", ".", "14", ">", ">", ">", "from", "lightning", ".", "pytorch", ".", "core", ".", "mixins", "import", "HyperparametersMixin", ">", ">", ">", "class", "AutomaticArgsModel", "(", "HyperparametersMixin", ")", ":", ".", ".", ".", "def", "__init__", "(", "self", ",", "arg1", ",", "arg2", ",", "arg3", ")", ":", ".", ".", ".", "super", "(", ")", ".", "__init__", "(", ")", ".", ".", ".", ".", ".", ".", "self", ".", "save_hyperparameters", "(", ")", ".", ".", ".", "def", "forward", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", ".", ".", ".", ".", ".", ".", ">", ">", ">", "model", "=", "AutomaticArgsModel", "(", "1", ",", "'", "abc", "'", ",", "3", ".", "14", ")", ">", ">", ">", "model", ".", "hparams", "\"", "arg1", "\"", ":", "1", "\"", "arg2", "\"", ":", "abc", "\"", "arg3", "\"", ":", "3", ".", "14", ">", ">", ">", "from", "lightning", ".", "pytorch", ".", "core", ".", "mixins", "import", "HyperparametersMixin", ">", ">", ">", "class", "SingleArgModel", "(", "HyperparametersMixin", ")", ":", ".", ".", ".", "def", "__init__", "(", "self", ",", "params", ")", ":", ".", ".", ".", "super", "(", ")", ".", "__init__", "(", ")", ".", ".", ".", ".", ".", ".", "self", ".", "save_hyperparameters", "(", "params", ")", ".", ".", ".", "def", "forward", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", ".", ".", ".", ".", ".", ".", ">", ">", ">", "model", "=", "SingleArgModel", "(", "Namespace", "(", "p1", "=", "1", ",", "p2", "=", "'", "abc", "'", ",", "p3", "=", "3", ".", "14", ")", ")", ">", ">", ">", "model", ".", "hparams", "\"", "p1", "\"", ":", "1", "\"", "p2", "\"", ":", "abc", "\"", "p3", "\"", ":", "3", ".", "14", ">", ">", ">", "from", "lightning", ".", "pytorch", ".", "core", ".", "mixins", "import", "HyperparametersMixin", ">", ">", ">", "class", "ManuallyArgsModel", "(", "HyperparametersMixin", ")", ":", ".", ".", ".", "def", "__init__", "(", "self", ",", "arg1", ",", "arg2", ",", "arg3", ")", ":", ".", ".", ".", "super", "(", ")", ".", "__init__", "(", ")", ".", ".", ".", ".", ".", ".", "self", ".", "save_hyperparameters", "(", "ignore", "=", "'", "arg2", "'", ")", ".", ".", ".", "def", "forward", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", ".", ".", ".", ".", ".", ".", ">", ">", ">", "model", "=", "ManuallyArgsModel", "(", "1", ",", "'", "abc", "'", ",", "3", ".", "14", ")", ">", ">", ">", "model", ".", "hparams", "\"", "arg1", "\"", ":", "1", "\"", "arg3", "\"", ":", "3", ".", "14", "\"", "\"", "\"", "self", ".", "_log_hyperparams", "=", "logger", "given_hparams", "=", "_given_hyperparameters", ".", "get", "(", ")", "if", "given_hparams", "is", "None", "and", "not", "frame", ":", "current_frame", "=", "inspect", ".", "currentframe", "(", ")", "if", "current_frame", ":", "frame", "=", "current_frame", ".", "f_back", "save_hyperparameters", "(", "self", ",", "*", "args", ",", "ignore", "=", "ignore", ",", "frame", "=", "frame", ",", "given_hparams", "=", "given_hparams", ")"], "docstring": "Save arguments to ``hparams`` attribute.\r\n\r\n        Args:\r\n            args: single object of `dict`, `NameSpace` or `OmegaConf`\r\n                or string names or arguments from class ``__init__``\r\n            ignore: an argument name or a list of argument names from\r\n                class ``__init__`` to be ignored\r\n            frame: a frame object. Default is None\r\n            logger: Whether to send the hyperparameters to the logger. Default: True\r\n\r\n        Example::\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # manually assign arguments\r\n            ...         self.save_hyperparameters('arg1', 'arg3')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class AutomaticArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # equivalent automatic\r\n            ...         self.save_hyperparameters()\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = AutomaticArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg2\": abc\r\n            \"arg3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class SingleArgModel(HyperparametersMixin):\r\n            ...     def __init__(self, params):\r\n            ...         super().__init__()\r\n            ...         # manually assign single argument\r\n            ...         self.save_hyperparameters(params)\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))\r\n            >>> model.hparams\r\n            \"p1\": 1\r\n            \"p2\": abc\r\n            \"p3\": 3.14\r\n\r\n            >>> from lightning.pytorch.core.mixins import HyperparametersMixin\r\n            >>> class ManuallyArgsModel(HyperparametersMixin):\r\n            ...     def __init__(self, arg1, arg2, arg3):\r\n            ...         super().__init__()\r\n            ...         # pass argument(s) to ignore as a string or in a list\r\n            ...         self.save_hyperparameters(ignore='arg2')\r\n            ...     def forward(self, *args, **kwargs):\r\n            ...         ...\r\n            >>> model = ManuallyArgsModel(1, 'abc', 3.14)\r\n            >>> model.hparams\r\n            \"arg1\": 1\r\n            \"arg3\": 3.14", "docstring_tokens": ["save", "arguments", "to", "hparams", "attribute", "args", "args", "single", "object", "of", "dict", "namespace", "or", "omegaconf", "or", "string", "names", "or", "arguments", "from", "class", "__init__", "ignore", "an", "argument", "name", "or", "a", "list", "of", "argument", "names", "from", "class", "__init__", "to", "be", "ignored", "frame", "a", "frame", "object", "default", "is", "none", "logger", "whether", "to", "send", "the", "hyperparameters", "to", "the", "logger", "default", "true", "example", "from", "lightning", "pytorch", "core", "mixins", "import", "hyperparametersmixin", "class", "manuallyargsmodel", "hyperparametersmixin", "def", "__init__", "self", "arg1", "arg2", "arg3", "super", "__init__", "manually", "assign", "arguments", "self", "save_hyperparameters", "arg1", "arg3", "def", "forward", "self", "args", "kwargs", "model", "manuallyargsmodel", "1", "abc", "3", "14", "model", "hparams", "arg1", "1", "arg3", "3", "14", "from", "lightning", "pytorch", "core", "mixins", "import", "hyperparametersmixin", "class", "automaticargsmodel", "hyperparametersmixin", "def", "__init__", "self", "arg1", "arg2", "arg3", "super", "__init__", "equivalent", "automatic", "self", "save_hyperparameters", "def", "forward", "self", "args", "kwargs", "model", "automaticargsmodel", "1", "abc", "3", "14", "model", "hparams", "arg1", "1", "arg2", "abc", "arg3", "3", "14", "from", "lightning", "pytorch", "core", "mixins", "import", "hyperparametersmixin", "class", "singleargmodel", "hyperparametersmixin", "def", "__init__", "self", "params", "super", "__init__", "manually", "assign", "single", "argument", "self", "save_hyperparameters", "params", "def", "forward", "self", "args", "kwargs", "model", "singleargmodel", "namespace", "p1", "1", "p2", "abc", "p3", "3", "14", "model", "hparams", "p1", "1", "p2", "abc", "p3", "3", "14", "from", "lightning", "pytorch", "core", "mixins", "import", "hyperparametersmixin", "class", "manuallyargsmodel", "hyperparametersmixin", "def", "__init__", "self", "arg1", "arg2", "arg3", "super", "__init__", "pass", "argument", "s", "to", "ignore", "as", "a", "string", "or", "in", "a", "list", "self", "save_hyperparameters", "ignore", "arg2", "def", "forward", "self", "args", "kwargs", "model", "manuallyargsmodel", "1", "abc", "3", "14", "model", "hparams", "arg1", "1", "arg3", "3", "14"], "docstring_summary": "Save arguments to ``hparams`` attribute.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "HyperparametersMixin", "start_line": 50, "end_line": 130, "hash": "5cf08a8cc32298d21125f6e8a460915b", "complexity": 4, "parameters": ["*args", "ignore", "str]]", "frame", "logger"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "func_name": "hparams", "original_string": "def hparams(self) -> Union[AttributeDict, MutableMapping]:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For\r\n        the frozen set of initial hyperparameters, use :attr:`hparams_initial`.\r\n\r\n        Returns:\r\n            Mutable hyperparameters dictionary\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams\"):\r\n            self._hparams = AttributeDict()\r\n        return self._hparams", "language": "python", "code": "def hparams(self) -> Union[AttributeDict, MutableMapping]:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For\r\n        the frozen set of initial hyperparameters, use :attr:`hparams_initial`.\r\n\r\n        Returns:\r\n            Mutable hyperparameters dictionary\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams\"):\r\n            self._hparams = AttributeDict()\r\n        return self._hparams", "code_tokens": ["def", "hparams", "(", "self", ")", "-", ">", "Union", "[", "AttributeDict", ",", "MutableMapping", "]", ":", "\"", "\"", "\"", "The", "collection", "of", "hyperparameters", "saved", "with", ":", "meth", ":", "`", "save_hyperparameters", "`", ".", "It", "is", "mutable", "by", "the", "user", ".", "For", "the", "frozen", "set", "of", "initial", "hyperparameters", ",", "use", ":", "attr", ":", "`", "hparams_initial", "`", ".", "Returns", ":", "Mutable", "hyperparameters", "dictionary", "\"", "\"", "\"", "if", "not", "hasattr", "(", "self", ",", "\"", "_hparams", "\"", ")", ":", "self", ".", "_hparams", "=", "AttributeDict", "(", ")", "return", "self", ".", "_hparams"], "docstring": "The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For\r\n        the frozen set of initial hyperparameters, use :attr:`hparams_initial`.\r\n\r\n        Returns:\r\n            Mutable hyperparameters dictionary", "docstring_tokens": ["the", "collection", "of", "hyperparameters", "saved", "with", "meth", "save_hyperparameters", "it", "is", "mutable", "by", "the", "user", "for", "the", "frozen", "set", "of", "initial", "hyperparameters", "use", "attr", "hparams_initial", "returns", "mutable", "hyperparameters", "dictionary"], "docstring_summary": "The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "HyperparametersMixin", "start_line": 153, "end_line": 163, "hash": "4283fbb18b3da79dbfd86001d556e004", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "func_name": "hparams_initial", "original_string": "def hparams_initial(self) -> AttributeDict:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.\r\n        Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.\r\n\r\n        Returns:\r\n            AttributeDict: immutable initial hyperparameters\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams_initial\"):\r\n            return AttributeDict()\r\n        # prevent any change\r\n        return copy.deepcopy(self._hparams_initial)", "language": "python", "code": "def hparams_initial(self) -> AttributeDict:\r\n        \"\"\"The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.\r\n        Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.\r\n\r\n        Returns:\r\n            AttributeDict: immutable initial hyperparameters\r\n\r\n        \"\"\"\r\n        if not hasattr(self, \"_hparams_initial\"):\r\n            return AttributeDict()\r\n        # prevent any change\r\n        return copy.deepcopy(self._hparams_initial)", "code_tokens": ["def", "hparams_initial", "(", "self", ")", "-", ">", "AttributeDict", ":", "\"", "\"", "\"", "The", "collection", "of", "hyperparameters", "saved", "with", ":", "meth", ":", "`", "save_hyperparameters", "`", ".", "These", "contents", "are", "read", "-", "only", ".", "Manual", "updates", "to", "the", "saved", "hyperparameters", "can", "instead", "be", "performed", "through", ":", "attr", ":", "`", "hparams", "`", ".", "Returns", ":", "AttributeDict", ":", "immutable", "initial", "hyperparameters", "\"", "\"", "\"", "if", "not", "hasattr", "(", "self", ",", "\"", "_hparams_initial", "\"", ")", ":", "return", "AttributeDict", "(", ")", "return", "copy", ".", "deepcopy", "(", "self", ".", "_hparams_initial", ")"], "docstring": "The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.\r\n        Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.\r\n\r\n        Returns:\r\n            AttributeDict: immutable initial hyperparameters", "docstring_tokens": ["the", "collection", "of", "hyperparameters", "saved", "with", "meth", "save_hyperparameters", "these", "contents", "are", "read", "only", "manual", "updates", "to", "the", "saved", "hyperparameters", "can", "instead", "be", "performed", "through", "attr", "hparams", "returns", "attributedict", "immutable", "initial", "hyperparameters"], "docstring_summary": "The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\core\\mixins\\hparams_mixin.py", "partition": "train", "function_type": "class_method", "class_name": "HyperparametersMixin", "start_line": 166, "end_line": 177, "hash": "7ea92ea0e2b3a6a40784a41c38529063", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "_try_load", "original_string": "def _try_load(path_data: str, trials: int = 30, delta: float = 1.0) -> tuple[Tensor, Tensor]:\r\n        \"\"\"Resolving loading from the same time from multiple concurrent processes.\"\"\"\r\n        res, exception = None, None\r\n        assert trials, \"at least some trial has to be set\"\r\n        assert os.path.isfile(path_data), f\"missing file: {path_data}\"\r\n        for _ in range(trials):\r\n            try:\r\n                res = torch.load(path_data)\r\n            # todo: specify the possible exception\r\n            except Exception as ex:\r\n                exception = ex\r\n                time.sleep(delta * random.random())  # noqa: S311\r\n            else:\r\n                break\r\n        assert res is not None\r\n        if exception is not None:\r\n            # raise the caught exception\r\n            raise exception\r\n        return res", "language": "python", "code": "def _try_load(path_data: str, trials: int = 30, delta: float = 1.0) -> tuple[Tensor, Tensor]:\r\n        \"\"\"Resolving loading from the same time from multiple concurrent processes.\"\"\"\r\n        res, exception = None, None\r\n        assert trials, \"at least some trial has to be set\"\r\n        assert os.path.isfile(path_data), f\"missing file: {path_data}\"\r\n        for _ in range(trials):\r\n            try:\r\n                res = torch.load(path_data)\r\n            # todo: specify the possible exception\r\n            except Exception as ex:\r\n                exception = ex\r\n                time.sleep(delta * random.random())  # noqa: S311\r\n            else:\r\n                break\r\n        assert res is not None\r\n        if exception is not None:\r\n            # raise the caught exception\r\n            raise exception\r\n        return res", "code_tokens": ["def", "_try_load", "(", "path_data", ":", "str", ",", "trials", ":", "int", "=", "30", ",", "delta", ":", "float", "=", "1", ".", "0", ")", "-", ">", "tuple", "[", "Tensor", ",", "Tensor", "]", ":", "\"", "\"", "\"", "Resolving", "loading", "from", "the", "same", "time", "from", "multiple", "concurrent", "processes", ".", "\"", "\"", "\"", "res", ",", "exception", "=", "None", ",", "None", "assert", "trials", ",", "\"", "at", "least", "some", "trial", "has", "to", "be", "set", "\"", "assert", "os", ".", "path", ".", "isfile", "(", "path_data", ")", ",", "f", "\"", "missing", "file", ":", "{", "path_data", "}", "\"", "for", "_", "in", "range", "(", "trials", ")", ":", "try", ":", "res", "=", "torch", ".", "load", "(", "path_data", ")", "except", "Exception", "as", "ex", ":", "exception", "=", "ex", "time", ".", "sleep", "(", "delta", "*", "random", ".", "random", "(", ")", ")", "else", ":", "break", "assert", "res", "is", "not", "None", "if", "exception", "is", "not", "None", ":", "raise", "exception", "return", "res"], "docstring": "Resolving loading from the same time from multiple concurrent processes.", "docstring_tokens": ["resolving", "loading", "from", "the", "same", "time", "from", "multiple", "concurrent", "processes"], "docstring_summary": "Resolving loading from the same time from multiple concurrent processes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "_MNIST", "start_line": 102, "end_line": 120, "hash": "04c9df9f6089eb7961469e831cbe9136", "complexity": 4, "parameters": ["path_data", "trials", "delta"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        data_dir: str = _DATASETS_PATH,\r\n        val_split: int = 5000,\r\n        num_workers: int = 16,\r\n        normalize: bool = False,\r\n        seed: int = 42,\r\n        batch_size: int = 32,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            data_dir: where to save/load the data\r\n            val_split: how many of the training images to use for the validation split\r\n            num_workers: how many workers to use for loading data\r\n            normalize: If true applies image normalize\r\n            seed: starting seed for RNG.\r\n            batch_size: desired batch size.\r\n        \"\"\"\r\n        super().__init__()\r\n        if num_workers and _IS_WINDOWS:\r\n            # see: https://stackoverflow.com/a/59680818\r\n            warn(\r\n                f\"You have requested num_workers={num_workers} on Windows,\"\r\n                \" but currently recommended is 0, so we set it for you\"\r\n            )\r\n            num_workers = 0\r\n\r\n        self.data_dir = data_dir\r\n        self.val_split = val_split\r\n        self.num_workers = num_workers\r\n        self.normalize = normalize\r\n        self.seed = seed\r\n        self.batch_size = batch_size", "language": "python", "code": "def __init__(\r\n        self,\r\n        data_dir: str = _DATASETS_PATH,\r\n        val_split: int = 5000,\r\n        num_workers: int = 16,\r\n        normalize: bool = False,\r\n        seed: int = 42,\r\n        batch_size: int = 32,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            data_dir: where to save/load the data\r\n            val_split: how many of the training images to use for the validation split\r\n            num_workers: how many workers to use for loading data\r\n            normalize: If true applies image normalize\r\n            seed: starting seed for RNG.\r\n            batch_size: desired batch size.\r\n        \"\"\"\r\n        super().__init__()\r\n        if num_workers and _IS_WINDOWS:\r\n            # see: https://stackoverflow.com/a/59680818\r\n            warn(\r\n                f\"You have requested num_workers={num_workers} on Windows,\"\r\n                \" but currently recommended is 0, so we set it for you\"\r\n            )\r\n            num_workers = 0\r\n\r\n        self.data_dir = data_dir\r\n        self.val_split = val_split\r\n        self.num_workers = num_workers\r\n        self.normalize = normalize\r\n        self.seed = seed\r\n        self.batch_size = batch_size", "code_tokens": ["def", "__init__", "(", "self", ",", "data_dir", ":", "str", "=", "_DATASETS_PATH", ",", "val_split", ":", "int", "=", "5000", ",", "num_workers", ":", "int", "=", "16", ",", "normalize", ":", "bool", "=", "False", ",", "seed", ":", "int", "=", "42", ",", "batch_size", ":", "int", "=", "32", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "data_dir", ":", "where", "to", "save", "/", "load", "the", "data", "val_split", ":", "how", "many", "of", "the", "training", "images", "to", "use", "for", "the", "validation", "split", "num_workers", ":", "how", "many", "workers", "to", "use", "for", "loading", "data", "normalize", ":", "If", "true", "applies", "image", "normalize", "seed", ":", "starting", "seed", "for", "RNG", ".", "batch_size", ":", "desired", "batch", "size", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", ")", "if", "num_workers", "and", "_IS_WINDOWS", ":", "warn", "(", "f", "\"", "You", "have", "requested", "num_workers", "=", "{", "num_workers", "}", "on", "Windows", ",", "\"", "\"", "but", "currently", "recommended", "is", "0", ",", "so", "we", "set", "it", "for", "you", "\"", ")", "num_workers", "=", "0", "self", ".", "data_dir", "=", "data_dir", "self", ".", "val_split", "=", "val_split", "self", ".", "num_workers", "=", "num_workers", "self", ".", "normalize", "=", "normalize", "self", ".", "seed", "=", "seed", "self", ".", "batch_size", "=", "batch_size"], "docstring": "Args:\r\n            data_dir: where to save/load the data\r\n            val_split: how many of the training images to use for the validation split\r\n            num_workers: how many workers to use for loading data\r\n            normalize: If true applies image normalize\r\n            seed: starting seed for RNG.\r\n            batch_size: desired batch size.", "docstring_tokens": ["args", "data_dir", "where", "to", "save", "load", "the", "data", "val_split", "how", "many", "of", "the", "training", "images", "to", "use", "for", "the", "validation", "split", "num_workers", "how", "many", "workers", "to", "use", "for", "loading", "data", "normalize", "if", "true", "applies", "image", "normalize", "seed", "starting", "seed", "for", "rng", "batch_size", "desired", "batch", "size"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "MNISTDataModule", "start_line": 155, "end_line": 187, "hash": "2a08872d9cfa924a766caecf21f02d76", "complexity": 3, "parameters": ["data_dir", "val_split", "num_workers", "normalize", "seed", "batch_size"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "prepare_data", "original_string": "def prepare_data(self) -> None:\r\n        \"\"\"Saves MNIST files to `data_dir`\"\"\"\r\n        MNIST(self.data_dir, train=True, download=True)\r\n        MNIST(self.data_dir, train=False, download=True)", "language": "python", "code": "def prepare_data(self) -> None:\r\n        \"\"\"Saves MNIST files to `data_dir`\"\"\"\r\n        MNIST(self.data_dir, train=True, download=True)\r\n        MNIST(self.data_dir, train=False, download=True)", "code_tokens": ["def", "prepare_data", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Saves", "MNIST", "files", "to", "`", "data_dir", "`", "\"", "\"", "\"", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "True", ",", "download", "=", "True", ")", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "False", ",", "download", "=", "True", ")"], "docstring": "Saves MNIST files to `data_dir`", "docstring_tokens": ["saves", "mnist", "files", "to", "data_dir"], "docstring_summary": "Saves MNIST files to `data_dir`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "MNISTDataModule", "start_line": 193, "end_line": 196, "hash": "45ccb403f7b90d7f216e1370677b2c31", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "setup", "original_string": "def setup(self, stage: str) -> None:\r\n        \"\"\"Split the train and valid dataset.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset: Dataset = MNIST(self.data_dir, train=True, download=False, **extra)\r\n        assert isinstance(dataset, Sized)\r\n        train_length = len(dataset)\r\n        self.dataset_train, self.dataset_val = random_split(\r\n            dataset, [train_length - self.val_split, self.val_split], generator=torch.Generator().manual_seed(42)\r\n        )", "language": "python", "code": "def setup(self, stage: str) -> None:\r\n        \"\"\"Split the train and valid dataset.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset: Dataset = MNIST(self.data_dir, train=True, download=False, **extra)\r\n        assert isinstance(dataset, Sized)\r\n        train_length = len(dataset)\r\n        self.dataset_train, self.dataset_val = random_split(\r\n            dataset, [train_length - self.val_split, self.val_split], generator=torch.Generator().manual_seed(42)\r\n        )", "code_tokens": ["def", "setup", "(", "self", ",", "stage", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Split", "the", "train", "and", "valid", "dataset", ".", "\"", "\"", "\"", "extra", "=", "{", "\"", "transform", "\"", ":", "self", ".", "default_transforms", "}", "if", "self", ".", "default_transforms", "else", "{", "}", "dataset", ":", "Dataset", "=", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "True", ",", "download", "=", "False", ",", "*", "*", "extra", ")", "assert", "isinstance", "(", "dataset", ",", "Sized", ")", "train_length", "=", "len", "(", "dataset", ")", "self", ".", "dataset_train", ",", "self", ".", "dataset_val", "=", "random_split", "(", "dataset", ",", "[", "train_length", "-", "self", ".", "val_split", ",", "self", ".", "val_split", "]", ",", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "42", ")", ")"], "docstring": "Split the train and valid dataset.", "docstring_tokens": ["split", "the", "train", "and", "valid", "dataset"], "docstring_summary": "Split the train and valid dataset.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "MNISTDataModule", "start_line": 198, "end_line": 206, "hash": "8549fe2c06a7833b04eb1e4fe17a6e19", "complexity": 2, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "train_dataloader", "original_string": "def train_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST train set removes a subset to use for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_train,\r\n            batch_size=self.batch_size,\r\n            shuffle=True,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "language": "python", "code": "def train_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST train set removes a subset to use for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_train,\r\n            batch_size=self.batch_size,\r\n            shuffle=True,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "code_tokens": ["def", "train_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "\"", "\"", "\"", "MNIST", "train", "set", "removes", "a", "subset", "to", "use", "for", "validation", ".", "\"", "\"", "\"", "return", "DataLoader", "(", "self", ".", "dataset_train", ",", "batch_size", "=", "self", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "self", ".", "num_workers", ",", "drop_last", "=", "True", ",", "pin_memory", "=", "True", ",", ")"], "docstring": "MNIST train set removes a subset to use for validation.", "docstring_tokens": ["mnist", "train", "set", "removes", "a", "subset", "to", "use", "for", "validation"], "docstring_summary": "MNIST train set removes a subset to use for validation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "MNISTDataModule", "start_line": 208, "end_line": 217, "hash": "2f1b03272adb5f96f69c613715c62c31", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "val_dataloader", "original_string": "def val_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST val set uses a subset of the training set for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_val,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "language": "python", "code": "def val_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST val set uses a subset of the training set for validation.\"\"\"\r\n        return DataLoader(\r\n            self.dataset_val,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "code_tokens": ["def", "val_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "\"", "\"", "\"", "MNIST", "val", "set", "uses", "a", "subset", "of", "the", "training", "set", "for", "validation", ".", "\"", "\"", "\"", "return", "DataLoader", "(", "self", ".", "dataset_val", ",", "batch_size", "=", "self", ".", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "self", ".", "num_workers", ",", "drop_last", "=", "True", ",", "pin_memory", "=", "True", ",", ")"], "docstring": "MNIST val set uses a subset of the training set for validation.", "docstring_tokens": ["mnist", "val", "set", "uses", "a", "subset", "of", "the", "training", "set", "for", "validation"], "docstring_summary": "MNIST val set uses a subset of the training set for validation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "MNISTDataModule", "start_line": 219, "end_line": 228, "hash": "38334b2f8bf0a4e1c604adc59a0851e7", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "func_name": "test_dataloader", "original_string": "def test_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST test set uses the test split.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset = MNIST(self.data_dir, train=False, download=False, **extra)\r\n        return DataLoader(\r\n            dataset,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "language": "python", "code": "def test_dataloader(self) -> DataLoader:\r\n        \"\"\"MNIST test set uses the test split.\"\"\"\r\n        extra = {\"transform\": self.default_transforms} if self.default_transforms else {}\r\n        dataset = MNIST(self.data_dir, train=False, download=False, **extra)\r\n        return DataLoader(\r\n            dataset,\r\n            batch_size=self.batch_size,\r\n            shuffle=False,\r\n            num_workers=self.num_workers,\r\n            drop_last=True,\r\n            pin_memory=True,\r\n        )", "code_tokens": ["def", "test_dataloader", "(", "self", ")", "-", ">", "DataLoader", ":", "\"", "\"", "\"", "MNIST", "test", "set", "uses", "the", "test", "split", ".", "\"", "\"", "\"", "extra", "=", "{", "\"", "transform", "\"", ":", "self", ".", "default_transforms", "}", "if", "self", ".", "default_transforms", "else", "{", "}", "dataset", "=", "MNIST", "(", "self", ".", "data_dir", ",", "train", "=", "False", ",", "download", "=", "False", ",", "*", "*", "extra", ")", "return", "DataLoader", "(", "dataset", ",", "batch_size", "=", "self", ".", "batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "self", ".", "num_workers", ",", "drop_last", "=", "True", ",", "pin_memory", "=", "True", ",", ")"], "docstring": "MNIST test set uses the test split.", "docstring_tokens": ["mnist", "test", "set", "uses", "the", "test", "split"], "docstring_summary": "MNIST test set uses the test split.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\mnist_datamodule.py", "partition": "train", "function_type": "class_method", "class_name": "MNISTDataModule", "start_line": 230, "end_line": 241, "hash": "d6fdee32103aad0f9c0c0bb2696e7365", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\demos\\transformer.py", "func_name": "generate_square_subsequent_mask", "original_string": "def generate_square_subsequent_mask(self, size: int) -> Tensor:\r\n        \"\"\"Generate a square mask for the sequence to prevent future tokens from being seen.\"\"\"\r\n        mask = torch.triu(torch.ones(size, size), diagonal=1)\r\n        mask = mask.float().masked_fill(mask == 1, float(\"-inf\")).masked_fill(mask == 0, 0.0)\r\n        return mask", "language": "python", "code": "def generate_square_subsequent_mask(self, size: int) -> Tensor:\r\n        \"\"\"Generate a square mask for the sequence to prevent future tokens from being seen.\"\"\"\r\n        mask = torch.triu(torch.ones(size, size), diagonal=1)\r\n        mask = mask.float().masked_fill(mask == 1, float(\"-inf\")).masked_fill(mask == 0, 0.0)\r\n        return mask", "code_tokens": ["def", "generate_square_subsequent_mask", "(", "self", ",", "size", ":", "int", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Generate", "a", "square", "mask", "for", "the", "sequence", "to", "prevent", "future", "tokens", "from", "being", "seen", ".", "\"", "\"", "\"", "mask", "=", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "size", ",", "size", ")", ",", "diagonal", "=", "1", ")", "mask", "=", "mask", ".", "float", "(", ")", ".", "masked_fill", "(", "mask", "=", "=", "1", ",", "float", "(", "\"", "-", "inf", "\"", ")", ")", ".", "masked_fill", "(", "mask", "=", "=", "0", ",", "0", ".", "0", ")", "return", "mask"], "docstring": "Generate a square mask for the sequence to prevent future tokens from being seen.", "docstring_tokens": ["generate", "a", "square", "mask", "for", "the", "sequence", "to", "prevent", "future", "tokens", "from", "being", "seen"], "docstring_summary": "Generate a square mask for the sequence to prevent future tokens from being seen.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\demos\\transformer.py", "partition": "train", "function_type": "class_method", "class_name": "Transformer", "start_line": 58, "end_line": 62, "hash": "655697cef017416488e6976c90c7f4ba", "complexity": 1, "parameters": ["size"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "experiment", "original_string": "def experiment(self) -> comet_experiment:\r\n        r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_comet_function()\r\n\r\n        \"\"\"\r\n\r\n        # if by some chance there is no experiment created yet (for example, when strategy=ddp_spawn)\r\n        # then we will create a new one\r\n        if not self._experiment:\r\n            self._create_experiment()\r\n\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> comet_experiment:\r\n        r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_comet_function()\r\n\r\n        \"\"\"\r\n\r\n        # if by some chance there is no experiment created yet (for example, when strategy=ddp_spawn)\r\n        # then we will create a new one\r\n        if not self._experiment:\r\n            self._create_experiment()\r\n\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "comet_experiment", ":", "r", "\"", "\"", "\"", "Actual", "Comet", "object", ".", "To", "use", "Comet", "features", "in", "your", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "do", "the", "following", ".", "Example", ":", ":", "self", ".", "logger", ".", "experiment", ".", "some_comet_function", "(", ")", "\"", "\"", "\"", "if", "not", "self", ".", "_experiment", ":", "self", ".", "_create_experiment", "(", ")", "return", "self", ".", "_experiment"], "docstring": "r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_comet_function()\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "actual", "comet", "object", "to", "use", "comet", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the", "following", "example", "self", "logger", "experiment", "some_comet_function"], "docstring_summary": "r\"\"\"Actual Comet object. To use Comet features in your :class:`~lightning.pytorch.core.LightningModule` do the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\comet.py", "partition": "train", "function_type": "class_method", "class_name": "CometLogger", "start_line": 305, "end_line": 320, "hash": "1b3701283d63fbdf29570dd89133c1a5", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "finalize", "original_string": "def finalize(self, status: str) -> None:\r\n        \"\"\"We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using\r\n        it after training is complete but instead of ending we will upload/save all the data.\"\"\"\r\n        if self._experiment is None:\r\n            # When using multiprocessing, finalize() should be a no-op on the main process, as no experiment has been\r\n            # initialized there\r\n            return\r\n\r\n        # just save the data\r\n        self.experiment.flush()", "language": "python", "code": "def finalize(self, status: str) -> None:\r\n        \"\"\"We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using\r\n        it after training is complete but instead of ending we will upload/save all the data.\"\"\"\r\n        if self._experiment is None:\r\n            # When using multiprocessing, finalize() should be a no-op on the main process, as no experiment has been\r\n            # initialized there\r\n            return\r\n\r\n        # just save the data\r\n        self.experiment.flush()", "code_tokens": ["def", "finalize", "(", "self", ",", "status", ":", "str", ")", "-", ">", "None", ":", "\"", "\"", "\"", "We", "will", "not", "end", "experiment", "(", "will", "not", "call", "self", ".", "_experiment", ".", "end", "(", ")", ")", "here", "to", "have", "an", "ability", "to", "continue", "using", "it", "after", "training", "is", "complete", "but", "instead", "of", "ending", "we", "will", "upload", "/", "save", "all", "the", "data", ".", "\"", "\"", "\"", "if", "self", ".", "_experiment", "is", "None", ":", "return", "self", ".", "experiment", ".", "flush", "(", ")"], "docstring": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using\r\n        it after training is complete but instead of ending we will upload/save all the data.", "docstring_tokens": ["we", "will", "not", "end", "experiment", "will", "not", "call", "self", "_experiment", "end", "here", "to", "have", "an", "ability", "to", "continue", "using", "it", "after", "training", "is", "complete", "but", "instead", "of", "ending", "we", "will", "upload", "save", "all", "the", "data"], "docstring_summary": "We will not end experiment (will not call self._experiment.end()) here to have an ability to continue using", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\comet.py", "partition": "train", "function_type": "class_method", "class_name": "CometLogger", "start_line": 354, "end_line": 363, "hash": "2278306137cc1adb566e8f86d28890fb", "complexity": 2, "parameters": ["status"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._comet_config.offline_directory", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._comet_config.offline_directory", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "save", "directory", ".", "Returns", ":", "The", "path", "to", "the", "save", "directory", ".", "\"", "\"", "\"", "return", "self", ".", "_comet_config", ".", "offline_directory"], "docstring": "Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.", "docstring_tokens": ["gets", "the", "save", "directory", "returns", "the", "path", "to", "the", "save", "directory"], "docstring_summary": "Gets the save directory.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\comet.py", "partition": "train", "function_type": "class_method", "class_name": "CometLogger", "start_line": 367, "end_line": 374, "hash": "71e6ddcddadd11c0935880e6f50b8918", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "name", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"Gets the project name.\r\n\r\n        Returns:\r\n            The project name if it is specified.\r\n\r\n        \"\"\"\r\n        return self._project_name", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Gets the project name.\r\n\r\n        Returns:\r\n            The project name if it is specified.\r\n\r\n        \"\"\"\r\n        return self._project_name", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "project", "name", ".", "Returns", ":", "The", "project", "name", "if", "it", "is", "specified", ".", "\"", "\"", "\"", "return", "self", ".", "_project_name"], "docstring": "Gets the project name.\r\n\r\n        Returns:\r\n            The project name if it is specified.", "docstring_tokens": ["gets", "the", "project", "name", "returns", "the", "project", "name", "if", "it", "is", "specified"], "docstring_summary": "Gets the project name.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\comet.py", "partition": "train", "function_type": "class_method", "class_name": "CometLogger", "start_line": 378, "end_line": 385, "hash": "dfe90a8df1cdc588c0e4369bf2c4472d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\comet.py", "func_name": "version", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the version.\r\n\r\n        Returns:\r\n            The experiment key if present\r\n\r\n        \"\"\"\r\n        # Don't create an experiment if we don't have one\r\n        if self._experiment is not None:\r\n            return self._experiment.get_key()", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the version.\r\n\r\n        Returns:\r\n            The experiment key if present\r\n\r\n        \"\"\"\r\n        # Don't create an experiment if we don't have one\r\n        if self._experiment is not None:\r\n            return self._experiment.get_key()", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "version", ".", "Returns", ":", "The", "experiment", "key", "if", "present", "\"", "\"", "\"", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", ".", "get_key", "(", ")"], "docstring": "Gets the version.\r\n\r\n        Returns:\r\n            The experiment key if present", "docstring_tokens": ["gets", "the", "version", "returns", "the", "experiment", "key", "if", "present"], "docstring_summary": "Gets the version.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\comet.py", "partition": "train", "function_type": "class_method", "class_name": "CometLogger", "start_line": 389, "end_line": 398, "hash": "6d31ac20aeedff3eca5d97e9b72f6356", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "log_hparams", "original_string": "def log_hparams(self, params: dict[str, Any]) -> None:\r\n        \"\"\"Record hparams and save into files.\"\"\"\r\n        self.hparams.update(params)\r\n        hparams_file = os.path.join(self.log_dir, self.NAME_HPARAMS_FILE)\r\n        save_hparams_to_yaml(hparams_file, self.hparams)", "language": "python", "code": "def log_hparams(self, params: dict[str, Any]) -> None:\r\n        \"\"\"Record hparams and save into files.\"\"\"\r\n        self.hparams.update(params)\r\n        hparams_file = os.path.join(self.log_dir, self.NAME_HPARAMS_FILE)\r\n        save_hparams_to_yaml(hparams_file, self.hparams)", "code_tokens": ["def", "log_hparams", "(", "self", ",", "params", ":", "dict", "[", "str", ",", "Any", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Record", "hparams", "and", "save", "into", "files", ".", "\"", "\"", "\"", "self", ".", "hparams", ".", "update", "(", "params", ")", "hparams_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "log_dir", ",", "self", ".", "NAME_HPARAMS_FILE", ")", "save_hparams_to_yaml", "(", "hparams_file", ",", "self", ".", "hparams", ")"], "docstring": "Record hparams and save into files.", "docstring_tokens": ["record", "hparams", "and", "save", "into", "files"], "docstring_summary": "Record hparams and save into files.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "ExperimentWriter", "start_line": 56, "end_line": 60, "hash": "b65bd9944ce5c65316082e3044369f97", "complexity": 1, "parameters": ["params", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "root_dir", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(self.save_dir, self.name)", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(self.save_dir, self.name)", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Parent", "directory", "for", "all", "checkpoint", "subdirectories", ".", "If", "the", "experiment", "name", "parameter", "is", "an", "empty", "string", ",", "no", "experiment", "subdirectory", "is", "used", "and", "the", "checkpoint", "will", "be", "saved", "in", "\"", "save_dir", "/", "version", "\"", "\"", "\"", "\"", "return", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "self", ".", "name", ")"], "docstring": "Parent directory for all checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"", "docstring_tokens": ["parent", "directory", "for", "all", "checkpoint", "subdirectories", "if", "the", "experiment", "name", "parameter", "is", "an", "empty", "string", "no", "experiment", "subdirectory", "is", "used", "and", "the", "checkpoint", "will", "be", "saved", "in", "save_dir", "version"], "docstring_summary": "Parent directory for all checkpoint subdirectories.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 106, "end_line": 113, "hash": "53177aa23eb90e893ba7422197004700", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "log_dir", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self.root_dir, version)", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        return os.path.join(self.root_dir, version)", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "log", "directory", "for", "this", "run", ".", "By", "default", ",", "it", "is", "named", "`", "`", "'", "version_", "$", "{", "self", ".", "version", "}", "'", "`", "`", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "'", "s", "version", "parameter", "instead", "of", "`", "`", "None", "`", "`", "or", "an", "int", ".", "\"", "\"", "\"", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "f", "\"", "version_", "{", "self", ".", "version", "}", "\"", "return", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "version", ")"], "docstring": "The log directory for this run.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.", "docstring_tokens": ["the", "log", "directory", "for", "this", "run", "by", "default", "it", "is", "named", "version_", "self", "version", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "s", "version", "parameter", "instead", "of", "none", "or", "an", "int"], "docstring_summary": "The log directory for this run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 117, "end_line": 126, "hash": "355459d968efe96f2bad0772ab432817", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> str:\r\n        \"\"\"The current directory where logs are saved.\r\n\r\n        Returns:\r\n            The path to current directory where logs are saved.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "language": "python", "code": "def save_dir(self) -> str:\r\n        \"\"\"The current directory where logs are saved.\r\n\r\n        Returns:\r\n            The path to current directory where logs are saved.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "current", "directory", "where", "logs", "are", "saved", ".", "Returns", ":", "The", "path", "to", "current", "directory", "where", "logs", "are", "saved", ".", "\"", "\"", "\"", "return", "self", ".", "_save_dir"], "docstring": "The current directory where logs are saved.\r\n\r\n        Returns:\r\n            The path to current directory where logs are saved.", "docstring_tokens": ["the", "current", "directory", "where", "logs", "are", "saved", "returns", "the", "path", "to", "current", "directory", "where", "logs", "are", "saved"], "docstring_summary": "The current directory where logs are saved.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 130, "end_line": 137, "hash": "0ceb01ddb1b32044d3a53df184e09441", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\csv_logs.py", "func_name": "experiment", "original_string": "def experiment(self) -> _FabricExperimentWriter:\r\n        r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your\r\n        :class:`~lightning.pytorch.core.LightningModule` do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        self._fs.makedirs(self.root_dir, exist_ok=True)\r\n        self._experiment = ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> _FabricExperimentWriter:\r\n        r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your\r\n        :class:`~lightning.pytorch.core.LightningModule` do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"\r\n        if self._experiment is not None:\r\n            return self._experiment\r\n\r\n        self._fs.makedirs(self.root_dir, exist_ok=True)\r\n        self._experiment = ExperimentWriter(log_dir=self.log_dir)\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "_FabricExperimentWriter", ":", "r", "\"", "\"", "\"", "Actual", "_ExperimentWriter", "object", ".", "To", "use", "_ExperimentWriter", "features", "in", "your", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "do", "the", "following", ".", "Example", ":", ":", "self", ".", "logger", ".", "experiment", ".", "some_experiment_writer_function", "(", ")", "\"", "\"", "\"", "if", "self", ".", "_experiment", "is", "not", "None", ":", "return", "self", ".", "_experiment", "self", ".", "_fs", ".", "makedirs", "(", "self", ".", "root_dir", ",", "exist_ok", "=", "True", ")", "self", ".", "_experiment", "=", "ExperimentWriter", "(", "log_dir", "=", "self", ".", "log_dir", ")", "return", "self", ".", "_experiment"], "docstring": "r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your\r\n        :class:`~lightning.pytorch.core.LightningModule` do the following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_experiment_writer_function()\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "actual", "_experimentwriter", "object", "to", "use", "_experimentwriter", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the", "following", "example", "self", "logger", "experiment", "some_experiment_writer_function"], "docstring_summary": "r\"\"\"Actual _ExperimentWriter object. To use _ExperimentWriter features in your", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\csv_logs.py", "partition": "train", "function_type": "class_method", "class_name": "CSVLogger", "start_line": 148, "end_line": 162, "hash": "71dce99c97a0f8d2b213aecd4141e0ce", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "after_save_checkpoint", "original_string": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "after_save_checkpoint", "(", "self", ",", "checkpoint_callback", ":", "ModelCheckpoint", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint", ".", "Args", ":", "checkpoint_callback", ":", "the", "model", "checkpoint", "callback", "instance", "\"", "\"", "\"", "pass"], "docstring": "Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance", "docstring_tokens": ["called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint", "args", "checkpoint_callback", "the", "model", "checkpoint", "callback", "instance"], "docstring_summary": "Called after model checkpoint callback saves a new checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 34, "end_line": 41, "hash": "0129a392fa7d3f9512dd1c726945396c", "complexity": 1, "parameters": ["checkpoint_callback"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where experiment logs get saved, or `None` if the logger does not save data\r\n        locally.\"\"\"\r\n        return None", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Return the root directory where experiment logs get saved, or `None` if the logger does not save data\r\n        locally.\"\"\"\r\n        return None", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Return", "the", "root", "directory", "where", "experiment", "logs", "get", "saved", ",", "or", "`", "None", "`", "if", "the", "logger", "does", "not", "save", "data", "locally", ".", "\"", "\"", "\"", "return", "None"], "docstring": "Return the root directory where experiment logs get saved, or `None` if the logger does not save data\r\n        locally.", "docstring_tokens": ["return", "the", "root", "directory", "where", "experiment", "logs", "get", "saved", "or", "none", "if", "the", "logger", "does", "not", "save", "data", "locally"], "docstring_summary": "Return the root directory where experiment logs get saved, or `None` if the logger does not save data", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "Logger", "start_line": 44, "end_line": 47, "hash": "10743a7c30d905c79508474174530f8e", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "__getattr__", "original_string": "def __getattr__(self, name: str) -> Callable:\r\n        \"\"\"Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.\"\"\"\r\n\r\n        def method(*args: Any, **kwargs: Any) -> None:\r\n            return None\r\n\r\n        return method", "language": "python", "code": "def __getattr__(self, name: str) -> Callable:\r\n        \"\"\"Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.\"\"\"\r\n\r\n        def method(*args: Any, **kwargs: Any) -> None:\r\n            return None\r\n\r\n        return method", "code_tokens": ["def", "__getattr__", "(", "self", ",", "name", ":", "str", ")", "-", ">", "Callable", ":", "\"", "\"", "\"", "Allows", "the", "DummyLogger", "to", "be", "called", "with", "arbitrary", "methods", ",", "to", "avoid", "AttributeErrors", ".", "\"", "\"", "\"", "def", "method", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "return", "None", "return", "method"], "docstring": "Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.", "docstring_tokens": ["allows", "the", "dummylogger", "to", "be", "called", "with", "arbitrary", "methods", "to", "avoid", "attributeerrors"], "docstring_summary": "Allows the DummyLogger to be called with arbitrary methods, to avoid AttributeErrors.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\logger.py", "partition": "train", "function_type": "class_method", "class_name": "DummyLogger", "start_line": 90, "end_line": 96, "hash": "dbb28453c27148005ad6e2cdf657ba18", "complexity": 1, "parameters": ["name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\logger.py", "func_name": "merge_dicts", "original_string": "def merge_dicts(  # pragma: no cover\r\n    dicts: Sequence[Mapping],\r\n    agg_key_funcs: Optional[Mapping] = None,\r\n    default_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n) -> dict:\r\n    \"\"\"Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.\r\n\r\n    Args:\r\n        dicts:\r\n            Sequence of dictionaries to be merged.\r\n        agg_key_funcs:\r\n            Mapping from key name to function. This function will aggregate a\r\n            list of values, obtained from the same key of all dictionaries.\r\n            If some key has no specified aggregation function, the default one\r\n            will be used. Default is: ``None`` (all keys will be aggregated by the\r\n            default function).\r\n        default_func:\r\n            Default function to aggregate keys, which are not presented in the\r\n            `agg_key_funcs` map.\r\n\r\n    Returns:\r\n        Dictionary with merged values.\r\n\r\n    Examples:\r\n        >>> import pprint\r\n        >>> d1 = {'a': 1.7, 'b': 2.0, 'c': 1, 'd': {'d1': 1, 'd3': 3}}\r\n        >>> d2 = {'a': 1.1, 'b': 2.2, 'v': 1, 'd': {'d1': 2, 'd2': 3}}\r\n        >>> d3 = {'a': 1.1, 'v': 2.3, 'd': {'d3': 3, 'd4': {'d5': 1}}}\r\n        >>> dflt_func = min\r\n        >>> agg_funcs = {'a': statistics.mean, 'v': max, 'd': {'d1': sum}}\r\n        >>> pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func))\r\n        {'a': 1.3,\r\n         'b': 2.0,\r\n         'c': 1,\r\n         'd': {'d1': 3, 'd2': 3, 'd3': 3, 'd4': {'d5': 1}},\r\n         'v': 2.3}\r\n\r\n    \"\"\"\r\n    agg_key_funcs = agg_key_funcs or {}\r\n    keys = list(functools.reduce(operator.or_, [set(d.keys()) for d in dicts]))\r\n    d_out: dict = defaultdict(dict)\r\n    for k in keys:\r\n        fn = agg_key_funcs.get(k)\r\n        values_to_agg = [v for v in [d_in.get(k) for d_in in dicts] if v is not None]\r\n\r\n        if isinstance(values_to_agg[0], dict):\r\n            d_out[k] = merge_dicts(values_to_agg, fn, default_func)\r\n        else:\r\n            d_out[k] = (fn or default_func)(values_to_agg)\r\n\r\n    return dict(d_out)", "language": "python", "code": "def merge_dicts(  # pragma: no cover\r\n    dicts: Sequence[Mapping],\r\n    agg_key_funcs: Optional[Mapping] = None,\r\n    default_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n) -> dict:\r\n    \"\"\"Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.\r\n\r\n    Args:\r\n        dicts:\r\n            Sequence of dictionaries to be merged.\r\n        agg_key_funcs:\r\n            Mapping from key name to function. This function will aggregate a\r\n            list of values, obtained from the same key of all dictionaries.\r\n            If some key has no specified aggregation function, the default one\r\n            will be used. Default is: ``None`` (all keys will be aggregated by the\r\n            default function).\r\n        default_func:\r\n            Default function to aggregate keys, which are not presented in the\r\n            `agg_key_funcs` map.\r\n\r\n    Returns:\r\n        Dictionary with merged values.\r\n\r\n    Examples:\r\n        >>> import pprint\r\n        >>> d1 = {'a': 1.7, 'b': 2.0, 'c': 1, 'd': {'d1': 1, 'd3': 3}}\r\n        >>> d2 = {'a': 1.1, 'b': 2.2, 'v': 1, 'd': {'d1': 2, 'd2': 3}}\r\n        >>> d3 = {'a': 1.1, 'v': 2.3, 'd': {'d3': 3, 'd4': {'d5': 1}}}\r\n        >>> dflt_func = min\r\n        >>> agg_funcs = {'a': statistics.mean, 'v': max, 'd': {'d1': sum}}\r\n        >>> pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func))\r\n        {'a': 1.3,\r\n         'b': 2.0,\r\n         'c': 1,\r\n         'd': {'d1': 3, 'd2': 3, 'd3': 3, 'd4': {'d5': 1}},\r\n         'v': 2.3}\r\n\r\n    \"\"\"\r\n    agg_key_funcs = agg_key_funcs or {}\r\n    keys = list(functools.reduce(operator.or_, [set(d.keys()) for d in dicts]))\r\n    d_out: dict = defaultdict(dict)\r\n    for k in keys:\r\n        fn = agg_key_funcs.get(k)\r\n        values_to_agg = [v for v in [d_in.get(k) for d_in in dicts] if v is not None]\r\n\r\n        if isinstance(values_to_agg[0], dict):\r\n            d_out[k] = merge_dicts(values_to_agg, fn, default_func)\r\n        else:\r\n            d_out[k] = (fn or default_func)(values_to_agg)\r\n\r\n    return dict(d_out)", "code_tokens": ["def", "merge_dicts", "(", "dicts", ":", "Sequence", "[", "Mapping", "]", ",", "agg_key_funcs", ":", "Optional", "[", "Mapping", "]", "=", "None", ",", "default_func", ":", "Callable", "[", "[", "Sequence", "[", "float", "]", "]", ",", "float", "]", "=", "statistics", ".", "mean", ",", ")", "-", ">", "dict", ":", "\"", "\"", "\"", "Merge", "a", "sequence", "with", "dictionaries", "into", "one", "dictionary", "by", "aggregating", "the", "same", "keys", "with", "some", "given", "function", ".", "Args", ":", "dicts", ":", "Sequence", "of", "dictionaries", "to", "be", "merged", ".", "agg_key_funcs", ":", "Mapping", "from", "key", "name", "to", "function", ".", "This", "function", "will", "aggregate", "a", "list", "of", "values", ",", "obtained", "from", "the", "same", "key", "of", "all", "dictionaries", ".", "If", "some", "key", "has", "no", "specified", "aggregation", "function", ",", "the", "default", "one", "will", "be", "used", ".", "Default", "is", ":", "`", "`", "None", "`", "`", "(", "all", "keys", "will", "be", "aggregated", "by", "the", "default", "function", ")", ".", "default_func", ":", "Default", "function", "to", "aggregate", "keys", ",", "which", "are", "not", "presented", "in", "the", "`", "agg_key_funcs", "`", "map", ".", "Returns", ":", "Dictionary", "with", "merged", "values", ".", "Examples", ":", ">", ">", ">", "import", "pprint", ">", ">", ">", "d1", "=", "{", "'", "a", "'", ":", "1", ".", "7", ",", "'", "b", "'", ":", "2", ".", "0", ",", "'", "c", "'", ":", "1", ",", "'", "d", "'", ":", "{", "'", "d1", "'", ":", "1", ",", "'", "d3", "'", ":", "3", "}", "}", ">", ">", ">", "d2", "=", "{", "'", "a", "'", ":", "1", ".", "1", ",", "'", "b", "'", ":", "2", ".", "2", ",", "'", "v", "'", ":", "1", ",", "'", "d", "'", ":", "{", "'", "d1", "'", ":", "2", ",", "'", "d2", "'", ":", "3", "}", "}", ">", ">", ">", "d3", "=", "{", "'", "a", "'", ":", "1", ".", "1", ",", "'", "v", "'", ":", "2", ".", "3", ",", "'", "d", "'", ":", "{", "'", "d3", "'", ":", "3", ",", "'", "d4", "'", ":", "{", "'", "d5", "'", ":", "1", "}", "}", "}", ">", ">", ">", "dflt_func", "=", "min", ">", ">", ">", "agg_funcs", "=", "{", "'", "a", "'", ":", "statistics", ".", "mean", ",", "'", "v", "'", ":", "max", ",", "'", "d", "'", ":", "{", "'", "d1", "'", ":", "sum", "}", "}", ">", ">", ">", "pprint", ".", "pprint", "(", "merge_dicts", "(", "[", "d1", ",", "d2", ",", "d3", "]", ",", "agg_funcs", ",", "dflt_func", ")", ")", "{", "'", "a", "'", ":", "1", ".", "3", ",", "'", "b", "'", ":", "2", ".", "0", ",", "'", "c", "'", ":", "1", ",", "'", "d", "'", ":", "{", "'", "d1", "'", ":", "3", ",", "'", "d2", "'", ":", "3", ",", "'", "d3", "'", ":", "3", ",", "'", "d4", "'", ":", "{", "'", "d5", "'", ":", "1", "}", "}", ",", "'", "v", "'", ":", "2", ".", "3", "}", "\"", "\"", "\"", "agg_key_funcs", "=", "agg_key_funcs", "or", "{", "}", "keys", "=", "list", "(", "functools", ".", "reduce", "(", "operator", ".", "or_", ",", "[", "set", "(", "d", ".", "keys", "(", ")", ")", "for", "d", "in", "dicts", "]", ")", ")", "d_out", ":", "dict", "=", "defaultdict", "(", "dict", ")", "for", "k", "in", "keys", ":", "fn", "=", "agg_key_funcs", ".", "get", "(", "k", ")", "values_to_agg", "=", "[", "v", "for", "v", "in", "[", "d_in", ".", "get", "(", "k", ")", "for", "d_in", "in", "dicts", "]", "if", "v", "is", "not", "None", "]", "if", "isinstance", "(", "values_to_agg", "[", "0", "]", ",", "dict", ")", ":", "d_out", "[", "k", "]", "=", "merge_dicts", "(", "values_to_agg", ",", "fn", ",", "default_func", ")", "else", ":", "d_out", "[", "k", "]", "=", "(", "fn", "or", "default_func", ")", "(", "values_to_agg", ")", "return", "dict", "(", "d_out", ")"], "docstring": "Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.\r\n\r\n    Args:\r\n        dicts:\r\n            Sequence of dictionaries to be merged.\r\n        agg_key_funcs:\r\n            Mapping from key name to function. This function will aggregate a\r\n            list of values, obtained from the same key of all dictionaries.\r\n            If some key has no specified aggregation function, the default one\r\n            will be used. Default is: ``None`` (all keys will be aggregated by the\r\n            default function).\r\n        default_func:\r\n            Default function to aggregate keys, which are not presented in the\r\n            `agg_key_funcs` map.\r\n\r\n    Returns:\r\n        Dictionary with merged values.\r\n\r\n    Examples:\r\n        >>> import pprint\r\n        >>> d1 = {'a': 1.7, 'b': 2.0, 'c': 1, 'd': {'d1': 1, 'd3': 3}}\r\n        >>> d2 = {'a': 1.1, 'b': 2.2, 'v': 1, 'd': {'d1': 2, 'd2': 3}}\r\n        >>> d3 = {'a': 1.1, 'v': 2.3, 'd': {'d3': 3, 'd4': {'d5': 1}}}\r\n        >>> dflt_func = min\r\n        >>> agg_funcs = {'a': statistics.mean, 'v': max, 'd': {'d1': sum}}\r\n        >>> pprint.pprint(merge_dicts([d1, d2, d3], agg_funcs, dflt_func))\r\n        {'a': 1.3,\r\n         'b': 2.0,\r\n         'c': 1,\r\n         'd': {'d1': 3, 'd2': 3, 'd3': 3, 'd4': {'d5': 1}},\r\n         'v': 2.3}", "docstring_tokens": ["merge", "a", "sequence", "with", "dictionaries", "into", "one", "dictionary", "by", "aggregating", "the", "same", "keys", "with", "some", "given", "function", "args", "dicts", "sequence", "of", "dictionaries", "to", "be", "merged", "agg_key_funcs", "mapping", "from", "key", "name", "to", "function", "this", "function", "will", "aggregate", "a", "list", "of", "values", "obtained", "from", "the", "same", "key", "of", "all", "dictionaries", "if", "some", "key", "has", "no", "specified", "aggregation", "function", "the", "default", "one", "will", "be", "used", "default", "is", "none", "all", "keys", "will", "be", "aggregated", "by", "the", "default", "function", "default_func", "default", "function", "to", "aggregate", "keys", "which", "are", "not", "presented", "in", "the", "agg_key_funcs", "map", "returns", "dictionary", "with", "merged", "values", "examples", "import", "pprint", "d1", "a", "1", "7", "b", "2", "0", "c", "1", "d", "d1", "1", "d3", "3", "d2", "a", "1", "1", "b", "2", "2", "v", "1", "d", "d1", "2", "d2", "3", "d3", "a", "1", "1", "v", "2", "3", "d", "d3", "3", "d4", "d5", "1", "dflt_func", "min", "agg_funcs", "a", "statistics", "mean", "v", "max", "d", "d1", "sum", "pprint", "pprint", "merge_dicts", "d1", "d2", "d3", "agg_funcs", "dflt_func", "a", "1", "3", "b", "2", "0", "c", "1", "d", "d1", "3", "d2", "3", "d3", "3", "d4", "d5", "1", "v", "2", "3"], "docstring_summary": "Merge a sequence with dictionaries into one dictionary by aggregating the same keys with some given function.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\logger.py", "partition": "train", "function_type": "function", "start_line": 100, "end_line": 150, "hash": "722cabf7b15ed61a9361cdff09c5f106", "complexity": 9, "parameters": ["# pragma", "agg_key_funcs", "default_func", "float]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "experiment", "original_string": "def experiment(self) -> \"MlflowClient\":\r\n        r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_mlflow_function()\r\n\r\n        \"\"\"\r\n        import mlflow\r\n\r\n        if self._initialized:\r\n            return self._mlflow_client\r\n\r\n        mlflow.set_tracking_uri(self._tracking_uri)\r\n\r\n        if self._run_id is not None:\r\n            run = self._mlflow_client.get_run(self._run_id)\r\n            self._experiment_id = run.info.experiment_id\r\n            self._initialized = True\r\n            return self._mlflow_client\r\n\r\n        if self._experiment_id is None:\r\n            expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n            if expt is not None and expt.lifecycle_stage != \"deleted\":\r\n                self._experiment_id = expt.experiment_id\r\n            else:\r\n                log.warning(f\"Experiment with name {self._experiment_name} not found. Creating it.\")\r\n                self._experiment_id = self._mlflow_client.create_experiment(\r\n                    name=self._experiment_name, artifact_location=self._artifact_location\r\n                )\r\n\r\n        if self._run_id is None:\r\n            if self._run_name is not None:\r\n                self.tags = self.tags or {}\r\n\r\n                from mlflow.utils.mlflow_tags import MLFLOW_RUN_NAME\r\n\r\n                if MLFLOW_RUN_NAME in self.tags:\r\n                    log.warning(\r\n                        f\"The tag {MLFLOW_RUN_NAME} is found in tags. The value will be overridden by {self._run_name}.\"\r\n                    )\r\n                self.tags[MLFLOW_RUN_NAME] = self._run_name\r\n\r\n            resolve_tags = _get_resolve_tags()\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n            self._run_id = run.info.run_id\r\n        self._initialized = True\r\n        return self._mlflow_client", "language": "python", "code": "def experiment(self) -> \"MlflowClient\":\r\n        r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_mlflow_function()\r\n\r\n        \"\"\"\r\n        import mlflow\r\n\r\n        if self._initialized:\r\n            return self._mlflow_client\r\n\r\n        mlflow.set_tracking_uri(self._tracking_uri)\r\n\r\n        if self._run_id is not None:\r\n            run = self._mlflow_client.get_run(self._run_id)\r\n            self._experiment_id = run.info.experiment_id\r\n            self._initialized = True\r\n            return self._mlflow_client\r\n\r\n        if self._experiment_id is None:\r\n            expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n            if expt is not None and expt.lifecycle_stage != \"deleted\":\r\n                self._experiment_id = expt.experiment_id\r\n            else:\r\n                log.warning(f\"Experiment with name {self._experiment_name} not found. Creating it.\")\r\n                self._experiment_id = self._mlflow_client.create_experiment(\r\n                    name=self._experiment_name, artifact_location=self._artifact_location\r\n                )\r\n\r\n        if self._run_id is None:\r\n            if self._run_name is not None:\r\n                self.tags = self.tags or {}\r\n\r\n                from mlflow.utils.mlflow_tags import MLFLOW_RUN_NAME\r\n\r\n                if MLFLOW_RUN_NAME in self.tags:\r\n                    log.warning(\r\n                        f\"The tag {MLFLOW_RUN_NAME} is found in tags. The value will be overridden by {self._run_name}.\"\r\n                    )\r\n                self.tags[MLFLOW_RUN_NAME] = self._run_name\r\n\r\n            resolve_tags = _get_resolve_tags()\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n            self._run_id = run.info.run_id\r\n        self._initialized = True\r\n        return self._mlflow_client", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "\"", "MlflowClient", "\"", ":", "r", "\"", "\"", "\"", "Actual", "MLflow", "object", ".", "To", "use", "MLflow", "features", "in", "your", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "do", "the", "following", ".", "Example", ":", ":", "self", ".", "logger", ".", "experiment", ".", "some_mlflow_function", "(", ")", "\"", "\"", "\"", "import", "mlflow", "if", "self", ".", "_initialized", ":", "return", "self", ".", "_mlflow_client", "mlflow", ".", "set_tracking_uri", "(", "self", ".", "_tracking_uri", ")", "if", "self", ".", "_run_id", "is", "not", "None", ":", "run", "=", "self", ".", "_mlflow_client", ".", "get_run", "(", "self", ".", "_run_id", ")", "self", ".", "_experiment_id", "=", "run", ".", "info", ".", "experiment_id", "self", ".", "_initialized", "=", "True", "return", "self", ".", "_mlflow_client", "if", "self", ".", "_experiment_id", "is", "None", ":", "expt", "=", "self", ".", "_mlflow_client", ".", "get_experiment_by_name", "(", "self", ".", "_experiment_name", ")", "if", "expt", "is", "not", "None", "and", "expt", ".", "lifecycle_stage", "!", "=", "\"", "deleted", "\"", ":", "self", ".", "_experiment_id", "=", "expt", ".", "experiment_id", "else", ":", "log", ".", "warning", "(", "f", "\"", "Experiment", "with", "name", "{", "self", ".", "_experiment_name", "}", "not", "found", ".", "Creating", "it", ".", "\"", ")", "self", ".", "_experiment_id", "=", "self", ".", "_mlflow_client", ".", "create_experiment", "(", "name", "=", "self", ".", "_experiment_name", ",", "artifact_location", "=", "self", ".", "_artifact_location", ")", "if", "self", ".", "_run_id", "is", "None", ":", "if", "self", ".", "_run_name", "is", "not", "None", ":", "self", ".", "tags", "=", "self", ".", "tags", "or", "{", "}", "from", "mlflow", ".", "utils", ".", "mlflow_tags", "import", "MLFLOW_RUN_NAME", "if", "MLFLOW_RUN_NAME", "in", "self", ".", "tags", ":", "log", ".", "warning", "(", "f", "\"", "The", "tag", "{", "MLFLOW_RUN_NAME", "}", "is", "found", "in", "tags", ".", "The", "value", "will", "be", "overridden", "by", "{", "self", ".", "_run_name", "}", ".", "\"", ")", "self", ".", "tags", "[", "MLFLOW_RUN_NAME", "]", "=", "self", ".", "_run_name", "resolve_tags", "=", "_get_resolve_tags", "(", ")", "run", "=", "self", ".", "_mlflow_client", ".", "create_run", "(", "experiment_id", "=", "self", ".", "_experiment_id", ",", "tags", "=", "resolve_tags", "(", "self", ".", "tags", ")", ")", "self", ".", "_run_id", "=", "run", ".", "info", ".", "run_id", "self", ".", "_initialized", "=", "True", "return", "self", ".", "_mlflow_client"], "docstring": "r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n            self.logger.experiment.some_mlflow_function()\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "actual", "mlflow", "object", "to", "use", "mlflow", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the", "following", "example", "self", "logger", "experiment", "some_mlflow_function"], "docstring_summary": "r\"\"\"Actual MLflow object. To use MLflow features in your :class:`~lightning.pytorch.core.LightningModule` do the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\mlflow.py", "partition": "train", "function_type": "class_method", "class_name": "MLFlowLogger", "start_line": 156, "end_line": 204, "hash": "914c1eef00aa87acb95c5330e8e7d620", "complexity": 10, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "run_id", "original_string": "def run_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._run_id", "language": "python", "code": "def run_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._run_id", "code_tokens": ["def", "run_id", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Create", "the", "experiment", "if", "it", "does", "not", "exist", "to", "get", "the", "run", "id", ".", "Returns", ":", "The", "run", "id", ".", "\"", "\"", "\"", "_", "=", "self", ".", "experiment", "return", "self", ".", "_run_id"], "docstring": "Create the experiment if it does not exist to get the run id.\r\n\r\n        Returns:\r\n            The run id.", "docstring_tokens": ["create", "the", "experiment", "if", "it", "does", "not", "exist", "to", "get", "the", "run", "id", "returns", "the", "run", "id"], "docstring_summary": "Create the experiment if it does not exist to get the run id.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\mlflow.py", "partition": "train", "function_type": "class_method", "class_name": "MLFlowLogger", "start_line": 207, "end_line": 215, "hash": "7132f74e096e55623259168067676eaa", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "experiment_id", "original_string": "def experiment_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._experiment_id", "language": "python", "code": "def experiment_id(self) -> Optional[str]:\r\n        \"\"\"Create the experiment if it does not exist to get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        _ = self.experiment\r\n        return self._experiment_id", "code_tokens": ["def", "experiment_id", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Create", "the", "experiment", "if", "it", "does", "not", "exist", "to", "get", "the", "experiment", "id", ".", "Returns", ":", "The", "experiment", "id", ".", "\"", "\"", "\"", "_", "=", "self", ".", "experiment", "return", "self", ".", "_experiment_id"], "docstring": "Create the experiment if it does not exist to get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.", "docstring_tokens": ["create", "the", "experiment", "if", "it", "does", "not", "exist", "to", "get", "the", "experiment", "id", "returns", "the", "experiment", "id"], "docstring_summary": "Create the experiment if it does not exist to get the experiment id.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\mlflow.py", "partition": "train", "function_type": "class_method", "class_name": "MLFlowLogger", "start_line": 218, "end_line": 226, "hash": "a148cb2d6368052a33402491fd56e07e", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"The root file directory in which MLflow experiments are saved.\r\n\r\n        Return:\r\n            Local path to the root experiment directory if the tracking uri is local.\r\n            Otherwise returns `None`.\r\n\r\n        \"\"\"\r\n        if self._tracking_uri.startswith(LOCAL_FILE_URI_PREFIX):\r\n            return self._tracking_uri[len(LOCAL_FILE_URI_PREFIX) :]\r\n        return None", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"The root file directory in which MLflow experiments are saved.\r\n\r\n        Return:\r\n            Local path to the root experiment directory if the tracking uri is local.\r\n            Otherwise returns `None`.\r\n\r\n        \"\"\"\r\n        if self._tracking_uri.startswith(LOCAL_FILE_URI_PREFIX):\r\n            return self._tracking_uri[len(LOCAL_FILE_URI_PREFIX) :]\r\n        return None", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "The", "root", "file", "directory", "in", "which", "MLflow", "experiments", "are", "saved", ".", "Return", ":", "Local", "path", "to", "the", "root", "experiment", "directory", "if", "the", "tracking", "uri", "is", "local", ".", "Otherwise", "returns", "`", "None", "`", ".", "\"", "\"", "\"", "if", "self", ".", "_tracking_uri", ".", "startswith", "(", "LOCAL_FILE_URI_PREFIX", ")", ":", "return", "self", ".", "_tracking_uri", "[", "len", "(", "LOCAL_FILE_URI_PREFIX", ")", ":", "]", "return", "None"], "docstring": "The root file directory in which MLflow experiments are saved.\r\n\r\n        Return:\r\n            Local path to the root experiment directory if the tracking uri is local.\r\n            Otherwise returns `None`.", "docstring_tokens": ["the", "root", "file", "directory", "in", "which", "mlflow", "experiments", "are", "saved", "return", "local", "path", "to", "the", "root", "experiment", "directory", "if", "the", "tracking", "uri", "is", "local", "otherwise", "returns", "none"], "docstring_summary": "The root file directory in which MLflow experiments are saved.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\mlflow.py", "partition": "train", "function_type": "class_method", "class_name": "MLFlowLogger", "start_line": 293, "end_line": 303, "hash": "99fcecb1e0dbdee4379ae3f2517b961f", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "name", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"Get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        return self.experiment_id", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"Get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.\r\n\r\n        \"\"\"\r\n        return self.experiment_id", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Get", "the", "experiment", "id", ".", "Returns", ":", "The", "experiment", "id", ".", "\"", "\"", "\"", "return", "self", ".", "experiment_id"], "docstring": "Get the experiment id.\r\n\r\n        Returns:\r\n            The experiment id.", "docstring_tokens": ["get", "the", "experiment", "id", "returns", "the", "experiment", "id"], "docstring_summary": "Get the experiment id.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\mlflow.py", "partition": "train", "function_type": "class_method", "class_name": "MLFlowLogger", "start_line": 307, "end_line": 314, "hash": "54178963da555b87061a1bed07c99723", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\mlflow.py", "func_name": "version", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        return self.run_id", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Get the run id.\r\n\r\n        Returns:\r\n            The run id.\r\n\r\n        \"\"\"\r\n        return self.run_id", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Get", "the", "run", "id", ".", "Returns", ":", "The", "run", "id", ".", "\"", "\"", "\"", "return", "self", ".", "run_id"], "docstring": "Get the run id.\r\n\r\n        Returns:\r\n            The run id.", "docstring_tokens": ["get", "the", "run", "id", "returns", "the", "run", "id"], "docstring_summary": "Get the run id.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\mlflow.py", "partition": "train", "function_type": "class_method", "class_name": "MLFlowLogger", "start_line": 318, "end_line": 325, "hash": "5fd6567a632df8c3f6f8df2ba1fa3bfc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "_construct_path_with_prefix", "original_string": "def _construct_path_with_prefix(self, *keys: str) -> str:\r\n        \"\"\"Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.\"\"\"\r\n        if self._prefix:\r\n            return self.LOGGER_JOIN_CHAR.join([self._prefix, *keys])\r\n        return self.LOGGER_JOIN_CHAR.join(keys)", "language": "python", "code": "def _construct_path_with_prefix(self, *keys: str) -> str:\r\n        \"\"\"Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.\"\"\"\r\n        if self._prefix:\r\n            return self.LOGGER_JOIN_CHAR.join([self._prefix, *keys])\r\n        return self.LOGGER_JOIN_CHAR.join(keys)", "code_tokens": ["def", "_construct_path_with_prefix", "(", "self", ",", "*", "keys", ":", "str", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Return", "sequence", "of", "keys", "joined", "by", "`", "LOGGER_JOIN_CHAR", "`", ",", "started", "with", "`", "_prefix", "`", "if", "defined", ".", "\"", "\"", "\"", "if", "self", ".", "_prefix", ":", "return", "self", ".", "LOGGER_JOIN_CHAR", ".", "join", "(", "[", "self", ".", "_prefix", ",", "*", "keys", "]", ")", "return", "self", ".", "LOGGER_JOIN_CHAR", ".", "join", "(", "keys", ")"], "docstring": "Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.", "docstring_tokens": ["return", "sequence", "of", "keys", "joined", "by", "logger_join_char", "started", "with", "_prefix", "if", "defined"], "docstring_summary": "Return sequence of keys joined by `LOGGER_JOIN_CHAR`, started with `_prefix` if defined.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 311, "end_line": 315, "hash": "7d032a895628ea5c53d210bff3dd8d59", "complexity": 2, "parameters": ["*keys"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "experiment", "original_string": "def experiment(self) -> \"Run\":\r\n        r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your\r\n        :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Example::\r\n\r\n            class LitModel(LightningModule):\r\n                def training_step(self, batch, batch_idx):\r\n                    # log metrics\r\n                    acc = ...\r\n                    self.logger.experiment[\"train/acc\"].append(acc)\r\n\r\n                    # log images\r\n                    img = ...\r\n                    self.logger.experiment[\"train/misclassified_images\"].append(File.as_image(img))\r\n\r\n        Note that the syntax ``self.logger.experiment[\"your/metadata/structure\"].append(metadata)``\r\n        is specific to Neptune and extends the logger capabilities.\r\n        It lets you log various types of metadata, such as scores, files,\r\n        images, interactive visuals, and CSVs. Refer to the\r\n        `Neptune docs <https://docs.neptune.ai/logging/methods>`_\r\n        for more detailed explanations.\r\n        You can also use the regular logger methods ``log_metrics()``, and ``log_hyperparams()``\r\n        with NeptuneLogger.\r\n\r\n        \"\"\"\r\n        return self.run", "language": "python", "code": "def experiment(self) -> \"Run\":\r\n        r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your\r\n        :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Example::\r\n\r\n            class LitModel(LightningModule):\r\n                def training_step(self, batch, batch_idx):\r\n                    # log metrics\r\n                    acc = ...\r\n                    self.logger.experiment[\"train/acc\"].append(acc)\r\n\r\n                    # log images\r\n                    img = ...\r\n                    self.logger.experiment[\"train/misclassified_images\"].append(File.as_image(img))\r\n\r\n        Note that the syntax ``self.logger.experiment[\"your/metadata/structure\"].append(metadata)``\r\n        is specific to Neptune and extends the logger capabilities.\r\n        It lets you log various types of metadata, such as scores, files,\r\n        images, interactive visuals, and CSVs. Refer to the\r\n        `Neptune docs <https://docs.neptune.ai/logging/methods>`_\r\n        for more detailed explanations.\r\n        You can also use the regular logger methods ``log_metrics()``, and ``log_hyperparams()``\r\n        with NeptuneLogger.\r\n\r\n        \"\"\"\r\n        return self.run", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "\"", "Run", "\"", ":", "r", "\"", "\"", "\"", "Actual", "Neptune", "run", "object", ".", "Allows", "you", "to", "use", "neptune", "logging", "features", "in", "your", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", ".", "Example", ":", ":", "class", "LitModel", "(", "LightningModule", ")", ":", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "acc", "=", ".", ".", ".", "self", ".", "logger", ".", "experiment", "[", "\"", "train", "/", "acc", "\"", "]", ".", "append", "(", "acc", ")", "img", "=", ".", ".", ".", "self", ".", "logger", ".", "experiment", "[", "\"", "train", "/", "misclassified_images", "\"", "]", ".", "append", "(", "File", ".", "as_image", "(", "img", ")", ")", "Note", "that", "the", "syntax", "`", "`", "self", ".", "logger", ".", "experiment", "[", "\"", "your", "/", "metadata", "/", "structure", "\"", "]", ".", "append", "(", "metadata", ")", "`", "`", "is", "specific", "to", "Neptune", "and", "extends", "the", "logger", "capabilities", ".", "It", "lets", "you", "log", "various", "types", "of", "metadata", ",", "such", "as", "scores", ",", "files", ",", "images", ",", "interactive", "visuals", ",", "and", "CSVs", ".", "Refer", "to", "the", "`", "Neptune", "docs", "<", "https", ":", "/", "/", "docs", ".", "neptune", ".", "ai", "/", "logging", "/", "methods", ">", "`", "_", "for", "more", "detailed", "explanations", ".", "You", "can", "also", "use", "the", "regular", "logger", "methods", "`", "`", "log_metrics", "(", ")", "`", "`", ",", "and", "`", "`", "log_hyperparams", "(", ")", "`", "`", "with", "NeptuneLogger", ".", "\"", "\"", "\"", "return", "self", ".", "run"], "docstring": "r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your\r\n        :class:`~lightning.pytorch.core.LightningModule`.\r\n\r\n        Example::\r\n\r\n            class LitModel(LightningModule):\r\n                def training_step(self, batch, batch_idx):\r\n                    # log metrics\r\n                    acc = ...\r\n                    self.logger.experiment[\"train/acc\"].append(acc)\r\n\r\n                    # log images\r\n                    img = ...\r\n                    self.logger.experiment[\"train/misclassified_images\"].append(File.as_image(img))\r\n\r\n        Note that the syntax ``self.logger.experiment[\"your/metadata/structure\"].append(metadata)``\r\n        is specific to Neptune and extends the logger capabilities.\r\n        It lets you log various types of metadata, such as scores, files,\r\n        images, interactive visuals, and CSVs. Refer to the\r\n        `Neptune docs <https://docs.neptune.ai/logging/methods>`_\r\n        for more detailed explanations.\r\n        You can also use the regular logger methods ``log_metrics()``, and ``log_hyperparams()``\r\n        with NeptuneLogger.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "actual", "neptune", "run", "object", "allows", "you", "to", "use", "neptune", "logging", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "example", "class", "litmodel", "lightningmodule", "def", "training_step", "self", "batch", "batch_idx", "log", "metrics", "acc", "self", "logger", "experiment", "train", "acc", "append", "acc", "log", "images", "img", "self", "logger", "experiment", "train", "misclassified_images", "append", "file", "as_image", "img", "note", "that", "the", "syntax", "self", "logger", "experiment", "your", "metadata", "structure", "append", "metadata", "is", "specific", "to", "neptune", "and", "extends", "the", "logger", "capabilities", "it", "lets", "you", "log", "various", "types", "of", "metadata", "such", "as", "scores", "files", "images", "interactive", "visuals", "and", "csvs", "refer", "to", "the", "neptune", "docs", "https", "docs", "neptune", "ai", "logging", "methods", "_", "for", "more", "detailed", "explanations", "you", "can", "also", "use", "the", "regular", "logger", "methods", "log_metrics", "and", "log_hyperparams", "with", "neptunelogger"], "docstring_summary": "r\"\"\"Actual Neptune run object. Allows you to use neptune logging features in your", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 354, "end_line": 380, "hash": "04136e54486fbee55d0089903464979a", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "log_hyperparams", "original_string": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace]) -> None:\r\n        r\"\"\"Log hyperparameters to the run.\r\n\r\n        Hyperparameters will be logged under the \"<prefix>/hyperparams\" namespace.\r\n\r\n        Note:\r\n\r\n            You can also log parameters by directly using the logger instance:\r\n            ``neptune_logger.experiment[\"model/hyper-parameters\"] = params_dict``.\r\n\r\n            In this way you can keep hierarchical structure of the parameters.\r\n\r\n        Args:\r\n            params: `dict`.\r\n                Python dictionary structure with parameters.\r\n\r\n        Example::\r\n\r\n            from lightning.pytorch.loggers import NeptuneLogger\r\n            import neptune\r\n\r\n            PARAMS = {\r\n                \"batch_size\": 64,\r\n                \"lr\": 0.07,\r\n                \"decay_factor\": 0.97,\r\n            }\r\n\r\n            neptune_logger = NeptuneLogger(\r\n                api_key=neptune.ANONYMOUS_API_TOKEN,\r\n                project=\"common/pytorch-lightning-integration\"\r\n            )\r\n\r\n            neptune_logger.log_hyperparams(PARAMS)\r\n\r\n        \"\"\"\r\n        from neptune.utils import stringify_unsupported\r\n\r\n        params = _convert_params(params)\r\n        params = _sanitize_callable_params(params)\r\n\r\n        parameters_key = self.PARAMETERS_KEY\r\n        parameters_key = self._construct_path_with_prefix(parameters_key)\r\n\r\n        self.run[parameters_key] = stringify_unsupported(params)", "language": "python", "code": "def log_hyperparams(self, params: Union[dict[str, Any], Namespace]) -> None:\r\n        r\"\"\"Log hyperparameters to the run.\r\n\r\n        Hyperparameters will be logged under the \"<prefix>/hyperparams\" namespace.\r\n\r\n        Note:\r\n\r\n            You can also log parameters by directly using the logger instance:\r\n            ``neptune_logger.experiment[\"model/hyper-parameters\"] = params_dict``.\r\n\r\n            In this way you can keep hierarchical structure of the parameters.\r\n\r\n        Args:\r\n            params: `dict`.\r\n                Python dictionary structure with parameters.\r\n\r\n        Example::\r\n\r\n            from lightning.pytorch.loggers import NeptuneLogger\r\n            import neptune\r\n\r\n            PARAMS = {\r\n                \"batch_size\": 64,\r\n                \"lr\": 0.07,\r\n                \"decay_factor\": 0.97,\r\n            }\r\n\r\n            neptune_logger = NeptuneLogger(\r\n                api_key=neptune.ANONYMOUS_API_TOKEN,\r\n                project=\"common/pytorch-lightning-integration\"\r\n            )\r\n\r\n            neptune_logger.log_hyperparams(PARAMS)\r\n\r\n        \"\"\"\r\n        from neptune.utils import stringify_unsupported\r\n\r\n        params = _convert_params(params)\r\n        params = _sanitize_callable_params(params)\r\n\r\n        parameters_key = self.PARAMETERS_KEY\r\n        parameters_key = self._construct_path_with_prefix(parameters_key)\r\n\r\n        self.run[parameters_key] = stringify_unsupported(params)", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Log", "hyperparameters", "to", "the", "run", ".", "Hyperparameters", "will", "be", "logged", "under", "the", "\"", "<", "prefix", ">", "/", "hyperparams", "\"", "namespace", ".", "Note", ":", "You", "can", "also", "log", "parameters", "by", "directly", "using", "the", "logger", "instance", ":", "`", "`", "neptune_logger", ".", "experiment", "[", "\"", "model", "/", "hyper", "-", "parameters", "\"", "]", "=", "params_dict", "`", "`", ".", "In", "this", "way", "you", "can", "keep", "hierarchical", "structure", "of", "the", "parameters", ".", "Args", ":", "params", ":", "`", "dict", "`", ".", "Python", "dictionary", "structure", "with", "parameters", ".", "Example", ":", ":", "from", "lightning", ".", "pytorch", ".", "loggers", "import", "NeptuneLogger", "import", "neptune", "PARAMS", "=", "{", "\"", "batch_size", "\"", ":", "64", ",", "\"", "lr", "\"", ":", "0", ".", "07", ",", "\"", "decay_factor", "\"", ":", "0", ".", "97", ",", "}", "neptune_logger", "=", "NeptuneLogger", "(", "api_key", "=", "neptune", ".", "ANONYMOUS_API_TOKEN", ",", "project", "=", "\"", "common", "/", "pytorch", "-", "lightning", "-", "integration", "\"", ")", "neptune_logger", ".", "log_hyperparams", "(", "PARAMS", ")", "\"", "\"", "\"", "from", "neptune", ".", "utils", "import", "stringify_unsupported", "params", "=", "_convert_params", "(", "params", ")", "params", "=", "_sanitize_callable_params", "(", "params", ")", "parameters_key", "=", "self", ".", "PARAMETERS_KEY", "parameters_key", "=", "self", ".", "_construct_path_with_prefix", "(", "parameters_key", ")", "self", ".", "run", "[", "parameters_key", "]", "=", "stringify_unsupported", "(", "params", ")"], "docstring": "r\"\"\"Log hyperparameters to the run.\r\n\r\n        Hyperparameters will be logged under the \"<prefix>/hyperparams\" namespace.\r\n\r\n        Note:\r\n\r\n            You can also log parameters by directly using the logger instance:\r\n            ``neptune_logger.experiment[\"model/hyper-parameters\"] = params_dict``.\r\n\r\n            In this way you can keep hierarchical structure of the parameters.\r\n\r\n        Args:\r\n            params: `dict`.\r\n                Python dictionary structure with parameters.\r\n\r\n        Example::\r\n\r\n            from lightning.pytorch.loggers import NeptuneLogger\r\n            import neptune\r\n\r\n            PARAMS = {\r\n                \"batch_size\": 64,\r\n                \"lr\": 0.07,\r\n                \"decay_factor\": 0.97,\r\n            }\r\n\r\n            neptune_logger = NeptuneLogger(\r\n                api_key=neptune.ANONYMOUS_API_TOKEN,\r\n                project=\"common/pytorch-lightning-integration\"\r\n            )\r\n\r\n            neptune_logger.log_hyperparams(PARAMS)\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "log", "hyperparameters", "to", "the", "run", "hyperparameters", "will", "be", "logged", "under", "the", "prefix", "hyperparams", "namespace", "note", "you", "can", "also", "log", "parameters", "by", "directly", "using", "the", "logger", "instance", "neptune_logger", "experiment", "model", "hyper", "parameters", "params_dict", "in", "this", "way", "you", "can", "keep", "hierarchical", "structure", "of", "the", "parameters", "args", "params", "dict", "python", "dictionary", "structure", "with", "parameters", "example", "from", "lightning", "pytorch", "loggers", "import", "neptunelogger", "import", "neptune", "params", "batch_size", "64", "lr", "0", "07", "decay_factor", "0", "97", "neptune_logger", "neptunelogger", "api_key", "neptune", "anonymous_api_token", "project", "common", "pytorch", "lightning", "integration", "neptune_logger", "log_hyperparams", "params"], "docstring_summary": "r\"\"\"Log hyperparameters to the run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 398, "end_line": 441, "hash": "bd5b335d275f764ff1c9428164cc5edb", "complexity": 1, "parameters": ["params", "Any]", "Namespace]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "log_metrics", "original_string": "def log_metrics(self, metrics: dict[str, Union[Tensor, float]], step: Optional[int] = None) -> None:\r\n        \"\"\"Log metrics (numeric values) in Neptune runs.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values.\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        if rank_zero_only.rank != 0:\r\n            raise ValueError(\"run tried to log from global_rank != 0\")\r\n\r\n        metrics = _add_prefix(metrics, self._prefix, self.LOGGER_JOIN_CHAR)\r\n\r\n        for key, val in metrics.items():\r\n            self.run[key].append(val, step=step)", "language": "python", "code": "def log_metrics(self, metrics: dict[str, Union[Tensor, float]], step: Optional[int] = None) -> None:\r\n        \"\"\"Log metrics (numeric values) in Neptune runs.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values.\r\n            step: Step number at which the metrics should be recorded\r\n\r\n        \"\"\"\r\n        if rank_zero_only.rank != 0:\r\n            raise ValueError(\"run tried to log from global_rank != 0\")\r\n\r\n        metrics = _add_prefix(metrics, self._prefix, self.LOGGER_JOIN_CHAR)\r\n\r\n        for key, val in metrics.items():\r\n            self.run[key].append(val, step=step)", "code_tokens": ["def", "log_metrics", "(", "self", ",", "metrics", ":", "dict", "[", "str", ",", "Union", "[", "Tensor", ",", "float", "]", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "metrics", "(", "numeric", "values", ")", "in", "Neptune", "runs", ".", "Args", ":", "metrics", ":", "Dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", ".", "step", ":", "Step", "number", "at", "which", "the", "metrics", "should", "be", "recorded", "\"", "\"", "\"", "if", "rank_zero_only", ".", "rank", "!", "=", "0", ":", "raise", "ValueError", "(", "\"", "run", "tried", "to", "log", "from", "global_rank", "!", "=", "0", "\"", ")", "metrics", "=", "_add_prefix", "(", "metrics", ",", "self", ".", "_prefix", ",", "self", ".", "LOGGER_JOIN_CHAR", ")", "for", "key", ",", "val", "in", "metrics", ".", "items", "(", ")", ":", "self", ".", "run", "[", "key", "]", ".", "append", "(", "val", ",", "step", "=", "step", ")"], "docstring": "Log metrics (numeric values) in Neptune runs.\r\n\r\n        Args:\r\n            metrics: Dictionary with metric names as keys and measured quantities as values.\r\n            step: Step number at which the metrics should be recorded", "docstring_tokens": ["log", "metrics", "numeric", "values", "in", "neptune", "runs", "args", "metrics", "dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", "step", "number", "at", "which", "the", "metrics", "should", "be", "recorded"], "docstring_summary": "Log metrics (numeric values) in Neptune runs.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 446, "end_line": 460, "hash": "8c2aba0ee6b92302477b52bc6acd796b", "complexity": 3, "parameters": ["metrics", "Union[Tensor", "float]]", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save\r\n        locally.\r\n\r\n        Returns:\r\n            the root directory where experiment logs get saved\r\n\r\n        \"\"\"\r\n        return os.path.join(os.getcwd(), \".neptune\")", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save\r\n        locally.\r\n\r\n        Returns:\r\n            the root directory where experiment logs get saved\r\n\r\n        \"\"\"\r\n        return os.path.join(os.getcwd(), \".neptune\")", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "save", "directory", "of", "the", "experiment", "which", "in", "this", "case", "is", "`", "`", "None", "`", "`", "because", "Neptune", "does", "not", "save", "locally", ".", "Returns", ":", "the", "root", "directory", "where", "experiment", "logs", "get", "saved", "\"", "\"", "\"", "return", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "\"", ".", "neptune", "\"", ")"], "docstring": "Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save\r\n        locally.\r\n\r\n        Returns:\r\n            the root directory where experiment logs get saved", "docstring_tokens": ["gets", "the", "save", "directory", "of", "the", "experiment", "which", "in", "this", "case", "is", "none", "because", "neptune", "does", "not", "save", "locally", "returns", "the", "root", "directory", "where", "experiment", "logs", "get", "saved"], "docstring_summary": "Gets the save directory of the experiment which in this case is ``None`` because Neptune does not save", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 477, "end_line": 485, "hash": "e3b0976c511ea0ccae53c2591c7bb2fc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "after_save_checkpoint", "original_string": "def after_save_checkpoint(self, checkpoint_callback: Checkpoint) -> None:\r\n        \"\"\"Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        if not self._log_model_checkpoints:\r\n            return\r\n\r\n        file_names = set()\r\n        checkpoints_namespace = self._construct_path_with_prefix(\"model/checkpoints\")\r\n\r\n        # save last model\r\n        if hasattr(checkpoint_callback, \"last_model_path\") and checkpoint_callback.last_model_path:\r\n            model_last_name = self._get_full_model_name(checkpoint_callback.last_model_path, checkpoint_callback)\r\n            file_names.add(model_last_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_last_name}\"].upload(checkpoint_callback.last_model_path)\r\n\r\n        # save best k models\r\n        if hasattr(checkpoint_callback, \"best_k_models\"):\r\n            for key in checkpoint_callback.best_k_models:\r\n                model_name = self._get_full_model_name(key, checkpoint_callback)\r\n                file_names.add(model_name)\r\n                self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(key)\r\n\r\n        # log best model path and checkpoint\r\n        if hasattr(checkpoint_callback, \"best_model_path\") and checkpoint_callback.best_model_path:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_path\")] = checkpoint_callback.best_model_path\r\n\r\n            model_name = self._get_full_model_name(checkpoint_callback.best_model_path, checkpoint_callback)\r\n            file_names.add(model_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(checkpoint_callback.best_model_path)\r\n\r\n        # remove old models logged to experiment if they are not part of best k models at this point\r\n        if self.run.exists(checkpoints_namespace):\r\n            exp_structure = self.run.get_structure()\r\n            uploaded_model_names = self._get_full_model_names_from_exp_structure(exp_structure, checkpoints_namespace)\r\n\r\n            for file_to_drop in list(uploaded_model_names - file_names):\r\n                del self.run[f\"{checkpoints_namespace}/{file_to_drop}\"]\r\n\r\n        # log best model score\r\n        if hasattr(checkpoint_callback, \"best_model_score\") and checkpoint_callback.best_model_score:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_score\")] = (\r\n                checkpoint_callback.best_model_score.cpu().detach().numpy()\r\n            )", "language": "python", "code": "def after_save_checkpoint(self, checkpoint_callback: Checkpoint) -> None:\r\n        \"\"\"Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        if not self._log_model_checkpoints:\r\n            return\r\n\r\n        file_names = set()\r\n        checkpoints_namespace = self._construct_path_with_prefix(\"model/checkpoints\")\r\n\r\n        # save last model\r\n        if hasattr(checkpoint_callback, \"last_model_path\") and checkpoint_callback.last_model_path:\r\n            model_last_name = self._get_full_model_name(checkpoint_callback.last_model_path, checkpoint_callback)\r\n            file_names.add(model_last_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_last_name}\"].upload(checkpoint_callback.last_model_path)\r\n\r\n        # save best k models\r\n        if hasattr(checkpoint_callback, \"best_k_models\"):\r\n            for key in checkpoint_callback.best_k_models:\r\n                model_name = self._get_full_model_name(key, checkpoint_callback)\r\n                file_names.add(model_name)\r\n                self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(key)\r\n\r\n        # log best model path and checkpoint\r\n        if hasattr(checkpoint_callback, \"best_model_path\") and checkpoint_callback.best_model_path:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_path\")] = checkpoint_callback.best_model_path\r\n\r\n            model_name = self._get_full_model_name(checkpoint_callback.best_model_path, checkpoint_callback)\r\n            file_names.add(model_name)\r\n            self.run[f\"{checkpoints_namespace}/{model_name}\"].upload(checkpoint_callback.best_model_path)\r\n\r\n        # remove old models logged to experiment if they are not part of best k models at this point\r\n        if self.run.exists(checkpoints_namespace):\r\n            exp_structure = self.run.get_structure()\r\n            uploaded_model_names = self._get_full_model_names_from_exp_structure(exp_structure, checkpoints_namespace)\r\n\r\n            for file_to_drop in list(uploaded_model_names - file_names):\r\n                del self.run[f\"{checkpoints_namespace}/{file_to_drop}\"]\r\n\r\n        # log best model score\r\n        if hasattr(checkpoint_callback, \"best_model_score\") and checkpoint_callback.best_model_score:\r\n            self.run[self._construct_path_with_prefix(\"model/best_model_score\")] = (\r\n                checkpoint_callback.best_model_score.cpu().detach().numpy()\r\n            )", "code_tokens": ["def", "after_save_checkpoint", "(", "self", ",", "checkpoint_callback", ":", "Checkpoint", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Automatically", "log", "checkpointed", "model", ".", "Called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint", ".", "Args", ":", "checkpoint_callback", ":", "the", "model", "checkpoint", "callback", "instance", "\"", "\"", "\"", "if", "not", "self", ".", "_log_model_checkpoints", ":", "return", "file_names", "=", "set", "(", ")", "checkpoints_namespace", "=", "self", ".", "_construct_path_with_prefix", "(", "\"", "model", "/", "checkpoints", "\"", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "last_model_path", "\"", ")", "and", "checkpoint_callback", ".", "last_model_path", ":", "model_last_name", "=", "self", ".", "_get_full_model_name", "(", "checkpoint_callback", ".", "last_model_path", ",", "checkpoint_callback", ")", "file_names", ".", "add", "(", "model_last_name", ")", "self", ".", "run", "[", "f", "\"", "{", "checkpoints_namespace", "}", "/", "{", "model_last_name", "}", "\"", "]", ".", "upload", "(", "checkpoint_callback", ".", "last_model_path", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "best_k_models", "\"", ")", ":", "for", "key", "in", "checkpoint_callback", ".", "best_k_models", ":", "model_name", "=", "self", ".", "_get_full_model_name", "(", "key", ",", "checkpoint_callback", ")", "file_names", ".", "add", "(", "model_name", ")", "self", ".", "run", "[", "f", "\"", "{", "checkpoints_namespace", "}", "/", "{", "model_name", "}", "\"", "]", ".", "upload", "(", "key", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "best_model_path", "\"", ")", "and", "checkpoint_callback", ".", "best_model_path", ":", "self", ".", "run", "[", "self", ".", "_construct_path_with_prefix", "(", "\"", "model", "/", "best_model_path", "\"", ")", "]", "=", "checkpoint_callback", ".", "best_model_path", "model_name", "=", "self", ".", "_get_full_model_name", "(", "checkpoint_callback", ".", "best_model_path", ",", "checkpoint_callback", ")", "file_names", ".", "add", "(", "model_name", ")", "self", ".", "run", "[", "f", "\"", "{", "checkpoints_namespace", "}", "/", "{", "model_name", "}", "\"", "]", ".", "upload", "(", "checkpoint_callback", ".", "best_model_path", ")", "if", "self", ".", "run", ".", "exists", "(", "checkpoints_namespace", ")", ":", "exp_structure", "=", "self", ".", "run", ".", "get_structure", "(", ")", "uploaded_model_names", "=", "self", ".", "_get_full_model_names_from_exp_structure", "(", "exp_structure", ",", "checkpoints_namespace", ")", "for", "file_to_drop", "in", "list", "(", "uploaded_model_names", "-", "file_names", ")", ":", "del", "self", ".", "run", "[", "f", "\"", "{", "checkpoints_namespace", "}", "/", "{", "file_to_drop", "}", "\"", "]", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "best_model_score", "\"", ")", "and", "checkpoint_callback", ".", "best_model_score", ":", "self", ".", "run", "[", "self", ".", "_construct_path_with_prefix", "(", "\"", "model", "/", "best_model_score", "\"", ")", "]", "=", "(", "checkpoint_callback", ".", "best_model_score", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")"], "docstring": "Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance", "docstring_tokens": ["automatically", "log", "checkpointed", "model", "called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint", "args", "checkpoint_callback", "the", "model", "checkpoint", "callback", "instance"], "docstring_summary": "Automatically log checkpointed model. Called after model checkpoint callback saves a new checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 500, "end_line": 546, "hash": "7663ffc32f190814777f3cd2d50537f8", "complexity": 12, "parameters": ["checkpoint_callback"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "_get_full_model_name", "original_string": "def _get_full_model_name(model_path: str, checkpoint_callback: Checkpoint) -> str:\r\n        \"\"\"Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.\"\"\"\r\n        if hasattr(checkpoint_callback, \"dirpath\"):\r\n            model_path = os.path.normpath(model_path)\r\n            expected_model_path = os.path.normpath(checkpoint_callback.dirpath)\r\n            if not model_path.startswith(expected_model_path):\r\n                raise ValueError(f\"{model_path} was expected to start with {expected_model_path}.\")\r\n            # Remove extension from filepath\r\n            filepath, _ = os.path.splitext(model_path[len(expected_model_path) + 1 :])\r\n            return filepath.replace(os.sep, \"/\")\r\n        return model_path.replace(os.sep, \"/\")", "language": "python", "code": "def _get_full_model_name(model_path: str, checkpoint_callback: Checkpoint) -> str:\r\n        \"\"\"Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.\"\"\"\r\n        if hasattr(checkpoint_callback, \"dirpath\"):\r\n            model_path = os.path.normpath(model_path)\r\n            expected_model_path = os.path.normpath(checkpoint_callback.dirpath)\r\n            if not model_path.startswith(expected_model_path):\r\n                raise ValueError(f\"{model_path} was expected to start with {expected_model_path}.\")\r\n            # Remove extension from filepath\r\n            filepath, _ = os.path.splitext(model_path[len(expected_model_path) + 1 :])\r\n            return filepath.replace(os.sep, \"/\")\r\n        return model_path.replace(os.sep, \"/\")", "code_tokens": ["def", "_get_full_model_name", "(", "model_path", ":", "str", ",", "checkpoint_callback", ":", "Checkpoint", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Returns", "model", "name", "which", "is", "string", "`", "model_path", "`", "appended", "to", "`", "checkpoint_callback", ".", "dirpath", "`", ".", "\"", "\"", "\"", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "dirpath", "\"", ")", ":", "model_path", "=", "os", ".", "path", ".", "normpath", "(", "model_path", ")", "expected_model_path", "=", "os", ".", "path", ".", "normpath", "(", "checkpoint_callback", ".", "dirpath", ")", "if", "not", "model_path", ".", "startswith", "(", "expected_model_path", ")", ":", "raise", "ValueError", "(", "f", "\"", "{", "model_path", "}", "was", "expected", "to", "start", "with", "{", "expected_model_path", "}", ".", "\"", ")", "filepath", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "model_path", "[", "len", "(", "expected_model_path", ")", "+", "1", ":", "]", ")", "return", "filepath", ".", "replace", "(", "os", ".", "sep", ",", "\"", "/", "\"", ")", "return", "model_path", ".", "replace", "(", "os", ".", "sep", ",", "\"", "/", "\"", ")"], "docstring": "Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.", "docstring_tokens": ["returns", "model", "name", "which", "is", "string", "model_path", "appended", "to", "checkpoint_callback", "dirpath"], "docstring_summary": "Returns model name which is string `model_path` appended to `checkpoint_callback.dirpath`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 549, "end_line": 559, "hash": "5d0ed0d7c1691d3a912afdbb7d419776", "complexity": 3, "parameters": ["model_path", "checkpoint_callback"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "_get_full_model_names_from_exp_structure", "original_string": "def _get_full_model_names_from_exp_structure(cls, exp_structure: dict[str, Any], namespace: str) -> set[str]:\r\n        \"\"\"Returns all paths to properties which were already logged in `namespace`\"\"\"\r\n        structure_keys: list[str] = namespace.split(cls.LOGGER_JOIN_CHAR)\r\n        for key in structure_keys:\r\n            exp_structure = exp_structure[key]\r\n        uploaded_models_dict = exp_structure\r\n        return set(cls._dict_paths(uploaded_models_dict))", "language": "python", "code": "def _get_full_model_names_from_exp_structure(cls, exp_structure: dict[str, Any], namespace: str) -> set[str]:\r\n        \"\"\"Returns all paths to properties which were already logged in `namespace`\"\"\"\r\n        structure_keys: list[str] = namespace.split(cls.LOGGER_JOIN_CHAR)\r\n        for key in structure_keys:\r\n            exp_structure = exp_structure[key]\r\n        uploaded_models_dict = exp_structure\r\n        return set(cls._dict_paths(uploaded_models_dict))", "code_tokens": ["def", "_get_full_model_names_from_exp_structure", "(", "cls", ",", "exp_structure", ":", "dict", "[", "str", ",", "Any", "]", ",", "namespace", ":", "str", ")", "-", ">", "set", "[", "str", "]", ":", "\"", "\"", "\"", "Returns", "all", "paths", "to", "properties", "which", "were", "already", "logged", "in", "`", "namespace", "`", "\"", "\"", "\"", "structure_keys", ":", "list", "[", "str", "]", "=", "namespace", ".", "split", "(", "cls", ".", "LOGGER_JOIN_CHAR", ")", "for", "key", "in", "structure_keys", ":", "exp_structure", "=", "exp_structure", "[", "key", "]", "uploaded_models_dict", "=", "exp_structure", "return", "set", "(", "cls", ".", "_dict_paths", "(", "uploaded_models_dict", ")", ")"], "docstring": "Returns all paths to properties which were already logged in `namespace`", "docstring_tokens": ["returns", "all", "paths", "to", "properties", "which", "were", "already", "logged", "in", "namespace"], "docstring_summary": "Returns all paths to properties which were already logged in `namespace`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 562, "end_line": 568, "hash": "6c3ecdf76e02c329980492f62b79b226", "complexity": 2, "parameters": ["exp_structure", "Any]", "namespace"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\neptune.py", "func_name": "version", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Return the experiment version.\r\n\r\n        It's Neptune Run's short_id\r\n\r\n        \"\"\"\r\n        return self._run_short_id", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Return the experiment version.\r\n\r\n        It's Neptune Run's short_id\r\n\r\n        \"\"\"\r\n        return self._run_short_id", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Return", "the", "experiment", "version", ".", "It", "'", "s", "Neptune", "Run", "'", "s", "short_id", "\"", "\"", "\"", "return", "self", ".", "_run_short_id"], "docstring": "Return the experiment version.\r\n\r\n        It's Neptune Run's short_id", "docstring_tokens": ["return", "the", "experiment", "version", "it", "s", "neptune", "run", "s", "short_id"], "docstring_summary": "Return the experiment version.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\neptune.py", "partition": "train", "function_type": "class_method", "class_name": "NeptuneLogger", "start_line": 587, "end_line": 593, "hash": "b5e1b986327a7ff59a7713171eef2410", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "root_dir", "original_string": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all tensorboard checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(super().root_dir, self.name)", "language": "python", "code": "def root_dir(self) -> str:\r\n        \"\"\"Parent directory for all tensorboard checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"\r\n\r\n        \"\"\"\r\n        return os.path.join(super().root_dir, self.name)", "code_tokens": ["def", "root_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Parent", "directory", "for", "all", "tensorboard", "checkpoint", "subdirectories", ".", "If", "the", "experiment", "name", "parameter", "is", "an", "empty", "string", ",", "no", "experiment", "subdirectory", "is", "used", "and", "the", "checkpoint", "will", "be", "saved", "in", "\"", "save_dir", "/", "version", "\"", "\"", "\"", "\"", "return", "os", ".", "path", ".", "join", "(", "super", "(", ")", ".", "root_dir", ",", "self", ".", "name", ")"], "docstring": "Parent directory for all tensorboard checkpoint subdirectories.\r\n\r\n        If the experiment name parameter is an empty string, no experiment subdirectory is used and the checkpoint will\r\n        be saved in \"save_dir/version\"", "docstring_tokens": ["parent", "directory", "for", "all", "tensorboard", "checkpoint", "subdirectories", "if", "the", "experiment", "name", "parameter", "is", "an", "empty", "string", "no", "experiment", "subdirectory", "is", "used", "and", "the", "checkpoint", "will", "be", "saved", "in", "save_dir", "version"], "docstring_summary": "Parent directory for all tensorboard checkpoint subdirectories.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 114, "end_line": 121, "hash": "d56e8fba5e9d775d54af4ff0d6a5759b", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "log_dir", "original_string": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path ala test-tube\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "language": "python", "code": "def log_dir(self) -> str:\r\n        \"\"\"The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.\r\n\r\n        \"\"\"\r\n        # create a pseudo standard path ala test-tube\r\n        version = self.version if isinstance(self.version, str) else f\"version_{self.version}\"\r\n        log_dir = os.path.join(self.root_dir, version)\r\n        if isinstance(self.sub_dir, str):\r\n            log_dir = os.path.join(log_dir, self.sub_dir)\r\n        log_dir = os.path.expandvars(log_dir)\r\n        log_dir = os.path.expanduser(log_dir)\r\n        return log_dir", "code_tokens": ["def", "log_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "The", "directory", "for", "this", "run", "'", "s", "tensorboard", "checkpoint", ".", "By", "default", ",", "it", "is", "named", "`", "`", "'", "version_", "$", "{", "self", ".", "version", "}", "'", "`", "`", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "'", "s", "version", "parameter", "instead", "of", "`", "`", "None", "`", "`", "or", "an", "int", ".", "\"", "\"", "\"", "version", "=", "self", ".", "version", "if", "isinstance", "(", "self", ".", "version", ",", "str", ")", "else", "f", "\"", "version_", "{", "self", ".", "version", "}", "\"", "log_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "version", ")", "if", "isinstance", "(", "self", ".", "sub_dir", ",", "str", ")", ":", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "self", ".", "sub_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expandvars", "(", "log_dir", ")", "log_dir", "=", "os", ".", "path", ".", "expanduser", "(", "log_dir", ")", "return", "log_dir"], "docstring": "The directory for this run's tensorboard checkpoint.\r\n\r\n        By default, it is named ``'version_${self.version}'`` but it can be overridden by passing a string value for the\r\n        constructor's version parameter instead of ``None`` or an int.", "docstring_tokens": ["the", "directory", "for", "this", "run", "s", "tensorboard", "checkpoint", "by", "default", "it", "is", "named", "version_", "self", "version", "but", "it", "can", "be", "overridden", "by", "passing", "a", "string", "value", "for", "the", "constructor", "s", "version", "parameter", "instead", "of", "none", "or", "an", "int"], "docstring_summary": "The directory for this run's tensorboard checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 125, "end_line": 139, "hash": "64073fcd660ecb26dabd9448ca4742df", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "language": "python", "code": "def save_dir(self) -> str:\r\n        \"\"\"Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.\r\n\r\n        \"\"\"\r\n        return self._root_dir", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Gets", "the", "save", "directory", "where", "the", "TensorBoard", "experiments", "are", "saved", ".", "Returns", ":", "The", "local", "path", "to", "the", "save", "directory", "where", "the", "TensorBoard", "experiments", "are", "saved", ".", "\"", "\"", "\"", "return", "self", ".", "_root_dir"], "docstring": "Gets the save directory where the TensorBoard experiments are saved.\r\n\r\n        Returns:\r\n            The local path to the save directory where the TensorBoard experiments are saved.", "docstring_tokens": ["gets", "the", "save", "directory", "where", "the", "tensorboard", "experiments", "are", "saved", "returns", "the", "local", "path", "to", "the", "save", "directory", "where", "the", "tensorboard", "experiments", "are", "saved"], "docstring_summary": "Gets the save directory where the TensorBoard experiments are saved.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 143, "end_line": 150, "hash": "12a511dcb5250aed50f3fcf4c0b79c75", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "log_hyperparams", "original_string": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container, OmegaConf\r\n\r\n        params = _convert_params(params)\r\n\r\n        # store params to output\r\n        if _OMEGACONF_AVAILABLE and isinstance(params, Container):\r\n            self.hparams = OmegaConf.merge(self.hparams, params)\r\n        else:\r\n            self.hparams.update(params)\r\n\r\n        return super().log_hyperparams(params=params, metrics=metrics, step=step)", "language": "python", "code": "def log_hyperparams(\r\n        self,\r\n        params: Union[dict[str, Any], Namespace],\r\n        metrics: Optional[dict[str, Any]] = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics\r\n\r\n        \"\"\"\r\n        if _OMEGACONF_AVAILABLE:\r\n            from omegaconf import Container, OmegaConf\r\n\r\n        params = _convert_params(params)\r\n\r\n        # store params to output\r\n        if _OMEGACONF_AVAILABLE and isinstance(params, Container):\r\n            self.hparams = OmegaConf.merge(self.hparams, params)\r\n        else:\r\n            self.hparams.update(params)\r\n\r\n        return super().log_hyperparams(params=params, metrics=metrics, step=step)", "code_tokens": ["def", "log_hyperparams", "(", "self", ",", "params", ":", "Union", "[", "dict", "[", "str", ",", "Any", "]", ",", "Namespace", "]", ",", "metrics", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Record", "hyperparameters", ".", "TensorBoard", "logs", "with", "and", "without", "saved", "hyperparameters", "are", "incompatible", ",", "the", "hyperparameters", "are", "then", "not", "displayed", "in", "the", "TensorBoard", ".", "Please", "delete", "or", "move", "the", "previously", "saved", "logs", "to", "display", "the", "new", "ones", "with", "hyperparameters", ".", "Args", ":", "params", ":", "A", "dictionary", "-", "like", "container", "with", "the", "hyperparameters", "metrics", ":", "Dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", ":", "Optional", "global", "step", "number", "for", "the", "logged", "metrics", "\"", "\"", "\"", "if", "_OMEGACONF_AVAILABLE", ":", "from", "omegaconf", "import", "Container", ",", "OmegaConf", "params", "=", "_convert_params", "(", "params", ")", "if", "_OMEGACONF_AVAILABLE", "and", "isinstance", "(", "params", ",", "Container", ")", ":", "self", ".", "hparams", "=", "OmegaConf", ".", "merge", "(", "self", ".", "hparams", ",", "params", ")", "else", ":", "self", ".", "hparams", ".", "update", "(", "params", ")", "return", "super", "(", ")", ".", "log_hyperparams", "(", "params", "=", "params", ",", "metrics", "=", "metrics", ",", "step", "=", "step", ")"], "docstring": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the\r\n        hyperparameters are then not displayed in the TensorBoard. Please delete or move the previously saved logs to\r\n        display the new ones with hyperparameters.\r\n\r\n        Args:\r\n            params: A dictionary-like container with the hyperparameters\r\n            metrics: Dictionary with metric names as keys and measured quantities as values\r\n            step: Optional global step number for the logged metrics", "docstring_tokens": ["record", "hyperparameters", "tensorboard", "logs", "with", "and", "without", "saved", "hyperparameters", "are", "incompatible", "the", "hyperparameters", "are", "then", "not", "displayed", "in", "the", "tensorboard", "please", "delete", "or", "move", "the", "previously", "saved", "logs", "to", "display", "the", "new", "ones", "with", "hyperparameters", "args", "params", "a", "dictionary", "like", "container", "with", "the", "hyperparameters", "metrics", "dictionary", "with", "metric", "names", "as", "keys", "and", "measured", "quantities", "as", "values", "step", "optional", "global", "step", "number", "for", "the", "logged", "metrics"], "docstring_summary": "Record hyperparameters. TensorBoard logs with and without saved hyperparameters are incompatible, the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 154, "end_line": 181, "hash": "3d6f90cce40b5867f3f33506624c060b", "complexity": 4, "parameters": ["params", "Any]", "Namespace]", "metrics", "Any]]", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\tensorboard.py", "func_name": "after_save_checkpoint", "original_string": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "language": "python", "code": "def after_save_checkpoint(self, checkpoint_callback: ModelCheckpoint) -> None:\r\n        \"\"\"Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance\r\n\r\n        \"\"\"\r\n        pass", "code_tokens": ["def", "after_save_checkpoint", "(", "self", ",", "checkpoint_callback", ":", "ModelCheckpoint", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint", ".", "Args", ":", "checkpoint_callback", ":", "the", "model", "checkpoint", "callback", "instance", "\"", "\"", "\"", "pass"], "docstring": "Called after model checkpoint callback saves a new checkpoint.\r\n\r\n        Args:\r\n            checkpoint_callback: the model checkpoint callback instance", "docstring_tokens": ["called", "after", "model", "checkpoint", "callback", "saves", "a", "new", "checkpoint", "args", "checkpoint_callback", "the", "model", "checkpoint", "callback", "instance"], "docstring_summary": "Called after model checkpoint callback saves a new checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\tensorboard.py", "partition": "train", "function_type": "class_method", "class_name": "TensorBoardLogger", "start_line": 232, "end_line": 239, "hash": "0129a392fa7d3f9512dd1c726945396c", "complexity": 1, "parameters": ["checkpoint_callback"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\utilities.py", "func_name": "_scan_checkpoints", "original_string": "def _scan_checkpoints(checkpoint_callback: Checkpoint, logged_model_time: dict) -> list[tuple[float, str, float, str]]:\r\n    \"\"\"Return the checkpoints to be logged.\r\n\r\n    Args:\r\n        checkpoint_callback: Checkpoint callback reference.\r\n        logged_model_time: dictionary containing the logged model times.\r\n\r\n    \"\"\"\r\n    # get checkpoints to be saved with associated score\r\n    checkpoints = {}\r\n    if hasattr(checkpoint_callback, \"last_model_path\") and hasattr(checkpoint_callback, \"current_score\"):\r\n        checkpoints[checkpoint_callback.last_model_path] = (checkpoint_callback.current_score, \"latest\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_model_path\") and hasattr(checkpoint_callback, \"best_model_score\"):\r\n        checkpoints[checkpoint_callback.best_model_path] = (checkpoint_callback.best_model_score, \"best\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_k_models\"):\r\n        for key, value in checkpoint_callback.best_k_models.items():\r\n            checkpoints[key] = (value, \"best_k\")\r\n\r\n    checkpoints = sorted(\r\n        (Path(p).stat().st_mtime, p, s, tag) for p, (s, tag) in checkpoints.items() if Path(p).is_file()\r\n    )\r\n    checkpoints = [c for c in checkpoints if c[1] not in logged_model_time or logged_model_time[c[1]] < c[0]]\r\n    return checkpoints", "language": "python", "code": "def _scan_checkpoints(checkpoint_callback: Checkpoint, logged_model_time: dict) -> list[tuple[float, str, float, str]]:\r\n    \"\"\"Return the checkpoints to be logged.\r\n\r\n    Args:\r\n        checkpoint_callback: Checkpoint callback reference.\r\n        logged_model_time: dictionary containing the logged model times.\r\n\r\n    \"\"\"\r\n    # get checkpoints to be saved with associated score\r\n    checkpoints = {}\r\n    if hasattr(checkpoint_callback, \"last_model_path\") and hasattr(checkpoint_callback, \"current_score\"):\r\n        checkpoints[checkpoint_callback.last_model_path] = (checkpoint_callback.current_score, \"latest\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_model_path\") and hasattr(checkpoint_callback, \"best_model_score\"):\r\n        checkpoints[checkpoint_callback.best_model_path] = (checkpoint_callback.best_model_score, \"best\")\r\n\r\n    if hasattr(checkpoint_callback, \"best_k_models\"):\r\n        for key, value in checkpoint_callback.best_k_models.items():\r\n            checkpoints[key] = (value, \"best_k\")\r\n\r\n    checkpoints = sorted(\r\n        (Path(p).stat().st_mtime, p, s, tag) for p, (s, tag) in checkpoints.items() if Path(p).is_file()\r\n    )\r\n    checkpoints = [c for c in checkpoints if c[1] not in logged_model_time or logged_model_time[c[1]] < c[0]]\r\n    return checkpoints", "code_tokens": ["def", "_scan_checkpoints", "(", "checkpoint_callback", ":", "Checkpoint", ",", "logged_model_time", ":", "dict", ")", "-", ">", "list", "[", "tuple", "[", "float", ",", "str", ",", "float", ",", "str", "]", "]", ":", "\"", "\"", "\"", "Return", "the", "checkpoints", "to", "be", "logged", ".", "Args", ":", "checkpoint_callback", ":", "Checkpoint", "callback", "reference", ".", "logged_model_time", ":", "dictionary", "containing", "the", "logged", "model", "times", ".", "\"", "\"", "\"", "checkpoints", "=", "{", "}", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "last_model_path", "\"", ")", "and", "hasattr", "(", "checkpoint_callback", ",", "\"", "current_score", "\"", ")", ":", "checkpoints", "[", "checkpoint_callback", ".", "last_model_path", "]", "=", "(", "checkpoint_callback", ".", "current_score", ",", "\"", "latest", "\"", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "best_model_path", "\"", ")", "and", "hasattr", "(", "checkpoint_callback", ",", "\"", "best_model_score", "\"", ")", ":", "checkpoints", "[", "checkpoint_callback", ".", "best_model_path", "]", "=", "(", "checkpoint_callback", ".", "best_model_score", ",", "\"", "best", "\"", ")", "if", "hasattr", "(", "checkpoint_callback", ",", "\"", "best_k_models", "\"", ")", ":", "for", "key", ",", "value", "in", "checkpoint_callback", ".", "best_k_models", ".", "items", "(", ")", ":", "checkpoints", "[", "key", "]", "=", "(", "value", ",", "\"", "best_k", "\"", ")", "checkpoints", "=", "sorted", "(", "(", "Path", "(", "p", ")", ".", "stat", "(", ")", ".", "st_mtime", ",", "p", ",", "s", ",", "tag", ")", "for", "p", ",", "(", "s", ",", "tag", ")", "in", "checkpoints", ".", "items", "(", ")", "if", "Path", "(", "p", ")", ".", "is_file", "(", ")", ")", "checkpoints", "=", "[", "c", "for", "c", "in", "checkpoints", "if", "c", "[", "1", "]", "not", "in", "logged_model_time", "or", "logged_model_time", "[", "c", "[", "1", "]", "]", "<", "c", "[", "0", "]", "]", "return", "checkpoints"], "docstring": "Return the checkpoints to be logged.\r\n\r\n    Args:\r\n        checkpoint_callback: Checkpoint callback reference.\r\n        logged_model_time: dictionary containing the logged model times.", "docstring_tokens": ["return", "the", "checkpoints", "to", "be", "logged", "args", "checkpoint_callback", "checkpoint", "callback", "reference", "logged_model_time", "dictionary", "containing", "the", "logged", "model", "times"], "docstring_summary": "Return the checkpoints to be logged.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\utilities.py", "partition": "train", "function_type": "function", "start_line": 31, "end_line": 55, "hash": "7c8cfc7505d86037e61512524d525a05", "complexity": 12, "parameters": ["checkpoint_callback", "logged_model_time"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "experiment", "original_string": "def experiment(self) -> Union[\"Run\", \"RunDisabled\"]:\r\n        r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n        .. code-block:: python\r\n\r\n            self.logger.experiment.some_wandb_function()\r\n\r\n        \"\"\"\r\n        import wandb\r\n        from wandb.sdk.lib import RunDisabled\r\n        from wandb.wandb_run import Run\r\n\r\n        if self._experiment is None:\r\n            if self._offline:\r\n                os.environ[\"WANDB_MODE\"] = \"dryrun\"\r\n\r\n            attach_id = getattr(self, \"_attach_id\", None)\r\n            if wandb.run is not None:\r\n                # wandb process already created in this instance\r\n                rank_zero_warn(\r\n                    \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\r\n                    \" this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\"\r\n                )\r\n                self._experiment = wandb.run\r\n            elif attach_id is not None and hasattr(wandb, \"_attach\"):\r\n                # attach to wandb process referenced\r\n                self._experiment = wandb._attach(attach_id)\r\n            else:\r\n                # create new wandb process\r\n                self._experiment = wandb.init(**self._wandb_init)\r\n\r\n                # define default x-axis\r\n                if isinstance(self._experiment, (Run, RunDisabled)) and getattr(\r\n                    self._experiment, \"define_metric\", None\r\n                ):\r\n                    if self._wandb_init.get(\"sync_tensorboard\"):\r\n                        self._experiment.define_metric(\"*\", step_metric=\"global_step\")\r\n                    else:\r\n                        self._experiment.define_metric(\"trainer/global_step\")\r\n                        self._experiment.define_metric(\"*\", step_metric=\"trainer/global_step\", step_sync=True)\r\n\r\n        return self._experiment", "language": "python", "code": "def experiment(self) -> Union[\"Run\", \"RunDisabled\"]:\r\n        r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n        .. code-block:: python\r\n\r\n            self.logger.experiment.some_wandb_function()\r\n\r\n        \"\"\"\r\n        import wandb\r\n        from wandb.sdk.lib import RunDisabled\r\n        from wandb.wandb_run import Run\r\n\r\n        if self._experiment is None:\r\n            if self._offline:\r\n                os.environ[\"WANDB_MODE\"] = \"dryrun\"\r\n\r\n            attach_id = getattr(self, \"_attach_id\", None)\r\n            if wandb.run is not None:\r\n                # wandb process already created in this instance\r\n                rank_zero_warn(\r\n                    \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\r\n                    \" this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\"\r\n                )\r\n                self._experiment = wandb.run\r\n            elif attach_id is not None and hasattr(wandb, \"_attach\"):\r\n                # attach to wandb process referenced\r\n                self._experiment = wandb._attach(attach_id)\r\n            else:\r\n                # create new wandb process\r\n                self._experiment = wandb.init(**self._wandb_init)\r\n\r\n                # define default x-axis\r\n                if isinstance(self._experiment, (Run, RunDisabled)) and getattr(\r\n                    self._experiment, \"define_metric\", None\r\n                ):\r\n                    if self._wandb_init.get(\"sync_tensorboard\"):\r\n                        self._experiment.define_metric(\"*\", step_metric=\"global_step\")\r\n                    else:\r\n                        self._experiment.define_metric(\"trainer/global_step\")\r\n                        self._experiment.define_metric(\"*\", step_metric=\"trainer/global_step\", step_sync=True)\r\n\r\n        return self._experiment", "code_tokens": ["def", "experiment", "(", "self", ")", "-", ">", "Union", "[", "\"", "Run", "\"", ",", "\"", "RunDisabled", "\"", "]", ":", "r", "\"", "\"", "\"", "Actual", "wandb", "object", ".", "To", "use", "wandb", "features", "in", "your", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "core", ".", "LightningModule", "`", "do", "the", "following", ".", "Example", ":", ":", ".", ".", "code", "-", "block", ":", ":", "python", "self", ".", "logger", ".", "experiment", ".", "some_wandb_function", "(", ")", "\"", "\"", "\"", "import", "wandb", "from", "wandb", ".", "sdk", ".", "lib", "import", "RunDisabled", "from", "wandb", ".", "wandb_run", "import", "Run", "if", "self", ".", "_experiment", "is", "None", ":", "if", "self", ".", "_offline", ":", "os", ".", "environ", "[", "\"", "WANDB_MODE", "\"", "]", "=", "\"", "dryrun", "\"", "attach_id", "=", "getattr", "(", "self", ",", "\"", "_attach_id", "\"", ",", "None", ")", "if", "wandb", ".", "run", "is", "not", "None", ":", "rank_zero_warn", "(", "\"", "There", "is", "a", "wandb", "run", "already", "in", "progress", "and", "newly", "created", "instances", "of", "`", "WandbLogger", "`", "will", "reuse", "\"", "\"", "this", "run", ".", "If", "this", "is", "not", "desired", ",", "call", "`", "wandb", ".", "finish", "(", ")", "`", "before", "instantiating", "`", "WandbLogger", "`", ".", "\"", ")", "self", ".", "_experiment", "=", "wandb", ".", "run", "elif", "attach_id", "is", "not", "None", "and", "hasattr", "(", "wandb", ",", "\"", "_attach", "\"", ")", ":", "self", ".", "_experiment", "=", "wandb", ".", "_attach", "(", "attach_id", ")", "else", ":", "self", ".", "_experiment", "=", "wandb", ".", "init", "(", "*", "*", "self", ".", "_wandb_init", ")", "if", "isinstance", "(", "self", ".", "_experiment", ",", "(", "Run", ",", "RunDisabled", ")", ")", "and", "getattr", "(", "self", ".", "_experiment", ",", "\"", "define_metric", "\"", ",", "None", ")", ":", "if", "self", ".", "_wandb_init", ".", "get", "(", "\"", "sync_tensorboard", "\"", ")", ":", "self", ".", "_experiment", ".", "define_metric", "(", "\"", "*", "\"", ",", "step_metric", "=", "\"", "global_step", "\"", ")", "else", ":", "self", ".", "_experiment", ".", "define_metric", "(", "\"", "trainer", "/", "global_step", "\"", ")", "self", ".", "_experiment", ".", "define_metric", "(", "\"", "*", "\"", ",", "step_metric", "=", "\"", "trainer", "/", "global_step", "\"", ",", "step_sync", "=", "True", ")", "return", "self", ".", "_experiment"], "docstring": "r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the\r\n        following.\r\n\r\n        Example::\r\n\r\n        .. code-block:: python\r\n\r\n            self.logger.experiment.some_wandb_function()\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "actual", "wandb", "object", "to", "use", "wandb", "features", "in", "your", "class", "lightning", "pytorch", "core", "lightningmodule", "do", "the", "following", "example", "code", "block", "python", "self", "logger", "experiment", "some_wandb_function"], "docstring_summary": "r\"\"\"Actual wandb object. To use wandb features in your :class:`~lightning.pytorch.core.LightningModule` do the", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 377, "end_line": 421, "hash": "96b849ec3b499aeb4cedbd07be3c867a", "complexity": 9, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "log_table", "original_string": "def log_table(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[Any]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log a Table containing any object type (text, image, audio, video, molecule, html, etc).\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        metrics = {key: wandb.Table(columns=columns, data=data, dataframe=dataframe)}\r\n        self.log_metrics(metrics, step)", "language": "python", "code": "def log_table(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[Any]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log a Table containing any object type (text, image, audio, video, molecule, html, etc).\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        metrics = {key: wandb.Table(columns=columns, data=data, dataframe=dataframe)}\r\n        self.log_metrics(metrics, step)", "code_tokens": ["def", "log_table", "(", "self", ",", "key", ":", "str", ",", "columns", ":", "Optional", "[", "list", "[", "str", "]", "]", "=", "None", ",", "data", ":", "Optional", "[", "list", "[", "list", "[", "Any", "]", "]", "]", "=", "None", ",", "dataframe", ":", "Any", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "a", "Table", "containing", "any", "object", "type", "(", "text", ",", "image", ",", "audio", ",", "video", ",", "molecule", ",", "html", ",", "etc", ")", ".", "Can", "be", "defined", "either", "with", "`", "columns", "`", "and", "`", "data", "`", "or", "with", "`", "dataframe", "`", ".", "\"", "\"", "\"", "import", "wandb", "metrics", "=", "{", "key", ":", "wandb", ".", "Table", "(", "columns", "=", "columns", ",", "data", "=", "data", ",", "dataframe", "=", "dataframe", ")", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")"], "docstring": "Log a Table containing any object type (text, image, audio, video, molecule, html, etc).\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.", "docstring_tokens": ["log", "a", "table", "containing", "any", "object", "type", "text", "image", "audio", "video", "molecule", "html", "etc", "can", "be", "defined", "either", "with", "columns", "and", "data", "or", "with", "dataframe"], "docstring_summary": "Log a Table containing any object type (text, image, audio, video, molecule, html, etc).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 448, "end_line": 464, "hash": "d59c9a3a11a091869f3907be25df9759", "complexity": 1, "parameters": ["key", "columns", "data", "dataframe", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "log_text", "original_string": "def log_text(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[str]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log text as a Table.\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n\r\n        self.log_table(key, columns, data, dataframe, step)", "language": "python", "code": "def log_text(\r\n        self,\r\n        key: str,\r\n        columns: Optional[list[str]] = None,\r\n        data: Optional[list[list[str]]] = None,\r\n        dataframe: Any = None,\r\n        step: Optional[int] = None,\r\n    ) -> None:\r\n        \"\"\"Log text as a Table.\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.\r\n\r\n        \"\"\"\r\n\r\n        self.log_table(key, columns, data, dataframe, step)", "code_tokens": ["def", "log_text", "(", "self", ",", "key", ":", "str", ",", "columns", ":", "Optional", "[", "list", "[", "str", "]", "]", "=", "None", ",", "data", ":", "Optional", "[", "list", "[", "list", "[", "str", "]", "]", "]", "=", "None", ",", "dataframe", ":", "Any", "=", "None", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "text", "as", "a", "Table", ".", "Can", "be", "defined", "either", "with", "`", "columns", "`", "and", "`", "data", "`", "or", "with", "`", "dataframe", "`", ".", "\"", "\"", "\"", "self", ".", "log_table", "(", "key", ",", "columns", ",", "data", ",", "dataframe", ",", "step", ")"], "docstring": "Log text as a Table.\r\n\r\n        Can be defined either with `columns` and `data` or with `dataframe`.", "docstring_tokens": ["log", "text", "as", "a", "table", "can", "be", "defined", "either", "with", "columns", "and", "data", "or", "with", "dataframe"], "docstring_summary": "Log text as a Table.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 467, "end_line": 481, "hash": "6c4301cc68faa5d54ab25a817ffb8734", "complexity": 1, "parameters": ["key", "columns", "data", "dataframe", "step"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "log_image", "original_string": "def log_image(self, key: str, images: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log images (tensors, numpy arrays, PIL Images or file paths).\r\n\r\n        Optional kwargs are lists passed to each image (ex: caption, masks, boxes).\r\n\r\n        \"\"\"\r\n        if not isinstance(images, list):\r\n            raise TypeError(f'Expected a list as \"images\", found {type(images)}')\r\n        n = len(images)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Image(img, **kwarg) for img, kwarg in zip(images, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "language": "python", "code": "def log_image(self, key: str, images: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log images (tensors, numpy arrays, PIL Images or file paths).\r\n\r\n        Optional kwargs are lists passed to each image (ex: caption, masks, boxes).\r\n\r\n        \"\"\"\r\n        if not isinstance(images, list):\r\n            raise TypeError(f'Expected a list as \"images\", found {type(images)}')\r\n        n = len(images)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Image(img, **kwarg) for img, kwarg in zip(images, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "code_tokens": ["def", "log_image", "(", "self", ",", "key", ":", "str", ",", "images", ":", "list", "[", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "images", "(", "tensors", ",", "numpy", "arrays", ",", "PIL", "Images", "or", "file", "paths", ")", ".", "Optional", "kwargs", "are", "lists", "passed", "to", "each", "image", "(", "ex", ":", "caption", ",", "masks", ",", "boxes", ")", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "images", ",", "list", ")", ":", "raise", "TypeError", "(", "f", "'", "Expected", "a", "list", "as", "\"", "images", "\"", ",", "found", "{", "type", "(", "images", ")", "}", "'", ")", "n", "=", "len", "(", "images", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "len", "(", "v", ")", "!", "=", "n", ":", "raise", "ValueError", "(", "f", "\"", "Expected", "{", "n", "}", "items", "but", "only", "found", "{", "len", "(", "v", ")", "}", "for", "{", "k", "}", "\"", ")", "kwarg_list", "=", "[", "{", "k", ":", "kwargs", "[", "k", "]", "[", "i", "]", "for", "k", "in", "kwargs", "}", "for", "i", "in", "range", "(", "n", ")", "]", "import", "wandb", "metrics", "=", "{", "key", ":", "[", "wandb", ".", "Image", "(", "img", ",", "*", "*", "kwarg", ")", "for", "img", ",", "kwarg", "in", "zip", "(", "images", ",", "kwarg_list", ")", "]", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")"], "docstring": "Log images (tensors, numpy arrays, PIL Images or file paths).\r\n\r\n        Optional kwargs are lists passed to each image (ex: caption, masks, boxes).", "docstring_tokens": ["log", "images", "tensors", "numpy", "arrays", "pil", "images", "or", "file", "paths", "optional", "kwargs", "are", "lists", "passed", "to", "each", "image", "ex", "caption", "masks", "boxes"], "docstring_summary": "Log images (tensors, numpy arrays, PIL Images or file paths).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 484, "end_line": 501, "hash": "e36b5d58f944b2102cbcdb4400bf8964", "complexity": 7, "parameters": ["key", "images", "step", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "log_audio", "original_string": "def log_audio(self, key: str, audios: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Log audios (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the audio files\r\n            audios: The list of audio file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the audio files\r\n            \\**kwargs: Optional kwargs are lists passed to each ``Wandb.Audio`` instance (ex: caption, sample_rate).\r\n\r\n        Optional kwargs are lists passed to each audio (ex: caption, sample_rate).\r\n\r\n        \"\"\"\r\n        if not isinstance(audios, list):\r\n            raise TypeError(f'Expected a list as \"audios\", found {type(audios)}')\r\n        n = len(audios)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Audio(audio, **kwarg) for audio, kwarg in zip(audios, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "language": "python", "code": "def log_audio(self, key: str, audios: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        r\"\"\"Log audios (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the audio files\r\n            audios: The list of audio file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the audio files\r\n            \\**kwargs: Optional kwargs are lists passed to each ``Wandb.Audio`` instance (ex: caption, sample_rate).\r\n\r\n        Optional kwargs are lists passed to each audio (ex: caption, sample_rate).\r\n\r\n        \"\"\"\r\n        if not isinstance(audios, list):\r\n            raise TypeError(f'Expected a list as \"audios\", found {type(audios)}')\r\n        n = len(audios)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Audio(audio, **kwarg) for audio, kwarg in zip(audios, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "code_tokens": ["def", "log_audio", "(", "self", ",", "key", ":", "str", ",", "audios", ":", "list", "[", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Log", "audios", "(", "numpy", "arrays", ",", "or", "file", "paths", ")", ".", "Args", ":", "key", ":", "The", "key", "to", "be", "used", "for", "logging", "the", "audio", "files", "audios", ":", "The", "list", "of", "audio", "file", "paths", ",", "or", "numpy", "arrays", "to", "be", "logged", "step", ":", "The", "step", "number", "to", "be", "used", "for", "logging", "the", "audio", "files", "\\", "*", "*", "kwargs", ":", "Optional", "kwargs", "are", "lists", "passed", "to", "each", "`", "`", "Wandb", ".", "Audio", "`", "`", "instance", "(", "ex", ":", "caption", ",", "sample_rate", ")", ".", "Optional", "kwargs", "are", "lists", "passed", "to", "each", "audio", "(", "ex", ":", "caption", ",", "sample_rate", ")", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "audios", ",", "list", ")", ":", "raise", "TypeError", "(", "f", "'", "Expected", "a", "list", "as", "\"", "audios", "\"", ",", "found", "{", "type", "(", "audios", ")", "}", "'", ")", "n", "=", "len", "(", "audios", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "len", "(", "v", ")", "!", "=", "n", ":", "raise", "ValueError", "(", "f", "\"", "Expected", "{", "n", "}", "items", "but", "only", "found", "{", "len", "(", "v", ")", "}", "for", "{", "k", "}", "\"", ")", "kwarg_list", "=", "[", "{", "k", ":", "kwargs", "[", "k", "]", "[", "i", "]", "for", "k", "in", "kwargs", "}", "for", "i", "in", "range", "(", "n", ")", "]", "import", "wandb", "metrics", "=", "{", "key", ":", "[", "wandb", ".", "Audio", "(", "audio", ",", "*", "*", "kwarg", ")", "for", "audio", ",", "kwarg", "in", "zip", "(", "audios", ",", "kwarg_list", ")", "]", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")"], "docstring": "r\"\"\"Log audios (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the audio files\r\n            audios: The list of audio file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the audio files\r\n            \\**kwargs: Optional kwargs are lists passed to each ``Wandb.Audio`` instance (ex: caption, sample_rate).\r\n\r\n        Optional kwargs are lists passed to each audio (ex: caption, sample_rate).\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "log", "audios", "numpy", "arrays", "or", "file", "paths", "args", "key", "the", "key", "to", "be", "used", "for", "logging", "the", "audio", "files", "audios", "the", "list", "of", "audio", "file", "paths", "or", "numpy", "arrays", "to", "be", "logged", "step", "the", "step", "number", "to", "be", "used", "for", "logging", "the", "audio", "files", "kwargs", "optional", "kwargs", "are", "lists", "passed", "to", "each", "wandb", "audio", "instance", "ex", "caption", "sample_rate", "optional", "kwargs", "are", "lists", "passed", "to", "each", "audio", "ex", "caption", "sample_rate"], "docstring_summary": "r\"\"\"Log audios (numpy arrays, or file paths).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 504, "end_line": 527, "hash": "20cf565ab5b1dc032ae04c9fef4a7034", "complexity": 7, "parameters": ["key", "audios", "step", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "log_video", "original_string": "def log_video(self, key: str, videos: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log videos (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the video files\r\n            videos: The list of video file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the video files\r\n            **kwargs: Optional kwargs are lists passed to each Wandb.Video instance (ex: caption, fps, format).\r\n\r\n        Optional kwargs are lists passed to each video (ex: caption, fps, format).\r\n\r\n        \"\"\"\r\n        if not isinstance(videos, list):\r\n            raise TypeError(f'Expected a list as \"videos\", found {type(videos)}')\r\n        n = len(videos)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Video(video, **kwarg) for video, kwarg in zip(videos, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "language": "python", "code": "def log_video(self, key: str, videos: list[Any], step: Optional[int] = None, **kwargs: Any) -> None:\r\n        \"\"\"Log videos (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the video files\r\n            videos: The list of video file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the video files\r\n            **kwargs: Optional kwargs are lists passed to each Wandb.Video instance (ex: caption, fps, format).\r\n\r\n        Optional kwargs are lists passed to each video (ex: caption, fps, format).\r\n\r\n        \"\"\"\r\n        if not isinstance(videos, list):\r\n            raise TypeError(f'Expected a list as \"videos\", found {type(videos)}')\r\n        n = len(videos)\r\n        for k, v in kwargs.items():\r\n            if len(v) != n:\r\n                raise ValueError(f\"Expected {n} items but only found {len(v)} for {k}\")\r\n        kwarg_list = [{k: kwargs[k][i] for k in kwargs} for i in range(n)]\r\n\r\n        import wandb\r\n\r\n        metrics = {key: [wandb.Video(video, **kwarg) for video, kwarg in zip(videos, kwarg_list)]}\r\n        self.log_metrics(metrics, step)  # type: ignore[arg-type]\r", "code_tokens": ["def", "log_video", "(", "self", ",", "key", ":", "str", ",", "videos", ":", "list", "[", "Any", "]", ",", "step", ":", "Optional", "[", "int", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Log", "videos", "(", "numpy", "arrays", ",", "or", "file", "paths", ")", ".", "Args", ":", "key", ":", "The", "key", "to", "be", "used", "for", "logging", "the", "video", "files", "videos", ":", "The", "list", "of", "video", "file", "paths", ",", "or", "numpy", "arrays", "to", "be", "logged", "step", ":", "The", "step", "number", "to", "be", "used", "for", "logging", "the", "video", "files", "*", "*", "kwargs", ":", "Optional", "kwargs", "are", "lists", "passed", "to", "each", "Wandb", ".", "Video", "instance", "(", "ex", ":", "caption", ",", "fps", ",", "format", ")", ".", "Optional", "kwargs", "are", "lists", "passed", "to", "each", "video", "(", "ex", ":", "caption", ",", "fps", ",", "format", ")", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "videos", ",", "list", ")", ":", "raise", "TypeError", "(", "f", "'", "Expected", "a", "list", "as", "\"", "videos", "\"", ",", "found", "{", "type", "(", "videos", ")", "}", "'", ")", "n", "=", "len", "(", "videos", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "if", "len", "(", "v", ")", "!", "=", "n", ":", "raise", "ValueError", "(", "f", "\"", "Expected", "{", "n", "}", "items", "but", "only", "found", "{", "len", "(", "v", ")", "}", "for", "{", "k", "}", "\"", ")", "kwarg_list", "=", "[", "{", "k", ":", "kwargs", "[", "k", "]", "[", "i", "]", "for", "k", "in", "kwargs", "}", "for", "i", "in", "range", "(", "n", ")", "]", "import", "wandb", "metrics", "=", "{", "key", ":", "[", "wandb", ".", "Video", "(", "video", ",", "*", "*", "kwarg", ")", "for", "video", ",", "kwarg", "in", "zip", "(", "videos", ",", "kwarg_list", ")", "]", "}", "self", ".", "log_metrics", "(", "metrics", ",", "step", ")"], "docstring": "Log videos (numpy arrays, or file paths).\r\n\r\n        Args:\r\n            key: The key to be used for logging the video files\r\n            videos: The list of video file paths, or numpy arrays to be logged\r\n            step: The step number to be used for logging the video files\r\n            **kwargs: Optional kwargs are lists passed to each Wandb.Video instance (ex: caption, fps, format).\r\n\r\n        Optional kwargs are lists passed to each video (ex: caption, fps, format).", "docstring_tokens": ["log", "videos", "numpy", "arrays", "or", "file", "paths", "args", "key", "the", "key", "to", "be", "used", "for", "logging", "the", "video", "files", "videos", "the", "list", "of", "video", "file", "paths", "or", "numpy", "arrays", "to", "be", "logged", "step", "the", "step", "number", "to", "be", "used", "for", "logging", "the", "video", "files", "kwargs", "optional", "kwargs", "are", "lists", "passed", "to", "each", "wandb", "video", "instance", "ex", "caption", "fps", "format", "optional", "kwargs", "are", "lists", "passed", "to", "each", "video", "ex", "caption", "fps", "format"], "docstring_summary": "Log videos (numpy arrays, or file paths).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 530, "end_line": 553, "hash": "f1b3962e9e6fe1e5eea051b9438b7722", "complexity": 7, "parameters": ["key", "videos", "step", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "save_dir", "original_string": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "language": "python", "code": "def save_dir(self) -> Optional[str]:\r\n        \"\"\"Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.\r\n\r\n        \"\"\"\r\n        return self._save_dir", "code_tokens": ["def", "save_dir", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "save", "directory", ".", "Returns", ":", "The", "path", "to", "the", "save", "directory", ".", "\"", "\"", "\"", "return", "self", ".", "_save_dir"], "docstring": "Gets the save directory.\r\n\r\n        Returns:\r\n            The path to the save directory.", "docstring_tokens": ["gets", "the", "save", "directory", "returns", "the", "path", "to", "the", "save", "directory"], "docstring_summary": "Gets the save directory.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 557, "end_line": 564, "hash": "674a9f63f28274d554581fa05aa98d2f", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "name", "original_string": "def name(self) -> Optional[str]:\r\n        \"\"\"The project name of this experiment.\r\n\r\n        Returns:\r\n            The name of the project the current experiment belongs to. This name is not the same as `wandb.Run`'s\r\n            name. To access wandb's internal experiment name, use ``logger.experiment.name`` instead.\r\n\r\n        \"\"\"\r\n        return self._project", "language": "python", "code": "def name(self) -> Optional[str]:\r\n        \"\"\"The project name of this experiment.\r\n\r\n        Returns:\r\n            The name of the project the current experiment belongs to. This name is not the same as `wandb.Run`'s\r\n            name. To access wandb's internal experiment name, use ``logger.experiment.name`` instead.\r\n\r\n        \"\"\"\r\n        return self._project", "code_tokens": ["def", "name", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "The", "project", "name", "of", "this", "experiment", ".", "Returns", ":", "The", "name", "of", "the", "project", "the", "current", "experiment", "belongs", "to", ".", "This", "name", "is", "not", "the", "same", "as", "`", "wandb", ".", "Run", "`", "'", "s", "name", ".", "To", "access", "wandb", "'", "s", "internal", "experiment", "name", ",", "use", "`", "`", "logger", ".", "experiment", ".", "name", "`", "`", "instead", ".", "\"", "\"", "\"", "return", "self", ".", "_project"], "docstring": "The project name of this experiment.\r\n\r\n        Returns:\r\n            The name of the project the current experiment belongs to. This name is not the same as `wandb.Run`'s\r\n            name. To access wandb's internal experiment name, use ``logger.experiment.name`` instead.", "docstring_tokens": ["the", "project", "name", "of", "this", "experiment", "returns", "the", "name", "of", "the", "project", "the", "current", "experiment", "belongs", "to", "this", "name", "is", "not", "the", "same", "as", "wandb", "run", "s", "name", "to", "access", "wandb", "s", "internal", "experiment", "name", "use", "logger", "experiment", "name", "instead"], "docstring_summary": "The project name of this experiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 568, "end_line": 576, "hash": "4634808e9c6915c1415f86706a56d6bc", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "version", "original_string": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the id of the experiment.\r\n\r\n        Returns:\r\n            The id of the experiment if the experiment exists else the id given to the constructor.\r\n\r\n        \"\"\"\r\n        # don't create an experiment if we don't have one\r\n        return self._experiment.id if self._experiment else self._id", "language": "python", "code": "def version(self) -> Optional[str]:\r\n        \"\"\"Gets the id of the experiment.\r\n\r\n        Returns:\r\n            The id of the experiment if the experiment exists else the id given to the constructor.\r\n\r\n        \"\"\"\r\n        # don't create an experiment if we don't have one\r\n        return self._experiment.id if self._experiment else self._id", "code_tokens": ["def", "version", "(", "self", ")", "-", ">", "Optional", "[", "str", "]", ":", "\"", "\"", "\"", "Gets", "the", "id", "of", "the", "experiment", ".", "Returns", ":", "The", "id", "of", "the", "experiment", "if", "the", "experiment", "exists", "else", "the", "id", "given", "to", "the", "constructor", ".", "\"", "\"", "\"", "return", "self", ".", "_experiment", ".", "id", "if", "self", ".", "_experiment", "else", "self", ".", "_id"], "docstring": "Gets the id of the experiment.\r\n\r\n        Returns:\r\n            The id of the experiment if the experiment exists else the id given to the constructor.", "docstring_tokens": ["gets", "the", "id", "of", "the", "experiment", "returns", "the", "id", "of", "the", "experiment", "if", "the", "experiment", "exists", "else", "the", "id", "given", "to", "the", "constructor"], "docstring_summary": "Gets the id of the experiment.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 580, "end_line": 588, "hash": "94e78e830009abf71f4d2d768338cb14", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "download_artifact", "original_string": "def download_artifact(\r\n        artifact: str,\r\n        save_dir: Optional[_PATH] = None,\r\n        artifact_type: Optional[str] = None,\r\n        use_artifact: Optional[bool] = True,\r\n    ) -> str:\r\n        \"\"\"Downloads an artifact from the wandb server.\r\n\r\n        Args:\r\n            artifact: The path of the artifact to download.\r\n            save_dir: The directory to save the artifact to.\r\n            artifact_type: The type of artifact to download.\r\n            use_artifact: Whether to add an edge between the artifact graph.\r\n\r\n        Returns:\r\n            The path to the downloaded artifact.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        if wandb.run is not None and use_artifact:\r\n            artifact = wandb.run.use_artifact(artifact)\r\n        else:\r\n            api = wandb.Api()\r\n            artifact = api.artifact(artifact, type=artifact_type)\r\n\r\n        save_dir = None if save_dir is None else os.fspath(save_dir)\r\n        return artifact.download(root=save_dir)", "language": "python", "code": "def download_artifact(\r\n        artifact: str,\r\n        save_dir: Optional[_PATH] = None,\r\n        artifact_type: Optional[str] = None,\r\n        use_artifact: Optional[bool] = True,\r\n    ) -> str:\r\n        \"\"\"Downloads an artifact from the wandb server.\r\n\r\n        Args:\r\n            artifact: The path of the artifact to download.\r\n            save_dir: The directory to save the artifact to.\r\n            artifact_type: The type of artifact to download.\r\n            use_artifact: Whether to add an edge between the artifact graph.\r\n\r\n        Returns:\r\n            The path to the downloaded artifact.\r\n\r\n        \"\"\"\r\n        import wandb\r\n\r\n        if wandb.run is not None and use_artifact:\r\n            artifact = wandb.run.use_artifact(artifact)\r\n        else:\r\n            api = wandb.Api()\r\n            artifact = api.artifact(artifact, type=artifact_type)\r\n\r\n        save_dir = None if save_dir is None else os.fspath(save_dir)\r\n        return artifact.download(root=save_dir)", "code_tokens": ["def", "download_artifact", "(", "artifact", ":", "str", ",", "save_dir", ":", "Optional", "[", "_PATH", "]", "=", "None", ",", "artifact_type", ":", "Optional", "[", "str", "]", "=", "None", ",", "use_artifact", ":", "Optional", "[", "bool", "]", "=", "True", ",", ")", "-", ">", "str", ":", "\"", "\"", "\"", "Downloads", "an", "artifact", "from", "the", "wandb", "server", ".", "Args", ":", "artifact", ":", "The", "path", "of", "the", "artifact", "to", "download", ".", "save_dir", ":", "The", "directory", "to", "save", "the", "artifact", "to", ".", "artifact_type", ":", "The", "type", "of", "artifact", "to", "download", ".", "use_artifact", ":", "Whether", "to", "add", "an", "edge", "between", "the", "artifact", "graph", ".", "Returns", ":", "The", "path", "to", "the", "downloaded", "artifact", ".", "\"", "\"", "\"", "import", "wandb", "if", "wandb", ".", "run", "is", "not", "None", "and", "use_artifact", ":", "artifact", "=", "wandb", ".", "run", ".", "use_artifact", "(", "artifact", ")", "else", ":", "api", "=", "wandb", ".", "Api", "(", ")", "artifact", "=", "api", ".", "artifact", "(", "artifact", ",", "type", "=", "artifact_type", ")", "save_dir", "=", "None", "if", "save_dir", "is", "None", "else", "os", ".", "fspath", "(", "save_dir", ")", "return", "artifact", ".", "download", "(", "root", "=", "save_dir", ")"], "docstring": "Downloads an artifact from the wandb server.\r\n\r\n        Args:\r\n            artifact: The path of the artifact to download.\r\n            save_dir: The directory to save the artifact to.\r\n            artifact_type: The type of artifact to download.\r\n            use_artifact: Whether to add an edge between the artifact graph.\r\n\r\n        Returns:\r\n            The path to the downloaded artifact.", "docstring_tokens": ["downloads", "an", "artifact", "from", "the", "wandb", "server", "args", "artifact", "the", "path", "of", "the", "artifact", "to", "download", "save_dir", "the", "directory", "to", "save", "the", "artifact", "to", "artifact_type", "the", "type", "of", "artifact", "to", "download", "use_artifact", "whether", "to", "add", "an", "edge", "between", "the", "artifact", "graph", "returns", "the", "path", "to", "the", "downloaded", "artifact"], "docstring_summary": "Downloads an artifact from the wandb server.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 600, "end_line": 627, "hash": "47b747876887f7d7429d0f73db7fc2c2", "complexity": 4, "parameters": ["artifact", "save_dir", "artifact_type", "use_artifact"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loggers\\wandb.py", "func_name": "use_artifact", "original_string": "def use_artifact(self, artifact: str, artifact_type: Optional[str] = None) -> \"Artifact\":\r\n        \"\"\"Logs to the wandb dashboard that the mentioned artifact is used by the run.\r\n\r\n        Args:\r\n            artifact: The path of the artifact.\r\n            artifact_type: The type of artifact being used.\r\n\r\n        Returns:\r\n            wandb Artifact object for the artifact.\r\n\r\n        \"\"\"\r\n        return self.experiment.use_artifact(artifact, type=artifact_type)", "language": "python", "code": "def use_artifact(self, artifact: str, artifact_type: Optional[str] = None) -> \"Artifact\":\r\n        \"\"\"Logs to the wandb dashboard that the mentioned artifact is used by the run.\r\n\r\n        Args:\r\n            artifact: The path of the artifact.\r\n            artifact_type: The type of artifact being used.\r\n\r\n        Returns:\r\n            wandb Artifact object for the artifact.\r\n\r\n        \"\"\"\r\n        return self.experiment.use_artifact(artifact, type=artifact_type)", "code_tokens": ["def", "use_artifact", "(", "self", ",", "artifact", ":", "str", ",", "artifact_type", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "\"", "Artifact", "\"", ":", "\"", "\"", "\"", "Logs", "to", "the", "wandb", "dashboard", "that", "the", "mentioned", "artifact", "is", "used", "by", "the", "run", ".", "Args", ":", "artifact", ":", "The", "path", "of", "the", "artifact", ".", "artifact_type", ":", "The", "type", "of", "artifact", "being", "used", ".", "Returns", ":", "wandb", "Artifact", "object", "for", "the", "artifact", ".", "\"", "\"", "\"", "return", "self", ".", "experiment", ".", "use_artifact", "(", "artifact", ",", "type", "=", "artifact_type", ")"], "docstring": "Logs to the wandb dashboard that the mentioned artifact is used by the run.\r\n\r\n        Args:\r\n            artifact: The path of the artifact.\r\n            artifact_type: The type of artifact being used.\r\n\r\n        Returns:\r\n            wandb Artifact object for the artifact.", "docstring_tokens": ["logs", "to", "the", "wandb", "dashboard", "that", "the", "mentioned", "artifact", "is", "used", "by", "the", "run", "args", "artifact", "the", "path", "of", "the", "artifact", "artifact_type", "the", "type", "of", "artifact", "being", "used", "returns", "wandb", "artifact", "object", "for", "the", "artifact"], "docstring_summary": "Logs to the wandb dashboard that the mentioned artifact is used by the run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loggers\\wandb.py", "partition": "train", "function_type": "class_method", "class_name": "WandbLogger", "start_line": 629, "end_line": 640, "hash": "1cc631684597bab486ac18d8d25ee74c", "complexity": 1, "parameters": ["artifact", "artifact_type"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "num_dataloaders", "original_string": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "language": "python", "code": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "code_tokens": ["def", "num_dataloaders", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Returns", "the", "number", "of", "prediction", "dataloaders", ".", "\"", "\"", "\"", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "return", "len", "(", "combined_loader", ".", "flattened", ")"], "docstring": "Returns the number of prediction dataloaders.", "docstring_tokens": ["returns", "the", "number", "of", "prediction", "dataloaders"], "docstring_summary": "Returns the number of prediction dataloaders.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 88, "end_line": 92, "hash": "d70af2433fd1e06e186a3454e1377649", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "max_batches", "original_string": "def max_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The max number of batches to run per dataloader.\"\"\"\r\n        max_batches = self._max_batches\r\n        if not self.trainer.sanity_checking:\r\n            return max_batches\r\n        return [min(self.trainer.num_sanity_val_steps, batches) for batches in max_batches]", "language": "python", "code": "def max_batches(self) -> list[Union[int, float]]:\r\n        \"\"\"The max number of batches to run per dataloader.\"\"\"\r\n        max_batches = self._max_batches\r\n        if not self.trainer.sanity_checking:\r\n            return max_batches\r\n        return [min(self.trainer.num_sanity_val_steps, batches) for batches in max_batches]", "code_tokens": ["def", "max_batches", "(", "self", ")", "-", ">", "list", "[", "Union", "[", "int", ",", "float", "]", "]", ":", "\"", "\"", "\"", "The", "max", "number", "of", "batches", "to", "run", "per", "dataloader", ".", "\"", "\"", "\"", "max_batches", "=", "self", ".", "_max_batches", "if", "not", "self", ".", "trainer", ".", "sanity_checking", ":", "return", "max_batches", "return", "[", "min", "(", "self", ".", "trainer", ".", "num_sanity_val_steps", ",", "batches", ")", "for", "batches", "in", "max_batches", "]"], "docstring": "The max number of batches to run per dataloader.", "docstring_tokens": ["the", "max", "number", "of", "batches", "to", "run", "per", "dataloader"], "docstring_summary": "The max number of batches to run per dataloader.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 95, "end_line": 100, "hash": "be1f542f8104ccfaf1adc7b091df17b2", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_should_reload_val_dl", "original_string": "def _should_reload_val_dl(self) -> bool:\r\n        \"\"\"Check if validation dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return bool(n_epochs and self.trainer.current_epoch - self._last_val_dl_reload_epoch >= n_epochs)", "language": "python", "code": "def _should_reload_val_dl(self) -> bool:\r\n        \"\"\"Check if validation dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return bool(n_epochs and self.trainer.current_epoch - self._last_val_dl_reload_epoch >= n_epochs)", "code_tokens": ["def", "_should_reload_val_dl", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Check", "if", "validation", "dataloader", "should", "be", "reloaded", ".", "\"", "\"", "\"", "n_epochs", "=", "self", ".", "trainer", ".", "reload_dataloaders_every_n_epochs", "return", "bool", "(", "n_epochs", "and", "self", ".", "trainer", ".", "current_epoch", "-", "self", ".", "_last_val_dl_reload_epoch", ">", "=", "n_epochs", ")"], "docstring": "Check if validation dataloader should be reloaded.", "docstring_tokens": ["check", "if", "validation", "dataloader", "should", "be", "reloaded"], "docstring_summary": "Check if validation dataloader should be reloaded.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 108, "end_line": 111, "hash": "090bcb1a8085f3f4013cc37c1130048e", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "reset", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        self._has_run = False\r\n        self._logged_outputs = []\r\n\r\n        if not self.restarting:\r\n            self.batch_progress.reset_on_run()\r\n        else:\r\n            self.batch_progress.reset_on_restart()\r\n        fn = trainer.state.fn\r\n        assert fn is not None\r\n        # when restarting, if we are running `validate` or `test` twice, since there's no concept of `max_epochs` we\r\n        # need to reset the current state when the loop has finished running\r\n        if fn != TrainerFn.FITTING:\r\n            self.batch_progress.reset_on_run()\r\n\r\n        assert trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(trainer, trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n\r\n        if fn == TrainerFn.FITTING:\r\n            for i, dl in enumerate(combined_loader.flattened):\r\n                # some users want validation shuffling based on the training progress\r\n                _set_sampler_epoch(dl, trainer.fit_loop.epoch_progress.current.processed)\r\n\r\n        # set the per-dataloader limits\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        # add the previous `fetched` value to properly track `is_last_batch` with no prefetching\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        self._has_run = False\r\n        self._logged_outputs = []\r\n\r\n        if not self.restarting:\r\n            self.batch_progress.reset_on_run()\r\n        else:\r\n            self.batch_progress.reset_on_restart()\r\n        fn = trainer.state.fn\r\n        assert fn is not None\r\n        # when restarting, if we are running `validate` or `test` twice, since there's no concept of `max_epochs` we\r\n        # need to reset the current state when the loop has finished running\r\n        if fn != TrainerFn.FITTING:\r\n            self.batch_progress.reset_on_run()\r\n\r\n        assert trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(trainer, trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n\r\n        if fn == TrainerFn.FITTING:\r\n            for i, dl in enumerate(combined_loader.flattened):\r\n                # some users want validation shuffling based on the training progress\r\n                _set_sampler_epoch(dl, trainer.fit_loop.epoch_progress.current.processed)\r\n\r\n        # set the per-dataloader limits\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        # add the previous `fetched` value to properly track `is_last_batch` with no prefetching\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resets", "the", "internal", "state", "of", "the", "loop", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "self", ".", "_has_run", "=", "False", "self", ".", "_logged_outputs", "=", "[", "]", "if", "not", "self", ".", "restarting", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "else", ":", "self", ".", "batch_progress", ".", "reset_on_restart", "(", ")", "fn", "=", "trainer", ".", "state", ".", "fn", "assert", "fn", "is", "not", "None", "if", "fn", "!", "=", "TrainerFn", ".", "FITTING", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "assert", "trainer", ".", "state", ".", "stage", "is", "not", "None", "data_fetcher", "=", "_select_data_fetcher", "(", "trainer", ",", "trainer", ".", "state", ".", "stage", ")", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "if", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "for", "i", ",", "dl", "in", "enumerate", "(", "combined_loader", ".", "flattened", ")", ":", "_set_sampler_epoch", "(", "dl", ",", "trainer", ".", "fit_loop", ".", "epoch_progress", ".", "current", ".", "processed", ")", "combined_loader", ".", "limits", "=", "self", ".", "max_batches", "data_fetcher", ".", "setup", "(", "combined_loader", ")", "iter", "(", "data_fetcher", ")", "data_fetcher", ".", "fetched", "+", "=", "self", ".", "batch_progress", ".", "current", ".", "ready", "data_fetcher", ".", "_start_profiler", "=", "self", ".", "_on_before_fetch", "data_fetcher", ".", "_stop_profiler", "=", "self", ".", "_on_after_fetch", "self", ".", "_data_fetcher", "=", "data_fetcher"], "docstring": "Resets the internal state of the loop.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "the", "loop"], "docstring_summary": "Resets the internal state of the loop.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 228, "end_line": 265, "hash": "85b0dbe14e81322d268f556782ff4178", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "on_run_start", "original_string": "def on_run_start(self) -> None:\r\n        \"\"\"Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``\r\n        hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_evaluation_model_eval()\r\n        self._on_evaluation_start()\r\n        self._on_evaluation_epoch_start()", "language": "python", "code": "def on_run_start(self) -> None:\r\n        \"\"\"Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``\r\n        hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_evaluation_model_eval()\r\n        self._on_evaluation_start()\r\n        self._on_evaluation_epoch_start()", "code_tokens": ["def", "on_run_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "the", "`", "`", "_on_evaluation_model_eval", "`", "`", ",", "`", "`", "_on_evaluation_start", "`", "`", "and", "`", "`", "_on_evaluation_epoch_start", "`", "`", "hooks", ".", "\"", "\"", "\"", "self", ".", "_verify_dataloader_idx_requirement", "(", ")", "self", ".", "_on_evaluation_model_eval", "(", ")", "self", ".", "_on_evaluation_start", "(", ")", "self", ".", "_on_evaluation_epoch_start", "(", ")"], "docstring": "Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``\r\n        hooks.", "docstring_tokens": ["runs", "the", "_on_evaluation_model_eval", "_on_evaluation_start", "and", "_on_evaluation_epoch_start", "hooks"], "docstring_summary": "Runs the ``_on_evaluation_model_eval``, ``_on_evaluation_start`` and ``_on_evaluation_epoch_start``", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 280, "end_line": 286, "hash": "3d608898365912248a393603393d9f89", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "on_run_end", "original_string": "def on_run_end(self) -> list[_OUT_DICT]:\r\n        \"\"\"Runs the ``_on_evaluation_epoch_end`` hook.\"\"\"\r\n        # if `done` returned True before any iterations were done, this won't have been called in `on_advance_end`\r\n        self.trainer._logger_connector.epoch_end_reached()\r\n        self.trainer._logger_connector._evaluation_epoch_end()\r\n\r\n        # hook\r\n        self._on_evaluation_epoch_end()\r\n\r\n        logged_outputs, self._logged_outputs = self._logged_outputs, []  # free memory\r\n        # include any logged outputs on epoch_end\r\n        epoch_end_logged_outputs = self.trainer._logger_connector.update_eval_epoch_metrics()\r\n        all_logged_outputs = dict(ChainMap(*logged_outputs))  # list[dict] -> dict\r\n        all_logged_outputs.update(epoch_end_logged_outputs)\r\n        for dl_outputs in logged_outputs:\r\n            dl_outputs.update(epoch_end_logged_outputs)\r\n\r\n        # log metrics\r\n        self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n\r\n        # hook\r\n        self._on_evaluation_end()\r\n\r\n        # enable train mode again\r\n        self._on_evaluation_model_train()\r\n\r\n        if self.verbose and self.trainer.is_global_zero:\r\n            self._print_results(logged_outputs, self._stage.value)\r\n\r\n        now = time.monotonic()\r\n        self.trainer._last_val_time = now\r\n\r\n        return logged_outputs", "language": "python", "code": "def on_run_end(self) -> list[_OUT_DICT]:\r\n        \"\"\"Runs the ``_on_evaluation_epoch_end`` hook.\"\"\"\r\n        # if `done` returned True before any iterations were done, this won't have been called in `on_advance_end`\r\n        self.trainer._logger_connector.epoch_end_reached()\r\n        self.trainer._logger_connector._evaluation_epoch_end()\r\n\r\n        # hook\r\n        self._on_evaluation_epoch_end()\r\n\r\n        logged_outputs, self._logged_outputs = self._logged_outputs, []  # free memory\r\n        # include any logged outputs on epoch_end\r\n        epoch_end_logged_outputs = self.trainer._logger_connector.update_eval_epoch_metrics()\r\n        all_logged_outputs = dict(ChainMap(*logged_outputs))  # list[dict] -> dict\r\n        all_logged_outputs.update(epoch_end_logged_outputs)\r\n        for dl_outputs in logged_outputs:\r\n            dl_outputs.update(epoch_end_logged_outputs)\r\n\r\n        # log metrics\r\n        self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n\r\n        # hook\r\n        self._on_evaluation_end()\r\n\r\n        # enable train mode again\r\n        self._on_evaluation_model_train()\r\n\r\n        if self.verbose and self.trainer.is_global_zero:\r\n            self._print_results(logged_outputs, self._stage.value)\r\n\r\n        now = time.monotonic()\r\n        self.trainer._last_val_time = now\r\n\r\n        return logged_outputs", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "list", "[", "_OUT_DICT", "]", ":", "\"", "\"", "\"", "Runs", "the", "`", "`", "_on_evaluation_epoch_end", "`", "`", "hook", ".", "\"", "\"", "\"", "self", ".", "trainer", ".", "_logger_connector", ".", "epoch_end_reached", "(", ")", "self", ".", "trainer", ".", "_logger_connector", ".", "_evaluation_epoch_end", "(", ")", "self", ".", "_on_evaluation_epoch_end", "(", ")", "logged_outputs", ",", "self", ".", "_logged_outputs", "=", "self", ".", "_logged_outputs", ",", "[", "]", "epoch_end_logged_outputs", "=", "self", ".", "trainer", ".", "_logger_connector", ".", "update_eval_epoch_metrics", "(", ")", "all_logged_outputs", "=", "dict", "(", "ChainMap", "(", "*", "logged_outputs", ")", ")", "all_logged_outputs", ".", "update", "(", "epoch_end_logged_outputs", ")", "for", "dl_outputs", "in", "logged_outputs", ":", "dl_outputs", ".", "update", "(", "epoch_end_logged_outputs", ")", "self", ".", "trainer", ".", "_logger_connector", ".", "log_eval_end_metrics", "(", "all_logged_outputs", ")", "self", ".", "_on_evaluation_end", "(", ")", "self", ".", "_on_evaluation_model_train", "(", ")", "if", "self", ".", "verbose", "and", "self", ".", "trainer", ".", "is_global_zero", ":", "self", ".", "_print_results", "(", "logged_outputs", ",", "self", ".", "_stage", ".", "value", ")", "now", "=", "time", ".", "monotonic", "(", ")", "self", ".", "trainer", ".", "_last_val_time", "=", "now", "return", "logged_outputs"], "docstring": "Runs the ``_on_evaluation_epoch_end`` hook.", "docstring_tokens": ["runs", "the", "_on_evaluation_epoch_end", "hook"], "docstring_summary": "Runs the ``_on_evaluation_epoch_end`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 288, "end_line": 320, "hash": "903a52e02801669bcdc7e32ff27dcf5f", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_on_evaluation_start", "original_string": "def _on_evaluation_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_start\" if trainer.testing else \"on_validation_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)", "language": "python", "code": "def _on_evaluation_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_start\" if trainer.testing else \"on_validation_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)", "code_tokens": ["def", "_on_evaluation_start", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "`", "`", "on_", "{", "validation", "/", "test", "}", "_start", "`", "`", "hooks", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "\"", "on_test_start", "\"", "if", "trainer", ".", "testing", "else", "\"", "on_validation_start", "\"", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Runs ``on_{validation/test}_start`` hooks.", "docstring_tokens": ["runs", "on_", "validation", "test", "_start", "hooks"], "docstring_summary": "Runs ``on_{validation/test}_start`` hooks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 328, "end_line": 335, "hash": "5504678d24b6bc6ae7dca7f35d27ba70", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_on_evaluation_model_eval", "original_string": "def _on_evaluation_model_eval(self) -> None:\r\n        \"\"\"Sets model to eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_eval\" if trainer.testing else \"on_validation_model_eval\"\r\n        self._module_mode.capture(trainer.lightning_module)\r\n        call._call_lightning_module_hook(trainer, hook_name)", "language": "python", "code": "def _on_evaluation_model_eval(self) -> None:\r\n        \"\"\"Sets model to eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_eval\" if trainer.testing else \"on_validation_model_eval\"\r\n        self._module_mode.capture(trainer.lightning_module)\r\n        call._call_lightning_module_hook(trainer, hook_name)", "code_tokens": ["def", "_on_evaluation_model_eval", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Sets", "model", "to", "eval", "mode", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "\"", "on_test_model_eval", "\"", "if", "trainer", ".", "testing", "else", "\"", "on_validation_model_eval", "\"", "self", ".", "_module_mode", ".", "capture", "(", "trainer", ".", "lightning_module", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ")"], "docstring": "Sets model to eval mode.", "docstring_tokens": ["sets", "model", "to", "eval", "mode"], "docstring_summary": "Sets model to eval mode.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 337, "end_line": 342, "hash": "301632c5a9926e4ffe70f392c5261e28", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_on_evaluation_model_train", "original_string": "def _on_evaluation_model_train(self) -> None:\r\n        \"\"\"Undoes the eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_train\" if trainer.testing else \"on_validation_model_train\"\r\n        if is_overridden(hook_name, trainer.lightning_module):\r\n            call._call_lightning_module_hook(trainer, hook_name)\r\n        else:\r\n            self._module_mode.restore(trainer.lightning_module)", "language": "python", "code": "def _on_evaluation_model_train(self) -> None:\r\n        \"\"\"Undoes the eval mode.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_model_train\" if trainer.testing else \"on_validation_model_train\"\r\n        if is_overridden(hook_name, trainer.lightning_module):\r\n            call._call_lightning_module_hook(trainer, hook_name)\r\n        else:\r\n            self._module_mode.restore(trainer.lightning_module)", "code_tokens": ["def", "_on_evaluation_model_train", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Undoes", "the", "eval", "mode", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "\"", "on_test_model_train", "\"", "if", "trainer", ".", "testing", "else", "\"", "on_validation_model_train", "\"", "if", "is_overridden", "(", "hook_name", ",", "trainer", ".", "lightning_module", ")", ":", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ")", "else", ":", "self", ".", "_module_mode", ".", "restore", "(", "trainer", ".", "lightning_module", ")"], "docstring": "Undoes the eval mode.", "docstring_tokens": ["undoes", "the", "eval", "mode"], "docstring_summary": "Undoes the eval mode.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 344, "end_line": 351, "hash": "cf22a7df1e568d6e447493f77e5a2cc8", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_on_evaluation_end", "original_string": "def _on_evaluation_end(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_end\" if trainer.testing else \"on_validation_end\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)\r\n\r\n        # reset the logger connector state\r\n        trainer._logger_connector.reset_results()", "language": "python", "code": "def _on_evaluation_end(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n        hook_name = \"on_test_end\" if trainer.testing else \"on_validation_end\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)\r\n        call._call_strategy_hook(trainer, hook_name, *args, **kwargs)\r\n\r\n        # reset the logger connector state\r\n        trainer._logger_connector.reset_results()", "code_tokens": ["def", "_on_evaluation_end", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "`", "`", "on_", "{", "validation", "/", "test", "}", "_end", "`", "`", "hook", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "\"", "on_test_end", "\"", "if", "trainer", ".", "testing", "else", "\"", "on_validation_end", "\"", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "trainer", ".", "_logger_connector", ".", "reset_results", "(", ")"], "docstring": "Runs ``on_{validation/test}_end`` hook.", "docstring_tokens": ["runs", "on_", "validation", "test", "_end", "hook"], "docstring_summary": "Runs ``on_{validation/test}_end`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 353, "end_line": 362, "hash": "1992f50f0009b5328d59659d7d755b9b", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_on_evaluation_epoch_start", "original_string": "def _on_evaluation_epoch_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs the ``on_{validation/test}_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_start\" if trainer.testing else \"on_validation_epoch_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)", "language": "python", "code": "def _on_evaluation_epoch_start(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Runs the ``on_{validation/test}_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_start\" if trainer.testing else \"on_validation_epoch_start\"\r\n        call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\r\n        call._call_lightning_module_hook(trainer, hook_name, *args, **kwargs)", "code_tokens": ["def", "_on_evaluation_epoch_start", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "the", "`", "`", "on_", "{", "validation", "/", "test", "}", "_epoch_start", "`", "`", "hooks", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "\"", "on_test_epoch_start", "\"", "if", "trainer", ".", "testing", "else", "\"", "on_validation_epoch_start", "\"", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Runs the ``on_{validation/test}_epoch_start`` hooks.", "docstring_tokens": ["runs", "the", "on_", "validation", "test", "_epoch_start", "hooks"], "docstring_summary": "Runs the ``on_{validation/test}_epoch_start`` hooks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 364, "end_line": 370, "hash": "b5ba54a9d94da02ad87d56db3a40ab87", "complexity": 2, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_on_evaluation_epoch_end", "original_string": "def _on_evaluation_epoch_end(self) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_epoch_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_end\" if trainer.testing else \"on_validation_epoch_end\"\r\n        call._call_callback_hooks(trainer, hook_name)\r\n        call._call_lightning_module_hook(trainer, hook_name)\r\n\r\n        trainer._logger_connector.on_epoch_end()", "language": "python", "code": "def _on_evaluation_epoch_end(self) -> None:\r\n        \"\"\"Runs ``on_{validation/test}_epoch_end`` hook.\"\"\"\r\n        trainer = self.trainer\r\n\r\n        hook_name = \"on_test_epoch_end\" if trainer.testing else \"on_validation_epoch_end\"\r\n        call._call_callback_hooks(trainer, hook_name)\r\n        call._call_lightning_module_hook(trainer, hook_name)\r\n\r\n        trainer._logger_connector.on_epoch_end()", "code_tokens": ["def", "_on_evaluation_epoch_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "`", "`", "on_", "{", "validation", "/", "test", "}", "_epoch_end", "`", "`", "hook", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "hook_name", "=", "\"", "on_test_epoch_end", "\"", "if", "trainer", ".", "testing", "else", "\"", "on_validation_epoch_end", "\"", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "hook_name", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "hook_name", ")", "trainer", ".", "_logger_connector", ".", "on_epoch_end", "(", ")"], "docstring": "Runs ``on_{validation/test}_epoch_end`` hook.", "docstring_tokens": ["runs", "on_", "validation", "test", "_epoch_end", "hook"], "docstring_summary": "Runs ``on_{validation/test}_epoch_end`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 372, "end_line": 380, "hash": "4ba5c86c66ba91d22e98426fd2ee8863", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_build_kwargs", "original_string": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            batch: the current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "language": "python", "code": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            batch: the current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "code_tokens": ["def", "_build_kwargs", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "Optional", "[", "int", "]", ")", "-", ">", "OrderedDict", ":", "\"", "\"", "\"", "Helper", "method", "to", "build", "the", "arguments", "for", "the", "current", "step", ".", "Args", ":", "batch", ":", "the", "current", "batch", "to", "run", "through", "the", "step", ".", "batch_idx", ":", "the", "index", "of", "the", "current", "batch", ".", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "producing", "the", "current", "batch", ".", "None", "if", "not", "multiple", "dataloaders", "in", "sequential", "mode", ".", "Returns", ":", "the", "dictionary", "containing", "all", "the", "keyboard", "arguments", "for", "the", "step", "\"", "\"", "\"", "step_kwargs", "=", "OrderedDict", "(", "[", "(", "\"", "batch", "\"", ",", "batch", ")", ",", "(", "\"", "batch_idx", "\"", ",", "batch_idx", ")", "]", ")", "if", "dataloader_idx", "is", "not", "None", ":", "step_kwargs", "[", "\"", "dataloader_idx", "\"", "]", "=", "dataloader_idx", "return", "step_kwargs"], "docstring": "Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            batch: the current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the step", "docstring_tokens": ["helper", "method", "to", "build", "the", "arguments", "for", "the", "current", "step", "args", "batch", "the", "current", "batch", "to", "run", "through", "the", "step", "batch_idx", "the", "index", "of", "the", "current", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "producing", "the", "current", "batch", "none", "if", "not", "multiple", "dataloaders", "in", "sequential", "mode", "returns", "the", "dictionary", "containing", "all", "the", "keyboard", "arguments", "for", "the", "step"], "docstring_summary": "Helper method to build the arguments for the current step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 472, "end_line": 488, "hash": "1afed985a86da9b1a39de521e972a264", "complexity": 2, "parameters": ["batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\evaluation_loop.py", "func_name": "_build_step_args_from_hook_kwargs", "original_string": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `test_step` or `validation_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "language": "python", "code": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `test_step` or `validation_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "code_tokens": ["def", "_build_step_args_from_hook_kwargs", "(", "self", ",", "hook_kwargs", ":", "OrderedDict", ",", "step_hook_name", ":", "str", ")", "-", ">", "tuple", ":", "\"", "\"", "\"", "Helper", "method", "to", "build", "args", "for", "`", "test_step", "`", "or", "`", "validation_step", "`", ".", "\"", "\"", "\"", "kwargs", "=", "hook_kwargs", ".", "copy", "(", ")", "step_hook_fx", "=", "getattr", "(", "self", ".", "trainer", ".", "lightning_module", ",", "step_hook_name", ")", "if", "not", "is_param_in_hook_signature", "(", "step_hook_fx", ",", "\"", "batch_idx", "\"", ",", "min_args", "=", "2", ")", ":", "kwargs", ".", "pop", "(", "\"", "batch_idx", "\"", ",", "None", ")", "return", "tuple", "(", "kwargs", ".", "values", "(", ")", ")"], "docstring": "Helper method to build args for `test_step` or `validation_step`.", "docstring_tokens": ["helper", "method", "to", "build", "args", "for", "test_step", "or", "validation_step"], "docstring_summary": "Helper method to build args for `test_step` or `validation_step`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\evaluation_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_EvaluationLoop", "start_line": 490, "end_line": 496, "hash": "81f4d73e520778d47712b54d13bd07dd", "complexity": 2, "parameters": ["hook_kwargs", "step_hook_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "_should_reload_train_dl", "original_string": "def _should_reload_train_dl(self) -> bool:\r\n        \"\"\"Check if train dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return n_epochs and self.trainer.current_epoch - self._last_train_dl_reload_epoch >= n_epochs", "language": "python", "code": "def _should_reload_train_dl(self) -> bool:\r\n        \"\"\"Check if train dataloader should be reloaded.\"\"\"\r\n        n_epochs = self.trainer.reload_dataloaders_every_n_epochs\r\n        return n_epochs and self.trainer.current_epoch - self._last_train_dl_reload_epoch >= n_epochs", "code_tokens": ["def", "_should_reload_train_dl", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Check", "if", "train", "dataloader", "should", "be", "reloaded", ".", "\"", "\"", "\"", "n_epochs", "=", "self", ".", "trainer", ".", "reload_dataloaders_every_n_epochs", "return", "n_epochs", "and", "self", ".", "trainer", ".", "current_epoch", "-", "self", ".", "_last_train_dl_reload_epoch", ">", "=", "n_epochs"], "docstring": "Check if train dataloader should be reloaded.", "docstring_tokens": ["check", "if", "train", "dataloader", "should", "be", "reloaded"], "docstring_summary": "Check if train dataloader should be reloaded.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 166, "end_line": 169, "hash": "8e8d6c71c92a471c92887657802e97c1", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "done", "original_string": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self.max_batches == 0:\r\n            rank_zero_info(\"`Trainer.fit` stopped: No training batches.\")\r\n            return True\r\n\r\n        # TODO: Move track steps inside training loop and move part of these condition inside training loop\r\n        stop_steps = _is_max_limit_reached(self.epoch_loop.global_step, self.max_steps)\r\n        if stop_steps:\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_steps={self.max_steps!r}` reached.\")\r\n            return True\r\n\r\n        # `processed` is increased before `on_train_epoch_end`, the hook where checkpoints are typically saved.\r\n        # we use it here because the checkpoint data won't have `completed` increased yet\r\n        assert isinstance(self.max_epochs, int)\r\n        stop_epochs = _is_max_limit_reached(self.epoch_progress.current.processed, self.max_epochs)\r\n        if stop_epochs:\r\n            # in case they are not equal, override so `trainer.current_epoch` has the expected value\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_epochs={self.max_epochs!r}` reached.\")\r\n            return True\r\n\r\n        if self.trainer.should_stop and self._can_stop_early:\r\n            rank_zero_debug(\"`Trainer.fit` stopped: `trainer.should_stop` was set.\")\r\n            return True\r\n\r\n        return False", "language": "python", "code": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self.max_batches == 0:\r\n            rank_zero_info(\"`Trainer.fit` stopped: No training batches.\")\r\n            return True\r\n\r\n        # TODO: Move track steps inside training loop and move part of these condition inside training loop\r\n        stop_steps = _is_max_limit_reached(self.epoch_loop.global_step, self.max_steps)\r\n        if stop_steps:\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_steps={self.max_steps!r}` reached.\")\r\n            return True\r\n\r\n        # `processed` is increased before `on_train_epoch_end`, the hook where checkpoints are typically saved.\r\n        # we use it here because the checkpoint data won't have `completed` increased yet\r\n        assert isinstance(self.max_epochs, int)\r\n        stop_epochs = _is_max_limit_reached(self.epoch_progress.current.processed, self.max_epochs)\r\n        if stop_epochs:\r\n            # in case they are not equal, override so `trainer.current_epoch` has the expected value\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n            rank_zero_info(f\"`Trainer.fit` stopped: `max_epochs={self.max_epochs!r}` reached.\")\r\n            return True\r\n\r\n        if self.trainer.should_stop and self._can_stop_early:\r\n            rank_zero_debug(\"`Trainer.fit` stopped: `trainer.should_stop` was set.\")\r\n            return True\r\n\r\n        return False", "code_tokens": ["def", "done", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Evaluates", "when", "to", "leave", "the", "loop", ".", "\"", "\"", "\"", "if", "self", ".", "max_batches", "=", "=", "0", ":", "rank_zero_info", "(", "\"", "`", "Trainer", ".", "fit", "`", "stopped", ":", "No", "training", "batches", ".", "\"", ")", "return", "True", "stop_steps", "=", "_is_max_limit_reached", "(", "self", ".", "epoch_loop", ".", "global_step", ",", "self", ".", "max_steps", ")", "if", "stop_steps", ":", "rank_zero_info", "(", "f", "\"", "`", "Trainer", ".", "fit", "`", "stopped", ":", "`", "max_steps", "=", "{", "self", ".", "max_steps", "!", "r", "}", "`", "reached", ".", "\"", ")", "return", "True", "assert", "isinstance", "(", "self", ".", "max_epochs", ",", "int", ")", "stop_epochs", "=", "_is_max_limit_reached", "(", "self", ".", "epoch_progress", ".", "current", ".", "processed", ",", "self", ".", "max_epochs", ")", "if", "stop_epochs", ":", "self", ".", "epoch_progress", ".", "current", ".", "completed", "=", "self", ".", "epoch_progress", ".", "current", ".", "processed", "rank_zero_info", "(", "f", "\"", "`", "Trainer", ".", "fit", "`", "stopped", ":", "`", "max_epochs", "=", "{", "self", ".", "max_epochs", "!", "r", "}", "`", "reached", ".", "\"", ")", "return", "True", "if", "self", ".", "trainer", ".", "should_stop", "and", "self", ".", "_can_stop_early", ":", "rank_zero_debug", "(", "\"", "`", "Trainer", ".", "fit", "`", "stopped", ":", "`", "trainer", ".", "should_stop", "`", "was", "set", ".", "\"", ")", "return", "True", "return", "False"], "docstring": "Evaluates when to leave the loop.", "docstring_tokens": ["evaluates", "when", "to", "leave", "the", "loop"], "docstring_summary": "Evaluates when to leave the loop.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 172, "end_line": 198, "hash": "8375ecae7135c9703964072596ef1fb9", "complexity": 6, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "skip", "original_string": "def skip(self) -> bool:\r\n        \"\"\"Whether we should skip the training and immediately return from the call to :meth:`run`.\"\"\"\r\n        # if `limit_train_batches == 0` then `setup_data` won't set the `self.max_batches` attribute (checked in `done`)\r\n        # so we cannot use it solely\r\n        return self.done or self.trainer.limit_train_batches == 0", "language": "python", "code": "def skip(self) -> bool:\r\n        \"\"\"Whether we should skip the training and immediately return from the call to :meth:`run`.\"\"\"\r\n        # if `limit_train_batches == 0` then `setup_data` won't set the `self.max_batches` attribute (checked in `done`)\r\n        # so we cannot use it solely\r\n        return self.done or self.trainer.limit_train_batches == 0", "code_tokens": ["def", "skip", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Whether", "we", "should", "skip", "the", "training", "and", "immediately", "return", "from", "the", "call", "to", ":", "meth", ":", "`", "run", "`", ".", "\"", "\"", "\"", "return", "self", ".", "done", "or", "self", ".", "trainer", ".", "limit_train_batches", "=", "=", "0"], "docstring": "Whether we should skip the training and immediately return from the call to :meth:`run`.", "docstring_tokens": ["whether", "we", "should", "skip", "the", "training", "and", "immediately", "return", "from", "the", "call", "to", "meth", "run"], "docstring_summary": "Whether we should skip the training and immediately return from the call to :meth:`run`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 201, "end_line": 205, "hash": "a5d1cecf35e8424317e16e330edb6280", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "reset", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of this loop.\"\"\"\r\n        assert self.trainer.model is not None\r\n        torch.set_grad_enabled(True)\r\n\r\n        self.update_restart_stage()\r\n\r\n        if self.restarted_on_epoch_start:\r\n            self.epoch_progress.reset_on_restart()\r\n\r\n        if self.resumed_on_epoch_end:\r\n            # when restarting from last without validation at end of epoch,\r\n            # self.restarting is False but it's still resuming\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.restarted_mid_epoch\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_processed()\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n            and not self.restarted_mid_epoch\r\n            and not self.epoch_loop.val_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_completed()", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of this loop.\"\"\"\r\n        assert self.trainer.model is not None\r\n        torch.set_grad_enabled(True)\r\n\r\n        self.update_restart_stage()\r\n\r\n        if self.restarted_on_epoch_start:\r\n            self.epoch_progress.reset_on_restart()\r\n\r\n        if self.resumed_on_epoch_end:\r\n            # when restarting from last without validation at end of epoch,\r\n            # self.restarting is False but it's still resuming\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.restarted_mid_epoch\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_processed()\r\n            self.epoch_progress.increment_completed()\r\n\r\n        if (\r\n            self.epoch_loop.restarted_on_train_batch_end\r\n            and self.epoch_loop.batch_progress.is_last_batch\r\n            and not self.restarted_mid_epoch\r\n            and not self.epoch_loop.val_loop.batch_progress.is_last_batch\r\n        ):\r\n            self.epoch_progress.increment_completed()", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resets", "the", "internal", "state", "of", "this", "loop", ".", "\"", "\"", "\"", "assert", "self", ".", "trainer", ".", "model", "is", "not", "None", "torch", ".", "set_grad_enabled", "(", "True", ")", "self", ".", "update_restart_stage", "(", ")", "if", "self", ".", "restarted_on_epoch_start", ":", "self", ".", "epoch_progress", ".", "reset_on_restart", "(", ")", "if", "self", ".", "resumed_on_epoch_end", ":", "self", ".", "epoch_progress", ".", "increment_completed", "(", ")", "if", "(", "self", ".", "epoch_loop", ".", "restarted_on_train_batch_end", "and", "self", ".", "restarted_mid_epoch", "and", "self", ".", "epoch_loop", ".", "batch_progress", ".", "is_last_batch", ")", ":", "self", ".", "epoch_progress", ".", "increment_processed", "(", ")", "self", ".", "epoch_progress", ".", "increment_completed", "(", ")", "if", "(", "self", ".", "epoch_loop", ".", "restarted_on_train_batch_end", "and", "self", ".", "epoch_loop", ".", "batch_progress", ".", "is_last_batch", "and", "not", "self", ".", "restarted_mid_epoch", "and", "not", "self", ".", "epoch_loop", ".", "val_loop", ".", "batch_progress", ".", "is_last_batch", ")", ":", "self", ".", "epoch_progress", ".", "increment_completed", "(", ")"], "docstring": "Resets the internal state of this loop.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "this", "loop"], "docstring_summary": "Resets the internal state of this loop.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 378, "end_line": 407, "hash": "8ca889f3e6f58116676ce77b97b208fa", "complexity": 10, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "on_run_start", "original_string": "def on_run_start(self) -> None:\r\n        \"\"\"Calls the ``on_train_start`` hook.\"\"\"\r\n        # update the current_epoch in-case of checkpoint reload\r\n        if not self._iteration_based_training():\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n\r\n        trainer = self.trainer\r\n\r\n        # reload the evaluation dataloaders too for proper display in the progress bar\r\n        if self.epoch_loop._should_check_val_epoch() and trainer.val_dataloaders is None:\r\n            trainer.validating = True\r\n            self.epoch_loop.val_loop.setup_data()\r\n            trainer.training = True\r\n\r\n        # Check for modules in eval mode at training start\r\n        self._warn_if_modules_in_eval_mode()\r\n\r\n        call._call_callback_hooks(trainer, \"on_train_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_start\")\r\n        call._call_strategy_hook(trainer, \"on_train_start\")", "language": "python", "code": "def on_run_start(self) -> None:\r\n        \"\"\"Calls the ``on_train_start`` hook.\"\"\"\r\n        # update the current_epoch in-case of checkpoint reload\r\n        if not self._iteration_based_training():\r\n            self.epoch_progress.current.completed = self.epoch_progress.current.processed\r\n\r\n        trainer = self.trainer\r\n\r\n        # reload the evaluation dataloaders too for proper display in the progress bar\r\n        if self.epoch_loop._should_check_val_epoch() and trainer.val_dataloaders is None:\r\n            trainer.validating = True\r\n            self.epoch_loop.val_loop.setup_data()\r\n            trainer.training = True\r\n\r\n        # Check for modules in eval mode at training start\r\n        self._warn_if_modules_in_eval_mode()\r\n\r\n        call._call_callback_hooks(trainer, \"on_train_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_start\")\r\n        call._call_strategy_hook(trainer, \"on_train_start\")", "code_tokens": ["def", "on_run_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "the", "`", "`", "on_train_start", "`", "`", "hook", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_iteration_based_training", "(", ")", ":", "self", ".", "epoch_progress", ".", "current", ".", "completed", "=", "self", ".", "epoch_progress", ".", "current", ".", "processed", "trainer", "=", "self", ".", "trainer", "if", "self", ".", "epoch_loop", ".", "_should_check_val_epoch", "(", ")", "and", "trainer", ".", "val_dataloaders", "is", "None", ":", "trainer", ".", "validating", "=", "True", "self", ".", "epoch_loop", ".", "val_loop", ".", "setup_data", "(", ")", "trainer", ".", "training", "=", "True", "self", ".", "_warn_if_modules_in_eval_mode", "(", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_train_start", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_train_start", "\"", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "on_train_start", "\"", ")"], "docstring": "Calls the ``on_train_start`` hook.", "docstring_tokens": ["calls", "the", "on_train_start", "hook"], "docstring_summary": "Calls the ``on_train_start`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 409, "end_line": 428, "hash": "63bd8e78c4a73dbc4971585324a1d089", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "on_advance_start", "original_string": "def on_advance_start(self) -> None:\r\n        \"\"\"Prepares the dataloader for training and calls the hook ``on_train_epoch_start``\"\"\"\r\n        trainer = self.trainer\r\n\r\n        # might need to setup data again depending on `trainer.reload_dataloaders_every_n_epochs`\r\n        self.setup_data()\r\n\r\n        # update the epoch value for all samplers\r\n        assert self._combined_loader is not None\r\n        for i, dl in enumerate(self._combined_loader.flattened):\r\n            _set_sampler_epoch(dl, self.epoch_progress.current.processed)\r\n\r\n        if not self.restarted_mid_epoch and not self.restarted_on_epoch_end:\r\n            if not self.restarted_on_epoch_start:\r\n                self.epoch_progress.increment_ready()\r\n\r\n            call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n            call._call_lightning_module_hook(trainer, \"on_train_epoch_start\")\r\n\r\n            self.epoch_progress.increment_started()", "language": "python", "code": "def on_advance_start(self) -> None:\r\n        \"\"\"Prepares the dataloader for training and calls the hook ``on_train_epoch_start``\"\"\"\r\n        trainer = self.trainer\r\n\r\n        # might need to setup data again depending on `trainer.reload_dataloaders_every_n_epochs`\r\n        self.setup_data()\r\n\r\n        # update the epoch value for all samplers\r\n        assert self._combined_loader is not None\r\n        for i, dl in enumerate(self._combined_loader.flattened):\r\n            _set_sampler_epoch(dl, self.epoch_progress.current.processed)\r\n\r\n        if not self.restarted_mid_epoch and not self.restarted_on_epoch_end:\r\n            if not self.restarted_on_epoch_start:\r\n                self.epoch_progress.increment_ready()\r\n\r\n            call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n            call._call_lightning_module_hook(trainer, \"on_train_epoch_start\")\r\n\r\n            self.epoch_progress.increment_started()", "code_tokens": ["def", "on_advance_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Prepares", "the", "dataloader", "for", "training", "and", "calls", "the", "hook", "`", "`", "on_train_epoch_start", "`", "`", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "self", ".", "setup_data", "(", ")", "assert", "self", ".", "_combined_loader", "is", "not", "None", "for", "i", ",", "dl", "in", "enumerate", "(", "self", ".", "_combined_loader", ".", "flattened", ")", ":", "_set_sampler_epoch", "(", "dl", ",", "self", ".", "epoch_progress", ".", "current", ".", "processed", ")", "if", "not", "self", ".", "restarted_mid_epoch", "and", "not", "self", ".", "restarted_on_epoch_end", ":", "if", "not", "self", ".", "restarted_on_epoch_start", ":", "self", ".", "epoch_progress", ".", "increment_ready", "(", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_train_epoch_start", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_train_epoch_start", "\"", ")", "self", ".", "epoch_progress", ".", "increment_started", "(", ")"], "docstring": "Prepares the dataloader for training and calls the hook ``on_train_epoch_start``", "docstring_tokens": ["prepares", "the", "dataloader", "for", "training", "and", "calls", "the", "hook", "on_train_epoch_start"], "docstring_summary": "Prepares the dataloader for training and calls the hook ``on_train_epoch_start``", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 430, "end_line": 449, "hash": "35d3b1b0319b25f445ad365c651cd784", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "advance", "original_string": "def advance(self) -> None:\r\n        \"\"\"Runs one whole epoch.\"\"\"\r\n        log.debug(f\"{type(self).__name__}: advancing loop\")\r\n\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode == \"sequential\":\r\n            raise ValueError(\r\n                f'`{type(self).__name__}` does not support the `CombinedLoader(mode=\"sequential\")` mode.'\r\n                f\" The available modes are: {[m for m in _SUPPORTED_MODES if m != 'sequential']}\"\r\n            )\r\n        with self.trainer.profiler.profile(\"run_training_epoch\"):\r\n            assert self._data_fetcher is not None\r\n            self.epoch_loop.run(self._data_fetcher)", "language": "python", "code": "def advance(self) -> None:\r\n        \"\"\"Runs one whole epoch.\"\"\"\r\n        log.debug(f\"{type(self).__name__}: advancing loop\")\r\n\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode == \"sequential\":\r\n            raise ValueError(\r\n                f'`{type(self).__name__}` does not support the `CombinedLoader(mode=\"sequential\")` mode.'\r\n                f\" The available modes are: {[m for m in _SUPPORTED_MODES if m != 'sequential']}\"\r\n            )\r\n        with self.trainer.profiler.profile(\"run_training_epoch\"):\r\n            assert self._data_fetcher is not None\r\n            self.epoch_loop.run(self._data_fetcher)", "code_tokens": ["def", "advance", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "one", "whole", "epoch", ".", "\"", "\"", "\"", "log", ".", "debug", "(", "f", "\"", "{", "type", "(", "self", ")", ".", "__name__", "}", ":", "advancing", "loop", "\"", ")", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "if", "combined_loader", ".", "_mode", "=", "=", "\"", "sequential", "\"", ":", "raise", "ValueError", "(", "f", "'", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", "`", "does", "not", "support", "the", "`", "CombinedLoader", "(", "mode", "=", "\"", "sequential", "\"", ")", "`", "mode", ".", "'", "f", "\"", "The", "available", "modes", "are", ":", "{", "[", "m", "for", "m", "in", "_SUPPORTED_MODES", "if", "m", "!", "=", "'", "sequential", "'", "]", "}", "\"", ")", "with", "self", ".", "trainer", ".", "profiler", ".", "profile", "(", "\"", "run_training_epoch", "\"", ")", ":", "assert", "self", ".", "_data_fetcher", "is", "not", "None", "self", ".", "epoch_loop", ".", "run", "(", "self", ".", "_data_fetcher", ")"], "docstring": "Runs one whole epoch.", "docstring_tokens": ["runs", "one", "whole", "epoch"], "docstring_summary": "Runs one whole epoch.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 451, "end_line": 464, "hash": "6439d2fa392985091528b6a2436433e9", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "on_run_end", "original_string": "def on_run_end(self) -> None:\r\n        \"\"\"Calls the ``on_train_end`` hook.\"\"\"\r\n        log.debug(f\"{self.__class__.__name__}: train run ended\")\r\n\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_train_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_end\")\r\n        call._call_strategy_hook(trainer, \"on_train_end\")", "language": "python", "code": "def on_run_end(self) -> None:\r\n        \"\"\"Calls the ``on_train_end`` hook.\"\"\"\r\n        log.debug(f\"{self.__class__.__name__}: train run ended\")\r\n\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_train_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_train_end\")\r\n        call._call_strategy_hook(trainer, \"on_train_end\")", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "the", "`", "`", "on_train_end", "`", "`", "hook", ".", "\"", "\"", "\"", "log", ".", "debug", "(", "f", "\"", "{", "self", ".", "__class__", ".", "__name__", "}", ":", "train", "run", "ended", "\"", ")", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_train_end", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_train_end", "\"", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "on_train_end", "\"", ")"], "docstring": "Calls the ``on_train_end`` hook.", "docstring_tokens": ["calls", "the", "on_train_end", "hook"], "docstring_summary": "Calls the ``on_train_end`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 500, "end_line": 507, "hash": "24ba983af2238191b8217c66acd0a621", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\fit_loop.py", "func_name": "_warn_if_modules_in_eval_mode", "original_string": "def _warn_if_modules_in_eval_mode(self) -> None:\r\n        \"\"\"Warn if any modules are in eval mode at the start of training.\"\"\"\r\n        model = self.trainer.lightning_module\r\n        eval_modules = [name for name, module in model.named_modules() if not module.training]\r\n\r\n        if eval_modules:\r\n            rank_zero_warn(\r\n                f\"Found {len(eval_modules)} module(s) in eval mode at the start of training.\"\r\n                \" This may lead to unexpected behavior during training. If this is intentional,\"\r\n                \" you can ignore this warning.\",\r\n                category=PossibleUserWarning,\r\n            )", "language": "python", "code": "def _warn_if_modules_in_eval_mode(self) -> None:\r\n        \"\"\"Warn if any modules are in eval mode at the start of training.\"\"\"\r\n        model = self.trainer.lightning_module\r\n        eval_modules = [name for name, module in model.named_modules() if not module.training]\r\n\r\n        if eval_modules:\r\n            rank_zero_warn(\r\n                f\"Found {len(eval_modules)} module(s) in eval mode at the start of training.\"\r\n                \" This may lead to unexpected behavior during training. If this is intentional,\"\r\n                \" you can ignore this warning.\",\r\n                category=PossibleUserWarning,\r\n            )", "code_tokens": ["def", "_warn_if_modules_in_eval_mode", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Warn", "if", "any", "modules", "are", "in", "eval", "mode", "at", "the", "start", "of", "training", ".", "\"", "\"", "\"", "model", "=", "self", ".", "trainer", ".", "lightning_module", "eval_modules", "=", "[", "name", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", "if", "not", "module", ".", "training", "]", "if", "eval_modules", ":", "rank_zero_warn", "(", "f", "\"", "Found", "{", "len", "(", "eval_modules", ")", "}", "module", "(", "s", ")", "in", "eval", "mode", "at", "the", "start", "of", "training", ".", "\"", "\"", "This", "may", "lead", "to", "unexpected", "behavior", "during", "training", ".", "If", "this", "is", "intentional", ",", "\"", "\"", "you", "can", "ignore", "this", "warning", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")"], "docstring": "Warn if any modules are in eval mode at the start of training.", "docstring_tokens": ["warn", "if", "any", "modules", "are", "in", "eval", "mode", "at", "the", "start", "of", "training"], "docstring_summary": "Warn if any modules are in eval mode at the start of training.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\fit_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_FitLoop", "start_line": 527, "end_line": 538, "hash": "3098c319460119f5e9e487c6b1d0c302", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "restarting", "original_string": "def restarting(self, restarting: bool) -> None:\r\n        \"\"\"Connects this loop's restarting value and its children.\"\"\"\r\n        self._restarting = restarting\r\n        for loop in vars(self).values():\r\n            if isinstance(loop, _Loop):\r\n                loop.restarting = restarting", "language": "python", "code": "def restarting(self, restarting: bool) -> None:\r\n        \"\"\"Connects this loop's restarting value and its children.\"\"\"\r\n        self._restarting = restarting\r\n        for loop in vars(self).values():\r\n            if isinstance(loop, _Loop):\r\n                loop.restarting = restarting", "code_tokens": ["def", "restarting", "(", "self", ",", "restarting", ":", "bool", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Connects", "this", "loop", "'", "s", "restarting", "value", "and", "its", "children", ".", "\"", "\"", "\"", "self", ".", "_restarting", "=", "restarting", "for", "loop", "in", "vars", "(", "self", ")", ".", "values", "(", ")", ":", "if", "isinstance", "(", "loop", ",", "_Loop", ")", ":", "loop", ".", "restarting", "=", "restarting"], "docstring": "Connects this loop's restarting value and its children.", "docstring_tokens": ["connects", "this", "loop", "s", "restarting", "value", "and", "its", "children"], "docstring_summary": "Connects this loop's restarting value and its children.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\loop.py", "partition": "train", "function_type": "class_method", "class_name": "_Loop", "start_line": 34, "end_line": 39, "hash": "08e27f8b4b7d4fff109ce9d4c6fb6952", "complexity": 3, "parameters": ["restarting"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "on_save_checkpoint", "original_string": "def on_save_checkpoint(self) -> dict:\r\n        \"\"\"Called when saving a model checkpoint, use to persist loop state.\r\n\r\n        Returns:\r\n            The current loop state.\r\n\r\n        \"\"\"\r\n        return {}", "language": "python", "code": "def on_save_checkpoint(self) -> dict:\r\n        \"\"\"Called when saving a model checkpoint, use to persist loop state.\r\n\r\n        Returns:\r\n            The current loop state.\r\n\r\n        \"\"\"\r\n        return {}", "code_tokens": ["def", "on_save_checkpoint", "(", "self", ")", "-", ">", "dict", ":", "\"", "\"", "\"", "Called", "when", "saving", "a", "model", "checkpoint", ",", "use", "to", "persist", "loop", "state", ".", "Returns", ":", "The", "current", "loop", "state", ".", "\"", "\"", "\"", "return", "{", "}"], "docstring": "Called when saving a model checkpoint, use to persist loop state.\r\n\r\n        Returns:\r\n            The current loop state.", "docstring_tokens": ["called", "when", "saving", "a", "model", "checkpoint", "use", "to", "persist", "loop", "state", "returns", "the", "current", "loop", "state"], "docstring_summary": "Called when saving a model checkpoint, use to persist loop state.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\loop.py", "partition": "train", "function_type": "class_method", "class_name": "_Loop", "start_line": 49, "end_line": 56, "hash": "f5cb8853049b4401327186b7c151cf67", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "state_dict", "original_string": "def state_dict(self, destination: Optional[dict] = None, prefix: str = \"\") -> dict:\r\n        \"\"\"The state dict is determined by the state and progress of this loop and all its children.\r\n\r\n        Args:\r\n            destination: An existing dictionary to update with this loop's state. By default a new dictionary\r\n                is returned.\r\n            prefix: A prefix for each key in the state dictionary\r\n\r\n        \"\"\"\r\n        if destination is None:\r\n            destination = {}\r\n\r\n        destination[prefix + \"state_dict\"] = self.on_save_checkpoint()\r\n\r\n        for k, v in self.__dict__.items():\r\n            key = prefix + k\r\n            if isinstance(v, _BaseProgress):\r\n                destination[key] = v.state_dict()\r\n            elif isinstance(v, _Loop):\r\n                v.state_dict(destination, key + \".\")\r\n        return destination", "language": "python", "code": "def state_dict(self, destination: Optional[dict] = None, prefix: str = \"\") -> dict:\r\n        \"\"\"The state dict is determined by the state and progress of this loop and all its children.\r\n\r\n        Args:\r\n            destination: An existing dictionary to update with this loop's state. By default a new dictionary\r\n                is returned.\r\n            prefix: A prefix for each key in the state dictionary\r\n\r\n        \"\"\"\r\n        if destination is None:\r\n            destination = {}\r\n\r\n        destination[prefix + \"state_dict\"] = self.on_save_checkpoint()\r\n\r\n        for k, v in self.__dict__.items():\r\n            key = prefix + k\r\n            if isinstance(v, _BaseProgress):\r\n                destination[key] = v.state_dict()\r\n            elif isinstance(v, _Loop):\r\n                v.state_dict(destination, key + \".\")\r\n        return destination", "code_tokens": ["def", "state_dict", "(", "self", ",", "destination", ":", "Optional", "[", "dict", "]", "=", "None", ",", "prefix", ":", "str", "=", "\"", "\"", ")", "-", ">", "dict", ":", "\"", "\"", "\"", "The", "state", "dict", "is", "determined", "by", "the", "state", "and", "progress", "of", "this", "loop", "and", "all", "its", "children", ".", "Args", ":", "destination", ":", "An", "existing", "dictionary", "to", "update", "with", "this", "loop", "'", "s", "state", ".", "By", "default", "a", "new", "dictionary", "is", "returned", ".", "prefix", ":", "A", "prefix", "for", "each", "key", "in", "the", "state", "dictionary", "\"", "\"", "\"", "if", "destination", "is", "None", ":", "destination", "=", "{", "}", "destination", "[", "prefix", "+", "\"", "state_dict", "\"", "]", "=", "self", ".", "on_save_checkpoint", "(", ")", "for", "k", ",", "v", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "key", "=", "prefix", "+", "k", "if", "isinstance", "(", "v", ",", "_BaseProgress", ")", ":", "destination", "[", "key", "]", "=", "v", ".", "state_dict", "(", ")", "elif", "isinstance", "(", "v", ",", "_Loop", ")", ":", "v", ".", "state_dict", "(", "destination", ",", "key", "+", "\"", ".", "\"", ")", "return", "destination"], "docstring": "The state dict is determined by the state and progress of this loop and all its children.\r\n\r\n        Args:\r\n            destination: An existing dictionary to update with this loop's state. By default a new dictionary\r\n                is returned.\r\n            prefix: A prefix for each key in the state dictionary", "docstring_tokens": ["the", "state", "dict", "is", "determined", "by", "the", "state", "and", "progress", "of", "this", "loop", "and", "all", "its", "children", "args", "destination", "an", "existing", "dictionary", "to", "update", "with", "this", "loop", "s", "state", "by", "default", "a", "new", "dictionary", "is", "returned", "prefix", "a", "prefix", "for", "each", "key", "in", "the", "state", "dictionary"], "docstring_summary": "The state dict is determined by the state and progress of this loop and all its children.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\loop.py", "partition": "train", "function_type": "class_method", "class_name": "_Loop", "start_line": 61, "end_line": 81, "hash": "d864866b983add916ae1de4d9569e10c", "complexity": 5, "parameters": ["destination", "prefix"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\loop.py", "func_name": "load_state_dict", "original_string": "def load_state_dict(\r\n        self,\r\n        state_dict: dict,\r\n        prefix: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Loads the state of this loop and all its children.\"\"\"\r\n        self._load_from_state_dict(state_dict.copy(), prefix)\r\n        for k, v in self.__dict__.items():\r\n            if isinstance(v, _Loop):\r\n                v.load_state_dict(state_dict.copy(), prefix + k + \".\")\r\n        self.restarting = True\r\n        self._loaded_from_state_dict = True\r\n        self._resuming_from_checkpoint = True", "language": "python", "code": "def load_state_dict(\r\n        self,\r\n        state_dict: dict,\r\n        prefix: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Loads the state of this loop and all its children.\"\"\"\r\n        self._load_from_state_dict(state_dict.copy(), prefix)\r\n        for k, v in self.__dict__.items():\r\n            if isinstance(v, _Loop):\r\n                v.load_state_dict(state_dict.copy(), prefix + k + \".\")\r\n        self.restarting = True\r\n        self._loaded_from_state_dict = True\r\n        self._resuming_from_checkpoint = True", "code_tokens": ["def", "load_state_dict", "(", "self", ",", "state_dict", ":", "dict", ",", "prefix", ":", "str", "=", "\"", "\"", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Loads", "the", "state", "of", "this", "loop", "and", "all", "its", "children", ".", "\"", "\"", "\"", "self", ".", "_load_from_state_dict", "(", "state_dict", ".", "copy", "(", ")", ",", "prefix", ")", "for", "k", ",", "v", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "if", "isinstance", "(", "v", ",", "_Loop", ")", ":", "v", ".", "load_state_dict", "(", "state_dict", ".", "copy", "(", ")", ",", "prefix", "+", "k", "+", "\"", ".", "\"", ")", "self", ".", "restarting", "=", "True", "self", ".", "_loaded_from_state_dict", "=", "True", "self", ".", "_resuming_from_checkpoint", "=", "True"], "docstring": "Loads the state of this loop and all its children.", "docstring_tokens": ["loads", "the", "state", "of", "this", "loop", "and", "all", "its", "children"], "docstring_summary": "Loads the state of this loop and all its children.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\loop.py", "partition": "train", "function_type": "class_method", "class_name": "_Loop", "start_line": 83, "end_line": 95, "hash": "fec72c031ff1adb87e6595ad3b056e26", "complexity": 3, "parameters": ["state_dict", "prefix"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "predictions", "original_string": "def predictions(self) -> list[Any]:\r\n        \"\"\"The cached predictions.\"\"\"\r\n        if self._predictions == []:\r\n            return self._predictions\r\n        return self._predictions[0] if self.num_dataloaders == 1 else self._predictions", "language": "python", "code": "def predictions(self) -> list[Any]:\r\n        \"\"\"The cached predictions.\"\"\"\r\n        if self._predictions == []:\r\n            return self._predictions\r\n        return self._predictions[0] if self.num_dataloaders == 1 else self._predictions", "code_tokens": ["def", "predictions", "(", "self", ")", "-", ">", "list", "[", "Any", "]", ":", "\"", "\"", "\"", "The", "cached", "predictions", ".", "\"", "\"", "\"", "if", "self", ".", "_predictions", "=", "=", "[", "]", ":", "return", "self", ".", "_predictions", "return", "self", ".", "_predictions", "[", "0", "]", "if", "self", ".", "num_dataloaders", "=", "=", "1", "else", "self", ".", "_predictions"], "docstring": "The cached predictions.", "docstring_tokens": ["the", "cached", "predictions"], "docstring_summary": "The cached predictions.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 85, "end_line": 89, "hash": "9b5d01795ab208f446abf44cbaa8d131", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "num_dataloaders", "original_string": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "language": "python", "code": "def num_dataloaders(self) -> int:\r\n        \"\"\"Returns the number of prediction dataloaders.\"\"\"\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        return len(combined_loader.flattened)", "code_tokens": ["def", "num_dataloaders", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Returns", "the", "number", "of", "prediction", "dataloaders", ".", "\"", "\"", "\"", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "return", "len", "(", "combined_loader", ".", "flattened", ")"], "docstring": "Returns the number of prediction dataloaders.", "docstring_tokens": ["returns", "the", "number", "of", "prediction", "dataloaders"], "docstring_summary": "Returns the number of prediction dataloaders.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 92, "end_line": 96, "hash": "d70af2433fd1e06e186a3454e1377649", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "reset", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        self.batch_progress.reset_on_run()\r\n\r\n        assert self.trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(self.trainer, self.trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode != \"sequential\":\r\n            raise ValueError('`trainer.predict()` only supports the `CombinedLoader(mode=\"sequential\")` mode.')\r\n\r\n        # set the per-dataloader limits\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        # add the previous `fetched` value to properly track `is_last_batch` with no prefetching\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher\r\n\r\n        num_dataloaders = self.num_dataloaders\r\n        self.epoch_batch_indices = [[] for _ in range(num_dataloaders)]\r\n        self._predictions = [[] for _ in range(num_dataloaders)]", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        self.batch_progress.reset_on_run()\r\n\r\n        assert self.trainer.state.stage is not None\r\n        data_fetcher = _select_data_fetcher(self.trainer, self.trainer.state.stage)\r\n        combined_loader = self._combined_loader\r\n        assert combined_loader is not None\r\n        if combined_loader._mode != \"sequential\":\r\n            raise ValueError('`trainer.predict()` only supports the `CombinedLoader(mode=\"sequential\")` mode.')\r\n\r\n        # set the per-dataloader limits\r\n        combined_loader.limits = self.max_batches\r\n        data_fetcher.setup(combined_loader)\r\n        iter(data_fetcher)  # creates the iterator inside the fetcher\r\n\r\n        # add the previous `fetched` value to properly track `is_last_batch` with no prefetching\r\n        data_fetcher.fetched += self.batch_progress.current.ready\r\n        data_fetcher._start_profiler = self._on_before_fetch\r\n        data_fetcher._stop_profiler = self._on_after_fetch\r\n        self._data_fetcher = data_fetcher\r\n\r\n        num_dataloaders = self.num_dataloaders\r\n        self.epoch_batch_indices = [[] for _ in range(num_dataloaders)]\r\n        self._predictions = [[] for _ in range(num_dataloaders)]", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resets", "the", "internal", "state", "of", "the", "loop", "for", "a", "new", "run", ".", "\"", "\"", "\"", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "assert", "self", ".", "trainer", ".", "state", ".", "stage", "is", "not", "None", "data_fetcher", "=", "_select_data_fetcher", "(", "self", ".", "trainer", ",", "self", ".", "trainer", ".", "state", ".", "stage", ")", "combined_loader", "=", "self", ".", "_combined_loader", "assert", "combined_loader", "is", "not", "None", "if", "combined_loader", ".", "_mode", "!", "=", "\"", "sequential", "\"", ":", "raise", "ValueError", "(", "'", "`", "trainer", ".", "predict", "(", ")", "`", "only", "supports", "the", "`", "CombinedLoader", "(", "mode", "=", "\"", "sequential", "\"", ")", "`", "mode", ".", "'", ")", "combined_loader", ".", "limits", "=", "self", ".", "max_batches", "data_fetcher", ".", "setup", "(", "combined_loader", ")", "iter", "(", "data_fetcher", ")", "data_fetcher", ".", "fetched", "+", "=", "self", ".", "batch_progress", ".", "current", ".", "ready", "data_fetcher", ".", "_start_profiler", "=", "self", ".", "_on_before_fetch", "data_fetcher", ".", "_stop_profiler", "=", "self", ".", "_on_after_fetch", "self", ".", "_data_fetcher", "=", "data_fetcher", "num_dataloaders", "=", "self", ".", "num_dataloaders", "self", ".", "epoch_batch_indices", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_dataloaders", ")", "]", "self", ".", "_predictions", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_dataloaders", ")", "]"], "docstring": "Resets the internal state of the loop for a new run.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "the", "loop", "for", "a", "new", "run"], "docstring_summary": "Resets the internal state of the loop for a new run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 167, "end_line": 191, "hash": "b43f156e9beb7030148a9a2480c2e79b", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "on_run_start", "original_string": "def on_run_start(self) -> None:\r\n        \"\"\"Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_predict_model_eval()\r\n        self._on_predict_start()\r\n        self._on_predict_epoch_start()", "language": "python", "code": "def on_run_start(self) -> None:\r\n        \"\"\"Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.\"\"\"\r\n        self._verify_dataloader_idx_requirement()\r\n        self._on_predict_model_eval()\r\n        self._on_predict_start()\r\n        self._on_predict_epoch_start()", "code_tokens": ["def", "on_run_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "`", "`", "_on_predict_model_eval", "`", "`", ",", "`", "`", "_on_predict_start", "`", "`", "and", "`", "`", "_on_predict_epoch_start", "`", "`", "hooks", ".", "\"", "\"", "\"", "self", ".", "_verify_dataloader_idx_requirement", "(", ")", "self", ".", "_on_predict_model_eval", "(", ")", "self", ".", "_on_predict_start", "(", ")", "self", ".", "_on_predict_epoch_start", "(", ")"], "docstring": "Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.", "docstring_tokens": ["calls", "_on_predict_model_eval", "_on_predict_start", "and", "_on_predict_epoch_start", "hooks"], "docstring_summary": "Calls ``_on_predict_model_eval``, ``_on_predict_start`` and ``_on_predict_epoch_start`` hooks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 193, "end_line": 198, "hash": "0905b0879ba5f269e4158cf3e859e8f2", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "on_run_end", "original_string": "def on_run_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.\"\"\"\r\n        results = self._on_predict_epoch_end()\r\n        self._on_predict_end()\r\n        self._on_predict_model_train()\r\n        return results", "language": "python", "code": "def on_run_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.\"\"\"\r\n        results = self._on_predict_epoch_end()\r\n        self._on_predict_end()\r\n        self._on_predict_model_train()\r\n        return results", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "Optional", "[", "_PREDICT_OUTPUT", "]", ":", "\"", "\"", "\"", "Calls", "`", "`", "on_predict_epoch_end", "`", "`", "and", "`", "`", "on_predict_end", "`", "`", "hooks", "and", "returns", "results", "from", "all", "dataloaders", ".", "\"", "\"", "\"", "results", "=", "self", ".", "_on_predict_epoch_end", "(", ")", "self", ".", "_on_predict_end", "(", ")", "self", ".", "_on_predict_model_train", "(", ")", "return", "results"], "docstring": "Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.", "docstring_tokens": ["calls", "on_predict_epoch_end", "and", "on_predict_end", "hooks", "and", "returns", "results", "from", "all", "dataloaders"], "docstring_summary": "Calls ``on_predict_epoch_end`` and ``on_predict_end`` hooks and returns results from all dataloaders.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 200, "end_line": 205, "hash": "3bef429cb7e497ca9404c7d68ed63dfe", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_predict_step", "original_string": "def _predict_step(\r\n        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]\r\n    ) -> None:\r\n        \"\"\"Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        data_fetcher = self._data_fetcher\r\n        assert data_fetcher is not None\r\n\r\n        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        any_on_epoch = (\r\n            self._store_data_for_prediction_writer(batch_idx, dataloader_idx) if not using_dataloader_iter else False\r\n        )\r\n\r\n        # the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,\r\n        # so we need different kwargs\r\n        hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_started()\r\n\r\n        # configure step_kwargs\r\n        step_args = (\r\n            self._build_step_args_from_hook_kwargs(hook_kwargs, \"predict_step\")\r\n            if not using_dataloader_iter\r\n            else (dataloader_iter,)\r\n        )\r\n        predictions = call._call_strategy_hook(trainer, \"predict_step\", *step_args)\r\n        if predictions is None:\r\n            self._warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        if using_dataloader_iter:\r\n            # update the hook kwargs now that the step method might have consumed the iterator\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            dataloader_idx = data_fetcher._dataloader_idx\r\n            hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        if self._return_predictions or any_on_epoch:\r\n            self._predictions[dataloader_idx].append(move_data_to_device(predictions, torch.device(\"cpu\")))", "language": "python", "code": "def _predict_step(\r\n        self, batch: Any, batch_idx: int, dataloader_idx: int, dataloader_iter: Optional[Iterator]\r\n    ) -> None:\r\n        \"\"\"Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        data_fetcher = self._data_fetcher\r\n        assert data_fetcher is not None\r\n\r\n        if not (using_dataloader_iter := isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            batch = trainer.precision_plugin.convert_input(batch)\r\n            batch = trainer.lightning_module._on_before_batch_transfer(batch, dataloader_idx=dataloader_idx)\r\n            batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        any_on_epoch = (\r\n            self._store_data_for_prediction_writer(batch_idx, dataloader_idx) if not using_dataloader_iter else False\r\n        )\r\n\r\n        # the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,\r\n        # so we need different kwargs\r\n        hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_start\", *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_started()\r\n\r\n        # configure step_kwargs\r\n        step_args = (\r\n            self._build_step_args_from_hook_kwargs(hook_kwargs, \"predict_step\")\r\n            if not using_dataloader_iter\r\n            else (dataloader_iter,)\r\n        )\r\n        predictions = call._call_strategy_hook(trainer, \"predict_step\", *step_args)\r\n        if predictions is None:\r\n            self._warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\r\n\r\n        self.batch_progress.increment_processed()\r\n\r\n        if using_dataloader_iter:\r\n            # update the hook kwargs now that the step method might have consumed the iterator\r\n            batch = data_fetcher._batch\r\n            batch_idx = data_fetcher._batch_idx\r\n            dataloader_idx = data_fetcher._dataloader_idx\r\n            hook_kwargs = self._build_kwargs(batch, batch_idx, dataloader_idx if self.num_dataloaders > 1 else None)\r\n\r\n        call._call_callback_hooks(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n        call._call_lightning_module_hook(trainer, \"on_predict_batch_end\", predictions, *hook_kwargs.values())\r\n\r\n        self.batch_progress.increment_completed()\r\n\r\n        if self._return_predictions or any_on_epoch:\r\n            self._predictions[dataloader_idx].append(move_data_to_device(predictions, torch.device(\"cpu\")))", "code_tokens": ["def", "_predict_step", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "int", ",", "dataloader_iter", ":", "Optional", "[", "Iterator", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Runs", "the", "actual", "predict", "step", "together", "with", "all", "the", "necessary", "bookkeeping", "and", "the", "hooks", "tied", "to", "it", ".", "Args", ":", "batch", ":", "the", "current", "batch", "to", "run", "the", "prediction", "on", "batch_idx", ":", "The", "index", "of", "the", "current", "batch", ".", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "producing", "the", "current", "batch", ".", "dataloader_iter", ":", "The", "iterator", "if", "using", "this", "step", "flavor", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "data_fetcher", "=", "self", ".", "_data_fetcher", "assert", "data_fetcher", "is", "not", "None", "if", "not", "(", "using_dataloader_iter", ":", "=", "isinstance", "(", "data_fetcher", ",", "_DataLoaderIterDataFetcher", ")", ")", ":", "batch", "=", "trainer", ".", "precision_plugin", ".", "convert_input", "(", "batch", ")", "batch", "=", "trainer", ".", "lightning_module", ".", "_on_before_batch_transfer", "(", "batch", ",", "dataloader_idx", "=", "dataloader_idx", ")", "batch", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "batch_to_device", "\"", ",", "batch", ",", "dataloader_idx", "=", "dataloader_idx", ")", "self", ".", "batch_progress", ".", "increment_ready", "(", ")", "any_on_epoch", "=", "(", "self", ".", "_store_data_for_prediction_writer", "(", "batch_idx", ",", "dataloader_idx", ")", "if", "not", "using_dataloader_iter", "else", "False", ")", "hook_kwargs", "=", "self", ".", "_build_kwargs", "(", "batch", ",", "batch_idx", ",", "dataloader_idx", "if", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_predict_batch_start", "\"", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_predict_batch_start", "\"", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "self", ".", "batch_progress", ".", "increment_started", "(", ")", "step_args", "=", "(", "self", ".", "_build_step_args_from_hook_kwargs", "(", "hook_kwargs", ",", "\"", "predict_step", "\"", ")", "if", "not", "using_dataloader_iter", "else", "(", "dataloader_iter", ",", ")", ")", "predictions", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "predict_step", "\"", ",", "*", "step_args", ")", "if", "predictions", "is", "None", ":", "self", ".", "_warning_cache", ".", "warn", "(", "\"", "predict", "returned", "None", "if", "it", "was", "on", "purpose", ",", "ignore", "this", "warning", ".", ".", ".", "\"", ")", "self", ".", "batch_progress", ".", "increment_processed", "(", ")", "if", "using_dataloader_iter", ":", "batch", "=", "data_fetcher", ".", "_batch", "batch_idx", "=", "data_fetcher", ".", "_batch_idx", "dataloader_idx", "=", "data_fetcher", ".", "_dataloader_idx", "hook_kwargs", "=", "self", ".", "_build_kwargs", "(", "batch", ",", "batch_idx", ",", "dataloader_idx", "if", "self", ".", "num_dataloaders", ">", "1", "else", "None", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_predict_batch_end", "\"", ",", "predictions", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_predict_batch_end", "\"", ",", "predictions", ",", "*", "hook_kwargs", ".", "values", "(", ")", ")", "self", ".", "batch_progress", ".", "increment_completed", "(", ")", "if", "self", ".", "_return_predictions", "or", "any_on_epoch", ":", "self", ".", "_predictions", "[", "dataloader_idx", "]", ".", "append", "(", "move_data_to_device", "(", "predictions", ",", "torch", ".", "device", "(", "\"", "cpu", "\"", ")", ")", ")"], "docstring": "Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: The index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch.\r\n            dataloader_iter: The iterator if using this step flavor.", "docstring_tokens": ["runs", "the", "actual", "predict", "step", "together", "with", "all", "the", "necessary", "bookkeeping", "and", "the", "hooks", "tied", "to", "it", "args", "batch", "the", "current", "batch", "to", "run", "the", "prediction", "on", "batch_idx", "the", "index", "of", "the", "current", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "producing", "the", "current", "batch", "dataloader_iter", "the", "iterator", "if", "using", "this", "step", "flavor"], "docstring_summary": "Runs the actual predict step together with all the necessary bookkeeping and the hooks tied to it.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 212, "end_line": 273, "hash": "bcb4deb0c4b57be431e63a837f5ac0ce", "complexity": 10, "parameters": ["batch", "batch_idx", "dataloader_idx", "dataloader_iter"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_build_kwargs", "original_string": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Assembles the keyword arguments for the ``predict_step``\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the predict step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "language": "python", "code": "def _build_kwargs(self, batch: Any, batch_idx: int, dataloader_idx: Optional[int]) -> OrderedDict:\r\n        \"\"\"Assembles the keyword arguments for the ``predict_step``\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the predict step\r\n\r\n        \"\"\"\r\n        step_kwargs = OrderedDict([(\"batch\", batch), (\"batch_idx\", batch_idx)])\r\n        if dataloader_idx is not None:\r\n            step_kwargs[\"dataloader_idx\"] = dataloader_idx\r\n        return step_kwargs", "code_tokens": ["def", "_build_kwargs", "(", "self", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ",", "dataloader_idx", ":", "Optional", "[", "int", "]", ")", "-", ">", "OrderedDict", ":", "\"", "\"", "\"", "Assembles", "the", "keyword", "arguments", "for", "the", "`", "`", "predict_step", "`", "`", "Args", ":", "batch", ":", "the", "current", "batch", "to", "run", "the", "prediction", "on", "batch_idx", ":", "the", "index", "of", "the", "current", "batch", ".", "dataloader_idx", ":", "the", "index", "of", "the", "dataloader", "producing", "the", "current", "batch", ".", "None", "if", "not", "multiple", "dataloaders", "in", "sequential", "mode", ".", "Returns", ":", "the", "dictionary", "containing", "all", "the", "keyboard", "arguments", "for", "the", "predict", "step", "\"", "\"", "\"", "step_kwargs", "=", "OrderedDict", "(", "[", "(", "\"", "batch", "\"", ",", "batch", ")", ",", "(", "\"", "batch_idx", "\"", ",", "batch_idx", ")", "]", ")", "if", "dataloader_idx", "is", "not", "None", ":", "step_kwargs", "[", "\"", "dataloader_idx", "\"", "]", "=", "dataloader_idx", "return", "step_kwargs"], "docstring": "Assembles the keyword arguments for the ``predict_step``\r\n\r\n        Args:\r\n            batch: the current batch to run the prediction on\r\n            batch_idx: the index of the current batch.\r\n            dataloader_idx: the index of the dataloader producing the current batch. None if not multiple dataloaders\r\n                in sequential mode.\r\n\r\n        Returns:\r\n            the dictionary containing all the keyboard arguments for the predict step", "docstring_tokens": ["assembles", "the", "keyword", "arguments", "for", "the", "predict_step", "args", "batch", "the", "current", "batch", "to", "run", "the", "prediction", "on", "batch_idx", "the", "index", "of", "the", "current", "batch", "dataloader_idx", "the", "index", "of", "the", "dataloader", "producing", "the", "current", "batch", "none", "if", "not", "multiple", "dataloaders", "in", "sequential", "mode", "returns", "the", "dictionary", "containing", "all", "the", "keyboard", "arguments", "for", "the", "predict", "step"], "docstring_summary": "Assembles the keyword arguments for the ``predict_step``", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 275, "end_line": 291, "hash": "ac108c4b40e89b74c89dd92e39546691", "complexity": 2, "parameters": ["batch", "batch_idx", "dataloader_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_build_step_args_from_hook_kwargs", "original_string": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `predict_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "language": "python", "code": "def _build_step_args_from_hook_kwargs(self, hook_kwargs: OrderedDict, step_hook_name: str) -> tuple:\r\n        \"\"\"Helper method to build args for `predict_step`.\"\"\"\r\n        kwargs = hook_kwargs.copy()\r\n        step_hook_fx = getattr(self.trainer.lightning_module, step_hook_name)\r\n        if not is_param_in_hook_signature(step_hook_fx, \"batch_idx\", min_args=2):\r\n            kwargs.pop(\"batch_idx\", None)\r\n        return tuple(kwargs.values())", "code_tokens": ["def", "_build_step_args_from_hook_kwargs", "(", "self", ",", "hook_kwargs", ":", "OrderedDict", ",", "step_hook_name", ":", "str", ")", "-", ">", "tuple", ":", "\"", "\"", "\"", "Helper", "method", "to", "build", "args", "for", "`", "predict_step", "`", ".", "\"", "\"", "\"", "kwargs", "=", "hook_kwargs", ".", "copy", "(", ")", "step_hook_fx", "=", "getattr", "(", "self", ".", "trainer", ".", "lightning_module", ",", "step_hook_name", ")", "if", "not", "is_param_in_hook_signature", "(", "step_hook_fx", ",", "\"", "batch_idx", "\"", ",", "min_args", "=", "2", ")", ":", "kwargs", ".", "pop", "(", "\"", "batch_idx", "\"", ",", "None", ")", "return", "tuple", "(", "kwargs", ".", "values", "(", ")", ")"], "docstring": "Helper method to build args for `predict_step`.", "docstring_tokens": ["helper", "method", "to", "build", "args", "for", "predict_step"], "docstring_summary": "Helper method to build args for `predict_step`.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 293, "end_line": 299, "hash": "d7e4e98443308bea99d632d4a2d7e380", "complexity": 2, "parameters": ["hook_kwargs", "step_hook_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_get_batch_indices", "original_string": "def _get_batch_indices(self, dataloader: object) -> list[list[int]]:  # batches x samples\r\n        \"\"\"Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\r\n        :class:`~lightning.pytorch.overrides.distributed._IndexBatchSamplerWrapper`.\"\"\"\r\n        batch_sampler = getattr(dataloader, \"batch_sampler\", None)\r\n        if not isinstance(batch_sampler, _IndexBatchSamplerWrapper):\r\n            self._warning_cache.warn(\r\n                f\"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`\"\r\n            )\r\n            return []\r\n        return batch_sampler.seen_batch_indices", "language": "python", "code": "def _get_batch_indices(self, dataloader: object) -> list[list[int]]:  # batches x samples\r\n        \"\"\"Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\r\n        :class:`~lightning.pytorch.overrides.distributed._IndexBatchSamplerWrapper`.\"\"\"\r\n        batch_sampler = getattr(dataloader, \"batch_sampler\", None)\r\n        if not isinstance(batch_sampler, _IndexBatchSamplerWrapper):\r\n            self._warning_cache.warn(\r\n                f\"Couldn't infer the batch indices fetched from your dataloader: `{type(dataloader).__name__}`\"\r\n            )\r\n            return []\r\n        return batch_sampler.seen_batch_indices", "code_tokens": ["def", "_get_batch_indices", "(", "self", ",", "dataloader", ":", "object", ")", "-", ">", "list", "[", "list", "[", "int", "]", "]", ":", "\"", "\"", "\"", "Returns", "a", "reference", "to", "the", "seen", "batch", "indices", "if", "the", "dataloader", "has", "a", "batch", "sampler", "wrapped", "by", "our", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "overrides", ".", "distributed", ".", "_IndexBatchSamplerWrapper", "`", ".", "\"", "\"", "\"", "batch_sampler", "=", "getattr", "(", "dataloader", ",", "\"", "batch_sampler", "\"", ",", "None", ")", "if", "not", "isinstance", "(", "batch_sampler", ",", "_IndexBatchSamplerWrapper", ")", ":", "self", ".", "_warning_cache", ".", "warn", "(", "f", "\"", "Couldn", "'", "t", "infer", "the", "batch", "indices", "fetched", "from", "your", "dataloader", ":", "`", "{", "type", "(", "dataloader", ")", ".", "__name__", "}", "`", "\"", ")", "return", "[", "]", "return", "batch_sampler", ".", "seen_batch_indices"], "docstring": "Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our\r\n        :class:`~lightning.pytorch.overrides.distributed._IndexBatchSamplerWrapper`.", "docstring_tokens": ["returns", "a", "reference", "to", "the", "seen", "batch", "indices", "if", "the", "dataloader", "has", "a", "batch", "sampler", "wrapped", "by", "our", "class", "lightning", "pytorch", "overrides", "distributed", "_indexbatchsamplerwrapper"], "docstring_summary": "Returns a reference to the seen batch indices if the dataloader has a batch sampler wrapped by our", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 301, "end_line": 310, "hash": "c41c2ad8cb698ebdc09d7e08e0599d0e", "complexity": 2, "parameters": ["dataloader"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_on_predict_start", "original_string": "def _on_predict_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_start\")\r\n        call._call_strategy_hook(trainer, \"on_predict_start\")", "language": "python", "code": "def _on_predict_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_start\")\r\n        call._call_strategy_hook(trainer, \"on_predict_start\")", "code_tokens": ["def", "_on_predict_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "`", "`", "on_predict_start", "`", "`", "hooks", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_predict_start", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_predict_start", "\"", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "on_predict_start", "\"", ")"], "docstring": "Calls ``on_predict_start`` hooks.", "docstring_tokens": ["calls", "on_predict_start", "hooks"], "docstring_summary": "Calls ``on_predict_start`` hooks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 340, "end_line": 345, "hash": "b754b1924dd530ef60df7160f6a3b54a", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_on_predict_epoch_start", "original_string": "def _on_predict_epoch_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_start\")", "language": "python", "code": "def _on_predict_epoch_start(self) -> None:\r\n        \"\"\"Calls ``on_predict_epoch_start`` hooks.\"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_start\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_start\")", "code_tokens": ["def", "_on_predict_epoch_start", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "`", "`", "on_predict_epoch_start", "`", "`", "hooks", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_predict_epoch_start", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_predict_epoch_start", "\"", ")"], "docstring": "Calls ``on_predict_epoch_start`` hooks.", "docstring_tokens": ["calls", "on_predict_epoch_start", "hooks"], "docstring_summary": "Calls ``on_predict_epoch_start`` hooks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 354, "end_line": 358, "hash": "2718b59b8c367e35d9629083234b080d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_on_predict_epoch_end", "original_string": "def _on_predict_epoch_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` hook.\r\n\r\n        Returns:\r\n            the results for all dataloaders\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_end\")\r\n\r\n        if self.return_predictions:\r\n            return self.predictions\r\n        return None", "language": "python", "code": "def _on_predict_epoch_end(self) -> Optional[_PREDICT_OUTPUT]:\r\n        \"\"\"Calls ``on_predict_epoch_end`` hook.\r\n\r\n        Returns:\r\n            the results for all dataloaders\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_callback_hooks(trainer, \"on_predict_epoch_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_epoch_end\")\r\n\r\n        if self.return_predictions:\r\n            return self.predictions\r\n        return None", "code_tokens": ["def", "_on_predict_epoch_end", "(", "self", ")", "-", ">", "Optional", "[", "_PREDICT_OUTPUT", "]", ":", "\"", "\"", "\"", "Calls", "`", "`", "on_predict_epoch_end", "`", "`", "hook", ".", "Returns", ":", "the", "results", "for", "all", "dataloaders", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_predict_epoch_end", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_predict_epoch_end", "\"", ")", "if", "self", ".", "return_predictions", ":", "return", "self", ".", "predictions", "return", "None"], "docstring": "Calls ``on_predict_epoch_end`` hook.\r\n\r\n        Returns:\r\n            the results for all dataloaders", "docstring_tokens": ["calls", "on_predict_epoch_end", "hook", "returns", "the", "results", "for", "all", "dataloaders"], "docstring_summary": "Calls ``on_predict_epoch_end`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 360, "end_line": 373, "hash": "0a18fc32619df4787ea72e6464b39190", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\prediction_loop.py", "func_name": "_on_predict_end", "original_string": "def _on_predict_end(self) -> None:\r\n        \"\"\"Resets previous gradient status and calls ``on_predict_end`` hook.\"\"\"\r\n        if not self.return_predictions:\r\n            self._predictions = []\r\n        self.epoch_batch_indices = []\r\n\r\n        trainer = self.trainer\r\n        # hook\r\n        call._call_callback_hooks(trainer, \"on_predict_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_end\")\r\n        call._call_strategy_hook(trainer, \"on_predict_end\")", "language": "python", "code": "def _on_predict_end(self) -> None:\r\n        \"\"\"Resets previous gradient status and calls ``on_predict_end`` hook.\"\"\"\r\n        if not self.return_predictions:\r\n            self._predictions = []\r\n        self.epoch_batch_indices = []\r\n\r\n        trainer = self.trainer\r\n        # hook\r\n        call._call_callback_hooks(trainer, \"on_predict_end\")\r\n        call._call_lightning_module_hook(trainer, \"on_predict_end\")\r\n        call._call_strategy_hook(trainer, \"on_predict_end\")", "code_tokens": ["def", "_on_predict_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resets", "previous", "gradient", "status", "and", "calls", "`", "`", "on_predict_end", "`", "`", "hook", ".", "\"", "\"", "\"", "if", "not", "self", ".", "return_predictions", ":", "self", ".", "_predictions", "=", "[", "]", "self", ".", "epoch_batch_indices", "=", "[", "]", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_predict_end", "\"", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_predict_end", "\"", ")", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "on_predict_end", "\"", ")"], "docstring": "Resets previous gradient status and calls ``on_predict_end`` hook.", "docstring_tokens": ["resets", "previous", "gradient", "status", "and", "calls", "on_predict_end", "hook"], "docstring_summary": "Resets previous gradient status and calls ``on_predict_end`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\prediction_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_PredictionLoop", "start_line": 375, "end_line": 385, "hash": "dbb77e9878ddc31c58db5c82c6726d62", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\progress.py", "func_name": "reset_on_restart", "original_string": "def reset_on_restart(self) -> None:\r\n        \"\"\"Reset the progress on restart.\r\n\r\n        If there is a failure before all attributes are increased, restore the attributes to the last fully completed\r\n        value.\r\n\r\n        \"\"\"\r\n        self.ready = self.completed", "language": "python", "code": "def reset_on_restart(self) -> None:\r\n        \"\"\"Reset the progress on restart.\r\n\r\n        If there is a failure before all attributes are increased, restore the attributes to the last fully completed\r\n        value.\r\n\r\n        \"\"\"\r\n        self.ready = self.completed", "code_tokens": ["def", "reset_on_restart", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Reset", "the", "progress", "on", "restart", ".", "If", "there", "is", "a", "failure", "before", "all", "attributes", "are", "increased", ",", "restore", "the", "attributes", "to", "the", "last", "fully", "completed", "value", ".", "\"", "\"", "\"", "self", ".", "ready", "=", "self", ".", "completed"], "docstring": "Reset the progress on restart.\r\n\r\n        If there is a failure before all attributes are increased, restore the attributes to the last fully completed\r\n        value.", "docstring_tokens": ["reset", "the", "progress", "on", "restart", "if", "there", "is", "a", "failure", "before", "all", "attributes", "are", "increased", "restore", "the", "attributes", "to", "the", "last", "fully", "completed", "value"], "docstring_summary": "Reset the progress on restart.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\progress.py", "partition": "train", "function_type": "class_method", "class_name": "_ReadyCompletedTracker", "start_line": 60, "end_line": 67, "hash": "7367bb86d2e33e10525faf3eb7a5a622", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "total_batch_idx", "original_string": "def total_batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (across epochs)\"\"\"\r\n        # use `ready` instead of `completed` in case this is accessed after `completed` has been increased\r\n        # but before the next `ready` increase\r\n        return self.batch_progress.total.ready - 1", "language": "python", "code": "def total_batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (across epochs)\"\"\"\r\n        # use `ready` instead of `completed` in case this is accessed after `completed` has been increased\r\n        # but before the next `ready` increase\r\n        return self.batch_progress.total.ready - 1", "code_tokens": ["def", "total_batch_idx", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Returns", "the", "current", "batch", "index", "(", "across", "epochs", ")", "\"", "\"", "\"", "return", "self", ".", "batch_progress", ".", "total", ".", "ready", "-", "1"], "docstring": "Returns the current batch index (across epochs)", "docstring_tokens": ["returns", "the", "current", "batch", "index", "across", "epochs"], "docstring_summary": "Returns the current batch index (across epochs)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 98, "end_line": 102, "hash": "83a3728c2a0215e2a062a2c322627bee", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "batch_idx", "original_string": "def batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (within this epoch)\"\"\"\r\n        # use `ready` instead of `completed` in case this is accessed after `completed` has been increased\r\n        # but before the next `ready` increase\r\n        return self.batch_progress.current.ready - 1", "language": "python", "code": "def batch_idx(self) -> int:\r\n        \"\"\"Returns the current batch index (within this epoch)\"\"\"\r\n        # use `ready` instead of `completed` in case this is accessed after `completed` has been increased\r\n        # but before the next `ready` increase\r\n        return self.batch_progress.current.ready - 1", "code_tokens": ["def", "batch_idx", "(", "self", ")", "-", ">", "int", ":", "\"", "\"", "\"", "Returns", "the", "current", "batch", "index", "(", "within", "this", "epoch", ")", "\"", "\"", "\"", "return", "self", ".", "batch_progress", ".", "current", ".", "ready", "-", "1"], "docstring": "Returns the current batch index (within this epoch)", "docstring_tokens": ["returns", "the", "current", "batch", "index", "within", "this", "epoch"], "docstring_summary": "Returns the current batch index (within this epoch)", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 105, "end_line": 109, "hash": "f737650d60417f0036b9b81cae4c7f2d", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "done", "original_string": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self._is_training_done and self._is_validation_done:\r\n            return True\r\n\r\n        if self.trainer.should_stop:\r\n            # early stopping\r\n            min_epochs = self.trainer.fit_loop.min_epochs\r\n            can_stop_early = self.trainer.fit_loop._can_stop_early\r\n            if not can_stop_early:\r\n                self._warning_cache.info(\r\n                    f\"Trainer was signaled to stop but the required `min_epochs={min_epochs!r}` or\"\r\n                    f\" `min_steps={self.min_steps!r}` has not been met. Training will continue...\"\r\n                )\r\n            return can_stop_early\r\n\r\n        return False", "language": "python", "code": "def done(self) -> bool:\r\n        \"\"\"Evaluates when to leave the loop.\"\"\"\r\n        if self._is_training_done and self._is_validation_done:\r\n            return True\r\n\r\n        if self.trainer.should_stop:\r\n            # early stopping\r\n            min_epochs = self.trainer.fit_loop.min_epochs\r\n            can_stop_early = self.trainer.fit_loop._can_stop_early\r\n            if not can_stop_early:\r\n                self._warning_cache.info(\r\n                    f\"Trainer was signaled to stop but the required `min_epochs={min_epochs!r}` or\"\r\n                    f\" `min_steps={self.min_steps!r}` has not been met. Training will continue...\"\r\n                )\r\n            return can_stop_early\r\n\r\n        return False", "code_tokens": ["def", "done", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Evaluates", "when", "to", "leave", "the", "loop", ".", "\"", "\"", "\"", "if", "self", ".", "_is_training_done", "and", "self", ".", "_is_validation_done", ":", "return", "True", "if", "self", ".", "trainer", ".", "should_stop", ":", "min_epochs", "=", "self", ".", "trainer", ".", "fit_loop", ".", "min_epochs", "can_stop_early", "=", "self", ".", "trainer", ".", "fit_loop", ".", "_can_stop_early", "if", "not", "can_stop_early", ":", "self", ".", "_warning_cache", ".", "info", "(", "f", "\"", "Trainer", "was", "signaled", "to", "stop", "but", "the", "required", "`", "min_epochs", "=", "{", "min_epochs", "!", "r", "}", "`", "or", "\"", "f", "\"", "`", "min_steps", "=", "{", "self", ".", "min_steps", "!", "r", "}", "`", "has", "not", "been", "met", ".", "Training", "will", "continue", ".", ".", ".", "\"", ")", "return", "can_stop_early", "return", "False"], "docstring": "Evaluates when to leave the loop.", "docstring_tokens": ["evaluates", "when", "to", "leave", "the", "loop"], "docstring_summary": "Evaluates when to leave the loop.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 129, "end_line": 145, "hash": "7995197b14dcce4924a70746842fb9d6", "complexity": 5, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "reset", "original_string": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        if (\r\n            self.restarting\r\n            and not self._should_accumulate()\r\n            and (self.restarted_on_train_batch_end or not self.restarted_on_last)\r\n        ):\r\n            # batches_that_stepped is never set prior to saving a checkpoint, even when saving\r\n            # happens on_validation_end\r\n            # we could set it in the checkpoint but we prefer to keep checkpoints backward compatible\r\n            self._batches_that_stepped += 1\r\n\r\n        if self.restarted_on_train_batch_end:\r\n            self.batch_progress.increment_completed()\r\n            # handle situation in which save happened on_train_batch_end and epoch is at end\r\n            if self.batch_progress.current.completed >= self.trainer.num_training_batches:\r\n                self.batch_progress.reset_on_run()\r\n                self.scheduler_progress.reset_on_run()\r\n                self.automatic_optimization.optim_progress.reset_on_run()\r\n                self.val_loop.batch_progress.total.reset()\r\n\r\n        if self.restarting:\r\n            self.batch_progress.reset_on_restart()\r\n            self.scheduler_progress.reset_on_restart()\r\n            self.automatic_optimization.optim_progress.reset_on_restart()\r\n\r\n            trainer = self.trainer\r\n            if trainer.num_training_batches != float(\"inf\"):\r\n                expected_steps = math.ceil(trainer.num_training_batches / trainer.accumulate_grad_batches)\r\n                loader = trainer.fit_loop._combined_loader\r\n                assert loader is not None\r\n                is_resumable_loader = all(isinstance(loader, _Stateful) for loader in loader.flattened)\r\n                if self.global_step % expected_steps != 0 and not is_resumable_loader:\r\n                    rank_zero_warn(\r\n                        \"You're resuming from a checkpoint that ended before the epoch ended and your dataloader is\"\r\n                        \" not resumable. This can cause unreliable results if further training is done.\"\r\n                        \" Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing\"\r\n                        \" the `state_dict` / `load_state_dict` interface.\",\r\n                        category=PossibleUserWarning,\r\n                    )\r\n        else:\r\n            self.batch_progress.reset_on_run()\r\n            self.scheduler_progress.reset_on_run()\r\n            self.automatic_optimization.optim_progress.reset_on_run()\r\n            # when the epoch starts, the total val batch progress should be reset as it's supposed to count the batches\r\n            # seen per epoch, this is useful for tracking when validation is run multiple times per epoch\r\n            self.val_loop.batch_progress.total.reset()", "language": "python", "code": "def reset(self) -> None:\r\n        \"\"\"Resets the internal state of the loop for a new run.\"\"\"\r\n        if (\r\n            self.restarting\r\n            and not self._should_accumulate()\r\n            and (self.restarted_on_train_batch_end or not self.restarted_on_last)\r\n        ):\r\n            # batches_that_stepped is never set prior to saving a checkpoint, even when saving\r\n            # happens on_validation_end\r\n            # we could set it in the checkpoint but we prefer to keep checkpoints backward compatible\r\n            self._batches_that_stepped += 1\r\n\r\n        if self.restarted_on_train_batch_end:\r\n            self.batch_progress.increment_completed()\r\n            # handle situation in which save happened on_train_batch_end and epoch is at end\r\n            if self.batch_progress.current.completed >= self.trainer.num_training_batches:\r\n                self.batch_progress.reset_on_run()\r\n                self.scheduler_progress.reset_on_run()\r\n                self.automatic_optimization.optim_progress.reset_on_run()\r\n                self.val_loop.batch_progress.total.reset()\r\n\r\n        if self.restarting:\r\n            self.batch_progress.reset_on_restart()\r\n            self.scheduler_progress.reset_on_restart()\r\n            self.automatic_optimization.optim_progress.reset_on_restart()\r\n\r\n            trainer = self.trainer\r\n            if trainer.num_training_batches != float(\"inf\"):\r\n                expected_steps = math.ceil(trainer.num_training_batches / trainer.accumulate_grad_batches)\r\n                loader = trainer.fit_loop._combined_loader\r\n                assert loader is not None\r\n                is_resumable_loader = all(isinstance(loader, _Stateful) for loader in loader.flattened)\r\n                if self.global_step % expected_steps != 0 and not is_resumable_loader:\r\n                    rank_zero_warn(\r\n                        \"You're resuming from a checkpoint that ended before the epoch ended and your dataloader is\"\r\n                        \" not resumable. This can cause unreliable results if further training is done.\"\r\n                        \" Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing\"\r\n                        \" the `state_dict` / `load_state_dict` interface.\",\r\n                        category=PossibleUserWarning,\r\n                    )\r\n        else:\r\n            self.batch_progress.reset_on_run()\r\n            self.scheduler_progress.reset_on_run()\r\n            self.automatic_optimization.optim_progress.reset_on_run()\r\n            # when the epoch starts, the total val batch progress should be reset as it's supposed to count the batches\r\n            # seen per epoch, this is useful for tracking when validation is run multiple times per epoch\r\n            self.val_loop.batch_progress.total.reset()", "code_tokens": ["def", "reset", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Resets", "the", "internal", "state", "of", "the", "loop", "for", "a", "new", "run", ".", "\"", "\"", "\"", "if", "(", "self", ".", "restarting", "and", "not", "self", ".", "_should_accumulate", "(", ")", "and", "(", "self", ".", "restarted_on_train_batch_end", "or", "not", "self", ".", "restarted_on_last", ")", ")", ":", "self", ".", "_batches_that_stepped", "+", "=", "1", "if", "self", ".", "restarted_on_train_batch_end", ":", "self", ".", "batch_progress", ".", "increment_completed", "(", ")", "if", "self", ".", "batch_progress", ".", "current", ".", "completed", ">", "=", "self", ".", "trainer", ".", "num_training_batches", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "self", ".", "scheduler_progress", ".", "reset_on_run", "(", ")", "self", ".", "automatic_optimization", ".", "optim_progress", ".", "reset_on_run", "(", ")", "self", ".", "val_loop", ".", "batch_progress", ".", "total", ".", "reset", "(", ")", "if", "self", ".", "restarting", ":", "self", ".", "batch_progress", ".", "reset_on_restart", "(", ")", "self", ".", "scheduler_progress", ".", "reset_on_restart", "(", ")", "self", ".", "automatic_optimization", ".", "optim_progress", ".", "reset_on_restart", "(", ")", "trainer", "=", "self", ".", "trainer", "if", "trainer", ".", "num_training_batches", "!", "=", "float", "(", "\"", "inf", "\"", ")", ":", "expected_steps", "=", "math", ".", "ceil", "(", "trainer", ".", "num_training_batches", "/", "trainer", ".", "accumulate_grad_batches", ")", "loader", "=", "trainer", ".", "fit_loop", ".", "_combined_loader", "assert", "loader", "is", "not", "None", "is_resumable_loader", "=", "all", "(", "isinstance", "(", "loader", ",", "_Stateful", ")", "for", "loader", "in", "loader", ".", "flattened", ")", "if", "self", ".", "global_step", "%", "expected_steps", "!", "=", "0", "and", "not", "is_resumable_loader", ":", "rank_zero_warn", "(", "\"", "You", "'", "re", "resuming", "from", "a", "checkpoint", "that", "ended", "before", "the", "epoch", "ended", "and", "your", "dataloader", "is", "\"", "\"", "not", "resumable", ".", "This", "can", "cause", "unreliable", "results", "if", "further", "training", "is", "done", ".", "\"", "\"", "Consider", "using", "an", "end", "-", "of", "-", "epoch", "checkpoint", "or", "make", "your", "dataloader", "resumable", "by", "implementing", "\"", "\"", "the", "`", "state_dict", "`", "/", "`", "load_state_dict", "`", "interface", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "else", ":", "self", ".", "batch_progress", ".", "reset_on_run", "(", ")", "self", ".", "scheduler_progress", ".", "reset_on_run", "(", ")", "self", ".", "automatic_optimization", ".", "optim_progress", ".", "reset_on_run", "(", ")", "self", ".", "val_loop", ".", "batch_progress", ".", "total", ".", "reset", "(", ")"], "docstring": "Resets the internal state of the loop for a new run.", "docstring_tokens": ["resets", "the", "internal", "state", "of", "the", "loop", "for", "a", "new", "run"], "docstring_summary": "Resets the internal state of the loop for a new run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 190, "end_line": 236, "hash": "9af4e52122348c906966fea48330094b", "complexity": 12, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "_num_ready_batches_reached", "original_string": "def _num_ready_batches_reached(self) -> bool:\r\n        \"\"\"Checks if we are in the last batch or if there are more batches to follow.\"\"\"\r\n        epoch_finished_on_ready = self.batch_progress.current.ready == self.trainer.num_training_batches\r\n        return epoch_finished_on_ready or self.batch_progress.is_last_batch", "language": "python", "code": "def _num_ready_batches_reached(self) -> bool:\r\n        \"\"\"Checks if we are in the last batch or if there are more batches to follow.\"\"\"\r\n        epoch_finished_on_ready = self.batch_progress.current.ready == self.trainer.num_training_batches\r\n        return epoch_finished_on_ready or self.batch_progress.is_last_batch", "code_tokens": ["def", "_num_ready_batches_reached", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Checks", "if", "we", "are", "in", "the", "last", "batch", "or", "if", "there", "are", "more", "batches", "to", "follow", ".", "\"", "\"", "\"", "epoch_finished_on_ready", "=", "self", ".", "batch_progress", ".", "current", ".", "ready", "=", "=", "self", ".", "trainer", ".", "num_training_batches", "return", "epoch_finished_on_ready", "or", "self", ".", "batch_progress", ".", "is_last_batch"], "docstring": "Checks if we are in the last batch or if there are more batches to follow.", "docstring_tokens": ["checks", "if", "we", "are", "in", "the", "last", "batch", "or", "if", "there", "are", "more", "batches", "to", "follow"], "docstring_summary": "Checks if we are in the last batch or if there are more batches to follow.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 434, "end_line": 437, "hash": "244fd8d6913afa96645b5d68b25f97f9", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "_should_accumulate", "original_string": "def _should_accumulate(self) -> bool:\r\n        \"\"\"Checks if the optimizer step should be performed or gradients should be accumulated for the current step.\"\"\"\r\n        accumulation_done = self._accumulated_batches_reached()\r\n        # Lightning steps on the final batch\r\n        is_final_batch = self._num_ready_batches_reached()\r\n        # but the strategy might not\r\n        strategy_accumulates_on_final_batch = self.trainer.strategy.handles_gradient_accumulation or not is_final_batch\r\n        return not accumulation_done and strategy_accumulates_on_final_batch", "language": "python", "code": "def _should_accumulate(self) -> bool:\r\n        \"\"\"Checks if the optimizer step should be performed or gradients should be accumulated for the current step.\"\"\"\r\n        accumulation_done = self._accumulated_batches_reached()\r\n        # Lightning steps on the final batch\r\n        is_final_batch = self._num_ready_batches_reached()\r\n        # but the strategy might not\r\n        strategy_accumulates_on_final_batch = self.trainer.strategy.handles_gradient_accumulation or not is_final_batch\r\n        return not accumulation_done and strategy_accumulates_on_final_batch", "code_tokens": ["def", "_should_accumulate", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Checks", "if", "the", "optimizer", "step", "should", "be", "performed", "or", "gradients", "should", "be", "accumulated", "for", "the", "current", "step", ".", "\"", "\"", "\"", "accumulation_done", "=", "self", ".", "_accumulated_batches_reached", "(", ")", "is_final_batch", "=", "self", ".", "_num_ready_batches_reached", "(", ")", "strategy_accumulates_on_final_batch", "=", "self", ".", "trainer", ".", "strategy", ".", "handles_gradient_accumulation", "or", "not", "is_final_batch", "return", "not", "accumulation_done", "and", "strategy_accumulates_on_final_batch"], "docstring": "Checks if the optimizer step should be performed or gradients should be accumulated for the current step.", "docstring_tokens": ["checks", "if", "the", "optimizer", "step", "should", "be", "performed", "or", "gradients", "should", "be", "accumulated", "for", "the", "current", "step"], "docstring_summary": "Checks if the optimizer step should be performed or gradients should be accumulated for the current step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 439, "end_line": 446, "hash": "c4bfd4bc6eb8956ee7efe1e6fbb40f5b", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "update_lr_schedulers", "original_string": "def update_lr_schedulers(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Updates the lr schedulers based on the given interval.\"\"\"\r\n        if interval == \"step\" and self._should_accumulate():\r\n            return\r\n        self._update_learning_rates(interval=interval, update_plateau_schedulers=update_plateau_schedulers)", "language": "python", "code": "def update_lr_schedulers(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Updates the lr schedulers based on the given interval.\"\"\"\r\n        if interval == \"step\" and self._should_accumulate():\r\n            return\r\n        self._update_learning_rates(interval=interval, update_plateau_schedulers=update_plateau_schedulers)", "code_tokens": ["def", "update_lr_schedulers", "(", "self", ",", "interval", ":", "str", ",", "update_plateau_schedulers", ":", "bool", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Updates", "the", "lr", "schedulers", "based", "on", "the", "given", "interval", ".", "\"", "\"", "\"", "if", "interval", "=", "=", "\"", "step", "\"", "and", "self", ".", "_should_accumulate", "(", ")", ":", "return", "self", ".", "_update_learning_rates", "(", "interval", "=", "interval", ",", "update_plateau_schedulers", "=", "update_plateau_schedulers", ")"], "docstring": "Updates the lr schedulers based on the given interval.", "docstring_tokens": ["updates", "the", "lr", "schedulers", "based", "on", "the", "given", "interval"], "docstring_summary": "Updates the lr schedulers based on the given interval.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 448, "end_line": 452, "hash": "f2cdf55056ccb5ff33e2fabf7acd6183", "complexity": 3, "parameters": ["interval", "update_plateau_schedulers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "_update_learning_rates", "original_string": "def _update_learning_rates(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Update learning rates.\r\n\r\n        Args:\r\n            interval: either 'epoch' or 'step'.\r\n            update_plateau_schedulers: control whether ``ReduceLROnPlateau`` or non-plateau schedulers get updated.\r\n                This is used so non-plateau schedulers can be updated before running validation. Checkpoints are\r\n                commonly saved during validation, however, on-plateau schedulers might monitor a validation metric\r\n                so they have to be updated separately.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        if not trainer.lr_scheduler_configs or not trainer.lightning_module.automatic_optimization:\r\n            return\r\n\r\n        for config in trainer.lr_scheduler_configs:\r\n            if update_plateau_schedulers ^ config.reduce_on_plateau:\r\n                continue\r\n\r\n            current_idx = self.batch_idx if interval == \"step\" else trainer.current_epoch\r\n            current_idx += 1  # account for both batch and epoch starts from 0\r\n            # Take step if call to update_learning_rates matches the interval key and\r\n            # the current step modulo the schedulers frequency is zero\r\n            if config.interval == interval and current_idx % config.frequency == 0:\r\n                monitor_val = None\r\n                if config.reduce_on_plateau:\r\n                    monitor_key = config.monitor\r\n                    assert monitor_key is not None\r\n                    monitor_val = self._get_monitor_value(monitor_key)\r\n                    if monitor_val is None:\r\n                        if config.strict:\r\n                            avail_metrics = list(trainer.callback_metrics)\r\n                            raise MisconfigurationException(\r\n                                f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                                f\" which is not available. Available metrics are: {avail_metrics}.\"\r\n                                \" Condition can be set using `monitor` key in lr scheduler dict\"\r\n                            )\r\n                        rank_zero_warn(\r\n                            f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                            \" which is not available but strict is set to `False`.\"\r\n                            \" Skipping learning rate update.\",\r\n                            category=RuntimeWarning,\r\n                        )\r\n                        continue\r\n\r\n                self.scheduler_progress.increment_ready()\r\n\r\n                # update LR\r\n                call._call_lightning_module_hook(\r\n                    trainer,\r\n                    \"lr_scheduler_step\",\r\n                    config.scheduler,\r\n                    monitor_val,\r\n                )\r\n                self.scheduler_progress.increment_completed()", "language": "python", "code": "def _update_learning_rates(self, interval: str, update_plateau_schedulers: bool) -> None:\r\n        \"\"\"Update learning rates.\r\n\r\n        Args:\r\n            interval: either 'epoch' or 'step'.\r\n            update_plateau_schedulers: control whether ``ReduceLROnPlateau`` or non-plateau schedulers get updated.\r\n                This is used so non-plateau schedulers can be updated before running validation. Checkpoints are\r\n                commonly saved during validation, however, on-plateau schedulers might monitor a validation metric\r\n                so they have to be updated separately.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        if not trainer.lr_scheduler_configs or not trainer.lightning_module.automatic_optimization:\r\n            return\r\n\r\n        for config in trainer.lr_scheduler_configs:\r\n            if update_plateau_schedulers ^ config.reduce_on_plateau:\r\n                continue\r\n\r\n            current_idx = self.batch_idx if interval == \"step\" else trainer.current_epoch\r\n            current_idx += 1  # account for both batch and epoch starts from 0\r\n            # Take step if call to update_learning_rates matches the interval key and\r\n            # the current step modulo the schedulers frequency is zero\r\n            if config.interval == interval and current_idx % config.frequency == 0:\r\n                monitor_val = None\r\n                if config.reduce_on_plateau:\r\n                    monitor_key = config.monitor\r\n                    assert monitor_key is not None\r\n                    monitor_val = self._get_monitor_value(monitor_key)\r\n                    if monitor_val is None:\r\n                        if config.strict:\r\n                            avail_metrics = list(trainer.callback_metrics)\r\n                            raise MisconfigurationException(\r\n                                f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                                f\" which is not available. Available metrics are: {avail_metrics}.\"\r\n                                \" Condition can be set using `monitor` key in lr scheduler dict\"\r\n                            )\r\n                        rank_zero_warn(\r\n                            f\"ReduceLROnPlateau conditioned on metric {monitor_key}\"\r\n                            \" which is not available but strict is set to `False`.\"\r\n                            \" Skipping learning rate update.\",\r\n                            category=RuntimeWarning,\r\n                        )\r\n                        continue\r\n\r\n                self.scheduler_progress.increment_ready()\r\n\r\n                # update LR\r\n                call._call_lightning_module_hook(\r\n                    trainer,\r\n                    \"lr_scheduler_step\",\r\n                    config.scheduler,\r\n                    monitor_val,\r\n                )\r\n                self.scheduler_progress.increment_completed()", "code_tokens": ["def", "_update_learning_rates", "(", "self", ",", "interval", ":", "str", ",", "update_plateau_schedulers", ":", "bool", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Update", "learning", "rates", ".", "Args", ":", "interval", ":", "either", "'", "epoch", "'", "or", "'", "step", "'", ".", "update_plateau_schedulers", ":", "control", "whether", "`", "`", "ReduceLROnPlateau", "`", "`", "or", "non", "-", "plateau", "schedulers", "get", "updated", ".", "This", "is", "used", "so", "non", "-", "plateau", "schedulers", "can", "be", "updated", "before", "running", "validation", ".", "Checkpoints", "are", "commonly", "saved", "during", "validation", ",", "however", ",", "on", "-", "plateau", "schedulers", "might", "monitor", "a", "validation", "metric", "so", "they", "have", "to", "be", "updated", "separately", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "if", "not", "trainer", ".", "lr_scheduler_configs", "or", "not", "trainer", ".", "lightning_module", ".", "automatic_optimization", ":", "return", "for", "config", "in", "trainer", ".", "lr_scheduler_configs", ":", "if", "update_plateau_schedulers", "^", "config", ".", "reduce_on_plateau", ":", "continue", "current_idx", "=", "self", ".", "batch_idx", "if", "interval", "=", "=", "\"", "step", "\"", "else", "trainer", ".", "current_epoch", "current_idx", "+", "=", "1", "if", "config", ".", "interval", "=", "=", "interval", "and", "current_idx", "%", "config", ".", "frequency", "=", "=", "0", ":", "monitor_val", "=", "None", "if", "config", ".", "reduce_on_plateau", ":", "monitor_key", "=", "config", ".", "monitor", "assert", "monitor_key", "is", "not", "None", "monitor_val", "=", "self", ".", "_get_monitor_value", "(", "monitor_key", ")", "if", "monitor_val", "is", "None", ":", "if", "config", ".", "strict", ":", "avail_metrics", "=", "list", "(", "trainer", ".", "callback_metrics", ")", "raise", "MisconfigurationException", "(", "f", "\"", "ReduceLROnPlateau", "conditioned", "on", "metric", "{", "monitor_key", "}", "\"", "f", "\"", "which", "is", "not", "available", ".", "Available", "metrics", "are", ":", "{", "avail_metrics", "}", ".", "\"", "\"", "Condition", "can", "be", "set", "using", "`", "monitor", "`", "key", "in", "lr", "scheduler", "dict", "\"", ")", "rank_zero_warn", "(", "f", "\"", "ReduceLROnPlateau", "conditioned", "on", "metric", "{", "monitor_key", "}", "\"", "\"", "which", "is", "not", "available", "but", "strict", "is", "set", "to", "`", "False", "`", ".", "\"", "\"", "Skipping", "learning", "rate", "update", ".", "\"", ",", "category", "=", "RuntimeWarning", ",", ")", "continue", "self", ".", "scheduler_progress", ".", "increment_ready", "(", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "lr_scheduler_step", "\"", ",", "config", ".", "scheduler", ",", "monitor_val", ",", ")", "self", ".", "scheduler_progress", ".", "increment_completed", "(", ")"], "docstring": "Update learning rates.\r\n\r\n        Args:\r\n            interval: either 'epoch' or 'step'.\r\n            update_plateau_schedulers: control whether ``ReduceLROnPlateau`` or non-plateau schedulers get updated.\r\n                This is used so non-plateau schedulers can be updated before running validation. Checkpoints are\r\n                commonly saved during validation, however, on-plateau schedulers might monitor a validation metric\r\n                so they have to be updated separately.", "docstring_tokens": ["update", "learning", "rates", "args", "interval", "either", "epoch", "or", "step", "update_plateau_schedulers", "control", "whether", "reducelronplateau", "or", "non", "plateau", "schedulers", "get", "updated", "this", "is", "used", "so", "non", "plateau", "schedulers", "can", "be", "updated", "before", "running", "validation", "checkpoints", "are", "commonly", "saved", "during", "validation", "however", "on", "plateau", "schedulers", "might", "monitor", "a", "validation", "metric", "so", "they", "have", "to", "be", "updated", "separately"], "docstring_summary": "Update learning rates.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 454, "end_line": 509, "hash": "c67b0d46492ec617500f41e333eff859", "complexity": 11, "parameters": ["interval", "update_plateau_schedulers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "_should_check_val_fx", "original_string": "def _should_check_val_fx(self, data_fetcher: _DataFetcher) -> bool:\r\n        \"\"\"Decide if we should run validation.\"\"\"\r\n        if not self._should_check_val_epoch():\r\n            return False\r\n\r\n        # val_check_batch is inf for iterable datasets with no length defined\r\n        is_infinite_dataset = self.trainer.val_check_batch == float(\"inf\")\r\n        is_last_batch = self.batch_progress.is_last_batch\r\n        if is_last_batch and (is_infinite_dataset or isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            return True\r\n\r\n        if self.trainer.should_stop and self.trainer.fit_loop._can_stop_early:\r\n            # allow validation if requesting to stop early through `Trainer.should_stop` (e.g. by early stopping)\r\n            # and when the loop allows to stop (min_epochs/steps met)\r\n            return True\r\n\r\n        interval = self.trainer._val_check_time_interval\r\n        if interval is not None:\r\n            now = time.monotonic()\r\n            # if time\u2019s up \u2192 tell Trainer to validate\r\n            return now - self.trainer._last_val_time >= interval\r\n        # TODO: let training/eval loop handle logic around limit_*_batches and val_check_batch\r\n        is_val_check_batch = is_last_batch\r\n        if isinstance(self.trainer.limit_train_batches, int) and is_infinite_dataset:\r\n            is_val_check_batch = (self.batch_idx + 1) % self.trainer.limit_train_batches == 0\r\n        elif self.trainer.val_check_batch != float(\"inf\"):\r\n            # if we got here, we\u2019re in batch-based mode, so this can\u2019t be None\r\n            assert self.trainer.val_check_batch is not None\r\n            # if `check_val_every_n_epoch is `None`, run a validation loop every n training batches\r\n            # else condition it based on the batch_idx of the current epoch\r\n            current_iteration = self.total_batch_idx if self.trainer.check_val_every_n_epoch is None else self.batch_idx\r\n            is_val_check_batch = (current_iteration + 1) % self.trainer.val_check_batch == 0\r\n\r\n        return is_val_check_batch", "language": "python", "code": "def _should_check_val_fx(self, data_fetcher: _DataFetcher) -> bool:\r\n        \"\"\"Decide if we should run validation.\"\"\"\r\n        if not self._should_check_val_epoch():\r\n            return False\r\n\r\n        # val_check_batch is inf for iterable datasets with no length defined\r\n        is_infinite_dataset = self.trainer.val_check_batch == float(\"inf\")\r\n        is_last_batch = self.batch_progress.is_last_batch\r\n        if is_last_batch and (is_infinite_dataset or isinstance(data_fetcher, _DataLoaderIterDataFetcher)):\r\n            return True\r\n\r\n        if self.trainer.should_stop and self.trainer.fit_loop._can_stop_early:\r\n            # allow validation if requesting to stop early through `Trainer.should_stop` (e.g. by early stopping)\r\n            # and when the loop allows to stop (min_epochs/steps met)\r\n            return True\r\n\r\n        interval = self.trainer._val_check_time_interval\r\n        if interval is not None:\r\n            now = time.monotonic()\r\n            # if time\u2019s up \u2192 tell Trainer to validate\r\n            return now - self.trainer._last_val_time >= interval\r\n        # TODO: let training/eval loop handle logic around limit_*_batches and val_check_batch\r\n        is_val_check_batch = is_last_batch\r\n        if isinstance(self.trainer.limit_train_batches, int) and is_infinite_dataset:\r\n            is_val_check_batch = (self.batch_idx + 1) % self.trainer.limit_train_batches == 0\r\n        elif self.trainer.val_check_batch != float(\"inf\"):\r\n            # if we got here, we\u2019re in batch-based mode, so this can\u2019t be None\r\n            assert self.trainer.val_check_batch is not None\r\n            # if `check_val_every_n_epoch is `None`, run a validation loop every n training batches\r\n            # else condition it based on the batch_idx of the current epoch\r\n            current_iteration = self.total_batch_idx if self.trainer.check_val_every_n_epoch is None else self.batch_idx\r\n            is_val_check_batch = (current_iteration + 1) % self.trainer.val_check_batch == 0\r\n\r\n        return is_val_check_batch", "code_tokens": ["def", "_should_check_val_fx", "(", "self", ",", "data_fetcher", ":", "_DataFetcher", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Decide", "if", "we", "should", "run", "validation", ".", "\"", "\"", "\"", "if", "not", "self", ".", "_should_check_val_epoch", "(", ")", ":", "return", "False", "is_infinite_dataset", "=", "self", ".", "trainer", ".", "val_check_batch", "=", "=", "float", "(", "\"", "inf", "\"", ")", "is_last_batch", "=", "self", ".", "batch_progress", ".", "is_last_batch", "if", "is_last_batch", "and", "(", "is_infinite_dataset", "or", "isinstance", "(", "data_fetcher", ",", "_DataLoaderIterDataFetcher", ")", ")", ":", "return", "True", "if", "self", ".", "trainer", ".", "should_stop", "and", "self", ".", "trainer", ".", "fit_loop", ".", "_can_stop_early", ":", "return", "True", "interval", "=", "self", ".", "trainer", ".", "_val_check_time_interval", "if", "interval", "is", "not", "None", ":", "now", "=", "time", ".", "monotonic", "(", ")", "return", "now", "-", "self", ".", "trainer", ".", "_last_val_time", ">", "=", "interval", "is_val_check_batch", "=", "is_last_batch", "if", "isinstance", "(", "self", ".", "trainer", ".", "limit_train_batches", ",", "int", ")", "and", "is_infinite_dataset", ":", "is_val_check_batch", "=", "(", "self", ".", "batch_idx", "+", "1", ")", "%", "self", ".", "trainer", ".", "limit_train_batches", "=", "=", "0", "elif", "self", ".", "trainer", ".", "val_check_batch", "!", "=", "float", "(", "\"", "inf", "\"", ")", ":", "assert", "self", ".", "trainer", ".", "val_check_batch", "is", "not", "None", "current_iteration", "=", "self", ".", "total_batch_idx", "if", "self", ".", "trainer", ".", "check_val_every_n_epoch", "is", "None", "else", "self", ".", "batch_idx", "is_val_check_batch", "=", "(", "current_iteration", "+", "1", ")", "%", "self", ".", "trainer", ".", "val_check_batch", "=", "=", "0", "return", "is_val_check_batch"], "docstring": "Decide if we should run validation.", "docstring_tokens": ["decide", "if", "we", "should", "run", "validation"], "docstring_summary": "Decide if we should run validation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 521, "end_line": 554, "hash": "68230d74e95d35726c3dce794370d34b", "complexity": 12, "parameters": ["data_fetcher"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "_save_loggers_on_train_batch_end", "original_string": "def _save_loggers_on_train_batch_end(self) -> None:\r\n        \"\"\"Flushes loggers to disk.\"\"\"\r\n        if self.trainer.should_stop:\r\n            for logger in self.trainer.loggers:\r\n                logger.save()", "language": "python", "code": "def _save_loggers_on_train_batch_end(self) -> None:\r\n        \"\"\"Flushes loggers to disk.\"\"\"\r\n        if self.trainer.should_stop:\r\n            for logger in self.trainer.loggers:\r\n                logger.save()", "code_tokens": ["def", "_save_loggers_on_train_batch_end", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Flushes", "loggers", "to", "disk", ".", "\"", "\"", "\"", "if", "self", ".", "trainer", ".", "should_stop", ":", "for", "logger", "in", "self", ".", "trainer", ".", "loggers", ":", "logger", ".", "save", "(", ")"], "docstring": "Flushes loggers to disk.", "docstring_tokens": ["flushes", "loggers", "to", "disk"], "docstring_summary": "Flushes loggers to disk.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 556, "end_line": 560, "hash": "a11e69669019cf938fe7d42dbe6f4de6", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "func_name": "_build_kwargs", "original_string": "def _build_kwargs(self, kwargs: OrderedDict, batch: Any, batch_idx: int) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n            batch: The current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n\r\n        Returns:\r\n            The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        kwargs[\"batch\"] = batch\r\n        training_step_fx = getattr(self.trainer.lightning_module, \"training_step\")\r\n        # the `batch_idx` is optional, but its name can be anything\r\n        # as long as there are two arguments after 'self', we assume they are the `batch` and `batch_idx`\r\n        if is_param_in_hook_signature(training_step_fx, \"batch_idx\", min_args=2):\r\n            kwargs[\"batch_idx\"] = batch_idx\r\n        return kwargs", "language": "python", "code": "def _build_kwargs(self, kwargs: OrderedDict, batch: Any, batch_idx: int) -> OrderedDict:\r\n        \"\"\"Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n            batch: The current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n\r\n        Returns:\r\n            The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        kwargs[\"batch\"] = batch\r\n        training_step_fx = getattr(self.trainer.lightning_module, \"training_step\")\r\n        # the `batch_idx` is optional, but its name can be anything\r\n        # as long as there are two arguments after 'self', we assume they are the `batch` and `batch_idx`\r\n        if is_param_in_hook_signature(training_step_fx, \"batch_idx\", min_args=2):\r\n            kwargs[\"batch_idx\"] = batch_idx\r\n        return kwargs", "code_tokens": ["def", "_build_kwargs", "(", "self", ",", "kwargs", ":", "OrderedDict", ",", "batch", ":", "Any", ",", "batch_idx", ":", "int", ")", "-", ">", "OrderedDict", ":", "\"", "\"", "\"", "Helper", "method", "to", "build", "the", "arguments", "for", "the", "current", "step", ".", "Args", ":", "kwargs", ":", "The", "kwargs", "passed", "down", "to", "the", "hooks", ".", "batch", ":", "The", "current", "batch", "to", "run", "through", "the", "step", ".", "batch_idx", ":", "the", "index", "of", "the", "current", "batch", ".", "Returns", ":", "The", "kwargs", "passed", "down", "to", "the", "hooks", ".", "\"", "\"", "\"", "kwargs", "[", "\"", "batch", "\"", "]", "=", "batch", "training_step_fx", "=", "getattr", "(", "self", ".", "trainer", ".", "lightning_module", ",", "\"", "training_step", "\"", ")", "if", "is_param_in_hook_signature", "(", "training_step_fx", ",", "\"", "batch_idx", "\"", ",", "min_args", "=", "2", ")", ":", "kwargs", "[", "\"", "batch_idx", "\"", "]", "=", "batch_idx", "return", "kwargs"], "docstring": "Helper method to build the arguments for the current step.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n            batch: The current batch to run through the step.\r\n            batch_idx: the index of the current batch.\r\n\r\n        Returns:\r\n            The kwargs passed down to the hooks.", "docstring_tokens": ["helper", "method", "to", "build", "the", "arguments", "for", "the", "current", "step", "args", "kwargs", "the", "kwargs", "passed", "down", "to", "the", "hooks", "batch", "the", "current", "batch", "to", "run", "through", "the", "step", "batch_idx", "the", "index", "of", "the", "current", "batch", "returns", "the", "kwargs", "passed", "down", "to", "the", "hooks"], "docstring_summary": "Helper method to build the arguments for the current step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\training_epoch_loop.py", "partition": "train", "function_type": "class_method", "class_name": "_TrainingEpochLoop", "start_line": 562, "end_line": 580, "hash": "321364aa388f5074b231de18b497afee", "complexity": 2, "parameters": ["kwargs", "batch", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "check_finite_loss", "original_string": "def check_finite_loss(loss: Optional[Tensor]) -> None:\r\n    \"\"\"Checks for finite loss value.\r\n\r\n    Args:\r\n        loss: the loss value to check to be finite\r\n\r\n    \"\"\"\r\n    if loss is not None and not torch.isfinite(loss).all():\r\n        raise ValueError(f\"The loss returned in `training_step` is {loss}.\")", "language": "python", "code": "def check_finite_loss(loss: Optional[Tensor]) -> None:\r\n    \"\"\"Checks for finite loss value.\r\n\r\n    Args:\r\n        loss: the loss value to check to be finite\r\n\r\n    \"\"\"\r\n    if loss is not None and not torch.isfinite(loss).all():\r\n        raise ValueError(f\"The loss returned in `training_step` is {loss}.\")", "code_tokens": ["def", "check_finite_loss", "(", "loss", ":", "Optional", "[", "Tensor", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Checks", "for", "finite", "loss", "value", ".", "Args", ":", "loss", ":", "the", "loss", "value", "to", "check", "to", "be", "finite", "\"", "\"", "\"", "if", "loss", "is", "not", "None", "and", "not", "torch", ".", "isfinite", "(", "loss", ")", ".", "all", "(", ")", ":", "raise", "ValueError", "(", "f", "\"", "The", "loss", "returned", "in", "`", "training_step", "`", "is", "{", "loss", "}", ".", "\"", ")"], "docstring": "Checks for finite loss value.\r\n\r\n    Args:\r\n        loss: the loss value to check to be finite", "docstring_tokens": ["checks", "for", "finite", "loss", "value", "args", "loss", "the", "loss", "value", "to", "check", "to", "be", "finite"], "docstring_summary": "Checks for finite loss value.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\utilities.py", "partition": "train", "function_type": "function", "start_line": 38, "end_line": 46, "hash": "00a4a443d8d8e6ae6332127afe4f9f48", "complexity": 3, "parameters": ["loss"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "_parse_loop_limits", "original_string": "def _parse_loop_limits(\r\n    min_steps: Optional[int],\r\n    max_steps: int,\r\n    min_epochs: Optional[int],\r\n    max_epochs: Optional[int],\r\n    trainer: \"pl.Trainer\",\r\n) -> tuple[int, int]:\r\n    \"\"\"This utility computes the default values for the minimum and maximum number of steps and epochs given the values\r\n    the user has selected.\r\n\r\n    Args:\r\n        min_steps: Minimum number of steps.\r\n        max_steps: Maximum number of steps.\r\n        min_epochs: Minimum number of epochs.\r\n        max_epochs: Maximum number of epochs.\r\n        trainer: Trainer instance.\r\n\r\n    Returns:\r\n        The parsed limits, with default values being set for the ones that the user did not specify.\r\n\r\n    \"\"\"\r\n    if max_epochs is None:\r\n        if max_steps == -1 and not any(isinstance(cb, Timer) for cb in trainer.callbacks):\r\n            rank_zero_warn(\r\n                \"`max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit,\"\r\n                \" set `max_epochs=-1`.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            max_epochs = 1000\r\n        else:\r\n            max_epochs = -1\r\n\r\n    if min_epochs is None and min_steps is not None:\r\n        # setting this allows FitLoop.done to re-evaluate should_stop when it gets triggered `on_fit_start`\r\n        min_epochs = 1\r\n\r\n    if min_epochs is None:\r\n        # the default value is 0 so no training will be done when should_stop is triggered `on_fit_start`\r\n        min_epochs = 0\r\n\r\n    return min_epochs, max_epochs", "language": "python", "code": "def _parse_loop_limits(\r\n    min_steps: Optional[int],\r\n    max_steps: int,\r\n    min_epochs: Optional[int],\r\n    max_epochs: Optional[int],\r\n    trainer: \"pl.Trainer\",\r\n) -> tuple[int, int]:\r\n    \"\"\"This utility computes the default values for the minimum and maximum number of steps and epochs given the values\r\n    the user has selected.\r\n\r\n    Args:\r\n        min_steps: Minimum number of steps.\r\n        max_steps: Maximum number of steps.\r\n        min_epochs: Minimum number of epochs.\r\n        max_epochs: Maximum number of epochs.\r\n        trainer: Trainer instance.\r\n\r\n    Returns:\r\n        The parsed limits, with default values being set for the ones that the user did not specify.\r\n\r\n    \"\"\"\r\n    if max_epochs is None:\r\n        if max_steps == -1 and not any(isinstance(cb, Timer) for cb in trainer.callbacks):\r\n            rank_zero_warn(\r\n                \"`max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit,\"\r\n                \" set `max_epochs=-1`.\",\r\n                category=PossibleUserWarning,\r\n            )\r\n            max_epochs = 1000\r\n        else:\r\n            max_epochs = -1\r\n\r\n    if min_epochs is None and min_steps is not None:\r\n        # setting this allows FitLoop.done to re-evaluate should_stop when it gets triggered `on_fit_start`\r\n        min_epochs = 1\r\n\r\n    if min_epochs is None:\r\n        # the default value is 0 so no training will be done when should_stop is triggered `on_fit_start`\r\n        min_epochs = 0\r\n\r\n    return min_epochs, max_epochs", "code_tokens": ["def", "_parse_loop_limits", "(", "min_steps", ":", "Optional", "[", "int", "]", ",", "max_steps", ":", "int", ",", "min_epochs", ":", "Optional", "[", "int", "]", ",", "max_epochs", ":", "Optional", "[", "int", "]", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ",", ")", "-", ">", "tuple", "[", "int", ",", "int", "]", ":", "\"", "\"", "\"", "This", "utility", "computes", "the", "default", "values", "for", "the", "minimum", "and", "maximum", "number", "of", "steps", "and", "epochs", "given", "the", "values", "the", "user", "has", "selected", ".", "Args", ":", "min_steps", ":", "Minimum", "number", "of", "steps", ".", "max_steps", ":", "Maximum", "number", "of", "steps", ".", "min_epochs", ":", "Minimum", "number", "of", "epochs", ".", "max_epochs", ":", "Maximum", "number", "of", "epochs", ".", "trainer", ":", "Trainer", "instance", ".", "Returns", ":", "The", "parsed", "limits", ",", "with", "default", "values", "being", "set", "for", "the", "ones", "that", "the", "user", "did", "not", "specify", ".", "\"", "\"", "\"", "if", "max_epochs", "is", "None", ":", "if", "max_steps", "=", "=", "-", "1", "and", "not", "any", "(", "isinstance", "(", "cb", ",", "Timer", ")", "for", "cb", "in", "trainer", ".", "callbacks", ")", ":", "rank_zero_warn", "(", "\"", "`", "max_epochs", "`", "was", "not", "set", ".", "Setting", "it", "to", "1000", "epochs", ".", "To", "train", "without", "an", "epoch", "limit", ",", "\"", "\"", "set", "`", "max_epochs", "=", "-", "1", "`", ".", "\"", ",", "category", "=", "PossibleUserWarning", ",", ")", "max_epochs", "=", "1000", "else", ":", "max_epochs", "=", "-", "1", "if", "min_epochs", "is", "None", "and", "min_steps", "is", "not", "None", ":", "min_epochs", "=", "1", "if", "min_epochs", "is", "None", ":", "min_epochs", "=", "0", "return", "min_epochs", ",", "max_epochs"], "docstring": "This utility computes the default values for the minimum and maximum number of steps and epochs given the values\r\n    the user has selected.\r\n\r\n    Args:\r\n        min_steps: Minimum number of steps.\r\n        max_steps: Maximum number of steps.\r\n        min_epochs: Minimum number of epochs.\r\n        max_epochs: Maximum number of epochs.\r\n        trainer: Trainer instance.\r\n\r\n    Returns:\r\n        The parsed limits, with default values being set for the ones that the user did not specify.", "docstring_tokens": ["this", "utility", "computes", "the", "default", "values", "for", "the", "minimum", "and", "maximum", "number", "of", "steps", "and", "epochs", "given", "the", "values", "the", "user", "has", "selected", "args", "min_steps", "minimum", "number", "of", "steps", "max_steps", "maximum", "number", "of", "steps", "min_epochs", "minimum", "number", "of", "epochs", "max_epochs", "maximum", "number", "of", "epochs", "trainer", "trainer", "instance", "returns", "the", "parsed", "limits", "with", "default", "values", "being", "set", "for", "the", "ones", "that", "the", "user", "did", "not", "specify"], "docstring_summary": "This utility computes the default values for the minimum and maximum number of steps and epochs given the values", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\utilities.py", "partition": "train", "function_type": "function", "start_line": 49, "end_line": 89, "hash": "dc2719326cd50bfce1d531e3a4e84b1c", "complexity": 8, "parameters": ["min_steps", "max_steps", "min_epochs", "max_epochs", "trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "_block_parallel_sync_behavior", "original_string": "def _block_parallel_sync_behavior(strategy: Strategy, block: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for\r\n    example when accumulating gradients to reduce communication when it is not needed.\r\n\r\n    Args:\r\n        strategy: the strategy instance to use.\r\n        block: whether the context manager is enabled or not\r\n\r\n    Returns:\r\n        context manager with sync behaviour off\r\n\r\n    \"\"\"\r\n    if isinstance(strategy, ParallelStrategy) and block:\r\n        with strategy.block_backward_sync():\r\n            yield None\r\n    else:\r\n        yield None", "language": "python", "code": "def _block_parallel_sync_behavior(strategy: Strategy, block: bool = True) -> Generator[None, None, None]:\r\n    \"\"\"Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for\r\n    example when accumulating gradients to reduce communication when it is not needed.\r\n\r\n    Args:\r\n        strategy: the strategy instance to use.\r\n        block: whether the context manager is enabled or not\r\n\r\n    Returns:\r\n        context manager with sync behaviour off\r\n\r\n    \"\"\"\r\n    if isinstance(strategy, ParallelStrategy) and block:\r\n        with strategy.block_backward_sync():\r\n            yield None\r\n    else:\r\n        yield None", "code_tokens": ["def", "_block_parallel_sync_behavior", "(", "strategy", ":", "Strategy", ",", "block", ":", "bool", "=", "True", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "Blocks", "synchronization", "in", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "strategies", ".", "parallel", ".", "ParallelStrategy", "`", ".", "This", "is", "useful", "for", "example", "when", "accumulating", "gradients", "to", "reduce", "communication", "when", "it", "is", "not", "needed", ".", "Args", ":", "strategy", ":", "the", "strategy", "instance", "to", "use", ".", "block", ":", "whether", "the", "context", "manager", "is", "enabled", "or", "not", "Returns", ":", "context", "manager", "with", "sync", "behaviour", "off", "\"", "\"", "\"", "if", "isinstance", "(", "strategy", ",", "ParallelStrategy", ")", "and", "block", ":", "with", "strategy", ".", "block_backward_sync", "(", ")", ":", "yield", "None", "else", ":", "yield", "None"], "docstring": "Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for\r\n    example when accumulating gradients to reduce communication when it is not needed.\r\n\r\n    Args:\r\n        strategy: the strategy instance to use.\r\n        block: whether the context manager is enabled or not\r\n\r\n    Returns:\r\n        context manager with sync behaviour off", "docstring_tokens": ["blocks", "synchronization", "in", "class", "lightning", "pytorch", "strategies", "parallel", "parallelstrategy", "this", "is", "useful", "for", "example", "when", "accumulating", "gradients", "to", "reduce", "communication", "when", "it", "is", "not", "needed", "args", "strategy", "the", "strategy", "instance", "to", "use", "block", "whether", "the", "context", "manager", "is", "enabled", "or", "not", "returns", "context", "manager", "with", "sync", "behaviour", "off"], "docstring_summary": "Blocks synchronization in :class:`~lightning.pytorch.strategies.parallel.ParallelStrategy`. This is useful for", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\utilities.py", "partition": "train", "function_type": "function", "start_line": 93, "end_line": 109, "hash": "a3694a24bb76d5b676c3df408ce90380", "complexity": 4, "parameters": ["strategy", "block"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\utilities.py", "func_name": "_is_max_limit_reached", "original_string": "def _is_max_limit_reached(current: int, maximum: int = -1) -> bool:\r\n    \"\"\"Check if the limit has been reached (if enabled).\r\n\r\n    Args:\r\n        current: the current value\r\n        maximum: the maximum value (or -1 to disable limit)\r\n\r\n    Returns:\r\n        bool: whether the limit has been reached\r\n\r\n    \"\"\"\r\n    return maximum != -1 and current >= maximum", "language": "python", "code": "def _is_max_limit_reached(current: int, maximum: int = -1) -> bool:\r\n    \"\"\"Check if the limit has been reached (if enabled).\r\n\r\n    Args:\r\n        current: the current value\r\n        maximum: the maximum value (or -1 to disable limit)\r\n\r\n    Returns:\r\n        bool: whether the limit has been reached\r\n\r\n    \"\"\"\r\n    return maximum != -1 and current >= maximum", "code_tokens": ["def", "_is_max_limit_reached", "(", "current", ":", "int", ",", "maximum", ":", "int", "=", "-", "1", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Check", "if", "the", "limit", "has", "been", "reached", "(", "if", "enabled", ")", ".", "Args", ":", "current", ":", "the", "current", "value", "maximum", ":", "the", "maximum", "value", "(", "or", "-", "1", "to", "disable", "limit", ")", "Returns", ":", "bool", ":", "whether", "the", "limit", "has", "been", "reached", "\"", "\"", "\"", "return", "maximum", "!", "=", "-", "1", "and", "current", ">", "=", "maximum"], "docstring": "Check if the limit has been reached (if enabled).\r\n\r\n    Args:\r\n        current: the current value\r\n        maximum: the maximum value (or -1 to disable limit)\r\n\r\n    Returns:\r\n        bool: whether the limit has been reached", "docstring_tokens": ["check", "if", "the", "limit", "has", "been", "reached", "if", "enabled", "args", "current", "the", "current", "value", "maximum", "the", "maximum", "value", "or", "1", "to", "disable", "limit", "returns", "bool", "whether", "the", "limit", "has", "been", "reached"], "docstring_summary": "Check if the limit has been reached (if enabled).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\utilities.py", "partition": "train", "function_type": "function", "start_line": 112, "end_line": 123, "hash": "cd26847ba6a7c2e1f81f468add225f20", "complexity": 2, "parameters": ["current", "maximum"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "run", "original_string": "def run(self, optimizer: Optimizer, batch_idx: int, kwargs: OrderedDict) -> _OUTPUTS_TYPE:\r\n        \"\"\"Runs closure (train step + backward) together with optimization if necessary.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks\r\n            batch_idx: the current batch index.\r\n            optimizer: the optimizer\r\n\r\n        \"\"\"\r\n        closure = self._make_closure(kwargs, optimizer, batch_idx)\r\n\r\n        if (\r\n            # when the strategy handles accumulation, we want to always call the optimizer step\r\n            not self.trainer.strategy.handles_gradient_accumulation and self.trainer.fit_loop._should_accumulate()\r\n        ):\r\n            # For gradient accumulation\r\n\r\n            # -------------------\r\n            # calculate loss (train step + train step end)\r\n            # -------------------\r\n            # automatic_optimization=True: perform ddp sync only when performing optimizer_step\r\n            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):\r\n                closure()\r\n\r\n        # ------------------------------\r\n        # BACKWARD PASS\r\n        # ------------------------------\r\n        # gradient update with accumulated gradients\r\n        else:\r\n            self._optimizer_step(batch_idx, closure)\r\n\r\n        result = closure.consume_result()\r\n        if result.loss is None:\r\n            return {}\r\n        return result.asdict()", "language": "python", "code": "def run(self, optimizer: Optimizer, batch_idx: int, kwargs: OrderedDict) -> _OUTPUTS_TYPE:\r\n        \"\"\"Runs closure (train step + backward) together with optimization if necessary.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks\r\n            batch_idx: the current batch index.\r\n            optimizer: the optimizer\r\n\r\n        \"\"\"\r\n        closure = self._make_closure(kwargs, optimizer, batch_idx)\r\n\r\n        if (\r\n            # when the strategy handles accumulation, we want to always call the optimizer step\r\n            not self.trainer.strategy.handles_gradient_accumulation and self.trainer.fit_loop._should_accumulate()\r\n        ):\r\n            # For gradient accumulation\r\n\r\n            # -------------------\r\n            # calculate loss (train step + train step end)\r\n            # -------------------\r\n            # automatic_optimization=True: perform ddp sync only when performing optimizer_step\r\n            with _block_parallel_sync_behavior(self.trainer.strategy, block=True):\r\n                closure()\r\n\r\n        # ------------------------------\r\n        # BACKWARD PASS\r\n        # ------------------------------\r\n        # gradient update with accumulated gradients\r\n        else:\r\n            self._optimizer_step(batch_idx, closure)\r\n\r\n        result = closure.consume_result()\r\n        if result.loss is None:\r\n            return {}\r\n        return result.asdict()", "code_tokens": ["def", "run", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "batch_idx", ":", "int", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "_OUTPUTS_TYPE", ":", "\"", "\"", "\"", "Runs", "closure", "(", "train", "step", "+", "backward", ")", "together", "with", "optimization", "if", "necessary", ".", "Args", ":", "kwargs", ":", "the", "kwargs", "passed", "down", "to", "the", "hooks", "batch_idx", ":", "the", "current", "batch", "index", ".", "optimizer", ":", "the", "optimizer", "\"", "\"", "\"", "closure", "=", "self", ".", "_make_closure", "(", "kwargs", ",", "optimizer", ",", "batch_idx", ")", "if", "(", "not", "self", ".", "trainer", ".", "strategy", ".", "handles_gradient_accumulation", "and", "self", ".", "trainer", ".", "fit_loop", ".", "_should_accumulate", "(", ")", ")", ":", "with", "_block_parallel_sync_behavior", "(", "self", ".", "trainer", ".", "strategy", ",", "block", "=", "True", ")", ":", "closure", "(", ")", "else", ":", "self", ".", "_optimizer_step", "(", "batch_idx", ",", "closure", ")", "result", "=", "closure", ".", "consume_result", "(", ")", "if", "result", ".", "loss", "is", "None", ":", "return", "{", "}", "return", "result", ".", "asdict", "(", ")"], "docstring": "Runs closure (train step + backward) together with optimization if necessary.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks\r\n            batch_idx: the current batch index.\r\n            optimizer: the optimizer", "docstring_tokens": ["runs", "closure", "train", "step", "backward", "together", "with", "optimization", "if", "necessary", "args", "kwargs", "the", "kwargs", "passed", "down", "to", "the", "hooks", "batch_idx", "the", "current", "batch", "index", "optimizer", "the", "optimizer"], "docstring_summary": "Runs closure (train step + backward) together with optimization if necessary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 162, "end_line": 196, "hash": "5f454783720698d1d9bf57539fed5517", "complexity": 5, "parameters": ["optimizer", "batch_idx", "kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_make_closure", "original_string": "def _make_closure(self, kwargs: OrderedDict, optimizer: Optimizer, batch_idx: int) -> Closure:\r\n        \"\"\"Build a closure object that captures the given arguments and runs the `training_step` function and\r\n        optionally other functions such as `backward` and `zero_grad`.\"\"\"\r\n        step_fn = self._make_step_fn(kwargs)\r\n        backward_fn = self._make_backward_fn(optimizer)\r\n        zero_grad_fn = self._make_zero_grad_fn(batch_idx, optimizer)\r\n        return Closure(step_fn=step_fn, backward_fn=backward_fn, zero_grad_fn=zero_grad_fn)", "language": "python", "code": "def _make_closure(self, kwargs: OrderedDict, optimizer: Optimizer, batch_idx: int) -> Closure:\r\n        \"\"\"Build a closure object that captures the given arguments and runs the `training_step` function and\r\n        optionally other functions such as `backward` and `zero_grad`.\"\"\"\r\n        step_fn = self._make_step_fn(kwargs)\r\n        backward_fn = self._make_backward_fn(optimizer)\r\n        zero_grad_fn = self._make_zero_grad_fn(batch_idx, optimizer)\r\n        return Closure(step_fn=step_fn, backward_fn=backward_fn, zero_grad_fn=zero_grad_fn)", "code_tokens": ["def", "_make_closure", "(", "self", ",", "kwargs", ":", "OrderedDict", ",", "optimizer", ":", "Optimizer", ",", "batch_idx", ":", "int", ")", "-", ">", "Closure", ":", "\"", "\"", "\"", "Build", "a", "closure", "object", "that", "captures", "the", "given", "arguments", "and", "runs", "the", "`", "training_step", "`", "function", "and", "optionally", "other", "functions", "such", "as", "`", "backward", "`", "and", "`", "zero_grad", "`", ".", "\"", "\"", "\"", "step_fn", "=", "self", ".", "_make_step_fn", "(", "kwargs", ")", "backward_fn", "=", "self", ".", "_make_backward_fn", "(", "optimizer", ")", "zero_grad_fn", "=", "self", ".", "_make_zero_grad_fn", "(", "batch_idx", ",", "optimizer", ")", "return", "Closure", "(", "step_fn", "=", "step_fn", ",", "backward_fn", "=", "backward_fn", ",", "zero_grad_fn", "=", "zero_grad_fn", ")"], "docstring": "Build a closure object that captures the given arguments and runs the `training_step` function and\r\n        optionally other functions such as `backward` and `zero_grad`.", "docstring_tokens": ["build", "a", "closure", "object", "that", "captures", "the", "given", "arguments", "and", "runs", "the", "training_step", "function", "and", "optionally", "other", "functions", "such", "as", "backward", "and", "zero_grad"], "docstring_summary": "Build a closure object that captures the given arguments and runs the `training_step` function and", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 198, "end_line": 204, "hash": "4db51dbac5e0e558ff4c951fa3035931", "complexity": 1, "parameters": ["kwargs", "optimizer", "batch_idx"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_make_zero_grad_fn", "original_string": "def _make_zero_grad_fn(self, batch_idx: int, optimizer: Optimizer) -> Optional[Callable[[], None]]:\r\n        \"\"\"Build a `zero_grad` function that zeroes the gradients before back-propagation.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        is_first_batch_to_accumulate = batch_idx % self.trainer.accumulate_grad_batches == 0\r\n        if not is_first_batch_to_accumulate:\r\n            return None\r\n\r\n        def zero_grad_fn() -> None:\r\n            self._on_before_zero_grad(optimizer)\r\n            self._optimizer_zero_grad(batch_idx, optimizer)\r\n\r\n        return zero_grad_fn", "language": "python", "code": "def _make_zero_grad_fn(self, batch_idx: int, optimizer: Optimizer) -> Optional[Callable[[], None]]:\r\n        \"\"\"Build a `zero_grad` function that zeroes the gradients before back-propagation.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        is_first_batch_to_accumulate = batch_idx % self.trainer.accumulate_grad_batches == 0\r\n        if not is_first_batch_to_accumulate:\r\n            return None\r\n\r\n        def zero_grad_fn() -> None:\r\n            self._on_before_zero_grad(optimizer)\r\n            self._optimizer_zero_grad(batch_idx, optimizer)\r\n\r\n        return zero_grad_fn", "code_tokens": ["def", "_make_zero_grad_fn", "(", "self", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optional", "[", "Callable", "[", "[", "]", ",", "None", "]", "]", ":", "\"", "\"", "\"", "Build", "a", "`", "zero_grad", "`", "function", "that", "zeroes", "the", "gradients", "before", "back", "-", "propagation", ".", "Returns", "`", "`", "None", "`", "`", "in", "the", "case", "backward", "needs", "to", "be", "skipped", ".", "\"", "\"", "\"", "if", "self", ".", "_skip_backward", ":", "return", "None", "is_first_batch_to_accumulate", "=", "batch_idx", "%", "self", ".", "trainer", ".", "accumulate_grad_batches", "=", "=", "0", "if", "not", "is_first_batch_to_accumulate", ":", "return", "None", "def", "zero_grad_fn", "(", ")", "-", ">", "None", ":", "self", ".", "_on_before_zero_grad", "(", "optimizer", ")", "self", ".", "_optimizer_zero_grad", "(", "batch_idx", ",", "optimizer", ")", "return", "zero_grad_fn"], "docstring": "Build a `zero_grad` function that zeroes the gradients before back-propagation.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.", "docstring_tokens": ["build", "a", "zero_grad", "function", "that", "zeroes", "the", "gradients", "before", "back", "propagation", "returns", "none", "in", "the", "case", "backward", "needs", "to", "be", "skipped"], "docstring_summary": "Build a `zero_grad` function that zeroes the gradients before back-propagation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 210, "end_line": 227, "hash": "71377ad91263063e0d05c011496b13ce", "complexity": 3, "parameters": ["batch_idx", "optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_make_backward_fn", "original_string": "def _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn", "language": "python", "code": "def _make_backward_fn(self, optimizer: Optimizer) -> Optional[Callable[[Tensor], None]]:\r\n        \"\"\"Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.\r\n\r\n        \"\"\"\r\n        if self._skip_backward:\r\n            return None\r\n\r\n        def backward_fn(loss: Tensor) -> None:\r\n            call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\r\n\r\n        return backward_fn", "code_tokens": ["def", "_make_backward_fn", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optional", "[", "Callable", "[", "[", "Tensor", "]", ",", "None", "]", "]", ":", "\"", "\"", "\"", "Build", "a", "`", "backward", "`", "function", "that", "handles", "back", "-", "propagation", "through", "the", "output", "produced", "by", "the", "`", "training_step", "`", "function", ".", "Returns", "`", "`", "None", "`", "`", "in", "the", "case", "backward", "needs", "to", "be", "skipped", ".", "\"", "\"", "\"", "if", "self", ".", "_skip_backward", ":", "return", "None", "def", "backward_fn", "(", "loss", ":", "Tensor", ")", "-", ">", "None", ":", "call", ".", "_call_strategy_hook", "(", "self", ".", "trainer", ",", "\"", "backward", "\"", ",", "loss", ",", "optimizer", ")", "return", "backward_fn"], "docstring": "Build a `backward` function that handles back-propagation through the output produced by the `training_step`\r\n        function.\r\n\r\n        Returns ``None`` in the case backward needs to be skipped.", "docstring_tokens": ["build", "a", "backward", "function", "that", "handles", "back", "propagation", "through", "the", "output", "produced", "by", "the", "training_step", "function", "returns", "none", "in", "the", "case", "backward", "needs", "to", "be", "skipped"], "docstring_summary": "Build a `backward` function that handles back-propagation through the output produced by the `training_step`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 229, "end_line": 242, "hash": "63ba737308e8aff68cb36fa27b65aee7", "complexity": 2, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_optimizer_step", "original_string": "def _optimizer_step(\r\n        self,\r\n        batch_idx: int,\r\n        train_step_and_backward_closure: Callable[[], Optional[Tensor]],\r\n    ) -> None:\r\n        \"\"\"Performs the optimizer step and some sanity checking.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            train_step_and_backward_closure: the closure function performing the train step and computing the\r\n                gradients. By default, called by the optimizer (if possible)\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        # wraps into LightningOptimizer only for running step\r\n        optimizer = trainer.strategy._lightning_optimizers[0]\r\n\r\n        # if `strategy.handles_gradient_accumulation`, this method will be called to route into the strategy, but we\r\n        # need to check again if `should_accumulate` before increasing the counters\r\n        should_accumulate = trainer.fit_loop._should_accumulate()\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_ready()\r\n\r\n        # model hook\r\n        call._call_lightning_module_hook(\r\n            trainer,\r\n            \"optimizer_step\",\r\n            trainer.current_epoch,\r\n            batch_idx,\r\n            optimizer,\r\n            train_step_and_backward_closure,\r\n        )\r\n\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_completed()", "language": "python", "code": "def _optimizer_step(\r\n        self,\r\n        batch_idx: int,\r\n        train_step_and_backward_closure: Callable[[], Optional[Tensor]],\r\n    ) -> None:\r\n        \"\"\"Performs the optimizer step and some sanity checking.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            train_step_and_backward_closure: the closure function performing the train step and computing the\r\n                gradients. By default, called by the optimizer (if possible)\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        # wraps into LightningOptimizer only for running step\r\n        optimizer = trainer.strategy._lightning_optimizers[0]\r\n\r\n        # if `strategy.handles_gradient_accumulation`, this method will be called to route into the strategy, but we\r\n        # need to check again if `should_accumulate` before increasing the counters\r\n        should_accumulate = trainer.fit_loop._should_accumulate()\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_ready()\r\n\r\n        # model hook\r\n        call._call_lightning_module_hook(\r\n            trainer,\r\n            \"optimizer_step\",\r\n            trainer.current_epoch,\r\n            batch_idx,\r\n            optimizer,\r\n            train_step_and_backward_closure,\r\n        )\r\n\r\n        if not should_accumulate:\r\n            self.optim_progress.optimizer.step.increment_completed()", "code_tokens": ["def", "_optimizer_step", "(", "self", ",", "batch_idx", ":", "int", ",", "train_step_and_backward_closure", ":", "Callable", "[", "[", "]", ",", "Optional", "[", "Tensor", "]", "]", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Performs", "the", "optimizer", "step", "and", "some", "sanity", "checking", ".", "Args", ":", "batch_idx", ":", "the", "index", "of", "the", "current", "batch", "train_step_and_backward_closure", ":", "the", "closure", "function", "performing", "the", "train", "step", "and", "computing", "the", "gradients", ".", "By", "default", ",", "called", "by", "the", "optimizer", "(", "if", "possible", ")", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "optimizer", "=", "trainer", ".", "strategy", ".", "_lightning_optimizers", "[", "0", "]", "should_accumulate", "=", "trainer", ".", "fit_loop", ".", "_should_accumulate", "(", ")", "if", "not", "should_accumulate", ":", "self", ".", "optim_progress", ".", "optimizer", ".", "step", ".", "increment_ready", "(", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "optimizer_step", "\"", ",", "trainer", ".", "current_epoch", ",", "batch_idx", ",", "optimizer", ",", "train_step_and_backward_closure", ",", ")", "if", "not", "should_accumulate", ":", "self", ".", "optim_progress", ".", "optimizer", ".", "step", ".", "increment_completed", "(", ")"], "docstring": "Performs the optimizer step and some sanity checking.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            train_step_and_backward_closure: the closure function performing the train step and computing the\r\n                gradients. By default, called by the optimizer (if possible)", "docstring_tokens": ["performs", "the", "optimizer", "step", "and", "some", "sanity", "checking", "args", "batch_idx", "the", "index", "of", "the", "current", "batch", "train_step_and_backward_closure", "the", "closure", "function", "performing", "the", "train", "step", "and", "computing", "the", "gradients", "by", "default", "called", "by", "the", "optimizer", "if", "possible"], "docstring_summary": "Performs the optimizer step and some sanity checking.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 244, "end_line": 279, "hash": "c46a596adc02c6c373e73bedba195435", "complexity": 3, "parameters": ["batch_idx", "train_step_and_backward_closure", "Optional[Tensor]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_on_before_zero_grad", "original_string": "def _on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Calls the ``on_before_zero_grad`` hook.\r\n\r\n        Args:\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        self.optim_progress.optimizer.zero_grad.increment_ready()\r\n        call._call_callback_hooks(trainer, \"on_before_zero_grad\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_zero_grad\", optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_started()", "language": "python", "code": "def _on_before_zero_grad(self, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Calls the ``on_before_zero_grad`` hook.\r\n\r\n        Args:\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        self.optim_progress.optimizer.zero_grad.increment_ready()\r\n        call._call_callback_hooks(trainer, \"on_before_zero_grad\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_zero_grad\", optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_started()", "code_tokens": ["def", "_on_before_zero_grad", "(", "self", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Calls", "the", "`", "`", "on_before_zero_grad", "`", "`", "hook", ".", "Args", ":", "optimizer", ":", "the", "current", "optimizer", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "self", ".", "optim_progress", ".", "optimizer", ".", "zero_grad", ".", "increment_ready", "(", ")", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_before_zero_grad", "\"", ",", "optimizer", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_before_zero_grad", "\"", ",", "optimizer", ")", "self", ".", "optim_progress", ".", "optimizer", ".", "zero_grad", ".", "increment_started", "(", ")"], "docstring": "Calls the ``on_before_zero_grad`` hook.\r\n\r\n        Args:\r\n            optimizer: the current optimizer", "docstring_tokens": ["calls", "the", "on_before_zero_grad", "hook", "args", "optimizer", "the", "current", "optimizer"], "docstring_summary": "Calls the ``on_before_zero_grad`` hook.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 281, "end_line": 292, "hash": "7f1f33c2fa98a1f003e15008008d9056", "complexity": 1, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_optimizer_zero_grad", "original_string": "def _optimizer_zero_grad(self, batch_idx: int, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Zeroes out all gradients of parameters optimized by the current optimizer.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_lightning_module_hook(trainer, \"optimizer_zero_grad\", trainer.current_epoch, batch_idx, optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_completed()", "language": "python", "code": "def _optimizer_zero_grad(self, batch_idx: int, optimizer: torch.optim.Optimizer) -> None:\r\n        \"\"\"Zeroes out all gradients of parameters optimized by the current optimizer.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            optimizer: the current optimizer\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n        call._call_lightning_module_hook(trainer, \"optimizer_zero_grad\", trainer.current_epoch, batch_idx, optimizer)\r\n        self.optim_progress.optimizer.zero_grad.increment_completed()", "code_tokens": ["def", "_optimizer_zero_grad", "(", "self", ",", "batch_idx", ":", "int", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Zeroes", "out", "all", "gradients", "of", "parameters", "optimized", "by", "the", "current", "optimizer", ".", "Args", ":", "batch_idx", ":", "the", "index", "of", "the", "current", "batch", "optimizer", ":", "the", "current", "optimizer", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "optimizer_zero_grad", "\"", ",", "trainer", ".", "current_epoch", ",", "batch_idx", ",", "optimizer", ")", "self", ".", "optim_progress", ".", "optimizer", ".", "zero_grad", ".", "increment_completed", "(", ")"], "docstring": "Zeroes out all gradients of parameters optimized by the current optimizer.\r\n\r\n        Args:\r\n            batch_idx: the index of the current batch\r\n            optimizer: the current optimizer", "docstring_tokens": ["zeroes", "out", "all", "gradients", "of", "parameters", "optimized", "by", "the", "current", "optimizer", "args", "batch_idx", "the", "index", "of", "the", "current", "batch", "optimizer", "the", "current", "optimizer"], "docstring_summary": "Zeroes out all gradients of parameters optimized by the current optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 294, "end_line": 304, "hash": "06c669dc716e62475b09faad60fee8bf", "complexity": 1, "parameters": ["batch_idx", "optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "func_name": "_training_step", "original_string": "def _training_step(self, kwargs: OrderedDict) -> ClosureResult:\r\n        \"\"\"Performs the actual train step with the tied hooks.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks.\r\n\r\n        Returns:\r\n            A ``ClosureResult`` containing the training step output.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n\r\n        if training_step_output is None and trainer.world_size > 1:\r\n            raise RuntimeError(\r\n                \"Skipping the `training_step` by returning None in distributed training is not supported.\"\r\n                \" It is recommended that you rewrite your training logic to avoid having to skip the step in the first\"\r\n                \" place.\"\r\n            )\r\n\r\n        return self.output_result_cls.from_training_step_output(training_step_output, trainer.accumulate_grad_batches)", "language": "python", "code": "def _training_step(self, kwargs: OrderedDict) -> ClosureResult:\r\n        \"\"\"Performs the actual train step with the tied hooks.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks.\r\n\r\n        Returns:\r\n            A ``ClosureResult`` containing the training step output.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n\r\n        if training_step_output is None and trainer.world_size > 1:\r\n            raise RuntimeError(\r\n                \"Skipping the `training_step` by returning None in distributed training is not supported.\"\r\n                \" It is recommended that you rewrite your training logic to avoid having to skip the step in the first\"\r\n                \" place.\"\r\n            )\r\n\r\n        return self.output_result_cls.from_training_step_output(training_step_output, trainer.accumulate_grad_batches)", "code_tokens": ["def", "_training_step", "(", "self", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "ClosureResult", ":", "\"", "\"", "\"", "Performs", "the", "actual", "train", "step", "with", "the", "tied", "hooks", ".", "Args", ":", "kwargs", ":", "the", "kwargs", "passed", "down", "to", "the", "hooks", ".", "Returns", ":", "A", "`", "`", "ClosureResult", "`", "`", "containing", "the", "training", "step", "output", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "training_step_output", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "training_step", "\"", ",", "*", "kwargs", ".", "values", "(", ")", ")", "self", ".", "trainer", ".", "strategy", ".", "post_training_step", "(", ")", "if", "training_step_output", "is", "None", "and", "trainer", ".", "world_size", ">", "1", ":", "raise", "RuntimeError", "(", "\"", "Skipping", "the", "`", "training_step", "`", "by", "returning", "None", "in", "distributed", "training", "is", "not", "supported", ".", "\"", "\"", "It", "is", "recommended", "that", "you", "rewrite", "your", "training", "logic", "to", "avoid", "having", "to", "skip", "the", "step", "in", "the", "first", "\"", "\"", "place", ".", "\"", ")", "return", "self", ".", "output_result_cls", ".", "from_training_step_output", "(", "training_step_output", ",", "trainer", ".", "accumulate_grad_batches", ")"], "docstring": "Performs the actual train step with the tied hooks.\r\n\r\n        Args:\r\n            kwargs: the kwargs passed down to the hooks.\r\n\r\n        Returns:\r\n            A ``ClosureResult`` containing the training step output.", "docstring_tokens": ["performs", "the", "actual", "train", "step", "with", "the", "tied", "hooks", "args", "kwargs", "the", "kwargs", "passed", "down", "to", "the", "hooks", "returns", "a", "closureresult", "containing", "the", "training", "step", "output"], "docstring_summary": "Performs the actual train step with the tied hooks.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\automatic.py", "partition": "train", "function_type": "class_method", "class_name": "_AutomaticOptimization", "start_line": 306, "end_line": 328, "hash": "c898eda62a4f9993b88ec49327d74d62", "complexity": 3, "parameters": ["kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\closure.py", "func_name": "consume_result", "original_string": "def consume_result(self) -> T:\r\n        \"\"\"The cached result from the last time the closure was called.\r\n\r\n        Once accessed, the internal reference gets reset and the consumer will have to hold on to the reference as long\r\n        as necessary.\r\n\r\n        \"\"\"\r\n        if self._result is None:\r\n            raise MisconfigurationException(\r\n                \"The closure hasn't been executed.\"\r\n                \" HINT: did you call `optimizer_closure()` in your `optimizer_step` hook? It could also happen because\"\r\n                \" the `optimizer.step(optimizer_closure)` call did not execute it internally.\"\r\n            )\r\n        result, self._result = self._result, None  # free memory\r\n        return result", "language": "python", "code": "def consume_result(self) -> T:\r\n        \"\"\"The cached result from the last time the closure was called.\r\n\r\n        Once accessed, the internal reference gets reset and the consumer will have to hold on to the reference as long\r\n        as necessary.\r\n\r\n        \"\"\"\r\n        if self._result is None:\r\n            raise MisconfigurationException(\r\n                \"The closure hasn't been executed.\"\r\n                \" HINT: did you call `optimizer_closure()` in your `optimizer_step` hook? It could also happen because\"\r\n                \" the `optimizer.step(optimizer_closure)` call did not execute it internally.\"\r\n            )\r\n        result, self._result = self._result, None  # free memory\r\n        return result", "code_tokens": ["def", "consume_result", "(", "self", ")", "-", ">", "T", ":", "\"", "\"", "\"", "The", "cached", "result", "from", "the", "last", "time", "the", "closure", "was", "called", ".", "Once", "accessed", ",", "the", "internal", "reference", "gets", "reset", "and", "the", "consumer", "will", "have", "to", "hold", "on", "to", "the", "reference", "as", "long", "as", "necessary", ".", "\"", "\"", "\"", "if", "self", ".", "_result", "is", "None", ":", "raise", "MisconfigurationException", "(", "\"", "The", "closure", "hasn", "'", "t", "been", "executed", ".", "\"", "\"", "HINT", ":", "did", "you", "call", "`", "optimizer_closure", "(", ")", "`", "in", "your", "`", "optimizer_step", "`", "hook", "?", "It", "could", "also", "happen", "because", "\"", "\"", "the", "`", "optimizer", ".", "step", "(", "optimizer_closure", ")", "`", "call", "did", "not", "execute", "it", "internally", ".", "\"", ")", "result", ",", "self", ".", "_result", "=", "self", ".", "_result", ",", "None", "return", "result"], "docstring": "The cached result from the last time the closure was called.\r\n\r\n        Once accessed, the internal reference gets reset and the consumer will have to hold on to the reference as long\r\n        as necessary.", "docstring_tokens": ["the", "cached", "result", "from", "the", "last", "time", "the", "closure", "was", "called", "once", "accessed", "the", "internal", "reference", "gets", "reset", "and", "the", "consumer", "will", "have", "to", "hold", "on", "to", "the", "reference", "as", "long", "as", "necessary"], "docstring_summary": "The cached result from the last time the closure was called.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\closure.py", "partition": "train", "function_type": "class_method", "class_name": "AbstractClosure", "start_line": 44, "end_line": 58, "hash": "4280b4113ee001ddde656ba66773c798", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py", "func_name": "advance", "original_string": "def advance(self, kwargs: OrderedDict) -> None:\r\n        \"\"\"Performs the training step for manual optimization.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        # manually capture logged metrics\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        del kwargs  # release the batch from memory\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n        result = self.output_result_cls.from_training_step_output(training_step_output)\r\n\r\n        self._output = result.asdict()", "language": "python", "code": "def advance(self, kwargs: OrderedDict) -> None:\r\n        \"\"\"Performs the training step for manual optimization.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.\r\n\r\n        \"\"\"\r\n        trainer = self.trainer\r\n\r\n        # manually capture logged metrics\r\n        training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n        del kwargs  # release the batch from memory\r\n        self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\r\n        result = self.output_result_cls.from_training_step_output(training_step_output)\r\n\r\n        self._output = result.asdict()", "code_tokens": ["def", "advance", "(", "self", ",", "kwargs", ":", "OrderedDict", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Performs", "the", "training", "step", "for", "manual", "optimization", ".", "Args", ":", "kwargs", ":", "The", "kwargs", "passed", "down", "to", "the", "hooks", ".", "\"", "\"", "\"", "trainer", "=", "self", ".", "trainer", "training_step_output", "=", "call", ".", "_call_strategy_hook", "(", "trainer", ",", "\"", "training_step", "\"", ",", "*", "kwargs", ".", "values", "(", ")", ")", "del", "kwargs", "self", ".", "trainer", ".", "strategy", ".", "post_training_step", "(", ")", "result", "=", "self", ".", "output_result_cls", ".", "from_training_step_output", "(", "training_step_output", ")", "self", ".", "_output", "=", "result", ".", "asdict", "(", ")"], "docstring": "Performs the training step for manual optimization.\r\n\r\n        Args:\r\n            kwargs: The kwargs passed down to the hooks.", "docstring_tokens": ["performs", "the", "training", "step", "for", "manual", "optimization", "args", "kwargs", "the", "kwargs", "passed", "down", "to", "the", "hooks"], "docstring_summary": "Performs the training step for manual optimization.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\manual.py", "partition": "train", "function_type": "class_method", "class_name": "_ManualOptimization", "start_line": 104, "end_line": 119, "hash": "d2e0f0f9b21def2ff7e5b0cb50f1a3b7", "complexity": 1, "parameters": ["kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\loops\\optimization\\manual.py", "func_name": "on_run_end", "original_string": "def on_run_end(self) -> _OUTPUTS_TYPE:\r\n        \"\"\"Returns the result of this loop, i.e., the post-processed outputs from the training step.\"\"\"\r\n        output, self._output = self._output, {}  # free memory\r\n        # reset logic around the optimizer step\r\n        for lightning_optimizer in self.trainer.strategy._lightning_optimizers:\r\n            lightning_optimizer._on_before_step = do_nothing_closure\r\n            lightning_optimizer._on_after_step = do_nothing_closure\r\n        return output", "language": "python", "code": "def on_run_end(self) -> _OUTPUTS_TYPE:\r\n        \"\"\"Returns the result of this loop, i.e., the post-processed outputs from the training step.\"\"\"\r\n        output, self._output = self._output, {}  # free memory\r\n        # reset logic around the optimizer step\r\n        for lightning_optimizer in self.trainer.strategy._lightning_optimizers:\r\n            lightning_optimizer._on_before_step = do_nothing_closure\r\n            lightning_optimizer._on_after_step = do_nothing_closure\r\n        return output", "code_tokens": ["def", "on_run_end", "(", "self", ")", "-", ">", "_OUTPUTS_TYPE", ":", "\"", "\"", "\"", "Returns", "the", "result", "of", "this", "loop", ",", "i", ".", "e", ".", ",", "the", "post", "-", "processed", "outputs", "from", "the", "training", "step", ".", "\"", "\"", "\"", "output", ",", "self", ".", "_output", "=", "self", ".", "_output", ",", "{", "}", "for", "lightning_optimizer", "in", "self", ".", "trainer", ".", "strategy", ".", "_lightning_optimizers", ":", "lightning_optimizer", ".", "_on_before_step", "=", "do_nothing_closure", "lightning_optimizer", ".", "_on_after_step", "=", "do_nothing_closure", "return", "output"], "docstring": "Returns the result of this loop, i.e., the post-processed outputs from the training step.", "docstring_tokens": ["returns", "the", "result", "of", "this", "loop", "i", "e", "the", "post", "processed", "outputs", "from", "the", "training", "step"], "docstring_summary": "Returns the result of this loop, i.e., the post-processed outputs from the training step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\loops\\optimization\\manual.py", "partition": "train", "function_type": "class_method", "class_name": "_ManualOptimization", "start_line": 121, "end_line": 128, "hash": "5fc088a5d56132d2fa6a484486828878", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\overrides\\distributed.py", "func_name": "_find_tensors", "original_string": "def _find_tensors(\r\n    obj: Union[Tensor, list, tuple, dict, Any],\r\n) -> Union[list[Tensor], itertools.chain]:  # pragma: no-cover\r\n    \"\"\"Recursively find all tensors contained in the specified object.\"\"\"\r\n    if isinstance(obj, Tensor):\r\n        return [obj]\r\n    if isinstance(obj, (list, tuple)):\r\n        return itertools.chain(*map(_find_tensors, obj))\r\n    if isinstance(obj, dict):\r\n        return itertools.chain(*map(_find_tensors, obj.values()))\r\n    return []", "language": "python", "code": "def _find_tensors(\r\n    obj: Union[Tensor, list, tuple, dict, Any],\r\n) -> Union[list[Tensor], itertools.chain]:  # pragma: no-cover\r\n    \"\"\"Recursively find all tensors contained in the specified object.\"\"\"\r\n    if isinstance(obj, Tensor):\r\n        return [obj]\r\n    if isinstance(obj, (list, tuple)):\r\n        return itertools.chain(*map(_find_tensors, obj))\r\n    if isinstance(obj, dict):\r\n        return itertools.chain(*map(_find_tensors, obj.values()))\r\n    return []", "code_tokens": ["def", "_find_tensors", "(", "obj", ":", "Union", "[", "Tensor", ",", "list", ",", "tuple", ",", "dict", ",", "Any", "]", ",", ")", "-", ">", "Union", "[", "list", "[", "Tensor", "]", ",", "itertools", ".", "chain", "]", ":", "\"", "\"", "\"", "Recursively", "find", "all", "tensors", "contained", "in", "the", "specified", "object", ".", "\"", "\"", "\"", "if", "isinstance", "(", "obj", ",", "Tensor", ")", ":", "return", "[", "obj", "]", "if", "isinstance", "(", "obj", ",", "(", "list", ",", "tuple", ")", ")", ":", "return", "itertools", ".", "chain", "(", "*", "map", "(", "_find_tensors", ",", "obj", ")", ")", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "return", "itertools", ".", "chain", "(", "*", "map", "(", "_find_tensors", ",", "obj", ".", "values", "(", ")", ")", ")", "return", "[", "]"], "docstring": "Recursively find all tensors contained in the specified object.", "docstring_tokens": ["recursively", "find", "all", "tensors", "contained", "in", "the", "specified", "object"], "docstring_summary": "Recursively find all tensors contained in the specified object.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\overrides\\distributed.py", "partition": "train", "function_type": "function", "start_line": 28, "end_line": 38, "hash": "c218aa0bdab78dacfacc3df0e172fb50", "complexity": 4, "parameters": ["obj", "list", "tuple", "dict", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\overrides\\distributed.py", "func_name": "_register_ddp_comm_hook", "original_string": "def _register_ddp_comm_hook(\r\n    model: DistributedDataParallel,\r\n    ddp_comm_state: Optional[object] = None,\r\n    ddp_comm_hook: Optional[Callable] = None,\r\n    ddp_comm_wrapper: Optional[Callable] = None,\r\n) -> None:\r\n    \"\"\"Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.\r\n\r\n    Args:\r\n        model:\r\n            DDP model\r\n        ddp_comm_state:\r\n            state is passed to the hook and can be used to maintain\r\n            and update any state information that users would like to\r\n            maintain as part of the training process. Examples: error\r\n            feedback in gradient compression, peers to communicate with\r\n            next in GossipGrad etc.\r\n        ddp_comm_hook:\r\n            hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future\r\n\r\n            This callable function is called once the bucket is ready. The\r\n            hook can perform whatever processing is needed and return\r\n            a Future indicating completion of any async work (ex: allreduce).\r\n            If the hook doesn't perform any communication, it can also\r\n            just return a completed Future. The Future should hold the\r\n            new value of grad bucket's tensors. Once a bucket is ready,\r\n            c10d reducer would call this hook and use the tensors returned\r\n            by the Future and copy grads to individual parameters.\r\n        ddp_comm_wrapper:\r\n            communication hook wrapper to support a communication hook such\r\n            as FP16 compression as wrapper, which could be combined with\r\n            ddp_comm_hook\r\n\r\n    Examples::\r\n\r\n        from torch.distributed.algorithms.ddp_comm_hooks import (\r\n            default_hooks as default,\r\n            powerSGD_hook as powerSGD,\r\n            post_localSGD_hook as post_localSGD,\r\n        )\r\n\r\n        # fp16_compress_hook for compress gradients\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_hook=default.fp16_compress_hook,\r\n        )\r\n\r\n        # powerSGD_hook\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n        )\r\n\r\n        # post_localSGD_hook\r\n        subgroup, _ = torch.distributed.new_subgroups()\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            state=post_localSGD.PostLocalSGDState(\r\n                process_group=None,\r\n                subgroup=subgroup,\r\n                start_localSGD_iter=1_000,\r\n            ),\r\n            ddp_comm_hook=post_localSGD.post_localSGD_hook,\r\n        )\r\n\r\n        # fp16_compress_wrapper combined with other communication hook\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n            ddp_comm_wrapper=default.fp16_compress_wrapper,\r\n        )\r\n\r\n    \"\"\"\r\n    if ddp_comm_hook is None:\r\n        return\r\n    # inform mypy that ddp_comm_hook is callable\r\n    ddp_comm_hook: Callable = ddp_comm_hook\r\n\r\n    if ddp_comm_wrapper is not None:\r\n        rank_zero_info(\r\n            f\"DDP comm wrapper is provided, apply {ddp_comm_wrapper.__qualname__}({ddp_comm_hook.__qualname__}).\"\r\n        )\r\n        ddp_comm_hook = ddp_comm_wrapper(ddp_comm_hook)\r\n\r\n    rank_zero_debug(f\"Registering DDP comm hook: {ddp_comm_hook.__qualname__}.\")\r\n    model.register_comm_hook(state=ddp_comm_state, hook=ddp_comm_hook)", "language": "python", "code": "def _register_ddp_comm_hook(\r\n    model: DistributedDataParallel,\r\n    ddp_comm_state: Optional[object] = None,\r\n    ddp_comm_hook: Optional[Callable] = None,\r\n    ddp_comm_wrapper: Optional[Callable] = None,\r\n) -> None:\r\n    \"\"\"Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.\r\n\r\n    Args:\r\n        model:\r\n            DDP model\r\n        ddp_comm_state:\r\n            state is passed to the hook and can be used to maintain\r\n            and update any state information that users would like to\r\n            maintain as part of the training process. Examples: error\r\n            feedback in gradient compression, peers to communicate with\r\n            next in GossipGrad etc.\r\n        ddp_comm_hook:\r\n            hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future\r\n\r\n            This callable function is called once the bucket is ready. The\r\n            hook can perform whatever processing is needed and return\r\n            a Future indicating completion of any async work (ex: allreduce).\r\n            If the hook doesn't perform any communication, it can also\r\n            just return a completed Future. The Future should hold the\r\n            new value of grad bucket's tensors. Once a bucket is ready,\r\n            c10d reducer would call this hook and use the tensors returned\r\n            by the Future and copy grads to individual parameters.\r\n        ddp_comm_wrapper:\r\n            communication hook wrapper to support a communication hook such\r\n            as FP16 compression as wrapper, which could be combined with\r\n            ddp_comm_hook\r\n\r\n    Examples::\r\n\r\n        from torch.distributed.algorithms.ddp_comm_hooks import (\r\n            default_hooks as default,\r\n            powerSGD_hook as powerSGD,\r\n            post_localSGD_hook as post_localSGD,\r\n        )\r\n\r\n        # fp16_compress_hook for compress gradients\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_hook=default.fp16_compress_hook,\r\n        )\r\n\r\n        # powerSGD_hook\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n        )\r\n\r\n        # post_localSGD_hook\r\n        subgroup, _ = torch.distributed.new_subgroups()\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            state=post_localSGD.PostLocalSGDState(\r\n                process_group=None,\r\n                subgroup=subgroup,\r\n                start_localSGD_iter=1_000,\r\n            ),\r\n            ddp_comm_hook=post_localSGD.post_localSGD_hook,\r\n        )\r\n\r\n        # fp16_compress_wrapper combined with other communication hook\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n            ddp_comm_wrapper=default.fp16_compress_wrapper,\r\n        )\r\n\r\n    \"\"\"\r\n    if ddp_comm_hook is None:\r\n        return\r\n    # inform mypy that ddp_comm_hook is callable\r\n    ddp_comm_hook: Callable = ddp_comm_hook\r\n\r\n    if ddp_comm_wrapper is not None:\r\n        rank_zero_info(\r\n            f\"DDP comm wrapper is provided, apply {ddp_comm_wrapper.__qualname__}({ddp_comm_hook.__qualname__}).\"\r\n        )\r\n        ddp_comm_hook = ddp_comm_wrapper(ddp_comm_hook)\r\n\r\n    rank_zero_debug(f\"Registering DDP comm hook: {ddp_comm_hook.__qualname__}.\")\r\n    model.register_comm_hook(state=ddp_comm_state, hook=ddp_comm_hook)", "code_tokens": ["def", "_register_ddp_comm_hook", "(", "model", ":", "DistributedDataParallel", ",", "ddp_comm_state", ":", "Optional", "[", "object", "]", "=", "None", ",", "ddp_comm_hook", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "ddp_comm_wrapper", ":", "Optional", "[", "Callable", "]", "=", "None", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Function", "to", "register", "communication", "hook", "for", "DDP", "model", "https", ":", "/", "/", "pytorch", ".", "org", "/", "docs", "/", "master", "/", "ddp_comm_hooks", ".", "html", ".", "Args", ":", "model", ":", "DDP", "model", "ddp_comm_state", ":", "state", "is", "passed", "to", "the", "hook", "and", "can", "be", "used", "to", "maintain", "and", "update", "any", "state", "information", "that", "users", "would", "like", "to", "maintain", "as", "part", "of", "the", "training", "process", ".", "Examples", ":", "error", "feedback", "in", "gradient", "compression", ",", "peers", "to", "communicate", "with", "next", "in", "GossipGrad", "etc", ".", "ddp_comm_hook", ":", "hook", "(", "state", ":", "object", ",", "bucket", ":", "dist", ".", "_GradBucket", ")", "-", ">", "torch", ".", "futures", ".", "Future", "This", "callable", "function", "is", "called", "once", "the", "bucket", "is", "ready", ".", "The", "hook", "can", "perform", "whatever", "processing", "is", "needed", "and", "return", "a", "Future", "indicating", "completion", "of", "any", "async", "work", "(", "ex", ":", "allreduce", ")", ".", "If", "the", "hook", "doesn", "'", "t", "perform", "any", "communication", ",", "it", "can", "also", "just", "return", "a", "completed", "Future", ".", "The", "Future", "should", "hold", "the", "new", "value", "of", "grad", "bucket", "'", "s", "tensors", ".", "Once", "a", "bucket", "is", "ready", ",", "c10d", "reducer", "would", "call", "this", "hook", "and", "use", "the", "tensors", "returned", "by", "the", "Future", "and", "copy", "grads", "to", "individual", "parameters", ".", "ddp_comm_wrapper", ":", "communication", "hook", "wrapper", "to", "support", "a", "communication", "hook", "such", "as", "FP16", "compression", "as", "wrapper", ",", "which", "could", "be", "combined", "with", "ddp_comm_hook", "Examples", ":", ":", "from", "torch", ".", "distributed", ".", "algorithms", ".", "ddp_comm_hooks", "import", "(", "default_hooks", "as", "default", ",", "powerSGD_hook", "as", "powerSGD", ",", "post_localSGD_hook", "as", "post_localSGD", ",", ")", "ddp_model", "=", ".", ".", ".", "_register_ddp_comm_hook", "(", "model", "=", "ddp_model", ",", "ddp_comm_hook", "=", "default", ".", "fp16_compress_hook", ",", ")", "ddp_model", "=", ".", ".", ".", "_register_ddp_comm_hook", "(", "model", "=", "ddp_model", ",", "ddp_comm_state", "=", "powerSGD", ".", "PowerSGDState", "(", "process_group", "=", "None", ",", "matrix_approximation_rank", "=", "1", ",", "start_powerSGD_iter", "=", "5000", ",", ")", ",", "ddp_comm_hook", "=", "powerSGD", ".", "powerSGD_hook", ",", ")", "subgroup", ",", "_", "=", "torch", ".", "distributed", ".", "new_subgroups", "(", ")", "ddp_model", "=", ".", ".", ".", "_register_ddp_comm_hook", "(", "model", "=", "ddp_model", ",", "state", "=", "post_localSGD", ".", "PostLocalSGDState", "(", "process_group", "=", "None", ",", "subgroup", "=", "subgroup", ",", "start_localSGD_iter", "=", "1_000", ",", ")", ",", "ddp_comm_hook", "=", "post_localSGD", ".", "post_localSGD_hook", ",", ")", "ddp_model", "=", ".", ".", ".", "_register_ddp_comm_hook", "(", "model", "=", "ddp_model", ",", "ddp_comm_state", "=", "powerSGD", ".", "PowerSGDState", "(", "process_group", "=", "None", ",", "matrix_approximation_rank", "=", "1", ",", "start_powerSGD_iter", "=", "5000", ",", ")", ",", "ddp_comm_hook", "=", "powerSGD", ".", "powerSGD_hook", ",", "ddp_comm_wrapper", "=", "default", ".", "fp16_compress_wrapper", ",", ")", "\"", "\"", "\"", "if", "ddp_comm_hook", "is", "None", ":", "return", "ddp_comm_hook", ":", "Callable", "=", "ddp_comm_hook", "if", "ddp_comm_wrapper", "is", "not", "None", ":", "rank_zero_info", "(", "f", "\"", "DDP", "comm", "wrapper", "is", "provided", ",", "apply", "{", "ddp_comm_wrapper", ".", "__qualname__", "}", "(", "{", "ddp_comm_hook", ".", "__qualname__", "}", ")", ".", "\"", ")", "ddp_comm_hook", "=", "ddp_comm_wrapper", "(", "ddp_comm_hook", ")", "rank_zero_debug", "(", "f", "\"", "Registering", "DDP", "comm", "hook", ":", "{", "ddp_comm_hook", ".", "__qualname__", "}", ".", "\"", ")", "model", ".", "register_comm_hook", "(", "state", "=", "ddp_comm_state", ",", "hook", "=", "ddp_comm_hook", ")"], "docstring": "Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.\r\n\r\n    Args:\r\n        model:\r\n            DDP model\r\n        ddp_comm_state:\r\n            state is passed to the hook and can be used to maintain\r\n            and update any state information that users would like to\r\n            maintain as part of the training process. Examples: error\r\n            feedback in gradient compression, peers to communicate with\r\n            next in GossipGrad etc.\r\n        ddp_comm_hook:\r\n            hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future\r\n\r\n            This callable function is called once the bucket is ready. The\r\n            hook can perform whatever processing is needed and return\r\n            a Future indicating completion of any async work (ex: allreduce).\r\n            If the hook doesn't perform any communication, it can also\r\n            just return a completed Future. The Future should hold the\r\n            new value of grad bucket's tensors. Once a bucket is ready,\r\n            c10d reducer would call this hook and use the tensors returned\r\n            by the Future and copy grads to individual parameters.\r\n        ddp_comm_wrapper:\r\n            communication hook wrapper to support a communication hook such\r\n            as FP16 compression as wrapper, which could be combined with\r\n            ddp_comm_hook\r\n\r\n    Examples::\r\n\r\n        from torch.distributed.algorithms.ddp_comm_hooks import (\r\n            default_hooks as default,\r\n            powerSGD_hook as powerSGD,\r\n            post_localSGD_hook as post_localSGD,\r\n        )\r\n\r\n        # fp16_compress_hook for compress gradients\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_hook=default.fp16_compress_hook,\r\n        )\r\n\r\n        # powerSGD_hook\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n        )\r\n\r\n        # post_localSGD_hook\r\n        subgroup, _ = torch.distributed.new_subgroups()\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            state=post_localSGD.PostLocalSGDState(\r\n                process_group=None,\r\n                subgroup=subgroup,\r\n                start_localSGD_iter=1_000,\r\n            ),\r\n            ddp_comm_hook=post_localSGD.post_localSGD_hook,\r\n        )\r\n\r\n        # fp16_compress_wrapper combined with other communication hook\r\n        ddp_model = ...\r\n        _register_ddp_comm_hook(\r\n            model=ddp_model,\r\n            ddp_comm_state=powerSGD.PowerSGDState(\r\n                process_group=None,\r\n                matrix_approximation_rank=1,\r\n                start_powerSGD_iter=5000,\r\n            ),\r\n            ddp_comm_hook=powerSGD.powerSGD_hook,\r\n            ddp_comm_wrapper=default.fp16_compress_wrapper,\r\n        )", "docstring_tokens": ["function", "to", "register", "communication", "hook", "for", "ddp", "model", "https", "pytorch", "org", "docs", "master", "ddp_comm_hooks", "html", "args", "model", "ddp", "model", "ddp_comm_state", "state", "is", "passed", "to", "the", "hook", "and", "can", "be", "used", "to", "maintain", "and", "update", "any", "state", "information", "that", "users", "would", "like", "to", "maintain", "as", "part", "of", "the", "training", "process", "examples", "error", "feedback", "in", "gradient", "compression", "peers", "to", "communicate", "with", "next", "in", "gossipgrad", "etc", "ddp_comm_hook", "hook", "state", "object", "bucket", "dist", "_gradbucket", "torch", "futures", "future", "this", "callable", "function", "is", "called", "once", "the", "bucket", "is", "ready", "the", "hook", "can", "perform", "whatever", "processing", "is", "needed", "and", "return", "a", "future", "indicating", "completion", "of", "any", "async", "work", "ex", "allreduce", "if", "the", "hook", "doesn", "t", "perform", "any", "communication", "it", "can", "also", "just", "return", "a", "completed", "future", "the", "future", "should", "hold", "the", "new", "value", "of", "grad", "bucket", "s", "tensors", "once", "a", "bucket", "is", "ready", "c10d", "reducer", "would", "call", "this", "hook", "and", "use", "the", "tensors", "returned", "by", "the", "future", "and", "copy", "grads", "to", "individual", "parameters", "ddp_comm_wrapper", "communication", "hook", "wrapper", "to", "support", "a", "communication", "hook", "such", "as", "fp16", "compression", "as", "wrapper", "which", "could", "be", "combined", "with", "ddp_comm_hook", "examples", "from", "torch", "distributed", "algorithms", "ddp_comm_hooks", "import", "default_hooks", "as", "default", "powersgd_hook", "as", "powersgd", "post_localsgd_hook", "as", "post_localsgd", "fp16_compress_hook", "for", "compress", "gradients", "ddp_model", "_register_ddp_comm_hook", "model", "ddp_model", "ddp_comm_hook", "default", "fp16_compress_hook", "powersgd_hook", "ddp_model", "_register_ddp_comm_hook", "model", "ddp_model", "ddp_comm_state", "powersgd", "powersgdstate", "process_group", "none", "matrix_approximation_rank", "1", "start_powersgd_iter", "5000", "ddp_comm_hook", "powersgd", "powersgd_hook", "post_localsgd_hook", "subgroup", "_", "torch", "distributed", "new_subgroups", "ddp_model", "_register_ddp_comm_hook", "model", "ddp_model", "state", "post_localsgd", "postlocalsgdstate", "process_group", "none", "subgroup", "subgroup", "start_localsgd_iter", "1_000", "ddp_comm_hook", "post_localsgd", "post_localsgd_hook", "fp16_compress_wrapper", "combined", "with", "other", "communication", "hook", "ddp_model", "_register_ddp_comm_hook", "model", "ddp_model", "ddp_comm_state", "powersgd", "powersgdstate", "process_group", "none", "matrix_approximation_rank", "1", "start_powersgd_iter", "5000", "ddp_comm_hook", "powersgd", "powersgd_hook", "ddp_comm_wrapper", "default", "fp16_compress_wrapper"], "docstring_summary": "Function to register communication hook for DDP model https://pytorch.org/docs/master/ddp_comm_hooks.html.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\overrides\\distributed.py", "partition": "train", "function_type": "function", "start_line": 61, "end_line": 160, "hash": "b72dd21e85006f2f78b85205ea8de1ec", "complexity": 3, "parameters": ["model", "ddp_comm_state", "ddp_comm_hook", "ddp_comm_wrapper"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\overrides\\distributed.py", "func_name": "_sync_module_states", "original_string": "def _sync_module_states(module: torch.nn.Module) -> None:\r\n    \"\"\"Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.\"\"\"\r\n    parameters_to_ignore = set(getattr(module, \"_ddp_params_and_buffers_to_ignore\", []))\r\n    from torch.distributed.distributed_c10d import _get_default_group\r\n    from torch.distributed.utils import _sync_module_states as torch_sync_module_states\r\n\r\n    torch_sync_module_states(\r\n        module,\r\n        _get_default_group(),\r\n        250 * 1024 * 1024,\r\n        src=0,\r\n        params_and_buffers_to_ignore=parameters_to_ignore,\r\n    )", "language": "python", "code": "def _sync_module_states(module: torch.nn.Module) -> None:\r\n    \"\"\"Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.\"\"\"\r\n    parameters_to_ignore = set(getattr(module, \"_ddp_params_and_buffers_to_ignore\", []))\r\n    from torch.distributed.distributed_c10d import _get_default_group\r\n    from torch.distributed.utils import _sync_module_states as torch_sync_module_states\r\n\r\n    torch_sync_module_states(\r\n        module,\r\n        _get_default_group(),\r\n        250 * 1024 * 1024,\r\n        src=0,\r\n        params_and_buffers_to_ignore=parameters_to_ignore,\r\n    )", "code_tokens": ["def", "_sync_module_states", "(", "module", ":", "torch", ".", "nn", ".", "Module", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Taken", "from", "https", ":", "/", "/", "github", ".", "com", "/", "pytorch", "/", "pytorch", "/", "blob", "/", "v2", ".", "0", ".", "0", "/", "torch", "/", "nn", "/", "parallel", "/", "distributed", ".", "py", "parameters_to_ignore", "=", "set", "(", "getattr", "(", "module", ",", "\"", "_ddp_params_and_buffers_to_ignore", "\"", ",", "[", "]", ")", ")", "from", "torch", ".", "distributed", ".", "distributed_c10d", "import", "_get_default_group", "from", "torch", ".", "distributed", ".", "utils", "import", "_sync_module_states", "as", "torch_sync_module_states", "torch_sync_module_states", "(", "module", ",", "_get_default_group", "(", ")", ",", "250", "*", "1024", "*", "1024", ",", "src", "=", "0", ",", "params_and_buffers_to_ignore", "=", "parameters_to_ignore", ",", ")"], "docstring": "Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.", "docstring_tokens": ["taken", "from", "https", "github", "com", "pytorch", "pytorch", "blob", "v2", "0", "0", "torch", "nn", "parallel", "distributed", "py", "l675", "l682"], "docstring_summary": "Taken from https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/parallel/distributed.py#L675-L682.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\overrides\\distributed.py", "partition": "train", "function_type": "function", "start_line": 163, "end_line": 175, "hash": "461dac5bfba5f345b5458e7fdcf31e35", "complexity": 1, "parameters": ["module"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "func_name": "apply", "original_string": "def apply(self, model: Module) -> Module:\r\n        \"\"\"Add global batchnorm for a model spread across multiple GPUs and nodes.\r\n\r\n        Override this method to synchronize batchnorm layers between specific process groups instead\r\n        of the whole world.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with batchnorm layers synchronized within the process groups.\r\n\r\n        \"\"\"\r\n        return torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)", "language": "python", "code": "def apply(self, model: Module) -> Module:\r\n        \"\"\"Add global batchnorm for a model spread across multiple GPUs and nodes.\r\n\r\n        Override this method to synchronize batchnorm layers between specific process groups instead\r\n        of the whole world.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with batchnorm layers synchronized within the process groups.\r\n\r\n        \"\"\"\r\n        return torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)", "code_tokens": ["def", "apply", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "\"", "\"", "\"", "Add", "global", "batchnorm", "for", "a", "model", "spread", "across", "multiple", "GPUs", "and", "nodes", ".", "Override", "this", "method", "to", "synchronize", "batchnorm", "layers", "between", "specific", "process", "groups", "instead", "of", "the", "whole", "world", ".", "Args", ":", "model", ":", "Reference", "to", "the", "current", "LightningModule", "Return", ":", "LightningModule", "with", "batchnorm", "layers", "synchronized", "within", "the", "process", "groups", ".", "\"", "\"", "\"", "return", "torch", ".", "nn", ".", "SyncBatchNorm", ".", "convert_sync_batchnorm", "(", "model", ")"], "docstring": "Add global batchnorm for a model spread across multiple GPUs and nodes.\r\n\r\n        Override this method to synchronize batchnorm layers between specific process groups instead\r\n        of the whole world.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with batchnorm layers synchronized within the process groups.", "docstring_tokens": ["add", "global", "batchnorm", "for", "a", "model", "spread", "across", "multiple", "gpus", "and", "nodes", "override", "this", "method", "to", "synchronize", "batchnorm", "layers", "between", "specific", "process", "groups", "instead", "of", "the", "whole", "world", "args", "model", "reference", "to", "the", "current", "lightningmodule", "return", "lightningmodule", "with", "batchnorm", "layers", "synchronized", "within", "the", "process", "groups"], "docstring_summary": "Add global batchnorm for a model spread across multiple GPUs and nodes.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\layer_sync.py", "partition": "train", "function_type": "class_method", "class_name": "TorchSyncBatchNorm", "start_line": 43, "end_line": 56, "hash": "b1e3bbd5068fdc019970075512f19c69", "complexity": 1, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\layer_sync.py", "func_name": "revert", "original_string": "def revert(self, model: Module) -> Module:\r\n        \"\"\"Convert the wrapped batchnorm layers back to regular batchnorm layers.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with regular batchnorm layers that will no longer sync across processes.\r\n\r\n        \"\"\"\r\n        # Code adapted from https://github.com/pytorch/pytorch/issues/41081#issuecomment-783961547\r\n        # Original author: Kapil Yedidi (@kapily)\r\n        converted_module = model\r\n        if isinstance(model, torch.nn.modules.batchnorm.SyncBatchNorm):\r\n            # Unfortunately, LayerSync does not store the original class - if it did\r\n            # we could return the one that was originally created.\r\n            converted_module = _BatchNormXd(\r\n                model.num_features, model.eps, model.momentum, model.affine, model.track_running_stats\r\n            )\r\n            if model.affine:\r\n                with torch.no_grad():\r\n                    converted_module.weight = model.weight\r\n                    converted_module.bias = model.bias\r\n            converted_module.running_mean = model.running_mean\r\n            converted_module.running_var = model.running_var\r\n            converted_module.num_batches_tracked = model.num_batches_tracked\r\n            if hasattr(model, \"qconfig\"):\r\n                converted_module.qconfig = model.qconfig\r\n        for name, child in model.named_children():\r\n            converted_module.add_module(name, self.revert(child))\r\n        del model\r\n        return converted_module", "language": "python", "code": "def revert(self, model: Module) -> Module:\r\n        \"\"\"Convert the wrapped batchnorm layers back to regular batchnorm layers.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with regular batchnorm layers that will no longer sync across processes.\r\n\r\n        \"\"\"\r\n        # Code adapted from https://github.com/pytorch/pytorch/issues/41081#issuecomment-783961547\r\n        # Original author: Kapil Yedidi (@kapily)\r\n        converted_module = model\r\n        if isinstance(model, torch.nn.modules.batchnorm.SyncBatchNorm):\r\n            # Unfortunately, LayerSync does not store the original class - if it did\r\n            # we could return the one that was originally created.\r\n            converted_module = _BatchNormXd(\r\n                model.num_features, model.eps, model.momentum, model.affine, model.track_running_stats\r\n            )\r\n            if model.affine:\r\n                with torch.no_grad():\r\n                    converted_module.weight = model.weight\r\n                    converted_module.bias = model.bias\r\n            converted_module.running_mean = model.running_mean\r\n            converted_module.running_var = model.running_var\r\n            converted_module.num_batches_tracked = model.num_batches_tracked\r\n            if hasattr(model, \"qconfig\"):\r\n                converted_module.qconfig = model.qconfig\r\n        for name, child in model.named_children():\r\n            converted_module.add_module(name, self.revert(child))\r\n        del model\r\n        return converted_module", "code_tokens": ["def", "revert", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "\"", "\"", "\"", "Convert", "the", "wrapped", "batchnorm", "layers", "back", "to", "regular", "batchnorm", "layers", ".", "Args", ":", "model", ":", "Reference", "to", "the", "current", "LightningModule", "Return", ":", "LightningModule", "with", "regular", "batchnorm", "layers", "that", "will", "no", "longer", "sync", "across", "processes", ".", "\"", "\"", "\"", "converted_module", "=", "model", "if", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ":", "converted_module", "=", "_BatchNormXd", "(", "model", ".", "num_features", ",", "model", ".", "eps", ",", "model", ".", "momentum", ",", "model", ".", "affine", ",", "model", ".", "track_running_stats", ")", "if", "model", ".", "affine", ":", "with", "torch", ".", "no_grad", "(", ")", ":", "converted_module", ".", "weight", "=", "model", ".", "weight", "converted_module", ".", "bias", "=", "model", ".", "bias", "converted_module", ".", "running_mean", "=", "model", ".", "running_mean", "converted_module", ".", "running_var", "=", "model", ".", "running_var", "converted_module", ".", "num_batches_tracked", "=", "model", ".", "num_batches_tracked", "if", "hasattr", "(", "model", ",", "\"", "qconfig", "\"", ")", ":", "converted_module", ".", "qconfig", "=", "model", ".", "qconfig", "for", "name", ",", "child", "in", "model", ".", "named_children", "(", ")", ":", "converted_module", ".", "add_module", "(", "name", ",", "self", ".", "revert", "(", "child", ")", ")", "del", "model", "return", "converted_module"], "docstring": "Convert the wrapped batchnorm layers back to regular batchnorm layers.\r\n\r\n        Args:\r\n            model: Reference to the current LightningModule\r\n\r\n        Return:\r\n            LightningModule with regular batchnorm layers that will no longer sync across processes.", "docstring_tokens": ["convert", "the", "wrapped", "batchnorm", "layers", "back", "to", "regular", "batchnorm", "layers", "args", "model", "reference", "to", "the", "current", "lightningmodule", "return", "lightningmodule", "with", "regular", "batchnorm", "layers", "that", "will", "no", "longer", "sync", "across", "processes"], "docstring_summary": "Convert the wrapped batchnorm layers back to regular batchnorm layers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\layer_sync.py", "partition": "train", "function_type": "class_method", "class_name": "TorchSyncBatchNorm", "start_line": 59, "end_line": 90, "hash": "889fc9b1718a2a59c421281adbb7e8e5", "complexity": 6, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "_ensure_setup", "original_string": "def _ensure_setup(self) -> None:\r\n        \"\"\"Ensures that the executor is setup.\r\n\r\n        We can't do setup in __init__ because if train or validate is called more than once, the teardown method deletes\r\n        the executor.\r\n\r\n        \"\"\"\r\n        if self._executor is None:\r\n            self._executor = ThreadPoolExecutor(max_workers=1)", "language": "python", "code": "def _ensure_setup(self) -> None:\r\n        \"\"\"Ensures that the executor is setup.\r\n\r\n        We can't do setup in __init__ because if train or validate is called more than once, the teardown method deletes\r\n        the executor.\r\n\r\n        \"\"\"\r\n        if self._executor is None:\r\n            self._executor = ThreadPoolExecutor(max_workers=1)", "code_tokens": ["def", "_ensure_setup", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Ensures", "that", "the", "executor", "is", "setup", ".", "We", "can", "'", "t", "do", "setup", "in", "__init__", "because", "if", "train", "or", "validate", "is", "called", "more", "than", "once", ",", "the", "teardown", "method", "deletes", "the", "executor", ".", "\"", "\"", "\"", "if", "self", ".", "_executor", "is", "None", ":", "self", ".", "_executor", "=", "ThreadPoolExecutor", "(", "max_workers", "=", "1", ")"], "docstring": "Ensures that the executor is setup.\r\n\r\n        We can't do setup in __init__ because if train or validate is called more than once, the teardown method deletes\r\n        the executor.", "docstring_tokens": ["ensures", "that", "the", "executor", "is", "setup", "we", "can", "t", "do", "setup", "in", "__init__", "because", "if", "train", "or", "validate", "is", "called", "more", "than", "once", "the", "teardown", "method", "deletes", "the", "executor"], "docstring_summary": "Ensures that the executor is setup.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "partition": "train", "function_type": "class_method", "class_name": "AsyncCheckpointIO", "start_line": 46, "end_line": 54, "hash": "d2ed2de657c40054ac8f5bb7eedbc832", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.\"\"\"\r\n\r\n        self._ensure_setup()\r\n\r\n        # rebuild args/kwargs with a cloned checkpoint (supports positional or kw form)\r\n        if \"checkpoint\" in kwargs:\r\n            kwargs = {**kwargs, \"checkpoint\": apply_to_collection(kwargs[\"checkpoint\"], torch.Tensor, _clone_tensor)}\r\n        elif len(args) >= 1:\r\n            args = (apply_to_collection(args[0], torch.Tensor, _clone_tensor), *args[1:])\r\n\r\n        def _save_checkpoint(*args: Any, **kwargs: Any) -> None:\r\n            try:\r\n                assert self.checkpoint_io is not None\r\n                self.checkpoint_io.save_checkpoint(*args, **kwargs)\r\n            except BaseException as ex:\r\n                self._error = ex\r\n\r\n        assert self._executor is not None\r\n        self._executor.submit(_save_checkpoint, *args, **kwargs)\r\n\r\n        # if an error was raised between the previous time `save_checkpoint`` was called and now,\r\n        # because `executor.submit` is not blocking\r\n        if self._error:\r\n            raise self._error", "language": "python", "code": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.\"\"\"\r\n\r\n        self._ensure_setup()\r\n\r\n        # rebuild args/kwargs with a cloned checkpoint (supports positional or kw form)\r\n        if \"checkpoint\" in kwargs:\r\n            kwargs = {**kwargs, \"checkpoint\": apply_to_collection(kwargs[\"checkpoint\"], torch.Tensor, _clone_tensor)}\r\n        elif len(args) >= 1:\r\n            args = (apply_to_collection(args[0], torch.Tensor, _clone_tensor), *args[1:])\r\n\r\n        def _save_checkpoint(*args: Any, **kwargs: Any) -> None:\r\n            try:\r\n                assert self.checkpoint_io is not None\r\n                self.checkpoint_io.save_checkpoint(*args, **kwargs)\r\n            except BaseException as ex:\r\n                self._error = ex\r\n\r\n        assert self._executor is not None\r\n        self._executor.submit(_save_checkpoint, *args, **kwargs)\r\n\r\n        # if an error was raised between the previous time `save_checkpoint`` was called and now,\r\n        # because `executor.submit` is not blocking\r\n        if self._error:\r\n            raise self._error", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Uses", "the", "`", "`", "ThreadPoolExecutor", "`", "`", "to", "save", "the", "checkpoints", "using", "the", "base", "`", "`", "checkpoint_io", "`", "`", ".", "\"", "\"", "\"", "self", ".", "_ensure_setup", "(", ")", "if", "\"", "checkpoint", "\"", "in", "kwargs", ":", "kwargs", "=", "{", "*", "*", "kwargs", ",", "\"", "checkpoint", "\"", ":", "apply_to_collection", "(", "kwargs", "[", "\"", "checkpoint", "\"", "]", ",", "torch", ".", "Tensor", ",", "_clone_tensor", ")", "}", "elif", "len", "(", "args", ")", ">", "=", "1", ":", "args", "=", "(", "apply_to_collection", "(", "args", "[", "0", "]", ",", "torch", ".", "Tensor", ",", "_clone_tensor", ")", ",", "*", "args", "[", "1", ":", "]", ")", "def", "_save_checkpoint", "(", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "try", ":", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")", "except", "BaseException", "as", "ex", ":", "self", ".", "_error", "=", "ex", "assert", "self", ".", "_executor", "is", "not", "None", "self", ".", "_executor", ".", "submit", "(", "_save_checkpoint", ",", "*", "args", ",", "*", "*", "kwargs", ")", "if", "self", ".", "_error", ":", "raise", "self", ".", "_error"], "docstring": "Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.", "docstring_tokens": ["uses", "the", "threadpoolexecutor", "to", "save", "the", "checkpoints", "using", "the", "base", "checkpoint_io"], "docstring_summary": "Uses the ``ThreadPoolExecutor`` to save the checkpoints using the base ``checkpoint_io``.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "partition": "train", "function_type": "class_method", "class_name": "AsyncCheckpointIO", "start_line": 57, "end_line": 81, "hash": "e99b98a9013def1ddca981bb3827c74f", "complexity": 5, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "teardown", "original_string": "def teardown(self) -> None:\r\n        \"\"\"This method is called to close the threads.\"\"\"\r\n        if self._executor is not None:\r\n            self._executor.shutdown(wait=True)\r\n            self._executor = None\r\n\r\n        # if an error was raised anytime in any of the `executor.submit` calls\r\n        if self._error:\r\n            raise self._error", "language": "python", "code": "def teardown(self) -> None:\r\n        \"\"\"This method is called to close the threads.\"\"\"\r\n        if self._executor is not None:\r\n            self._executor.shutdown(wait=True)\r\n            self._executor = None\r\n\r\n        # if an error was raised anytime in any of the `executor.submit` calls\r\n        if self._error:\r\n            raise self._error", "code_tokens": ["def", "teardown", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "method", "is", "called", "to", "close", "the", "threads", ".", "\"", "\"", "\"", "if", "self", ".", "_executor", "is", "not", "None", ":", "self", ".", "_executor", ".", "shutdown", "(", "wait", "=", "True", ")", "self", ".", "_executor", "=", "None", "if", "self", ".", "_error", ":", "raise", "self", ".", "_error"], "docstring": "This method is called to close the threads.", "docstring_tokens": ["this", "method", "is", "called", "to", "close", "the", "threads"], "docstring_summary": "This method is called to close the threads.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "partition": "train", "function_type": "class_method", "class_name": "AsyncCheckpointIO", "start_line": 84, "end_line": 92, "hash": "3326b8bdf4dab791990cc57284853115", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "func_name": "_clone_tensor", "original_string": "def _clone_tensor(t: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Clones a tensor on the caller thread.\"\"\"\r\n    # detach to avoid autograd history and clone to take a point-in-time copy\r\n    return t.detach().clone()", "language": "python", "code": "def _clone_tensor(t: torch.Tensor) -> torch.Tensor:\r\n    \"\"\"Clones a tensor on the caller thread.\"\"\"\r\n    # detach to avoid autograd history and clone to take a point-in-time copy\r\n    return t.detach().clone()", "code_tokens": ["def", "_clone_tensor", "(", "t", ":", "torch", ".", "Tensor", ")", "-", ">", "torch", ".", "Tensor", ":", "\"", "\"", "\"", "Clones", "a", "tensor", "on", "the", "caller", "thread", ".", "\"", "\"", "\"", "return", "t", ".", "detach", "(", ")", ".", "clone", "(", ")"], "docstring": "Clones a tensor on the caller thread.", "docstring_tokens": ["clones", "a", "tensor", "on", "the", "caller", "thread"], "docstring_summary": "Clones a tensor on the caller thread.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\async_plugin.py", "partition": "train", "function_type": "function", "start_line": 96, "end_line": 99, "hash": "79509091f9300083aa06e095230f9d52", "complexity": 1, "parameters": ["t"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to save the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.save_checkpoint(*args, **kwargs)", "language": "python", "code": "def save_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to save the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.save_checkpoint(*args, **kwargs)", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Uses", "the", "base", "`", "`", "checkpoint_io", "`", "`", "to", "save", "the", "checkpoint", ".", "\"", "\"", "\"", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "self", ".", "checkpoint_io", ".", "save_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Uses the base ``checkpoint_io`` to save the checkpoint.", "docstring_tokens": ["uses", "the", "base", "checkpoint_io", "to", "save", "the", "checkpoint"], "docstring_summary": "Uses the base ``checkpoint_io`` to save the checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "partition": "train", "function_type": "class_method", "class_name": "_WrappingCheckpointIO", "start_line": 56, "end_line": 59, "hash": "e76a2cc1361efc76dde894863739f6bd", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "func_name": "remove_checkpoint", "original_string": "def remove_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to remove the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.remove_checkpoint(*args, **kwargs)", "language": "python", "code": "def remove_checkpoint(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"Uses the base ``checkpoint_io`` to remove the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        self.checkpoint_io.remove_checkpoint(*args, **kwargs)", "code_tokens": ["def", "remove_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Uses", "the", "base", "`", "`", "checkpoint_io", "`", "`", "to", "remove", "the", "checkpoint", ".", "\"", "\"", "\"", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "self", ".", "checkpoint_io", ".", "remove_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Uses the base ``checkpoint_io`` to remove the checkpoint.", "docstring_tokens": ["uses", "the", "base", "checkpoint_io", "to", "remove", "the", "checkpoint"], "docstring_summary": "Uses the base ``checkpoint_io`` to remove the checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "partition": "train", "function_type": "class_method", "class_name": "_WrappingCheckpointIO", "start_line": 62, "end_line": 65, "hash": "4a1fa3f50b929112bcade4ab41261abe", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "func_name": "load_checkpoint", "original_string": "def load_checkpoint(self, *args: Any, **kwargs: Any) -> dict[str, Any]:\r\n        \"\"\"Uses the base ``checkpoint_io`` to load the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        return self.checkpoint_io.load_checkpoint(*args, **kwargs)", "language": "python", "code": "def load_checkpoint(self, *args: Any, **kwargs: Any) -> dict[str, Any]:\r\n        \"\"\"Uses the base ``checkpoint_io`` to load the checkpoint.\"\"\"\r\n        assert self.checkpoint_io is not None\r\n        return self.checkpoint_io.load_checkpoint(*args, **kwargs)", "code_tokens": ["def", "load_checkpoint", "(", "self", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Uses", "the", "base", "`", "`", "checkpoint_io", "`", "`", "to", "load", "the", "checkpoint", ".", "\"", "\"", "\"", "assert", "self", ".", "checkpoint_io", "is", "not", "None", "return", "self", ".", "checkpoint_io", ".", "load_checkpoint", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Uses the base ``checkpoint_io`` to load the checkpoint.", "docstring_tokens": ["uses", "the", "base", "checkpoint_io", "to", "load", "the", "checkpoint"], "docstring_summary": "Uses the base ``checkpoint_io`` to load the checkpoint.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\io\\wrapper.py", "partition": "train", "function_type": "class_method", "class_name": "_WrappingCheckpointIO", "start_line": 68, "end_line": 71, "hash": "ed9ad830576e85a9d69a2a70a17b1744", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\amp.py", "func_name": "forward_context", "original_string": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Enable autocast context.\"\"\"\r\n        with self.autocast_context_manager():\r\n            yield", "language": "python", "code": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"Enable autocast context.\"\"\"\r\n        with self.autocast_context_manager():\r\n            yield", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "Enable", "autocast", "context", ".", "\"", "\"", "\"", "with", "self", ".", "autocast_context_manager", "(", ")", ":", "yield"], "docstring": "Enable autocast context.", "docstring_tokens": ["enable", "autocast", "context"], "docstring_summary": "Enable autocast context.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\amp.py", "partition": "train", "function_type": "class_method", "class_name": "MixedPrecision", "start_line": 119, "end_line": 122, "hash": "45bbc4b400a5080ab58ee3e29335262d", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "func_name": "backward", "original_string": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs back-propagation using DeepSpeed's engine.\r\n\r\n        Args:\r\n            tensor: the loss tensor\r\n            model: the model to be optimized\r\n            optimizer: ignored for DeepSpeed\r\n            \\*args: additional positional arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n            \\**kwargs: additional keyword arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n\r\n        \"\"\"\r\n        if is_overridden(\"backward\", model):\r\n            warning_cache.warn(\r\n                \"You have overridden the `LightningModule.backward` hook but it will be ignored since DeepSpeed handles\"\r\n                \" the backward logic internally.\"\r\n            )\r\n        deepspeed_engine: deepspeed.DeepSpeedEngine = model.trainer.model\r\n        deepspeed_engine.backward(tensor, *args, **kwargs)", "language": "python", "code": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs back-propagation using DeepSpeed's engine.\r\n\r\n        Args:\r\n            tensor: the loss tensor\r\n            model: the model to be optimized\r\n            optimizer: ignored for DeepSpeed\r\n            \\*args: additional positional arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n            \\**kwargs: additional keyword arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n\r\n        \"\"\"\r\n        if is_overridden(\"backward\", model):\r\n            warning_cache.warn(\r\n                \"You have overridden the `LightningModule.backward` hook but it will be ignored since DeepSpeed handles\"\r\n                \" the backward logic internally.\"\r\n            )\r\n        deepspeed_engine: deepspeed.DeepSpeedEngine = model.trainer.model\r\n        deepspeed_engine.backward(tensor, *args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "optimizer", ":", "Optional", "[", "Steppable", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Performs", "back", "-", "propagation", "using", "DeepSpeed", "'", "s", "engine", ".", "Args", ":", "tensor", ":", "the", "loss", "tensor", "model", ":", "the", "model", "to", "be", "optimized", "optimizer", ":", "ignored", "for", "DeepSpeed", "\\", "*", "args", ":", "additional", "positional", "arguments", "for", "the", ":", "meth", ":", "`", "deepspeed", ".", "DeepSpeedEngine", ".", "backward", "`", "call", "\\", "*", "*", "kwargs", ":", "additional", "keyword", "arguments", "for", "the", ":", "meth", ":", "`", "deepspeed", ".", "DeepSpeedEngine", ".", "backward", "`", "call", "\"", "\"", "\"", "if", "is_overridden", "(", "\"", "backward", "\"", ",", "model", ")", ":", "warning_cache", ".", "warn", "(", "\"", "You", "have", "overridden", "the", "`", "LightningModule", ".", "backward", "`", "hook", "but", "it", "will", "be", "ignored", "since", "DeepSpeed", "handles", "\"", "\"", "the", "backward", "logic", "internally", ".", "\"", ")", "deepspeed_engine", ":", "deepspeed", ".", "DeepSpeedEngine", "=", "model", ".", "trainer", ".", "model", "deepspeed_engine", ".", "backward", "(", "tensor", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Performs back-propagation using DeepSpeed's engine.\r\n\r\n        Args:\r\n            tensor: the loss tensor\r\n            model: the model to be optimized\r\n            optimizer: ignored for DeepSpeed\r\n            \\*args: additional positional arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n            \\**kwargs: additional keyword arguments for the :meth:`deepspeed.DeepSpeedEngine.backward` call\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "performs", "back", "propagation", "using", "deepspeed", "s", "engine", "args", "tensor", "the", "loss", "tensor", "model", "the", "model", "to", "be", "optimized", "optimizer", "ignored", "for", "deepspeed", "args", "additional", "positional", "arguments", "for", "the", "meth", "deepspeed", "deepspeedengine", "backward", "call", "kwargs", "additional", "keyword", "arguments", "for", "the", "meth", "deepspeed", "deepspeedengine", "backward", "call"], "docstring_summary": "r\"\"\"Performs back-propagation using DeepSpeed's engine.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedPrecision", "start_line": 92, "end_line": 116, "hash": "eaf0bf4f351dc6a88e1143129883b952", "complexity": 2, "parameters": ["# type", "tensor", "model", "optimizer", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "func_name": "clip_gradients", "original_string": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"DeepSpeed handles gradient clipping internally.\"\"\"", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"DeepSpeed handles gradient clipping internally.\"\"\"", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", "=", "0", ".", "0", ",", "gradient_clip_algorithm", ":", "GradClipAlgorithmType", "=", "GradClipAlgorithmType", ".", "NORM", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "DeepSpeed", "handles", "gradient", "clipping", "internally", ".", "\"", "\"", "\""], "docstring": "DeepSpeed handles gradient clipping internally.", "docstring_tokens": ["deepspeed", "handles", "gradient", "clipping", "internally"], "docstring_summary": "DeepSpeed handles gradient clipping internally.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedPrecision", "start_line": 141, "end_line": 147, "hash": "bbfa585db0ec3854508ee15f7562a459", "complexity": 1, "parameters": ["optimizer", "clip_val", "float]", "gradient_clip_algorithm"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\double.py", "func_name": "forward_context", "original_string": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type.\r\n\r\n        See: :func:`torch.set_default_dtype`\r\n\r\n        \"\"\"\r\n        with self.tensor_init_context():\r\n            yield", "language": "python", "code": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type.\r\n\r\n        See: :func:`torch.set_default_dtype`\r\n\r\n        \"\"\"\r\n        with self.tensor_init_context():\r\n            yield", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "context", "manager", "to", "change", "the", "default", "tensor", "type", ".", "See", ":", ":", "func", ":", "`", "torch", ".", "set_default_dtype", "`", "\"", "\"", "\"", "with", "self", ".", "tensor_init_context", "(", ")", ":", "yield"], "docstring": "A context manager to change the default tensor type.\r\n\r\n        See: :func:`torch.set_default_dtype`", "docstring_tokens": ["a", "context", "manager", "to", "change", "the", "default", "tensor", "type", "see", "func", "torch", "set_default_dtype"], "docstring_summary": "A context manager to change the default tensor type.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\double.py", "partition": "train", "function_type": "class_method", "class_name": "DoublePrecision", "start_line": 49, "end_line": 56, "hash": "e312624c1320c51e9118dd89515536ab", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\half.py", "func_name": "forward_context", "original_string": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type when tensors get created during the module's forward.\r\n\r\n        See: :meth:`torch.set_default_tensor_type`\r\n\r\n        \"\"\"\r\n        default_dtype = torch.get_default_dtype()\r\n        torch.set_default_dtype(self._desired_input_dtype)\r\n        try:\r\n            yield\r\n        finally:\r\n            torch.set_default_dtype(default_dtype)", "language": "python", "code": "def forward_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A context manager to change the default tensor type when tensors get created during the module's forward.\r\n\r\n        See: :meth:`torch.set_default_tensor_type`\r\n\r\n        \"\"\"\r\n        default_dtype = torch.get_default_dtype()\r\n        torch.set_default_dtype(self._desired_input_dtype)\r\n        try:\r\n            yield\r\n        finally:\r\n            torch.set_default_dtype(default_dtype)", "code_tokens": ["def", "forward_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "context", "manager", "to", "change", "the", "default", "tensor", "type", "when", "tensors", "get", "created", "during", "the", "module", "'", "s", "forward", ".", "See", ":", ":", "meth", ":", "`", "torch", ".", "set_default_tensor_type", "`", "\"", "\"", "\"", "default_dtype", "=", "torch", ".", "get_default_dtype", "(", ")", "torch", ".", "set_default_dtype", "(", "self", ".", "_desired_input_dtype", ")", "try", ":", "yield", "finally", ":", "torch", ".", "set_default_dtype", "(", "default_dtype", ")"], "docstring": "A context manager to change the default tensor type when tensors get created during the module's forward.\r\n\r\n        See: :meth:`torch.set_default_tensor_type`", "docstring_tokens": ["a", "context", "manager", "to", "change", "the", "default", "tensor", "type", "when", "tensors", "get", "created", "during", "the", "module", "s", "forward", "see", "meth", "torch", "set_default_tensor_type"], "docstring_summary": "A context manager to change the default tensor type when tensors get created during the module's forward.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\half.py", "partition": "train", "function_type": "class_method", "class_name": "HalfPrecision", "start_line": 55, "end_line": 66, "hash": "4b1643a623ae4dbe87c796c62730bc90", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "connect", "original_string": "def connect(\r\n        self, model: Module, optimizers: list[Optimizer], lr_schedulers: list[Any]\r\n    ) -> tuple[Module, list[Optimizer], list[Any]]:\r\n        \"\"\"Connects this plugin to the accelerator and the training process.\"\"\"\r\n        return model, optimizers, lr_schedulers", "language": "python", "code": "def connect(\r\n        self, model: Module, optimizers: list[Optimizer], lr_schedulers: list[Any]\r\n    ) -> tuple[Module, list[Optimizer], list[Any]]:\r\n        \"\"\"Connects this plugin to the accelerator and the training process.\"\"\"\r\n        return model, optimizers, lr_schedulers", "code_tokens": ["def", "connect", "(", "self", ",", "model", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ",", "lr_schedulers", ":", "list", "[", "Any", "]", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", ",", "list", "[", "Any", "]", "]", ":", "\"", "\"", "\"", "Connects", "this", "plugin", "to", "the", "accelerator", "and", "the", "training", "process", ".", "\"", "\"", "\"", "return", "model", ",", "optimizers", ",", "lr_schedulers"], "docstring": "Connects this plugin to the accelerator and the training process.", "docstring_tokens": ["connects", "this", "plugin", "to", "the", "accelerator", "and", "the", "training", "process"], "docstring_summary": "Connects this plugin to the accelerator and the training process.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 39, "end_line": 43, "hash": "c74c972024b5ff12ffa41f6c80591375", "complexity": 1, "parameters": ["model", "optimizers", "lr_schedulers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "backward", "original_string": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: the loss value obtained from the closure\r\n            model: the model to be optimized\r\n            optimizer: current optimizer being used. ``None`` if using manual optimization\r\n            \\*args: Positional arguments intended for the actual function that performs the backward, like\r\n                :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        model.backward(tensor, *args, **kwargs)", "language": "python", "code": "def backward(  # type: ignore[override]\r\n        self,\r\n        tensor: Tensor,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Optional[Steppable],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: the loss value obtained from the closure\r\n            model: the model to be optimized\r\n            optimizer: current optimizer being used. ``None`` if using manual optimization\r\n            \\*args: Positional arguments intended for the actual function that performs the backward, like\r\n                :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        model.backward(tensor, *args, **kwargs)", "code_tokens": ["def", "backward", "(", "self", ",", "tensor", ":", "Tensor", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "optimizer", ":", "Optional", "[", "Steppable", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "Performs", "the", "actual", "backpropagation", ".", "Args", ":", "tensor", ":", "the", "loss", "value", "obtained", "from", "the", "closure", "model", ":", "the", "model", "to", "be", "optimized", "optimizer", ":", "current", "optimizer", "being", "used", ".", "`", "`", "None", "`", "`", "if", "using", "manual", "optimization", "\\", "*", "args", ":", "Positional", "arguments", "intended", "for", "the", "actual", "function", "that", "performs", "the", "backward", ",", "like", ":", "meth", ":", "`", "~", "torch", ".", "Tensor", ".", "backward", "`", ".", "\\", "*", "*", "kwargs", ":", "Keyword", "arguments", "for", "the", "same", "purpose", "as", "`", "`", "*", "args", "`", "`", ".", "\"", "\"", "\"", "model", ".", "backward", "(", "tensor", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Performs the actual backpropagation.\r\n\r\n        Args:\r\n            tensor: the loss value obtained from the closure\r\n            model: the model to be optimized\r\n            optimizer: current optimizer being used. ``None`` if using manual optimization\r\n            \\*args: Positional arguments intended for the actual function that performs the backward, like\r\n                :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "performs", "the", "actual", "backpropagation", "args", "tensor", "the", "loss", "value", "obtained", "from", "the", "closure", "model", "the", "model", "to", "be", "optimized", "optimizer", "current", "optimizer", "being", "used", "none", "if", "using", "manual", "optimization", "args", "positional", "arguments", "intended", "for", "the", "actual", "function", "that", "performs", "the", "backward", "like", "meth", "torch", "tensor", "backward", "kwargs", "keyword", "arguments", "for", "the", "same", "purpose", "as", "args"], "docstring_summary": "r\"\"\"Performs the actual backpropagation.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 53, "end_line": 72, "hash": "8fe8ec9ab7765dfa20a1f606ff109f15", "complexity": 1, "parameters": ["# type", "tensor", "model", "optimizer", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "_after_closure", "original_string": "def _after_closure(self, model: \"pl.LightningModule\", optimizer: Steppable) -> None:\r\n        \"\"\"Utility to share some code after the closure has been run.\"\"\"\r\n        trainer = model.trainer\r\n        call._call_callback_hooks(trainer, \"on_before_optimizer_step\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_optimizer_step\", optimizer)\r\n        self._clip_gradients(\r\n            model,\r\n            optimizer,\r\n            trainer.gradient_clip_val,\r\n            gradient_clip_algorithm=trainer.gradient_clip_algorithm,\r\n        )", "language": "python", "code": "def _after_closure(self, model: \"pl.LightningModule\", optimizer: Steppable) -> None:\r\n        \"\"\"Utility to share some code after the closure has been run.\"\"\"\r\n        trainer = model.trainer\r\n        call._call_callback_hooks(trainer, \"on_before_optimizer_step\", optimizer)\r\n        call._call_lightning_module_hook(trainer, \"on_before_optimizer_step\", optimizer)\r\n        self._clip_gradients(\r\n            model,\r\n            optimizer,\r\n            trainer.gradient_clip_val,\r\n            gradient_clip_algorithm=trainer.gradient_clip_algorithm,\r\n        )", "code_tokens": ["def", "_after_closure", "(", "self", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "optimizer", ":", "Steppable", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Utility", "to", "share", "some", "code", "after", "the", "closure", "has", "been", "run", ".", "\"", "\"", "\"", "trainer", "=", "model", ".", "trainer", "call", ".", "_call_callback_hooks", "(", "trainer", ",", "\"", "on_before_optimizer_step", "\"", ",", "optimizer", ")", "call", ".", "_call_lightning_module_hook", "(", "trainer", ",", "\"", "on_before_optimizer_step", "\"", ",", "optimizer", ")", "self", ".", "_clip_gradients", "(", "model", ",", "optimizer", ",", "trainer", ".", "gradient_clip_val", ",", "gradient_clip_algorithm", "=", "trainer", ".", "gradient_clip_algorithm", ",", ")"], "docstring": "Utility to share some code after the closure has been run.", "docstring_tokens": ["utility", "to", "share", "some", "code", "after", "the", "closure", "has", "been", "run"], "docstring_summary": "Utility to share some code after the closure has been run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 83, "end_line": 93, "hash": "6ff29e49b626b034878a7d49a125d046", "complexity": 1, "parameters": ["model", "optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "_wrap_closure", "original_string": "def _wrap_closure(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Steppable,\r\n        closure: Callable[[], Any],\r\n    ) -> Any:\r\n        \"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\r\n        hook is called.\r\n\r\n        The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\r\n        consistent with the ``Precision`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\r\n\r\n        \"\"\"\r\n        closure_result = closure()\r\n        self._after_closure(model, optimizer)\r\n        return closure_result", "language": "python", "code": "def _wrap_closure(\r\n        self,\r\n        model: \"pl.LightningModule\",\r\n        optimizer: Steppable,\r\n        closure: Callable[[], Any],\r\n    ) -> Any:\r\n        \"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\r\n        hook is called.\r\n\r\n        The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\r\n        consistent with the ``Precision`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\r\n\r\n        \"\"\"\r\n        closure_result = closure()\r\n        self._after_closure(model, optimizer)\r\n        return closure_result", "code_tokens": ["def", "_wrap_closure", "(", "self", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "optimizer", ":", "Steppable", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "This", "double", "-", "closure", "allows", "makes", "sure", "the", "`", "`", "closure", "`", "`", "is", "executed", "before", "the", "`", "`", "on_before_optimizer_step", "`", "`", "hook", "is", "called", ".", "The", "closure", "(", "generally", ")", "runs", "`", "`", "backward", "`", "`", "so", "this", "allows", "inspecting", "gradients", "in", "this", "hook", ".", "This", "structure", "is", "consistent", "with", "the", "`", "`", "Precision", "`", "`", "subclasses", "that", "cannot", "pass", "`", "`", "optimizer", ".", "step", "(", "closure", ")", "`", "`", "directly", ".", "\"", "\"", "\"", "closure_result", "=", "closure", "(", ")", "self", ".", "_after_closure", "(", "model", ",", "optimizer", ")", "return", "closure_result"], "docstring": "This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\r\n        hook is called.\r\n\r\n        The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\r\n        consistent with the ``Precision`` subclasses that cannot pass ``optimizer.step(closure)`` directly.", "docstring_tokens": ["this", "double", "closure", "allows", "makes", "sure", "the", "closure", "is", "executed", "before", "the", "on_before_optimizer_step", "hook", "is", "called", "the", "closure", "generally", "runs", "backward", "so", "this", "allows", "inspecting", "gradients", "in", "this", "hook", "this", "structure", "is", "consistent", "with", "the", "precision", "subclasses", "that", "cannot", "pass", "optimizer", "step", "closure", "directly"], "docstring_summary": "This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 95, "end_line": 110, "hash": "6d38f614b6fbd1f444614988bca55298", "complexity": 1, "parameters": ["model", "optimizer", "closure", "Any]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(  # type: ignore[override]\r\n        self,\r\n        optimizer: Steppable,\r\n        model: \"pl.LightningModule\",\r\n        closure: Callable[[], Any],\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        closure = partial(self._wrap_closure, model, optimizer, closure)\r\n        return optimizer.step(closure=closure, **kwargs)", "language": "python", "code": "def optimizer_step(  # type: ignore[override]\r\n        self,\r\n        optimizer: Steppable,\r\n        model: \"pl.LightningModule\",\r\n        closure: Callable[[], Any],\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Hook to run the optimizer step.\"\"\"\r\n        closure = partial(self._wrap_closure, model, optimizer, closure)\r\n        return optimizer.step(closure=closure, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Steppable", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Hook", "to", "run", "the", "optimizer", "step", ".", "\"", "\"", "\"", "closure", "=", "partial", "(", "self", ".", "_wrap_closure", ",", "model", ",", "optimizer", ",", "closure", ")", "return", "optimizer", ".", "step", "(", "closure", "=", "closure", ",", "*", "*", "kwargs", ")"], "docstring": "Hook to run the optimizer step.", "docstring_tokens": ["hook", "to", "run", "the", "optimizer", "step"], "docstring_summary": "Hook to run the optimizer step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 113, "end_line": 122, "hash": "ca76174f79fa475f900ee8c264eeeed5", "complexity": 1, "parameters": ["# type", "optimizer", "model", "closure", "Any]", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "clip_gradients", "original_string": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"Clips the gradients.\"\"\"\r\n        if clip_val <= 0:\r\n            return\r\n        if gradient_clip_algorithm == GradClipAlgorithmType.VALUE:\r\n            self.clip_grad_by_value(optimizer, clip_val)\r\n        elif gradient_clip_algorithm == GradClipAlgorithmType.NORM:\r\n            self.clip_grad_by_norm(optimizer, clip_val)", "language": "python", "code": "def clip_gradients(\r\n        self,\r\n        optimizer: Optimizer,\r\n        clip_val: Union[int, float] = 0.0,\r\n        gradient_clip_algorithm: GradClipAlgorithmType = GradClipAlgorithmType.NORM,\r\n    ) -> None:\r\n        \"\"\"Clips the gradients.\"\"\"\r\n        if clip_val <= 0:\r\n            return\r\n        if gradient_clip_algorithm == GradClipAlgorithmType.VALUE:\r\n            self.clip_grad_by_value(optimizer, clip_val)\r\n        elif gradient_clip_algorithm == GradClipAlgorithmType.NORM:\r\n            self.clip_grad_by_norm(optimizer, clip_val)", "code_tokens": ["def", "clip_gradients", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", "=", "0", ".", "0", ",", "gradient_clip_algorithm", ":", "GradClipAlgorithmType", "=", "GradClipAlgorithmType", ".", "NORM", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Clips", "the", "gradients", ".", "\"", "\"", "\"", "if", "clip_val", "<", "=", "0", ":", "return", "if", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "VALUE", ":", "self", ".", "clip_grad_by_value", "(", "optimizer", ",", "clip_val", ")", "elif", "gradient_clip_algorithm", "=", "=", "GradClipAlgorithmType", ".", "NORM", ":", "self", ".", "clip_grad_by_norm", "(", "optimizer", ",", "clip_val", ")"], "docstring": "Clips the gradients.", "docstring_tokens": ["clips", "the", "gradients"], "docstring_summary": "Clips the gradients.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 143, "end_line": 155, "hash": "33d99b4054e293ff080cd18f91f1cd68", "complexity": 4, "parameters": ["optimizer", "clip_val", "float]", "gradient_clip_algorithm"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "clip_grad_by_value", "original_string": "def clip_grad_by_value(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "language": "python", "code": "def clip_grad_by_value(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by value.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_value_(parameters, clip_value=clip_val)", "code_tokens": ["def", "clip_grad_by_value", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "value", ".", "\"", "\"", "\"", "parameters", "=", "self", ".", "main_params", "(", "optimizer", ")", "torch", ".", "nn", ".", "utils", ".", "clip_grad_value_", "(", "parameters", ",", "clip_value", "=", "clip_val", ")"], "docstring": "Clip gradients by value.", "docstring_tokens": ["clip", "gradients", "by", "value"], "docstring_summary": "Clip gradients by value.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 157, "end_line": 160, "hash": "6f4e9d35b5b6935c6460bf361c67bde7", "complexity": 1, "parameters": ["optimizer", "clip_val", "float]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "clip_grad_by_norm", "original_string": "def clip_grad_by_norm(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_norm_(parameters, clip_val)", "language": "python", "code": "def clip_grad_by_norm(self, optimizer: Optimizer, clip_val: Union[int, float]) -> None:\r\n        \"\"\"Clip gradients by norm.\"\"\"\r\n        parameters = self.main_params(optimizer)\r\n        torch.nn.utils.clip_grad_norm_(parameters, clip_val)", "code_tokens": ["def", "clip_grad_by_norm", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "clip_val", ":", "Union", "[", "int", ",", "float", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Clip", "gradients", "by", "norm", ".", "\"", "\"", "\"", "parameters", "=", "self", ".", "main_params", "(", "optimizer", ")", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "clip_val", ")"], "docstring": "Clip gradients by norm.", "docstring_tokens": ["clip", "gradients", "by", "norm"], "docstring_summary": "Clip gradients by norm.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 162, "end_line": 165, "hash": "229d974858333cb25b7734c2650c57e0", "complexity": 1, "parameters": ["optimizer", "clip_val", "float]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "train_step_context", "original_string": "def train_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the training step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def train_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the training step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "train_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "contextmanager", "for", "the", "training", "step", ".", "\"", "\"", "\"", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the training step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "training", "step"], "docstring_summary": "A contextmanager for the training step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 168, "end_line": 171, "hash": "b1ee1b7925fb5f57dcefa8e7f2499604", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "val_step_context", "original_string": "def val_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the validation step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def val_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the validation step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "val_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "contextmanager", "for", "the", "validation", "step", ".", "\"", "\"", "\"", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the validation step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "validation", "step"], "docstring_summary": "A contextmanager for the validation step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 174, "end_line": 177, "hash": "e8ac14be7a5b875322182a46ed04ebfc", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "test_step_context", "original_string": "def test_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the test step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def test_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the test step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "test_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "contextmanager", "for", "the", "test", "step", ".", "\"", "\"", "\"", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the test step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "test", "step"], "docstring_summary": "A contextmanager for the test step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 180, "end_line": 183, "hash": "ebe28fb8682f7f45c5119f369c9f9a97", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\plugins\\precision\\precision.py", "func_name": "predict_step_context", "original_string": "def predict_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the predict step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "language": "python", "code": "def predict_step_context(self) -> Generator[None, None, None]:\r\n        \"\"\"A contextmanager for the predict step.\"\"\"\r\n        with self.forward_context():\r\n            yield", "code_tokens": ["def", "predict_step_context", "(", "self", ")", "-", ">", "Generator", "[", "None", ",", "None", ",", "None", "]", ":", "\"", "\"", "\"", "A", "contextmanager", "for", "the", "predict", "step", ".", "\"", "\"", "\"", "with", "self", ".", "forward_context", "(", ")", ":", "yield"], "docstring": "A contextmanager for the predict step.", "docstring_tokens": ["a", "contextmanager", "for", "the", "predict", "step"], "docstring_summary": "A contextmanager for the predict step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\plugins\\precision\\precision.py", "partition": "train", "function_type": "class_method", "class_name": "Precision", "start_line": 186, "end_line": 189, "hash": "6490a0412657d3af5cab8be6923e109d", "complexity": 2, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\advanced.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        line_count_restriction: float = 1.0,\r\n        dump_stats: bool = False,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            line_count_restriction: this can be used to limit the number of functions\r\n                reported for each action. either an integer (to select a count of lines),\r\n                or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)\r\n\r\n            dump_stats: Whether to save raw profiler results. When ``True`` then ``dirpath`` must be provided.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.profiled_actions: dict[str, cProfile.Profile] = defaultdict(cProfile.Profile)\r\n        self.line_count_restriction = line_count_restriction\r\n        self.dump_stats = dump_stats", "language": "python", "code": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        line_count_restriction: float = 1.0,\r\n        dump_stats: bool = False,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            line_count_restriction: this can be used to limit the number of functions\r\n                reported for each action. either an integer (to select a count of lines),\r\n                or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)\r\n\r\n            dump_stats: Whether to save raw profiler results. When ``True`` then ``dirpath`` must be provided.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.profiled_actions: dict[str, cProfile.Profile] = defaultdict(cProfile.Profile)\r\n        self.line_count_restriction = line_count_restriction\r\n        self.dump_stats = dump_stats", "code_tokens": ["def", "__init__", "(", "self", ",", "dirpath", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "line_count_restriction", ":", "float", "=", "1", ".", "0", ",", "dump_stats", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "dirpath", ":", "Directory", "path", "for", "the", "`", "`", "filename", "`", "`", ".", "If", "`", "`", "dirpath", "`", "`", "is", "`", "`", "None", "`", "`", "but", "`", "`", "filename", "`", "`", "is", "present", ",", "the", "`", "`", "trainer", ".", "log_dir", "`", "`", "(", "from", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "loggers", ".", "tensorboard", ".", "TensorBoardLogger", "`", ")", "will", "be", "used", ".", "filename", ":", "If", "present", ",", "filename", "where", "the", "profiler", "results", "will", "be", "saved", "instead", "of", "printing", "to", "stdout", ".", "The", "`", "`", ".", "txt", "`", "`", "extension", "will", "be", "used", "automatically", ".", "line_count_restriction", ":", "this", "can", "be", "used", "to", "limit", "the", "number", "of", "functions", "reported", "for", "each", "action", ".", "either", "an", "integer", "(", "to", "select", "a", "count", "of", "lines", ")", ",", "or", "a", "decimal", "fraction", "between", "0", ".", "0", "and", "1", ".", "0", "inclusive", "(", "to", "select", "a", "percentage", "of", "lines", ")", "dump_stats", ":", "Whether", "to", "save", "raw", "profiler", "results", ".", "When", "`", "`", "True", "`", "`", "then", "`", "`", "dirpath", "`", "`", "must", "be", "provided", ".", "Raises", ":", "ValueError", ":", "If", "you", "attempt", "to", "stop", "recording", "an", "action", "which", "was", "never", "started", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "dirpath", ",", "filename", "=", "filename", ")", "self", ".", "profiled_actions", ":", "dict", "[", "str", ",", "cProfile", ".", "Profile", "]", "=", "defaultdict", "(", "cProfile", ".", "Profile", ")", "self", ".", "line_count_restriction", "=", "line_count_restriction", "self", ".", "dump_stats", "=", "dump_stats"], "docstring": "Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            line_count_restriction: this can be used to limit the number of functions\r\n                reported for each action. either an integer (to select a count of lines),\r\n                or a decimal fraction between 0.0 and 1.0 inclusive (to select a percentage of lines)\r\n\r\n            dump_stats: Whether to save raw profiler results. When ``True`` then ``dirpath`` must be provided.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to stop recording an action which was never started.", "docstring_tokens": ["args", "dirpath", "directory", "path", "for", "the", "filename", "if", "dirpath", "is", "none", "but", "filename", "is", "present", "the", "trainer", "log_dir", "from", "class", "lightning", "pytorch", "loggers", "tensorboard", "tensorboardlogger", "will", "be", "used", "filename", "if", "present", "filename", "where", "the", "profiler", "results", "will", "be", "saved", "instead", "of", "printing", "to", "stdout", "the", "txt", "extension", "will", "be", "used", "automatically", "line_count_restriction", "this", "can", "be", "used", "to", "limit", "the", "number", "of", "functions", "reported", "for", "each", "action", "either", "an", "integer", "to", "select", "a", "count", "of", "lines", "or", "a", "decimal", "fraction", "between", "0", "0", "and", "1", "0", "inclusive", "to", "select", "a", "percentage", "of", "lines", "dump_stats", "whether", "to", "save", "raw", "profiler", "results", "when", "true", "then", "dirpath", "must", "be", "provided", "raises", "valueerror", "if", "you", "attempt", "to", "stop", "recording", "an", "action", "which", "was", "never", "started"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\advanced.py", "partition": "train", "function_type": "class_method", "class_name": "AdvancedProfiler", "start_line": 42, "end_line": 71, "hash": "9e47cd54cc706047acf563d2c0609ff4", "complexity": 1, "parameters": ["dirpath", "Path]]", "filename", "line_count_restriction", "dump_stats"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "profile", "original_string": "def profile(self, action_name: str) -> Generator:\r\n        \"\"\"Yields a context manager to encapsulate the scope of a profiled action.\r\n\r\n        Example::\r\n\r\n            with self.profile('load training data'):\r\n                # load training data code\r\n\r\n        The profiler will start once you've entered the context and will automatically\r\n        stop once you exit the code block.\r\n\r\n        \"\"\"\r\n        try:\r\n            self.start(action_name)\r\n            yield action_name\r\n        finally:\r\n            self.stop(action_name)", "language": "python", "code": "def profile(self, action_name: str) -> Generator:\r\n        \"\"\"Yields a context manager to encapsulate the scope of a profiled action.\r\n\r\n        Example::\r\n\r\n            with self.profile('load training data'):\r\n                # load training data code\r\n\r\n        The profiler will start once you've entered the context and will automatically\r\n        stop once you exit the code block.\r\n\r\n        \"\"\"\r\n        try:\r\n            self.start(action_name)\r\n            yield action_name\r\n        finally:\r\n            self.stop(action_name)", "code_tokens": ["def", "profile", "(", "self", ",", "action_name", ":", "str", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Yields", "a", "context", "manager", "to", "encapsulate", "the", "scope", "of", "a", "profiled", "action", ".", "Example", ":", ":", "with", "self", ".", "profile", "(", "'", "load", "training", "data", "'", ")", ":", "The", "profiler", "will", "start", "once", "you", "'", "ve", "entered", "the", "context", "and", "will", "automatically", "stop", "once", "you", "exit", "the", "code", "block", ".", "\"", "\"", "\"", "try", ":", "self", ".", "start", "(", "action_name", ")", "yield", "action_name", "finally", ":", "self", ".", "stop", "(", "action_name", ")"], "docstring": "Yields a context manager to encapsulate the scope of a profiled action.\r\n\r\n        Example::\r\n\r\n            with self.profile('load training data'):\r\n                # load training data code\r\n\r\n        The profiler will start once you've entered the context and will automatically\r\n        stop once you exit the code block.", "docstring_tokens": ["yields", "a", "context", "manager", "to", "encapsulate", "the", "scope", "of", "a", "profiled", "action", "example", "with", "self", "profile", "load", "training", "data", "load", "training", "data", "code", "the", "profiler", "will", "start", "once", "you", "ve", "entered", "the", "context", "and", "will", "automatically", "stop", "once", "you", "exit", "the", "code", "block"], "docstring_summary": "Yields a context manager to encapsulate the scope of a profiled action.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\profiler.py", "partition": "train", "function_type": "class_method", "class_name": "Profiler", "start_line": 56, "end_line": 72, "hash": "0498ea81dfdac525569eef0ca83adc33", "complexity": 1, "parameters": ["action_name"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "describe", "original_string": "def describe(self) -> None:\r\n        \"\"\"Logs a profile report after the conclusion of run.\"\"\"\r\n        # users might call `describe` directly as the profilers can be used by themselves.\r\n        # to allow this, we open and close the files within this function by calling `_prepare_streams` and `teardown`\r\n        # manually instead of letting the `Trainer` do it through `setup` and `teardown`\r\n        self._prepare_streams()\r\n        summary = self.summary()\r\n        if summary and self._write_stream is not None:\r\n            self._write_stream(summary)\r\n        if self._output_file is not None:\r\n            self._output_file.flush()\r\n        self.teardown(stage=self._stage)", "language": "python", "code": "def describe(self) -> None:\r\n        \"\"\"Logs a profile report after the conclusion of run.\"\"\"\r\n        # users might call `describe` directly as the profilers can be used by themselves.\r\n        # to allow this, we open and close the files within this function by calling `_prepare_streams` and `teardown`\r\n        # manually instead of letting the `Trainer` do it through `setup` and `teardown`\r\n        self._prepare_streams()\r\n        summary = self.summary()\r\n        if summary and self._write_stream is not None:\r\n            self._write_stream(summary)\r\n        if self._output_file is not None:\r\n            self._output_file.flush()\r\n        self.teardown(stage=self._stage)", "code_tokens": ["def", "describe", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Logs", "a", "profile", "report", "after", "the", "conclusion", "of", "run", ".", "\"", "\"", "\"", "self", ".", "_prepare_streams", "(", ")", "summary", "=", "self", ".", "summary", "(", ")", "if", "summary", "and", "self", ".", "_write_stream", "is", "not", "None", ":", "self", ".", "_write_stream", "(", "summary", ")", "if", "self", ".", "_output_file", "is", "not", "None", ":", "self", ".", "_output_file", ".", "flush", "(", ")", "self", ".", "teardown", "(", "stage", "=", "self", ".", "_stage", ")"], "docstring": "Logs a profile report after the conclusion of run.", "docstring_tokens": ["logs", "a", "profile", "report", "after", "the", "conclusion", "of", "run"], "docstring_summary": "Logs a profile report after the conclusion of run.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\profiler.py", "partition": "train", "function_type": "class_method", "class_name": "Profiler", "start_line": 105, "end_line": 116, "hash": "ceaa1b20eba64148059046dd0da0bceb", "complexity": 4, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "setup", "original_string": "def setup(self, stage: str, local_rank: Optional[int] = None, log_dir: Optional[str] = None) -> None:\r\n        \"\"\"Execute arbitrary pre-profiling set-up steps.\"\"\"\r\n        self._stage = stage\r\n        self._local_rank = local_rank\r\n        self.dirpath = self.dirpath or log_dir", "language": "python", "code": "def setup(self, stage: str, local_rank: Optional[int] = None, log_dir: Optional[str] = None) -> None:\r\n        \"\"\"Execute arbitrary pre-profiling set-up steps.\"\"\"\r\n        self._stage = stage\r\n        self._local_rank = local_rank\r\n        self.dirpath = self.dirpath or log_dir", "code_tokens": ["def", "setup", "(", "self", ",", "stage", ":", "str", ",", "local_rank", ":", "Optional", "[", "int", "]", "=", "None", ",", "log_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Execute", "arbitrary", "pre", "-", "profiling", "set", "-", "up", "steps", ".", "\"", "\"", "\"", "self", ".", "_stage", "=", "stage", "self", ".", "_local_rank", "=", "local_rank", "self", ".", "dirpath", "=", "self", ".", "dirpath", "or", "log_dir"], "docstring": "Execute arbitrary pre-profiling set-up steps.", "docstring_tokens": ["execute", "arbitrary", "pre", "profiling", "set", "up", "steps"], "docstring_summary": "Execute arbitrary pre-profiling set-up steps.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\profiler.py", "partition": "train", "function_type": "class_method", "class_name": "Profiler", "start_line": 129, "end_line": 133, "hash": "97301b121d17fde8b2bf27f407c42b25", "complexity": 2, "parameters": ["stage", "local_rank", "log_dir"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\profiler.py", "func_name": "teardown", "original_string": "def teardown(self, stage: Optional[str]) -> None:\r\n        \"\"\"Execute arbitrary post-profiling tear-down steps.\r\n\r\n        Closes the currently open file and stream.\r\n\r\n        \"\"\"\r\n        self._write_stream = None\r\n        if self._output_file is not None:\r\n            self._output_file.close()\r\n            self._output_file = None  # can't pickle TextIOWrapper\r", "language": "python", "code": "def teardown(self, stage: Optional[str]) -> None:\r\n        \"\"\"Execute arbitrary post-profiling tear-down steps.\r\n\r\n        Closes the currently open file and stream.\r\n\r\n        \"\"\"\r\n        self._write_stream = None\r\n        if self._output_file is not None:\r\n            self._output_file.close()\r\n            self._output_file = None  # can't pickle TextIOWrapper\r", "code_tokens": ["def", "teardown", "(", "self", ",", "stage", ":", "Optional", "[", "str", "]", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Execute", "arbitrary", "post", "-", "profiling", "tear", "-", "down", "steps", ".", "Closes", "the", "currently", "open", "file", "and", "stream", ".", "\"", "\"", "\"", "self", ".", "_write_stream", "=", "None", "if", "self", ".", "_output_file", "is", "not", "None", ":", "self", ".", "_output_file", ".", "close", "(", ")", "self", ".", "_output_file", "=", "None"], "docstring": "Execute arbitrary post-profiling tear-down steps.\r\n\r\n        Closes the currently open file and stream.", "docstring_tokens": ["execute", "arbitrary", "post", "profiling", "tear", "down", "steps", "closes", "the", "currently", "open", "file", "and", "stream"], "docstring_summary": "Execute arbitrary post-profiling tear-down steps.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\profiler.py", "partition": "train", "function_type": "class_method", "class_name": "Profiler", "start_line": 135, "end_line": 144, "hash": "6ef12b910c5ffff374cacf6abfc26e3d", "complexity": 2, "parameters": ["stage"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\pytorch.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        group_by_input_shapes: bool = False,\r\n        emit_nvtx: bool = False,\r\n        export_to_chrome: bool = True,\r\n        row_limit: int = 20,\r\n        sort_by_key: Optional[str] = None,\r\n        record_module_names: bool = True,\r\n        table_kwargs: Optional[dict[str, Any]] = None,\r\n        **profiler_kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of\r\n        different operators inside your model - both on the CPU and GPU.\r\n\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            group_by_input_shapes: Include operator input shapes and group calls by shape.\r\n\r\n            emit_nvtx: Context manager that makes every autograd operation emit an NVTX range\r\n                Run::\r\n\r\n                    nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\r\n\r\n                To visualize, you can either use::\r\n\r\n                    nvvp trace_name.prof\r\n                    torch.autograd.profiler.load_nvprof(path)\r\n\r\n            export_to_chrome: Whether to export the sequence of profiled operators for Chrome.\r\n                It will generate a ``.json`` file which can be read by Chrome.\r\n\r\n            row_limit: Limit the number of rows in a table, ``-1`` is a special value that\r\n                removes the limit completely.\r\n\r\n            sort_by_key: Attribute used to sort entries. By default\r\n                they are printed in the same order as they were registered.\r\n                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,\r\n                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,\r\n                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.\r\n\r\n            record_module_names: Whether to add module names while recording autograd operation.\r\n\r\n            table_kwargs: Dictionary with keyword arguments for the summary table.\r\n\r\n            \\**profiler_kwargs: Keyword arguments for the PyTorch profiler. This depends on your PyTorch version\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If arg ``sort_by_key`` is not present in ``AVAILABLE_SORT_KEYS``.\r\n                If arg ``schedule`` is not a ``Callable``.\r\n                If arg ``schedule`` does not return a ``torch.profiler.ProfilerAction``.\r\n\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n\r\n        self._group_by_input_shapes = group_by_input_shapes and profiler_kwargs.get(\"record_shapes\", False)\r\n        self._emit_nvtx = emit_nvtx\r\n        self._export_to_chrome = export_to_chrome\r\n        self._row_limit = row_limit\r\n        self._sort_by_key = sort_by_key or _default_sort_by_key(profiler_kwargs)\r\n        self._record_module_names = record_module_names\r\n        self._profiler_kwargs = profiler_kwargs\r\n        self._table_kwargs = table_kwargs if table_kwargs is not None else {}\r\n\r\n        self.profiler: Optional[_PROFILER] = None\r\n        self.function_events: Optional[EventList] = None\r\n        self._lightning_module: Optional[LightningModule] = None  # set by ProfilerConnector\r\n        self._register: Optional[RegisterRecordFunction] = None\r\n        self._parent_profiler: Optional[AbstractContextManager] = None\r\n        self._recording_map: dict[str, record_function] = {}\r\n        self._start_action_name: Optional[str] = None\r\n        self._schedule: Optional[ScheduleWrapper] = None\r\n\r\n        if _KINETO_AVAILABLE:\r\n            self._init_kineto(profiler_kwargs)\r\n\r\n        if self._sort_by_key not in self.AVAILABLE_SORT_KEYS:\r\n            raise MisconfigurationException(\r\n                f\"Found sort_by_key: {self._sort_by_key}. Should be within {self.AVAILABLE_SORT_KEYS}. \"\r\n            )\r\n\r\n        for key in self._table_kwargs:\r\n            if key in {\"sort_by\", \"row_limit\"}:\r\n                raise KeyError(\r\n                    f\"Found invalid table_kwargs key: {key}. This is already a positional argument of the Profiler.\"\r\n                )\r\n            valid_table_keys = set(inspect.signature(EventList.table).parameters.keys()) - {\r\n                \"self\",\r\n                \"sort_by\",\r\n                \"row_limit\",\r\n            }\r\n            if key not in valid_table_keys:\r\n                raise KeyError(f\"Found invalid table_kwargs key: {key}. Should be within {valid_table_keys}.\")", "language": "python", "code": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        group_by_input_shapes: bool = False,\r\n        emit_nvtx: bool = False,\r\n        export_to_chrome: bool = True,\r\n        row_limit: int = 20,\r\n        sort_by_key: Optional[str] = None,\r\n        record_module_names: bool = True,\r\n        table_kwargs: Optional[dict[str, Any]] = None,\r\n        **profiler_kwargs: Any,\r\n    ) -> None:\r\n        r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of\r\n        different operators inside your model - both on the CPU and GPU.\r\n\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            group_by_input_shapes: Include operator input shapes and group calls by shape.\r\n\r\n            emit_nvtx: Context manager that makes every autograd operation emit an NVTX range\r\n                Run::\r\n\r\n                    nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\r\n\r\n                To visualize, you can either use::\r\n\r\n                    nvvp trace_name.prof\r\n                    torch.autograd.profiler.load_nvprof(path)\r\n\r\n            export_to_chrome: Whether to export the sequence of profiled operators for Chrome.\r\n                It will generate a ``.json`` file which can be read by Chrome.\r\n\r\n            row_limit: Limit the number of rows in a table, ``-1`` is a special value that\r\n                removes the limit completely.\r\n\r\n            sort_by_key: Attribute used to sort entries. By default\r\n                they are printed in the same order as they were registered.\r\n                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,\r\n                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,\r\n                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.\r\n\r\n            record_module_names: Whether to add module names while recording autograd operation.\r\n\r\n            table_kwargs: Dictionary with keyword arguments for the summary table.\r\n\r\n            \\**profiler_kwargs: Keyword arguments for the PyTorch profiler. This depends on your PyTorch version\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If arg ``sort_by_key`` is not present in ``AVAILABLE_SORT_KEYS``.\r\n                If arg ``schedule`` is not a ``Callable``.\r\n                If arg ``schedule`` does not return a ``torch.profiler.ProfilerAction``.\r\n\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n\r\n        self._group_by_input_shapes = group_by_input_shapes and profiler_kwargs.get(\"record_shapes\", False)\r\n        self._emit_nvtx = emit_nvtx\r\n        self._export_to_chrome = export_to_chrome\r\n        self._row_limit = row_limit\r\n        self._sort_by_key = sort_by_key or _default_sort_by_key(profiler_kwargs)\r\n        self._record_module_names = record_module_names\r\n        self._profiler_kwargs = profiler_kwargs\r\n        self._table_kwargs = table_kwargs if table_kwargs is not None else {}\r\n\r\n        self.profiler: Optional[_PROFILER] = None\r\n        self.function_events: Optional[EventList] = None\r\n        self._lightning_module: Optional[LightningModule] = None  # set by ProfilerConnector\r\n        self._register: Optional[RegisterRecordFunction] = None\r\n        self._parent_profiler: Optional[AbstractContextManager] = None\r\n        self._recording_map: dict[str, record_function] = {}\r\n        self._start_action_name: Optional[str] = None\r\n        self._schedule: Optional[ScheduleWrapper] = None\r\n\r\n        if _KINETO_AVAILABLE:\r\n            self._init_kineto(profiler_kwargs)\r\n\r\n        if self._sort_by_key not in self.AVAILABLE_SORT_KEYS:\r\n            raise MisconfigurationException(\r\n                f\"Found sort_by_key: {self._sort_by_key}. Should be within {self.AVAILABLE_SORT_KEYS}. \"\r\n            )\r\n\r\n        for key in self._table_kwargs:\r\n            if key in {\"sort_by\", \"row_limit\"}:\r\n                raise KeyError(\r\n                    f\"Found invalid table_kwargs key: {key}. This is already a positional argument of the Profiler.\"\r\n                )\r\n            valid_table_keys = set(inspect.signature(EventList.table).parameters.keys()) - {\r\n                \"self\",\r\n                \"sort_by\",\r\n                \"row_limit\",\r\n            }\r\n            if key not in valid_table_keys:\r\n                raise KeyError(f\"Found invalid table_kwargs key: {key}. Should be within {valid_table_keys}.\")", "code_tokens": ["def", "__init__", "(", "self", ",", "dirpath", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "group_by_input_shapes", ":", "bool", "=", "False", ",", "emit_nvtx", ":", "bool", "=", "False", ",", "export_to_chrome", ":", "bool", "=", "True", ",", "row_limit", ":", "int", "=", "20", ",", "sort_by_key", ":", "Optional", "[", "str", "]", "=", "None", ",", "record_module_names", ":", "bool", "=", "True", ",", "table_kwargs", ":", "Optional", "[", "dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "*", "*", "profiler_kwargs", ":", "Any", ",", ")", "-", ">", "None", ":", "r", "\"", "\"", "\"", "This", "profiler", "uses", "PyTorch", "'", "s", "Autograd", "Profiler", "and", "lets", "you", "inspect", "the", "cost", "of", "different", "operators", "inside", "your", "model", "-", "both", "on", "the", "CPU", "and", "GPU", ".", "Args", ":", "dirpath", ":", "Directory", "path", "for", "the", "`", "`", "filename", "`", "`", ".", "If", "`", "`", "dirpath", "`", "`", "is", "`", "`", "None", "`", "`", "but", "`", "`", "filename", "`", "`", "is", "present", ",", "the", "`", "`", "trainer", ".", "log_dir", "`", "`", "(", "from", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "loggers", ".", "tensorboard", ".", "TensorBoardLogger", "`", ")", "will", "be", "used", ".", "filename", ":", "If", "present", ",", "filename", "where", "the", "profiler", "results", "will", "be", "saved", "instead", "of", "printing", "to", "stdout", ".", "The", "`", "`", ".", "txt", "`", "`", "extension", "will", "be", "used", "automatically", ".", "group_by_input_shapes", ":", "Include", "operator", "input", "shapes", "and", "group", "calls", "by", "shape", ".", "emit_nvtx", ":", "Context", "manager", "that", "makes", "every", "autograd", "operation", "emit", "an", "NVTX", "range", "Run", ":", ":", "nvprof", "-", "-", "profile", "-", "from", "-", "start", "off", "-", "o", "trace_name", ".", "prof", "-", "-", "<", "regular", "command", "here", ">", "To", "visualize", ",", "you", "can", "either", "use", ":", ":", "nvvp", "trace_name", ".", "prof", "torch", ".", "autograd", ".", "profiler", ".", "load_nvprof", "(", "path", ")", "export_to_chrome", ":", "Whether", "to", "export", "the", "sequence", "of", "profiled", "operators", "for", "Chrome", ".", "It", "will", "generate", "a", "`", "`", ".", "json", "`", "`", "file", "which", "can", "be", "read", "by", "Chrome", ".", "row_limit", ":", "Limit", "the", "number", "of", "rows", "in", "a", "table", ",", "`", "`", "-", "1", "`", "`", "is", "a", "special", "value", "that", "removes", "the", "limit", "completely", ".", "sort_by_key", ":", "Attribute", "used", "to", "sort", "entries", ".", "By", "default", "they", "are", "printed", "in", "the", "same", "order", "as", "they", "were", "registered", ".", "Valid", "keys", "include", ":", "`", "`", "cpu_time", "`", "`", ",", "`", "`", "cuda_time", "`", "`", ",", "`", "`", "cpu_time_total", "`", "`", ",", "`", "`", "cuda_time_total", "`", "`", ",", "`", "`", "cpu_memory_usage", "`", "`", ",", "`", "`", "cuda_memory_usage", "`", "`", ",", "`", "`", "self_cpu_memory_usage", "`", "`", ",", "`", "`", "self_cuda_memory_usage", "`", "`", ",", "`", "`", "count", "`", "`", ".", "record_module_names", ":", "Whether", "to", "add", "module", "names", "while", "recording", "autograd", "operation", ".", "table_kwargs", ":", "Dictionary", "with", "keyword", "arguments", "for", "the", "summary", "table", ".", "\\", "*", "*", "profiler_kwargs", ":", "Keyword", "arguments", "for", "the", "PyTorch", "profiler", ".", "This", "depends", "on", "your", "PyTorch", "version", "Raises", ":", "MisconfigurationException", ":", "If", "arg", "`", "`", "sort_by_key", "`", "`", "is", "not", "present", "in", "`", "`", "AVAILABLE_SORT_KEYS", "`", "`", ".", "If", "arg", "`", "`", "schedule", "`", "`", "is", "not", "a", "`", "`", "Callable", "`", "`", ".", "If", "arg", "`", "`", "schedule", "`", "`", "does", "not", "return", "a", "`", "`", "torch", ".", "profiler", ".", "ProfilerAction", "`", "`", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "dirpath", ",", "filename", "=", "filename", ")", "self", ".", "_group_by_input_shapes", "=", "group_by_input_shapes", "and", "profiler_kwargs", ".", "get", "(", "\"", "record_shapes", "\"", ",", "False", ")", "self", ".", "_emit_nvtx", "=", "emit_nvtx", "self", ".", "_export_to_chrome", "=", "export_to_chrome", "self", ".", "_row_limit", "=", "row_limit", "self", ".", "_sort_by_key", "=", "sort_by_key", "or", "_default_sort_by_key", "(", "profiler_kwargs", ")", "self", ".", "_record_module_names", "=", "record_module_names", "self", ".", "_profiler_kwargs", "=", "profiler_kwargs", "self", ".", "_table_kwargs", "=", "table_kwargs", "if", "table_kwargs", "is", "not", "None", "else", "{", "}", "self", ".", "profiler", ":", "Optional", "[", "_PROFILER", "]", "=", "None", "self", ".", "function_events", ":", "Optional", "[", "EventList", "]", "=", "None", "self", ".", "_lightning_module", ":", "Optional", "[", "LightningModule", "]", "=", "None", "self", ".", "_register", ":", "Optional", "[", "RegisterRecordFunction", "]", "=", "None", "self", ".", "_parent_profiler", ":", "Optional", "[", "AbstractContextManager", "]", "=", "None", "self", ".", "_recording_map", ":", "dict", "[", "str", ",", "record_function", "]", "=", "{", "}", "self", ".", "_start_action_name", ":", "Optional", "[", "str", "]", "=", "None", "self", ".", "_schedule", ":", "Optional", "[", "ScheduleWrapper", "]", "=", "None", "if", "_KINETO_AVAILABLE", ":", "self", ".", "_init_kineto", "(", "profiler_kwargs", ")", "if", "self", ".", "_sort_by_key", "not", "in", "self", ".", "AVAILABLE_SORT_KEYS", ":", "raise", "MisconfigurationException", "(", "f", "\"", "Found", "sort_by_key", ":", "{", "self", ".", "_sort_by_key", "}", ".", "Should", "be", "within", "{", "self", ".", "AVAILABLE_SORT_KEYS", "}", ".", "\"", ")", "for", "key", "in", "self", ".", "_table_kwargs", ":", "if", "key", "in", "{", "\"", "sort_by", "\"", ",", "\"", "row_limit", "\"", "}", ":", "raise", "KeyError", "(", "f", "\"", "Found", "invalid", "table_kwargs", "key", ":", "{", "key", "}", ".", "This", "is", "already", "a", "positional", "argument", "of", "the", "Profiler", ".", "\"", ")", "valid_table_keys", "=", "set", "(", "inspect", ".", "signature", "(", "EventList", ".", "table", ")", ".", "parameters", ".", "keys", "(", ")", ")", "-", "{", "\"", "self", "\"", ",", "\"", "sort_by", "\"", ",", "\"", "row_limit", "\"", ",", "}", "if", "key", "not", "in", "valid_table_keys", ":", "raise", "KeyError", "(", "f", "\"", "Found", "invalid", "table_kwargs", "key", ":", "{", "key", "}", ".", "Should", "be", "within", "{", "valid_table_keys", "}", ".", "\"", ")"], "docstring": "r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of\r\n        different operators inside your model - both on the CPU and GPU.\r\n\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            group_by_input_shapes: Include operator input shapes and group calls by shape.\r\n\r\n            emit_nvtx: Context manager that makes every autograd operation emit an NVTX range\r\n                Run::\r\n\r\n                    nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\r\n\r\n                To visualize, you can either use::\r\n\r\n                    nvvp trace_name.prof\r\n                    torch.autograd.profiler.load_nvprof(path)\r\n\r\n            export_to_chrome: Whether to export the sequence of profiled operators for Chrome.\r\n                It will generate a ``.json`` file which can be read by Chrome.\r\n\r\n            row_limit: Limit the number of rows in a table, ``-1`` is a special value that\r\n                removes the limit completely.\r\n\r\n            sort_by_key: Attribute used to sort entries. By default\r\n                they are printed in the same order as they were registered.\r\n                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,\r\n                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,\r\n                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.\r\n\r\n            record_module_names: Whether to add module names while recording autograd operation.\r\n\r\n            table_kwargs: Dictionary with keyword arguments for the summary table.\r\n\r\n            \\**profiler_kwargs: Keyword arguments for the PyTorch profiler. This depends on your PyTorch version\r\n\r\n        Raises:\r\n            MisconfigurationException:\r\n                If arg ``sort_by_key`` is not present in ``AVAILABLE_SORT_KEYS``.\r\n                If arg ``schedule`` is not a ``Callable``.\r\n                If arg ``schedule`` does not return a ``torch.profiler.ProfilerAction``.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "this", "profiler", "uses", "pytorch", "s", "autograd", "profiler", "and", "lets", "you", "inspect", "the", "cost", "of", "different", "operators", "inside", "your", "model", "both", "on", "the", "cpu", "and", "gpu", "args", "dirpath", "directory", "path", "for", "the", "filename", "if", "dirpath", "is", "none", "but", "filename", "is", "present", "the", "trainer", "log_dir", "from", "class", "lightning", "pytorch", "loggers", "tensorboard", "tensorboardlogger", "will", "be", "used", "filename", "if", "present", "filename", "where", "the", "profiler", "results", "will", "be", "saved", "instead", "of", "printing", "to", "stdout", "the", "txt", "extension", "will", "be", "used", "automatically", "group_by_input_shapes", "include", "operator", "input", "shapes", "and", "group", "calls", "by", "shape", "emit_nvtx", "context", "manager", "that", "makes", "every", "autograd", "operation", "emit", "an", "nvtx", "range", "run", "nvprof", "profile", "from", "start", "off", "o", "trace_name", "prof", "regular", "command", "here", "to", "visualize", "you", "can", "either", "use", "nvvp", "trace_name", "prof", "torch", "autograd", "profiler", "load_nvprof", "path", "export_to_chrome", "whether", "to", "export", "the", "sequence", "of", "profiled", "operators", "for", "chrome", "it", "will", "generate", "a", "json", "file", "which", "can", "be", "read", "by", "chrome", "row_limit", "limit", "the", "number", "of", "rows", "in", "a", "table", "1", "is", "a", "special", "value", "that", "removes", "the", "limit", "completely", "sort_by_key", "attribute", "used", "to", "sort", "entries", "by", "default", "they", "are", "printed", "in", "the", "same", "order", "as", "they", "were", "registered", "valid", "keys", "include", "cpu_time", "cuda_time", "cpu_time_total", "cuda_time_total", "cpu_memory_usage", "cuda_memory_usage", "self_cpu_memory_usage", "self_cuda_memory_usage", "count", "record_module_names", "whether", "to", "add", "module", "names", "while", "recording", "autograd", "operation", "table_kwargs", "dictionary", "with", "keyword", "arguments", "for", "the", "summary", "table", "profiler_kwargs", "keyword", "arguments", "for", "the", "pytorch", "profiler", "this", "depends", "on", "your", "pytorch", "version", "raises", "misconfigurationexception", "if", "arg", "sort_by_key", "is", "not", "present", "in", "available_sort_keys", "if", "arg", "schedule", "is", "not", "a", "callable", "if", "arg", "schedule", "does", "not", "return", "a", "torch", "profiler", "profileraction"], "docstring_summary": "r\"\"\"This profiler uses PyTorch's Autograd Profiler and lets you inspect the cost of", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\pytorch.py", "partition": "train", "function_type": "class_method", "class_name": "PyTorchProfiler", "start_line": 232, "end_line": 332, "hash": "bb600792fa1d94b3823ab3cd2d9b0d9f", "complexity": 9, "parameters": ["dirpath", "Path]]", "filename", "group_by_input_shapes", "emit_nvtx", "export_to_chrome", "row_limit", "sort_by_key", "record_module_names", "table_kwargs", "Any]]", "**profiler_kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\simple.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        extended: bool = True,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            extended: If ``True``, adds extra columns representing number of calls and percentage of total time spent on\r\n                respective action.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to start an action which has already started, or\r\n                if you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.current_actions: dict[str, float] = {}\r\n        self.recorded_durations: dict = defaultdict(list)\r\n        self.extended = extended\r\n        self.start_time = time.monotonic()", "language": "python", "code": "def __init__(\r\n        self,\r\n        dirpath: Optional[Union[str, Path]] = None,\r\n        filename: Optional[str] = None,\r\n        extended: bool = True,\r\n    ) -> None:\r\n        \"\"\"\r\n        Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            extended: If ``True``, adds extra columns representing number of calls and percentage of total time spent on\r\n                respective action.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to start an action which has already started, or\r\n                if you attempt to stop recording an action which was never started.\r\n        \"\"\"\r\n        super().__init__(dirpath=dirpath, filename=filename)\r\n        self.current_actions: dict[str, float] = {}\r\n        self.recorded_durations: dict = defaultdict(list)\r\n        self.extended = extended\r\n        self.start_time = time.monotonic()", "code_tokens": ["def", "__init__", "(", "self", ",", "dirpath", ":", "Optional", "[", "Union", "[", "str", ",", "Path", "]", "]", "=", "None", ",", "filename", ":", "Optional", "[", "str", "]", "=", "None", ",", "extended", ":", "bool", "=", "True", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Args", ":", "dirpath", ":", "Directory", "path", "for", "the", "`", "`", "filename", "`", "`", ".", "If", "`", "`", "dirpath", "`", "`", "is", "`", "`", "None", "`", "`", "but", "`", "`", "filename", "`", "`", "is", "present", ",", "the", "`", "`", "trainer", ".", "log_dir", "`", "`", "(", "from", ":", "class", ":", "`", "~", "lightning", ".", "pytorch", ".", "loggers", ".", "tensorboard", ".", "TensorBoardLogger", "`", ")", "will", "be", "used", ".", "filename", ":", "If", "present", ",", "filename", "where", "the", "profiler", "results", "will", "be", "saved", "instead", "of", "printing", "to", "stdout", ".", "The", "`", "`", ".", "txt", "`", "`", "extension", "will", "be", "used", "automatically", ".", "extended", ":", "If", "`", "`", "True", "`", "`", ",", "adds", "extra", "columns", "representing", "number", "of", "calls", "and", "percentage", "of", "total", "time", "spent", "on", "respective", "action", ".", "Raises", ":", "ValueError", ":", "If", "you", "attempt", "to", "start", "an", "action", "which", "has", "already", "started", ",", "or", "if", "you", "attempt", "to", "stop", "recording", "an", "action", "which", "was", "never", "started", ".", "\"", "\"", "\"", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "dirpath", ",", "filename", "=", "filename", ")", "self", ".", "current_actions", ":", "dict", "[", "str", ",", "float", "]", "=", "{", "}", "self", ".", "recorded_durations", ":", "dict", "=", "defaultdict", "(", "list", ")", "self", ".", "extended", "=", "extended", "self", ".", "start_time", "=", "time", ".", "monotonic", "(", ")"], "docstring": "Args:\r\n            dirpath: Directory path for the ``filename``. If ``dirpath`` is ``None`` but ``filename`` is present, the\r\n                ``trainer.log_dir`` (from :class:`~lightning.pytorch.loggers.tensorboard.TensorBoardLogger`)\r\n                will be used.\r\n\r\n            filename: If present, filename where the profiler results will be saved instead of printing to stdout.\r\n                The ``.txt`` extension will be used automatically.\r\n\r\n            extended: If ``True``, adds extra columns representing number of calls and percentage of total time spent on\r\n                respective action.\r\n\r\n        Raises:\r\n            ValueError:\r\n                If you attempt to start an action which has already started, or\r\n                if you attempt to stop recording an action which was never started.", "docstring_tokens": ["args", "dirpath", "directory", "path", "for", "the", "filename", "if", "dirpath", "is", "none", "but", "filename", "is", "present", "the", "trainer", "log_dir", "from", "class", "lightning", "pytorch", "loggers", "tensorboard", "tensorboardlogger", "will", "be", "used", "filename", "if", "present", "filename", "where", "the", "profiler", "results", "will", "be", "saved", "instead", "of", "printing", "to", "stdout", "the", "txt", "extension", "will", "be", "used", "automatically", "extended", "if", "true", "adds", "extra", "columns", "representing", "number", "of", "calls", "and", "percentage", "of", "total", "time", "spent", "on", "respective", "action", "raises", "valueerror", "if", "you", "attempt", "to", "start", "an", "action", "which", "has", "already", "started", "or", "if", "you", "attempt", "to", "stop", "recording", "an", "action", "which", "was", "never", "started"], "docstring_summary": "Args:", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\simple.py", "partition": "train", "function_type": "class_method", "class_name": "SimpleProfiler", "start_line": 39, "end_line": 66, "hash": "f807639ec15e1d6a9d5e992bb960d9b6", "complexity": 1, "parameters": ["dirpath", "Path]]", "filename", "extended"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\profilers\\xla.py", "func_name": "__init__", "original_string": "def __init__(self, port: int = 9012) -> None:\r\n        \"\"\"XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU\r\n        performance tools.\r\n\r\n        Args:\r\n            port: the port to start the profiler server on. An exception is\r\n                raised if the provided port is invalid or busy.\r\n\r\n        \"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            raise ModuleNotFoundError(str(_XLA_AVAILABLE))\r\n        super().__init__(dirpath=None, filename=None)\r\n        self.port = port\r\n        self._recording_map: dict = {}\r\n        self._step_recoding_map: dict = {}\r\n        self._start_trace: bool = False", "language": "python", "code": "def __init__(self, port: int = 9012) -> None:\r\n        \"\"\"XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU\r\n        performance tools.\r\n\r\n        Args:\r\n            port: the port to start the profiler server on. An exception is\r\n                raised if the provided port is invalid or busy.\r\n\r\n        \"\"\"\r\n        if not _XLA_AVAILABLE:\r\n            raise ModuleNotFoundError(str(_XLA_AVAILABLE))\r\n        super().__init__(dirpath=None, filename=None)\r\n        self.port = port\r\n        self._recording_map: dict = {}\r\n        self._step_recoding_map: dict = {}\r\n        self._start_trace: bool = False", "code_tokens": ["def", "__init__", "(", "self", ",", "port", ":", "int", "=", "9012", ")", "-", ">", "None", ":", "\"", "\"", "\"", "XLA", "Profiler", "will", "help", "you", "debug", "and", "optimize", "training", "workload", "performance", "for", "your", "models", "using", "Cloud", "TPU", "performance", "tools", ".", "Args", ":", "port", ":", "the", "port", "to", "start", "the", "profiler", "server", "on", ".", "An", "exception", "is", "raised", "if", "the", "provided", "port", "is", "invalid", "or", "busy", ".", "\"", "\"", "\"", "if", "not", "_XLA_AVAILABLE", ":", "raise", "ModuleNotFoundError", "(", "str", "(", "_XLA_AVAILABLE", ")", ")", "super", "(", ")", ".", "__init__", "(", "dirpath", "=", "None", ",", "filename", "=", "None", ")", "self", ".", "port", "=", "port", "self", ".", "_recording_map", ":", "dict", "=", "{", "}", "self", ".", "_step_recoding_map", ":", "dict", "=", "{", "}", "self", ".", "_start_trace", ":", "bool", "=", "False"], "docstring": "XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU\r\n        performance tools.\r\n\r\n        Args:\r\n            port: the port to start the profiler server on. An exception is\r\n                raised if the provided port is invalid or busy.", "docstring_tokens": ["xla", "profiler", "will", "help", "you", "debug", "and", "optimize", "training", "workload", "performance", "for", "your", "models", "using", "cloud", "tpu", "performance", "tools", "args", "port", "the", "port", "to", "start", "the", "profiler", "server", "on", "an", "exception", "is", "raised", "if", "the", "provided", "port", "is", "invalid", "or", "busy"], "docstring_summary": "XLA Profiler will help you debug and optimize training workload performance for your models using Cloud TPU", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\profilers\\xla.py", "partition": "train", "function_type": "class_method", "class_name": "XLAProfiler", "start_line": 33, "end_line": 48, "hash": "3e361dd11a381da48af6281876a2f69f", "complexity": 2, "parameters": ["port"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module.py", "func_name": "configure_serialization", "original_string": "def configure_serialization(self) -> tuple[dict[str, Callable], dict[str, Callable]]:\r\n        \"\"\"Returns a tuple of dictionaries.\r\n\r\n        The first dictionary contains the name of the ``serve_step`` input variables name as its keys\r\n        and the associated de-serialization function (e.g function to convert a payload to tensors).\r\n\r\n        The second dictionary contains the name of the ``serve_step`` output variables name as its keys\r\n        and the associated serialization function (e.g function to convert a tensors into payload).\r\n\r\n        \"\"\"", "language": "python", "code": "def configure_serialization(self) -> tuple[dict[str, Callable], dict[str, Callable]]:\r\n        \"\"\"Returns a tuple of dictionaries.\r\n\r\n        The first dictionary contains the name of the ``serve_step`` input variables name as its keys\r\n        and the associated de-serialization function (e.g function to convert a payload to tensors).\r\n\r\n        The second dictionary contains the name of the ``serve_step`` output variables name as its keys\r\n        and the associated serialization function (e.g function to convert a tensors into payload).\r\n\r\n        \"\"\"", "code_tokens": ["def", "configure_serialization", "(", "self", ")", "-", ">", "tuple", "[", "dict", "[", "str", ",", "Callable", "]", ",", "dict", "[", "str", ",", "Callable", "]", "]", ":", "\"", "\"", "\"", "Returns", "a", "tuple", "of", "dictionaries", ".", "The", "first", "dictionary", "contains", "the", "name", "of", "the", "`", "`", "serve_step", "`", "`", "input", "variables", "name", "as", "its", "keys", "and", "the", "associated", "de", "-", "serialization", "function", "(", "e", ".", "g", "function", "to", "convert", "a", "payload", "to", "tensors", ")", ".", "The", "second", "dictionary", "contains", "the", "name", "of", "the", "`", "`", "serve_step", "`", "`", "output", "variables", "name", "as", "its", "keys", "and", "the", "associated", "serialization", "function", "(", "e", ".", "g", "function", "to", "convert", "a", "tensors", "into", "payload", ")", ".", "\"", "\"", "\""], "docstring": "Returns a tuple of dictionaries.\r\n\r\n        The first dictionary contains the name of the ``serve_step`` input variables name as its keys\r\n        and the associated de-serialization function (e.g function to convert a payload to tensors).\r\n\r\n        The second dictionary contains the name of the ``serve_step`` output variables name as its keys\r\n        and the associated serialization function (e.g function to convert a tensors into payload).", "docstring_tokens": ["returns", "a", "tuple", "of", "dictionaries", "the", "first", "dictionary", "contains", "the", "name", "of", "the", "serve_step", "input", "variables", "name", "as", "its", "keys", "and", "the", "associated", "de", "serialization", "function", "e", "g", "function", "to", "convert", "a", "payload", "to", "tensors", "the", "second", "dictionary", "contains", "the", "name", "of", "the", "serve_step", "output", "variables", "name", "as", "its", "keys", "and", "the", "associated", "serialization", "function", "e", "g", "function", "to", "convert", "a", "tensors", "into", "payload"], "docstring_summary": "Returns a tuple of dictionaries.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\serve\\servable_module.py", "partition": "train", "function_type": "class_method", "class_name": "ServableModule", "start_line": 62, "end_line": 71, "hash": "25e802103ee2096e95612ee5775bf7fe", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module.py", "func_name": "serve_step", "original_string": "def serve_step(self, *args: Tensor, **kwargs: Tensor) -> dict[str, Tensor]:\r\n        r\"\"\"Returns the predictions of your model as a dictionary.\r\n\r\n        .. code-block:: python\r\n\r\n            def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\r\n                return {\"predictions\": self(x)}\r\n\r\n        Args:\r\n            args: The output from de-serializer functions provided by the ``configure_serialization`` hook.\r\n            kwargs: The keyword output of the de-serializer functions provided by the ``configure_serialization`` hook.\r\n\r\n        Return:\r\n            - ``dict`` - A dictionary with their associated tensors.\r\n\r\n        \"\"\"", "language": "python", "code": "def serve_step(self, *args: Tensor, **kwargs: Tensor) -> dict[str, Tensor]:\r\n        r\"\"\"Returns the predictions of your model as a dictionary.\r\n\r\n        .. code-block:: python\r\n\r\n            def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\r\n                return {\"predictions\": self(x)}\r\n\r\n        Args:\r\n            args: The output from de-serializer functions provided by the ``configure_serialization`` hook.\r\n            kwargs: The keyword output of the de-serializer functions provided by the ``configure_serialization`` hook.\r\n\r\n        Return:\r\n            - ``dict`` - A dictionary with their associated tensors.\r\n\r\n        \"\"\"", "code_tokens": ["def", "serve_step", "(", "self", ",", "*", "args", ":", "Tensor", ",", "*", "*", "kwargs", ":", "Tensor", ")", "-", ">", "dict", "[", "str", ",", "Tensor", "]", ":", "r", "\"", "\"", "\"", "Returns", "the", "predictions", "of", "your", "model", "as", "a", "dictionary", ".", ".", ".", "code", "-", "block", ":", ":", "python", "def", "serve_step", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "-", ">", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "return", "{", "\"", "predictions", "\"", ":", "self", "(", "x", ")", "}", "Args", ":", "args", ":", "The", "output", "from", "de", "-", "serializer", "functions", "provided", "by", "the", "`", "`", "configure_serialization", "`", "`", "hook", ".", "kwargs", ":", "The", "keyword", "output", "of", "the", "de", "-", "serializer", "functions", "provided", "by", "the", "`", "`", "configure_serialization", "`", "`", "hook", ".", "Return", ":", "-", "`", "`", "dict", "`", "`", "-", "A", "dictionary", "with", "their", "associated", "tensors", ".", "\"", "\"", "\""], "docstring": "r\"\"\"Returns the predictions of your model as a dictionary.\r\n\r\n        .. code-block:: python\r\n\r\n            def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\r\n                return {\"predictions\": self(x)}\r\n\r\n        Args:\r\n            args: The output from de-serializer functions provided by the ``configure_serialization`` hook.\r\n            kwargs: The keyword output of the de-serializer functions provided by the ``configure_serialization`` hook.\r\n\r\n        Return:\r\n            - ``dict`` - A dictionary with their associated tensors.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "returns", "the", "predictions", "of", "your", "model", "as", "a", "dictionary", "code", "block", "python", "def", "serve_step", "self", "x", "torch", "tensor", "dict", "str", "torch", "tensor", "return", "predictions", "self", "x", "args", "args", "the", "output", "from", "de", "serializer", "functions", "provided", "by", "the", "configure_serialization", "hook", "kwargs", "the", "keyword", "output", "of", "the", "de", "serializer", "functions", "provided", "by", "the", "configure_serialization", "hook", "return", "dict", "a", "dictionary", "with", "their", "associated", "tensors"], "docstring_summary": "r\"\"\"Returns the predictions of your model as a dictionary.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\serve\\servable_module.py", "partition": "train", "function_type": "class_method", "class_name": "ServableModule", "start_line": 74, "end_line": 89, "hash": "f2cd859e45f07cf47e8e1848c365e27c", "complexity": 1, "parameters": ["*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\serve\\servable_module_validator.py", "func_name": "_start_server", "original_string": "def _start_server(servable_model: ServableModule, host: str, port: int, _: bool) -> None:\r\n        \"\"\"This method starts a server with a serve and ping endpoints.\"\"\"\r\n        from fastapi import Body, FastAPI\r\n        from uvicorn import run\r\n\r\n        app = FastAPI()\r\n\r\n        deserializers, serializers = servable_model.configure_serialization()\r\n\r\n        # Note: This isn't the original version, but a copy.\r\n        servable_model.eval()\r\n\r\n        @app.get(\"/ping\")\r\n        def ping() -> bool:\r\n            return True\r\n\r\n        @app.post(\"/serve\")\r\n        async def serve(payload: dict = Body(...)) -> dict[str, Any]:\r\n            body = payload[\"body\"]\r\n\r\n            for key, deserializer in deserializers.items():\r\n                body[key] = deserializer(body[key])\r\n\r\n            with torch.no_grad():\r\n                output = servable_model.serve_step(**body)\r\n\r\n            if not isinstance(output, dict):\r\n                raise Exception(f\"Please, return your outputs as a dictionary. Found {output}\")\r\n\r\n            for key, serializer in serializers.items():\r\n                output[key] = serializer(output[key])\r\n\r\n            return output\r\n\r\n        run(app, host=host, port=port, log_level=\"error\")", "language": "python", "code": "def _start_server(servable_model: ServableModule, host: str, port: int, _: bool) -> None:\r\n        \"\"\"This method starts a server with a serve and ping endpoints.\"\"\"\r\n        from fastapi import Body, FastAPI\r\n        from uvicorn import run\r\n\r\n        app = FastAPI()\r\n\r\n        deserializers, serializers = servable_model.configure_serialization()\r\n\r\n        # Note: This isn't the original version, but a copy.\r\n        servable_model.eval()\r\n\r\n        @app.get(\"/ping\")\r\n        def ping() -> bool:\r\n            return True\r\n\r\n        @app.post(\"/serve\")\r\n        async def serve(payload: dict = Body(...)) -> dict[str, Any]:\r\n            body = payload[\"body\"]\r\n\r\n            for key, deserializer in deserializers.items():\r\n                body[key] = deserializer(body[key])\r\n\r\n            with torch.no_grad():\r\n                output = servable_model.serve_step(**body)\r\n\r\n            if not isinstance(output, dict):\r\n                raise Exception(f\"Please, return your outputs as a dictionary. Found {output}\")\r\n\r\n            for key, serializer in serializers.items():\r\n                output[key] = serializer(output[key])\r\n\r\n            return output\r\n\r\n        run(app, host=host, port=port, log_level=\"error\")", "code_tokens": ["def", "_start_server", "(", "servable_model", ":", "ServableModule", ",", "host", ":", "str", ",", "port", ":", "int", ",", "_", ":", "bool", ")", "-", ">", "None", ":", "\"", "\"", "\"", "This", "method", "starts", "a", "server", "with", "a", "serve", "and", "ping", "endpoints", ".", "\"", "\"", "\"", "from", "fastapi", "import", "Body", ",", "FastAPI", "from", "uvicorn", "import", "run", "app", "=", "FastAPI", "(", ")", "deserializers", ",", "serializers", "=", "servable_model", ".", "configure_serialization", "(", ")", "servable_model", ".", "eval", "(", ")", "@", "app", ".", "get", "(", "\"", "/", "ping", "\"", ")", "def", "ping", "(", ")", "-", ">", "bool", ":", "return", "True", "@", "app", ".", "post", "(", "\"", "/", "serve", "\"", ")", "async", "def", "serve", "(", "payload", ":", "dict", "=", "Body", "(", ".", ".", ".", ")", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "body", "=", "payload", "[", "\"", "body", "\"", "]", "for", "key", ",", "deserializer", "in", "deserializers", ".", "items", "(", ")", ":", "body", "[", "key", "]", "=", "deserializer", "(", "body", "[", "key", "]", ")", "with", "torch", ".", "no_grad", "(", ")", ":", "output", "=", "servable_model", ".", "serve_step", "(", "*", "*", "body", ")", "if", "not", "isinstance", "(", "output", ",", "dict", ")", ":", "raise", "Exception", "(", "f", "\"", "Please", ",", "return", "your", "outputs", "as", "a", "dictionary", ".", "Found", "{", "output", "}", "\"", ")", "for", "key", ",", "serializer", "in", "serializers", ".", "items", "(", ")", ":", "output", "[", "key", "]", "=", "serializer", "(", "output", "[", "key", "]", ")", "return", "output", "run", "(", "app", ",", "host", "=", "host", ",", "port", "=", "port", ",", "log_level", "=", "\"", "error", "\"", ")"], "docstring": "This method starts a server with a serve and ping endpoints.", "docstring_tokens": ["this", "method", "starts", "a", "server", "with", "a", "serve", "and", "ping", "endpoints"], "docstring_summary": "This method starts a server with a serve and ping endpoints.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\serve\\servable_module_validator.py", "partition": "train", "function_type": "class_method", "class_name": "ServableModuleValidator", "start_line": 142, "end_line": 176, "hash": "b8b9a7c5335fb2954781460622fc8392", "complexity": 5, "parameters": ["servable_model", "host", "port", "_"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "is_distributed", "original_string": "def is_distributed(self) -> bool:  # pragma: no-cover\r\n        \"\"\"Legacy property kept for backwards compatibility.\"\"\"\r\n        rank_zero_deprecation(\r\n            f\"`{type(self).__name__}.is_distributed` is deprecated. Use is discouraged.\", stacklevel=6\r\n        )\r\n        return True", "language": "python", "code": "def is_distributed(self) -> bool:  # pragma: no-cover\r\n        \"\"\"Legacy property kept for backwards compatibility.\"\"\"\r\n        rank_zero_deprecation(\r\n            f\"`{type(self).__name__}.is_distributed` is deprecated. Use is discouraged.\", stacklevel=6\r\n        )\r\n        return True", "code_tokens": ["def", "is_distributed", "(", "self", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Legacy", "property", "kept", "for", "backwards", "compatibility", ".", "\"", "\"", "\"", "rank_zero_deprecation", "(", "f", "\"", "`", "{", "type", "(", "self", ")", ".", "__name__", "}", ".", "is_distributed", "`", "is", "deprecated", ".", "Use", "is", "discouraged", ".", "\"", ",", "stacklevel", "=", "6", ")", "return", "True"], "docstring": "Legacy property kept for backwards compatibility.", "docstring_tokens": ["legacy", "property", "kept", "for", "backwards", "compatibility"], "docstring_summary": "Legacy property kept for backwards compatibility.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 107, "end_line": 112, "hash": "2e0a0a12d3bf14b5c971ec0b8e44aa75", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "_setup_model", "original_string": "def _setup_model(self, model: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self.determine_ddp_device_ids()\r\n        log.debug(f\"setting up DDP model with device ids: {device_ids}, kwargs: {self._ddp_kwargs}\")\r\n        # https://pytorch.org/docs/stable/notes/cuda.html#id5\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=model, device_ids=device_ids, **self._ddp_kwargs)", "language": "python", "code": "def _setup_model(self, model: Module) -> DistributedDataParallel:\r\n        \"\"\"Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.\"\"\"\r\n        device_ids = self.determine_ddp_device_ids()\r\n        log.debug(f\"setting up DDP model with device ids: {device_ids}, kwargs: {self._ddp_kwargs}\")\r\n        # https://pytorch.org/docs/stable/notes/cuda.html#id5\r\n        ctx = torch.cuda.stream(torch.cuda.Stream()) if device_ids is not None else nullcontext()\r\n        with ctx:\r\n            return DistributedDataParallel(module=model, device_ids=device_ids, **self._ddp_kwargs)", "code_tokens": ["def", "_setup_model", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "DistributedDataParallel", ":", "\"", "\"", "\"", "Wraps", "the", "model", "into", "a", ":", "class", ":", "`", "~", "torch", ".", "nn", ".", "parallel", ".", "distributed", ".", "DistributedDataParallel", "`", "module", ".", "\"", "\"", "\"", "device_ids", "=", "self", ".", "determine_ddp_device_ids", "(", ")", "log", ".", "debug", "(", "f", "\"", "setting", "up", "DDP", "model", "with", "device", "ids", ":", "{", "device_ids", "}", ",", "kwargs", ":", "{", "self", ".", "_ddp_kwargs", "}", "\"", ")", "ctx", "=", "torch", ".", "cuda", ".", "stream", "(", "torch", ".", "cuda", ".", "Stream", "(", ")", ")", "if", "device_ids", "is", "not", "None", "else", "nullcontext", "(", ")", "with", "ctx", ":", "return", "DistributedDataParallel", "(", "module", "=", "model", ",", "device_ids", "=", "device_ids", ",", "*", "*", "self", ".", "_ddp_kwargs", ")"], "docstring": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "nn", "parallel", "distributed", "distributeddataparallel", "module"], "docstring_summary": "Wraps the model into a :class:`~torch.nn.parallel.distributed.DistributedDataParallel` module.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 187, "end_line": 194, "hash": "6c76ea44fc0e27a1127754085752f12a", "complexity": 3, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\r\n\r\n        if self._model_averager is None:\r\n            return optimizer_output\r\n\r\n        params = [param for group in optimizer.param_groups for param in group[\"params\"] if param.grad is not None]\r\n        self._model_averager.average_parameters(iter(params))\r\n\r\n        return optimizer_output", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        \"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            **kwargs: Any extra arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\r\n\r\n        if self._model_averager is None:\r\n            return optimizer_output\r\n\r\n        params = [param for group in optimizer.param_groups for param in group[\"params\"] if param.grad is not None]\r\n        self._model_averager.average_parameters(iter(params))\r\n\r\n        return optimizer_output", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "model", ":", "Optional", "[", "Union", "[", "\"", "pl", ".", "LightningModule", "\"", ",", "Module", "]", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Performs", "the", "actual", "optimizer", "step", ".", "Args", ":", "optimizer", ":", "the", "optimizer", "performing", "the", "step", "closure", ":", "closure", "calculating", "the", "loss", "value", "model", ":", "reference", "to", "the", "model", ",", "optionally", "defining", "optimizer", "step", "related", "hooks", "*", "*", "kwargs", ":", "Any", "extra", "arguments", "to", "`", "`", "optimizer", ".", "step", "`", "`", "\"", "\"", "\"", "optimizer_output", "=", "super", "(", ")", ".", "optimizer_step", "(", "optimizer", ",", "closure", ",", "model", ",", "*", "*", "kwargs", ")", "if", "self", ".", "_model_averager", "is", "None", ":", "return", "optimizer_output", "params", "=", "[", "param", "for", "group", "in", "optimizer", ".", "param_groups", "for", "param", "in", "group", "[", "\"", "params", "\"", "]", "if", "param", ".", "grad", "is", "not", "None", "]", "self", ".", "_model_averager", ".", "average_parameters", "(", "iter", "(", "params", ")", ")", "return", "optimizer_output"], "docstring": "Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            **kwargs: Any extra arguments to ``optimizer.step``", "docstring_tokens": ["performs", "the", "actual", "optimizer", "step", "args", "optimizer", "the", "optimizer", "performing", "the", "step", "closure", "closure", "calculating", "the", "loss", "value", "model", "reference", "to", "the", "model", "optionally", "defining", "optimizer", "step", "related", "hooks", "kwargs", "any", "extra", "arguments", "to", "optimizer", "step"], "docstring_summary": "Performs the actual optimizer step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 256, "end_line": 280, "hash": "97ce30c8743a630743ba3d1f5355322e", "complexity": 5, "parameters": ["optimizer", "closure", "Any]", "model", "Module]]", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "pre_backward", "original_string": "def pre_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run before precision plugin executes backward.\"\"\"\r\n        if not isinstance(self.model, DistributedDataParallel):\r\n            return\r\n        assert self.lightning_module is not None\r\n        if not self.lightning_module.automatic_optimization:\r\n            prepare_for_backward(self.model, closure_loss)", "language": "python", "code": "def pre_backward(self, closure_loss: Tensor) -> None:\r\n        \"\"\"Run before precision plugin executes backward.\"\"\"\r\n        if not isinstance(self.model, DistributedDataParallel):\r\n            return\r\n        assert self.lightning_module is not None\r\n        if not self.lightning_module.automatic_optimization:\r\n            prepare_for_backward(self.model, closure_loss)", "code_tokens": ["def", "pre_backward", "(", "self", ",", "closure_loss", ":", "Tensor", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Run", "before", "precision", "plugin", "executes", "backward", ".", "\"", "\"", "\"", "if", "not", "isinstance", "(", "self", ".", "model", ",", "DistributedDataParallel", ")", ":", "return", "assert", "self", ".", "lightning_module", "is", "not", "None", "if", "not", "self", ".", "lightning_module", ".", "automatic_optimization", ":", "prepare_for_backward", "(", "self", ".", "model", ",", "closure_loss", ")"], "docstring": "Run before precision plugin executes backward.", "docstring_tokens": ["run", "before", "precision", "plugin", "executes", "backward"], "docstring_summary": "Run before precision plugin executes backward.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 313, "end_line": 319, "hash": "1f95033b3401f61dab60fd67ca157686", "complexity": 3, "parameters": ["closure_loss"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\ddp.py", "func_name": "reduce", "original_string": "def reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "language": "python", "code": "def reduce(\r\n        self, tensor: Tensor, group: Optional[Any] = None, reduce_op: Optional[Union[ReduceOp, str]] = \"mean\"\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Tensor", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "\"", "mean", "\"", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "group", ":", "the", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "reduce_op", ":", "the", "reduction", "operation", ".", "Defaults", "to", "'", "mean", "'", "/", "'", "avg", "'", ".", "Can", "also", "be", "a", "string", "'", "sum", "'", "to", "calculate", "the", "sum", "during", "reduction", ".", "Return", ":", "reduced", "value", ",", "except", "when", "the", "input", "was", "not", "a", "tensor", "the", "output", "remains", "is", "unchanged", "\"", "\"", "\"", "if", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "return", "_sync_ddp_if_available", "(", "tensor", ",", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "reduce_op", "the", "reduction", "operation", "defaults", "to", "mean", "avg", "can", "also", "be", "a", "string", "sum", "to", "calculate", "the", "sum", "during", "reduction", "return", "reduced", "value", "except", "when", "the", "input", "was", "not", "a", "tensor", "the", "output", "remains", "is", "unchanged"], "docstring_summary": "Reduces a tensor from several distributed processes to one aggregated tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\ddp.py", "partition": "train", "function_type": "class_method", "class_name": "DDPStrategy", "start_line": 328, "end_line": 345, "hash": "3c3a7185435956fcf3e725935c6a3462", "complexity": 2, "parameters": ["tensor", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "__init__", "original_string": "def __init__(\r\n        self,\r\n        accelerator: Optional[\"pl.accelerators.Accelerator\"] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Union[str, int] = \"auto\",\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision_plugin: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. *For more information:* :ref:`deepspeed_advanced`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        *For more information:* https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either `precision=\"16-mixed\"` or\r\n                `precision=\"bf16-mixed\"`.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                If set to \"auto\", the strategy tries to infer this from\r\n                the train DataLoader's BatchSampler, else defaults to 1.\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size (trainer.batch_size).\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            # Starting with PyTorch 2.6, `torch.load` defaults to `weights_only=True` when loading full checkpoints.\r\n            # DeepSpeed added support for this behavior in version 0.16.0.\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision_plugin=precision_plugin,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            # User has not overridden config, set defaults\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        # default FP16 parameters.\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale", "language": "python", "code": "def __init__(\r\n        self,\r\n        accelerator: Optional[\"pl.accelerators.Accelerator\"] = None,\r\n        zero_optimization: bool = True,\r\n        stage: int = 2,\r\n        remote_device: Optional[str] = None,\r\n        offload_optimizer: bool = False,\r\n        offload_parameters: bool = False,\r\n        offload_params_device: str = \"cpu\",\r\n        nvme_path: str = \"/local_nvme\",\r\n        params_buffer_count: int = 5,\r\n        params_buffer_size: int = 100_000_000,\r\n        max_in_cpu: int = 1_000_000_000,\r\n        offload_optimizer_device: str = \"cpu\",\r\n        optimizer_buffer_count: int = 4,\r\n        block_size: int = 1048576,\r\n        queue_depth: int = 8,\r\n        single_submit: bool = False,\r\n        overlap_events: bool = True,\r\n        thread_count: int = 1,\r\n        pin_memory: bool = False,\r\n        sub_group_size: int = 1_000_000_000_000,\r\n        contiguous_gradients: bool = True,\r\n        overlap_comm: bool = True,\r\n        allgather_partitions: bool = True,\r\n        reduce_scatter: bool = True,\r\n        allgather_bucket_size: int = 200_000_000,\r\n        reduce_bucket_size: int = 200_000_000,\r\n        zero_allow_untested_optimizer: bool = True,\r\n        logging_batch_size_per_gpu: Union[str, int] = \"auto\",\r\n        config: Optional[Union[_PATH, dict[str, Any]]] = None,\r\n        logging_level: int = logging.WARN,\r\n        parallel_devices: Optional[list[torch.device]] = None,\r\n        cluster_environment: Optional[ClusterEnvironment] = None,\r\n        loss_scale: float = 0,\r\n        initial_scale_power: int = 16,\r\n        loss_scale_window: int = 1000,\r\n        hysteresis: int = 2,\r\n        min_loss_scale: int = 1,\r\n        partition_activations: bool = False,\r\n        cpu_checkpointing: bool = False,\r\n        contiguous_memory_optimization: bool = False,\r\n        synchronize_checkpoint_boundary: bool = False,\r\n        load_full_weights: bool = False,\r\n        precision_plugin: Optional[Precision] = None,\r\n        process_group_backend: Optional[str] = None,\r\n        timeout: Optional[timedelta] = default_pg_timeout,\r\n        exclude_frozen_parameters: bool = False,\r\n    ) -> None:\r\n        \"\"\"Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. *For more information:* :ref:`deepspeed_advanced`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        *For more information:* https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either `precision=\"16-mixed\"` or\r\n                `precision=\"bf16-mixed\"`.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                If set to \"auto\", the strategy tries to infer this from\r\n                the train DataLoader's BatchSampler, else defaults to 1.\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size (trainer.batch_size).\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.\r\n\r\n        \"\"\"\r\n        if not _DEEPSPEED_AVAILABLE:\r\n            raise MisconfigurationException(\r\n                \"To use the `DeepSpeedStrategy`, you must have DeepSpeed installed.\"\r\n                \" Install it by running `pip install -U deepspeed`.\"\r\n            )\r\n\r\n        if _TORCH_GREATER_EQUAL_2_6 and not _DEEPSPEED_GREATER_EQUAL_0_16:\r\n            # Starting with PyTorch 2.6, `torch.load` defaults to `weights_only=True` when loading full checkpoints.\r\n            # DeepSpeed added support for this behavior in version 0.16.0.\r\n            import deepspeed\r\n\r\n            deepspeed_version = deepspeed.__version__\r\n\r\n            raise ImportError(\r\n                f\"PyTorch >= 2.6 requires DeepSpeed >= 0.16.0. \"\r\n                f\"Detected DeepSpeed version: {deepspeed_version}. \"\r\n                \"Please upgrade by running `pip install -U 'deepspeed>=0.16.0'`.\"\r\n            )\r\n\r\n        super().__init__(\r\n            accelerator=accelerator,\r\n            parallel_devices=parallel_devices,\r\n            cluster_environment=cluster_environment,\r\n            precision_plugin=precision_plugin,\r\n            process_group_backend=process_group_backend,\r\n        )\r\n        self._timeout: Optional[timedelta] = timeout\r\n\r\n        self.config = self._load_config(config)\r\n        if self.config is None:\r\n            # User has not overridden config, set defaults\r\n            self.config = self._create_default_config(\r\n                zero_optimization,\r\n                zero_allow_untested_optimizer,\r\n                logging_batch_size_per_gpu,\r\n                offload_optimizer=offload_optimizer,\r\n                offload_parameters=offload_parameters,\r\n                nvme_path=nvme_path,\r\n                offload_params_device=offload_params_device,\r\n                params_buffer_count=params_buffer_count,\r\n                params_buffer_size=params_buffer_size,\r\n                max_in_cpu=max_in_cpu,\r\n                pin_memory=pin_memory,\r\n                offload_optimizer_device=offload_optimizer_device,\r\n                optimizer_buffer_count=optimizer_buffer_count,\r\n                block_size=block_size,\r\n                queue_depth=queue_depth,\r\n                single_submit=single_submit,\r\n                overlap_events=overlap_events,\r\n                thread_count=thread_count,\r\n                partition_activations=partition_activations,\r\n                cpu_checkpointing=cpu_checkpointing,\r\n                contiguous_memory_optimization=contiguous_memory_optimization,\r\n                synchronize_checkpoint_boundary=synchronize_checkpoint_boundary,\r\n                stage=stage,\r\n                contiguous_gradients=contiguous_gradients,\r\n                overlap_comm=overlap_comm,\r\n                allgather_partitions=allgather_partitions,\r\n                reduce_scatter=reduce_scatter,\r\n                allgather_bucket_size=allgather_bucket_size,\r\n                reduce_bucket_size=reduce_bucket_size,\r\n                sub_group_size=sub_group_size,\r\n            )\r\n        import deepspeed\r\n\r\n        self._config_initialized = False\r\n        deepspeed.utils.logging.logger.setLevel(logging_level)\r\n\r\n        self.remote_device = remote_device\r\n        self.load_full_weights = load_full_weights\r\n        self.exclude_frozen_parameters = exclude_frozen_parameters\r\n\r\n        # default FP16 parameters.\r\n        self.loss_scale = loss_scale\r\n        self.initial_scale_power = initial_scale_power\r\n        self.loss_scale_window = loss_scale_window\r\n        self.hysteresis = hysteresis\r\n        self.min_loss_scale = min_loss_scale", "code_tokens": ["def", "__init__", "(", "self", ",", "accelerator", ":", "Optional", "[", "\"", "pl", ".", "accelerators", ".", "Accelerator", "\"", "]", "=", "None", ",", "zero_optimization", ":", "bool", "=", "True", ",", "stage", ":", "int", "=", "2", ",", "remote_device", ":", "Optional", "[", "str", "]", "=", "None", ",", "offload_optimizer", ":", "bool", "=", "False", ",", "offload_parameters", ":", "bool", "=", "False", ",", "offload_params_device", ":", "str", "=", "\"", "cpu", "\"", ",", "nvme_path", ":", "str", "=", "\"", "/", "local_nvme", "\"", ",", "params_buffer_count", ":", "int", "=", "5", ",", "params_buffer_size", ":", "int", "=", "100_000_000", ",", "max_in_cpu", ":", "int", "=", "1_000_000_000", ",", "offload_optimizer_device", ":", "str", "=", "\"", "cpu", "\"", ",", "optimizer_buffer_count", ":", "int", "=", "4", ",", "block_size", ":", "int", "=", "1048576", ",", "queue_depth", ":", "int", "=", "8", ",", "single_submit", ":", "bool", "=", "False", ",", "overlap_events", ":", "bool", "=", "True", ",", "thread_count", ":", "int", "=", "1", ",", "pin_memory", ":", "bool", "=", "False", ",", "sub_group_size", ":", "int", "=", "1_000_000_000_000", ",", "contiguous_gradients", ":", "bool", "=", "True", ",", "overlap_comm", ":", "bool", "=", "True", ",", "allgather_partitions", ":", "bool", "=", "True", ",", "reduce_scatter", ":", "bool", "=", "True", ",", "allgather_bucket_size", ":", "int", "=", "200_000_000", ",", "reduce_bucket_size", ":", "int", "=", "200_000_000", ",", "zero_allow_untested_optimizer", ":", "bool", "=", "True", ",", "logging_batch_size_per_gpu", ":", "Union", "[", "str", ",", "int", "]", "=", "\"", "auto", "\"", ",", "config", ":", "Optional", "[", "Union", "[", "_PATH", ",", "dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "logging_level", ":", "int", "=", "logging", ".", "WARN", ",", "parallel_devices", ":", "Optional", "[", "list", "[", "torch", ".", "device", "]", "]", "=", "None", ",", "cluster_environment", ":", "Optional", "[", "ClusterEnvironment", "]", "=", "None", ",", "loss_scale", ":", "float", "=", "0", ",", "initial_scale_power", ":", "int", "=", "16", ",", "loss_scale_window", ":", "int", "=", "1000", ",", "hysteresis", ":", "int", "=", "2", ",", "min_loss_scale", ":", "int", "=", "1", ",", "partition_activations", ":", "bool", "=", "False", ",", "cpu_checkpointing", ":", "bool", "=", "False", ",", "contiguous_memory_optimization", ":", "bool", "=", "False", ",", "synchronize_checkpoint_boundary", ":", "bool", "=", "False", ",", "load_full_weights", ":", "bool", "=", "False", ",", "precision_plugin", ":", "Optional", "[", "Precision", "]", "=", "None", ",", "process_group_backend", ":", "Optional", "[", "str", "]", "=", "None", ",", "timeout", ":", "Optional", "[", "timedelta", "]", "=", "default_pg_timeout", ",", "exclude_frozen_parameters", ":", "bool", "=", "False", ",", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Provides", "capabilities", "to", "run", "training", "using", "the", "DeepSpeed", "library", ",", "with", "training", "optimizations", "for", "large", "billion", "parameter", "models", ".", "*", "For", "more", "information", ":", "*", ":", "ref", ":", "`", "deepspeed_advanced", "`", ".", ".", ".", "warning", ":", ":", "This", "is", "an", ":", "ref", ":", "`", "experimental", "<", "versioning", ":", "Experimental", "API", ">", "`", "feature", ".", "Defaults", "have", "been", "set", "to", "enable", "ZeRO", "-", "Offload", "and", "some", "have", "been", "taken", "from", "the", "link", "below", ".", "These", "defaults", "have", "been", "set", "generally", ",", "but", "may", "require", "tuning", "for", "optimum", "performance", "based", "on", "your", "model", "size", ".", "*", "For", "more", "information", ":", "*", "https", ":", "/", "/", "www", ".", "deepspeed", ".", "ai", "/", "docs", "/", "config", "-", "json", "/", "Arguments", ":", "zero_optimization", ":", "Enable", "ZeRO", "optimization", ".", "This", "is", "compatible", "with", "either", "`", "precision", "=", "\"", "16", "-", "mixed", "\"", "`", "or", "`", "precision", "=", "\"", "bf16", "-", "mixed", "\"", "`", ".", "stage", ":", "Different", "stages", "of", "the", "ZeRO", "Optimizer", ".", "0", "is", "disabled", ",", "1", "is", "optimizer", "state", "partitioning", ",", "2", "is", "optimizer", "+", "gradient", "state", "partitioning", ",", "3", "is", "optimizer", "+", "gradient_parameter", "partitioning", "using", "the", "infinity", "engine", ".", "remote_device", ":", "Device", "to", "instantiate", "the", "model", "on", "initially", "(", "`", "`", "cpu", "`", "`", "or", "`", "`", "nvme", "`", "`", ")", ".", "Defaults", "to", "GPU", ".", "offload_optimizer", ":", "Enable", "offloading", "optimizer", "memory", "and", "computation", "to", "CPU", "or", "NVMe", "based", "on", "`", "`", "offload_optimizer_device", "`", "`", ".", "offload_parameters", ":", "When", "using", "ZeRO", "Stage", "3", ",", "Enable", "offloading", "parameter", "memory", "and", "computation", "to", "CPU", "or", "NVMe", "based", "on", "`", "`", "offload_params_device", "`", "`", ".", "offload_params_device", ":", "When", "offloading", "parameters", "choose", "the", "device", "to", "offload", "to", ",", "`", "`", "cpu", "`", "`", "or", "`", "`", "nvme", "`", "`", ".", "offload_optimizer_device", ":", "When", "offloading", "optimizer", "state", "choose", "the", "device", "to", "offload", "to", ",", "`", "`", "cpu", "`", "`", "or", "`", "`", "nvme", "`", "`", ".", "params_buffer_count", ":", "Number", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "`", "`", "offload_params_device", "`", "`", "is", "`", "`", "nvme", "`", "`", ".", "params_buffer_size", ":", "Size", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "`", "`", "offload_params_device", "`", "`", "is", "`", "`", "nvme", "`", "`", ".", "max_in_cpu", ":", "Number", "of", "parameter", "elements", "to", "maintain", "in", "CPU", "memory", "when", "offloading", "to", "NVMe", "is", "enabled", ".", "nvme_path", ":", "Filesystem", "path", "for", "NVMe", "device", "for", "optimizer", "/", "parameter", "state", "offloading", ".", "optimizer_buffer_count", ":", "Number", "of", "buffers", "in", "buffer", "pool", "for", "optimizer", "state", "offloading", "when", "`", "`", "offload_optimizer_device", "`", "`", "is", "set", "to", "`", "`", "nvme", "`", "`", ".", "This", "should", "be", "at", "least", "the", "number", "of", "states", "maintained", "per", "parameter", "by", "the", "optimizer", ".", "For", "example", ",", "Adam", "optimizer", "has", "4", "states", "(", "parameter", ",", "gradient", ",", "momentum", ",", "and", "variance", ")", ".", "block_size", ":", "When", "using", "NVMe", "Offloading", ",", "the", "I", "/", "O", "block", "size", "in", "bytes", ".", "queue_depth", ":", "When", "using", "NVMe", "Offloading", ",", "the", "I", "/", "O", "queue", "depth", ".", "single_submit", ":", "When", "using", "NVMe", "Offloading", ",", "submit", "requests", "to", "storage", "device", "as", "multiple", "individual", "requests", ",", "as", "opposed", "to", "one", "block", "of", "requests", ".", "overlap_events", ":", "When", "using", "NVMe", "Offloading", ",", "submit", "requests", "to", "storage", "device", "in", "an", "overlapped", "fashion", "without", "waiting", "for", "completion", "of", "earlier", "requests", ".", "thread_count", ":", "When", "using", "NVMe", "Offloading", ",", "Intra", "-", "request", "parallelism", "for", "each", "read", "/", "write", "submitted", "by", "a", "user", "thread", ".", "pin_memory", ":", "When", "using", "ZeRO", "stage", "3", ",", "pin", "optimizer", "state", "memory", "on", "CPU", ".", "This", "could", "boost", "throughput", "at", "the", "cost", "of", "extra", "memory", "overhead", ".", "sub_group_size", ":", "When", "using", "ZeRO", "stage", "3", ",", "defines", "the", "number", "of", "parameters", "within", "a", "sub", "group", "to", "offload", "at", "a", "time", ".", "Smaller", "numbers", "require", "more", "communication", ",", "but", "improve", "memory", "efficiency", ".", "contiguous_gradients", ":", "Copies", "gradients", "to", "a", "continuous", "buffer", "as", "they", "are", "produced", ".", "Avoids", "memory", "fragmentation", "during", "backwards", ".", "Useful", "when", "training", "large", "models", ".", "overlap_comm", ":", "Overlap", "the", "reduction", "(", "synchronization", ")", "of", "gradients", "with", "the", "backwards", "computation", ".", "This", "is", "a", "speed", "optimization", "when", "training", "across", "multiple", "GPUs", "/", "machines", ".", "allgather_partitions", ":", "All", "gather", "updated", "parameters", "at", "the", "end", "of", "training", "step", ",", "instead", "of", "using", "a", "series", "of", "broadcast", "collectives", ".", "reduce_scatter", ":", "Use", "reduce", "/", "scatter", "instead", "of", "allreduce", "to", "average", "gradients", ".", "allgather_bucket_size", ":", "Number", "of", "elements", "to", "allgather", "at", "once", ".", "Used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", ",", "with", "a", "tradeoff", "with", "speed", ".", "reduce_bucket_size", ":", "Number", "of", "elements", "to", "reduce", "at", "once", ".", "Used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", ",", "with", "a", "tradeoff", "with", "speed", ".", "zero_allow_untested_optimizer", ":", "Allow", "untested", "optimizers", "to", "be", "used", "with", "ZeRO", ".", "Currently", "only", "Adam", "is", "a", "DeepSpeed", "supported", "optimizer", "when", "using", "ZeRO", ".", "logging_batch_size_per_gpu", ":", "Config", "used", "in", "DeepSpeed", "to", "calculate", "verbose", "timing", "for", "logging", "on", "a", "per", "sample", "per", "second", "basis", "(", "only", "displayed", "if", "logging", "=", "logging", ".", "INFO", ")", ".", "If", "set", "to", "\"", "auto", "\"", ",", "the", "strategy", "tries", "to", "infer", "this", "from", "the", "train", "DataLoader", "'", "s", "BatchSampler", ",", "else", "defaults", "to", "1", ".", "To", "obtain", "accurate", "logs", "when", "using", "datasets", "that", "do", "not", "support", "batch", "samplers", ",", "set", "this", "to", "the", "actual", "per", "gpu", "batch", "size", "(", "trainer", ".", "batch_size", ")", ".", "config", ":", "Pass", "in", "a", "deepspeed", "formatted", "config", "dict", ",", "or", "path", "to", "a", "deepspeed", "config", ":", "https", ":", "/", "/", "www", ".", "deepspeed", ".", "ai", "/", "docs", "/", "config", "-", "json", ".", "All", "defaults", "will", "be", "ignored", "if", "a", "config", "is", "passed", "in", ".", "logging_level", ":", "Set", "logging", "level", "for", "deepspeed", ".", "loss_scale", ":", "Loss", "scaling", "value", "for", "FP16", "training", ".", "0", ".", "0", "results", "in", "dynamic", "loss", "scaling", ",", "otherwise", "static", ".", "initial_scale_power", ":", "Power", "of", "the", "initial", "dynamic", "loss", "scale", "value", ".", "Loss", "scale", "is", "computed", "by", "`", "`", "2", "^", "initial_scale_power", "`", "`", ".", "loss_scale_window", ":", "Window", "in", "which", "to", "raise", "/", "lower", "the", "dynamic", "FP16", "loss", "scaling", "value", ".", "hysteresis", ":", "FP16", "Delay", "shift", "in", "Dynamic", "Loss", "scaling", ".", "min_loss_scale", ":", "The", "minimum", "FP16", "dynamic", "loss", "scaling", "value", ".", "partition_activations", ":", "Enables", "partition", "activation", "when", "used", "with", "ZeRO", "stage", "3", "and", "model", "parallelism", ".", "Still", "requires", "you", "to", "wrap", "your", "forward", "functions", "in", "deepspeed", ".", "checkpointing", ".", "checkpoint", ".", "See", "`", "deepspeed", "tutorial", "<", "https", ":", "/", "/", "www", ".", "deepspeed", ".", "ai", "/", "tutorials", "/", "megatron", "/", "cpu_checkpointing", ":", "Offloads", "partitioned", "activations", "to", "CPU", "if", "`", "`", "partition_activations", "`", "`", "is", "enabled", ".", "contiguous_memory_optimization", ":", "Copies", "partitioned", "activations", "so", "that", "they", "are", "contiguous", "in", "memory", ".", "Not", "supported", "by", "all", "models", ".", "synchronize_checkpoint_boundary", ":", "Insert", ":", "func", ":", "`", "torch", ".", "cuda", ".", "synchronize", "`", "at", "each", "checkpoint", "boundary", ".", "load_full_weights", ":", "True", "when", "loading", "a", "single", "checkpoint", "file", "containing", "the", "model", "state", "dict", "when", "using", "ZeRO", "Stage", "3", ".", "This", "differs", "from", "the", "DeepSpeed", "checkpoint", "which", "contains", "shards", "per", "worker", ".", "exclude_frozen_parameters", ":", "Exclude", "frozen", "parameters", "when", "saving", "checkpoints", ".", "\"", "\"", "\"", "if", "not", "_DEEPSPEED_AVAILABLE", ":", "raise", "MisconfigurationException", "(", "\"", "To", "use", "the", "`", "DeepSpeedStrategy", "`", ",", "you", "must", "have", "DeepSpeed", "installed", ".", "\"", "\"", "Install", "it", "by", "running", "`", "pip", "install", "-", "U", "deepspeed", "`", ".", "\"", ")", "if", "_TORCH_GREATER_EQUAL_2_6", "and", "not", "_DEEPSPEED_GREATER_EQUAL_0_16", ":", "import", "deepspeed", "deepspeed_version", "=", "deepspeed", ".", "__version__", "raise", "ImportError", "(", "f", "\"", "PyTorch", ">", "=", "2", ".", "6", "requires", "DeepSpeed", ">", "=", "0", ".", "16", ".", "0", ".", "\"", "f", "\"", "Detected", "DeepSpeed", "version", ":", "{", "deepspeed_version", "}", ".", "\"", "\"", "Please", "upgrade", "by", "running", "`", "pip", "install", "-", "U", "'", "deepspeed", ">", "=", "0", ".", "16", ".", "0", "'", "`", ".", "\"", ")", "super", "(", ")", ".", "__init__", "(", "accelerator", "=", "accelerator", ",", "parallel_devices", "=", "parallel_devices", ",", "cluster_environment", "=", "cluster_environment", ",", "precision_plugin", "=", "precision_plugin", ",", "process_group_backend", "=", "process_group_backend", ",", ")", "self", ".", "_timeout", ":", "Optional", "[", "timedelta", "]", "=", "timeout", "self", ".", "config", "=", "self", ".", "_load_config", "(", "config", ")", "if", "self", ".", "config", "is", "None", ":", "self", ".", "config", "=", "self", ".", "_create_default_config", "(", "zero_optimization", ",", "zero_allow_untested_optimizer", ",", "logging_batch_size_per_gpu", ",", "offload_optimizer", "=", "offload_optimizer", ",", "offload_parameters", "=", "offload_parameters", ",", "nvme_path", "=", "nvme_path", ",", "offload_params_device", "=", "offload_params_device", ",", "params_buffer_count", "=", "params_buffer_count", ",", "params_buffer_size", "=", "params_buffer_size", ",", "max_in_cpu", "=", "max_in_cpu", ",", "pin_memory", "=", "pin_memory", ",", "offload_optimizer_device", "=", "offload_optimizer_device", ",", "optimizer_buffer_count", "=", "optimizer_buffer_count", ",", "block_size", "=", "block_size", ",", "queue_depth", "=", "queue_depth", ",", "single_submit", "=", "single_submit", ",", "overlap_events", "=", "overlap_events", ",", "thread_count", "=", "thread_count", ",", "partition_activations", "=", "partition_activations", ",", "cpu_checkpointing", "=", "cpu_checkpointing", ",", "contiguous_memory_optimization", "=", "contiguous_memory_optimization", ",", "synchronize_checkpoint_boundary", "=", "synchronize_checkpoint_boundary", ",", "stage", "=", "stage", ",", "contiguous_gradients", "=", "contiguous_gradients", ",", "overlap_comm", "=", "overlap_comm", ",", "allgather_partitions", "=", "allgather_partitions", ",", "reduce_scatter", "=", "reduce_scatter", ",", "allgather_bucket_size", "=", "allgather_bucket_size", ",", "reduce_bucket_size", "=", "reduce_bucket_size", ",", "sub_group_size", "=", "sub_group_size", ",", ")", "import", "deepspeed", "self", ".", "_config_initialized", "=", "False", "deepspeed", ".", "utils", ".", "logging", ".", "logger", ".", "setLevel", "(", "logging_level", ")", "self", ".", "remote_device", "=", "remote_device", "self", ".", "load_full_weights", "=", "load_full_weights", "self", ".", "exclude_frozen_parameters", "=", "exclude_frozen_parameters", "self", ".", "loss_scale", "=", "loss_scale", "self", ".", "initial_scale_power", "=", "initial_scale_power", "self", ".", "loss_scale_window", "=", "loss_scale_window", "self", ".", "hysteresis", "=", "hysteresis", "self", ".", "min_loss_scale", "=", "min_loss_scale"], "docstring": "Provides capabilities to run training using the DeepSpeed library, with training optimizations for large\r\n        billion parameter models. *For more information:* :ref:`deepspeed_advanced`.\r\n\r\n        .. warning::  This is an :ref:`experimental <versioning:Experimental API>` feature.\r\n\r\n        Defaults have been set to enable ZeRO-Offload and some have been taken from the link below.\r\n        These defaults have been set generally, but may require tuning for optimum performance based on your model size.\r\n        *For more information:* https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training.\r\n\r\n        Arguments:\r\n\r\n            zero_optimization: Enable ZeRO optimization. This is compatible with either `precision=\"16-mixed\"` or\r\n                `precision=\"bf16-mixed\"`.\r\n\r\n            stage: Different stages of the ZeRO Optimizer. 0 is disabled,\r\n                1 is optimizer state partitioning, 2 is optimizer+gradient state partitioning,\r\n                3 is optimizer+gradient_parameter partitioning using the infinity engine.\r\n\r\n            remote_device: Device to instantiate the model on initially (``cpu`` or ``nvme``). Defaults to GPU.\r\n\r\n            offload_optimizer: Enable offloading optimizer memory and computation to CPU or NVMe\r\n                based on ``offload_optimizer_device``.\r\n\r\n            offload_parameters: When using ZeRO Stage 3, Enable offloading parameter memory and computation\r\n                to CPU or NVMe based on ``offload_params_device``.\r\n\r\n            offload_params_device: When offloading parameters choose the device to offload to, ``cpu`` or ``nvme``.\r\n\r\n            offload_optimizer_device: When offloading optimizer state choose the device to offload to,\r\n                ``cpu`` or ``nvme``.\r\n\r\n            params_buffer_count: Number of buffers in buffer pool for\r\n                parameter offloading when ``offload_params_device`` is ``nvme``.\r\n\r\n            params_buffer_size: Size of buffers in buffer pool for parameter offloading\r\n                when ``offload_params_device`` is ``nvme``.\r\n\r\n            max_in_cpu: Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.\r\n\r\n            nvme_path: Filesystem path for NVMe device for optimizer/parameter state offloading.\r\n\r\n            optimizer_buffer_count: Number of buffers in buffer pool for optimizer state offloading\r\n                when ``offload_optimizer_device`` is set to ``nvme``.\r\n                This should be at least the number of states maintained per parameter by the optimizer.\r\n                For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance).\r\n\r\n            block_size: When using NVMe Offloading, the I/O block size in bytes.\r\n\r\n            queue_depth: When using NVMe Offloading, the I/O queue depth.\r\n\r\n            single_submit: When using NVMe Offloading,\r\n                submit requests to storage device as multiple individual requests,\r\n                as opposed to one block of requests.\r\n\r\n            overlap_events: When using NVMe Offloading,\r\n                submit requests to storage device in an overlapped fashion\r\n                without waiting for completion of earlier requests.\r\n\r\n            thread_count: When using NVMe Offloading,\r\n                Intra-request parallelism for each read/write submitted by a user thread.\r\n\r\n            pin_memory: When using ZeRO stage 3, pin optimizer state memory on CPU.\r\n                This could boost throughput at the cost of extra memory overhead.\r\n\r\n            sub_group_size: When using ZeRO stage 3, defines the number of parameters\r\n                within a sub group to offload at a time.\r\n                Smaller numbers require more communication, but improve memory efficiency.\r\n\r\n            contiguous_gradients: Copies gradients to a continuous buffer as they are produced.\r\n                Avoids memory fragmentation during backwards. Useful when training large models.\r\n\r\n            overlap_comm: Overlap the reduction (synchronization) of gradients with the backwards computation.\r\n                This is a speed optimization when training across multiple GPUs/machines.\r\n\r\n            allgather_partitions: All gather updated parameters at the end of training step,\r\n                instead of using a series of broadcast collectives.\r\n\r\n            reduce_scatter: Use reduce/scatter instead of allreduce to average gradients.\r\n\r\n            allgather_bucket_size: Number of elements to allgather at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            reduce_bucket_size: Number of elements to reduce at once.\r\n                Used to limit the memory required for larger model sizes, with a tradeoff with speed.\r\n\r\n            zero_allow_untested_optimizer: Allow untested optimizers to be used with ZeRO. Currently only Adam is a\r\n                DeepSpeed supported optimizer when using ZeRO.\r\n\r\n            logging_batch_size_per_gpu: Config used in DeepSpeed to calculate verbose timing for logging\r\n                on a per sample per second basis (only displayed if logging=logging.INFO).\r\n                If set to \"auto\", the strategy tries to infer this from\r\n                the train DataLoader's BatchSampler, else defaults to 1.\r\n                To obtain accurate logs when using datasets that do not support batch samplers,\r\n                set this to the actual per gpu batch size (trainer.batch_size).\r\n\r\n            config: Pass in a deepspeed formatted config dict,\r\n                or path to a deepspeed config: https://www.deepspeed.ai/docs/config-json.\r\n                All defaults will be ignored if a config is passed in.\r\n\r\n            logging_level: Set logging level for deepspeed.\r\n\r\n            loss_scale: Loss scaling value for FP16 training.\r\n                0.0 results in dynamic loss scaling, otherwise static.\r\n\r\n            initial_scale_power: Power of the initial dynamic loss scale value. Loss scale is computed\r\n                by ``2^initial_scale_power``.\r\n\r\n            loss_scale_window: Window in which to raise/lower the dynamic FP16 loss scaling value.\r\n\r\n            hysteresis: FP16 Delay shift in Dynamic Loss scaling.\r\n\r\n            min_loss_scale: The minimum FP16 dynamic loss scaling value.\r\n\r\n            partition_activations: Enables partition activation when used with ZeRO stage 3 and model parallelism.\r\n                Still requires you to wrap your forward functions in deepspeed.checkpointing.checkpoint.\r\n                See `deepspeed tutorial\r\n                <https://www.deepspeed.ai/tutorials/megatron/#deepspeed-activation-checkpoints-optional>`_.\r\n\r\n            cpu_checkpointing: Offloads partitioned activations to CPU if ``partition_activations`` is enabled.\r\n\r\n            contiguous_memory_optimization: Copies partitioned activations so that they are contiguous in memory.\r\n                Not supported by all models.\r\n\r\n            synchronize_checkpoint_boundary: Insert :func:`torch.cuda.synchronize` at each checkpoint boundary.\r\n\r\n            load_full_weights: True when loading a single checkpoint file containing the model state dict\r\n                when using ZeRO Stage 3. This differs from the DeepSpeed checkpoint which contains shards\r\n                per worker.\r\n\r\n            exclude_frozen_parameters: Exclude frozen parameters when saving checkpoints.", "docstring_tokens": ["provides", "capabilities", "to", "run", "training", "using", "the", "deepspeed", "library", "with", "training", "optimizations", "for", "large", "billion", "parameter", "models", "for", "more", "information", "ref", "deepspeed_advanced", "warning", "this", "is", "an", "ref", "experimental", "versioning", "experimental", "api", "feature", "defaults", "have", "been", "set", "to", "enable", "zero", "offload", "and", "some", "have", "been", "taken", "from", "the", "link", "below", "these", "defaults", "have", "been", "set", "generally", "but", "may", "require", "tuning", "for", "optimum", "performance", "based", "on", "your", "model", "size", "for", "more", "information", "https", "www", "deepspeed", "ai", "docs", "config", "json", "zero", "optimizations", "for", "fp16", "training", "arguments", "zero_optimization", "enable", "zero", "optimization", "this", "is", "compatible", "with", "either", "precision", "16", "mixed", "or", "precision", "bf16", "mixed", "stage", "different", "stages", "of", "the", "zero", "optimizer", "0", "is", "disabled", "1", "is", "optimizer", "state", "partitioning", "2", "is", "optimizer", "gradient", "state", "partitioning", "3", "is", "optimizer", "gradient_parameter", "partitioning", "using", "the", "infinity", "engine", "remote_device", "device", "to", "instantiate", "the", "model", "on", "initially", "cpu", "or", "nvme", "defaults", "to", "gpu", "offload_optimizer", "enable", "offloading", "optimizer", "memory", "and", "computation", "to", "cpu", "or", "nvme", "based", "on", "offload_optimizer_device", "offload_parameters", "when", "using", "zero", "stage", "3", "enable", "offloading", "parameter", "memory", "and", "computation", "to", "cpu", "or", "nvme", "based", "on", "offload_params_device", "offload_params_device", "when", "offloading", "parameters", "choose", "the", "device", "to", "offload", "to", "cpu", "or", "nvme", "offload_optimizer_device", "when", "offloading", "optimizer", "state", "choose", "the", "device", "to", "offload", "to", "cpu", "or", "nvme", "params_buffer_count", "number", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "offload_params_device", "is", "nvme", "params_buffer_size", "size", "of", "buffers", "in", "buffer", "pool", "for", "parameter", "offloading", "when", "offload_params_device", "is", "nvme", "max_in_cpu", "number", "of", "parameter", "elements", "to", "maintain", "in", "cpu", "memory", "when", "offloading", "to", "nvme", "is", "enabled", "nvme_path", "filesystem", "path", "for", "nvme", "device", "for", "optimizer", "parameter", "state", "offloading", "optimizer_buffer_count", "number", "of", "buffers", "in", "buffer", "pool", "for", "optimizer", "state", "offloading", "when", "offload_optimizer_device", "is", "set", "to", "nvme", "this", "should", "be", "at", "least", "the", "number", "of", "states", "maintained", "per", "parameter", "by", "the", "optimizer", "for", "example", "adam", "optimizer", "has", "4", "states", "parameter", "gradient", "momentum", "and", "variance", "block_size", "when", "using", "nvme", "offloading", "the", "i", "o", "block", "size", "in", "bytes", "queue_depth", "when", "using", "nvme", "offloading", "the", "i", "o", "queue", "depth", "single_submit", "when", "using", "nvme", "offloading", "submit", "requests", "to", "storage", "device", "as", "multiple", "individual", "requests", "as", "opposed", "to", "one", "block", "of", "requests", "overlap_events", "when", "using", "nvme", "offloading", "submit", "requests", "to", "storage", "device", "in", "an", "overlapped", "fashion", "without", "waiting", "for", "completion", "of", "earlier", "requests", "thread_count", "when", "using", "nvme", "offloading", "intra", "request", "parallelism", "for", "each", "read", "write", "submitted", "by", "a", "user", "thread", "pin_memory", "when", "using", "zero", "stage", "3", "pin", "optimizer", "state", "memory", "on", "cpu", "this", "could", "boost", "throughput", "at", "the", "cost", "of", "extra", "memory", "overhead", "sub_group_size", "when", "using", "zero", "stage", "3", "defines", "the", "number", "of", "parameters", "within", "a", "sub", "group", "to", "offload", "at", "a", "time", "smaller", "numbers", "require", "more", "communication", "but", "improve", "memory", "efficiency", "contiguous_gradients", "copies", "gradients", "to", "a", "continuous", "buffer", "as", "they", "are", "produced", "avoids", "memory", "fragmentation", "during", "backwards", "useful", "when", "training", "large", "models", "overlap_comm", "overlap", "the", "reduction", "synchronization", "of", "gradients", "with", "the", "backwards", "computation", "this", "is", "a", "speed", "optimization", "when", "training", "across", "multiple", "gpus", "machines", "allgather_partitions", "all", "gather", "updated", "parameters", "at", "the", "end", "of", "training", "step", "instead", "of", "using", "a", "series", "of", "broadcast", "collectives", "reduce_scatter", "use", "reduce", "scatter", "instead", "of", "allreduce", "to", "average", "gradients", "allgather_bucket_size", "number", "of", "elements", "to", "allgather", "at", "once", "used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", "with", "a", "tradeoff", "with", "speed", "reduce_bucket_size", "number", "of", "elements", "to", "reduce", "at", "once", "used", "to", "limit", "the", "memory", "required", "for", "larger", "model", "sizes", "with", "a", "tradeoff", "with", "speed", "zero_allow_untested_optimizer", "allow", "untested", "optimizers", "to", "be", "used", "with", "zero", "currently", "only", "adam", "is", "a", "deepspeed", "supported", "optimizer", "when", "using", "zero", "logging_batch_size_per_gpu", "config", "used", "in", "deepspeed", "to", "calculate", "verbose", "timing", "for", "logging", "on", "a", "per", "sample", "per", "second", "basis", "only", "displayed", "if", "logging", "logging", "info", "if", "set", "to", "auto", "the", "strategy", "tries", "to", "infer", "this", "from", "the", "train", "dataloader", "s", "batchsampler", "else", "defaults", "to", "1", "to", "obtain", "accurate", "logs", "when", "using", "datasets", "that", "do", "not", "support", "batch", "samplers", "set", "this", "to", "the", "actual", "per", "gpu", "batch", "size", "trainer", "batch_size", "config", "pass", "in", "a", "deepspeed", "formatted", "config", "dict", "or", "path", "to", "a", "deepspeed", "config", "https", "www", "deepspeed", "ai", "docs", "config", "json", "all", "defaults", "will", "be", "ignored", "if", "a", "config", "is", "passed", "in", "logging_level", "set", "logging", "level", "for", "deepspeed", "loss_scale", "loss", "scaling", "value", "for", "fp16", "training", "0", "0", "results", "in", "dynamic", "loss", "scaling", "otherwise", "static", "initial_scale_power", "power", "of", "the", "initial", "dynamic", "loss", "scale", "value", "loss", "scale", "is", "computed", "by", "2", "initial_scale_power", "loss_scale_window", "window", "in", "which", "to", "raise", "lower", "the", "dynamic", "fp16", "loss", "scaling", "value", "hysteresis", "fp16", "delay", "shift", "in", "dynamic", "loss", "scaling", "min_loss_scale", "the", "minimum", "fp16", "dynamic", "loss", "scaling", "value", "partition_activations", "enables", "partition", "activation", "when", "used", "with", "zero", "stage", "3", "and", "model", "parallelism", "still", "requires", "you", "to", "wrap", "your", "forward", "functions", "in", "deepspeed", "checkpointing", "checkpoint", "see", "deepspeed", "tutorial", "https", "www", "deepspeed", "ai", "tutorials", "megatron", "deepspeed", "activation", "checkpoints", "optional", "_", "cpu_checkpointing", "offloads", "partitioned", "activations", "to", "cpu", "if", "partition_activations", "is", "enabled", "contiguous_memory_optimization", "copies", "partitioned", "activations", "so", "that", "they", "are", "contiguous", "in", "memory", "not", "supported", "by", "all", "models", "synchronize_checkpoint_boundary", "insert", "func", "torch", "cuda", "synchronize", "at", "each", "checkpoint", "boundary", "load_full_weights", "true", "when", "loading", "a", "single", "checkpoint", "file", "containing", "the", "model", "state", "dict", "when", "using", "zero", "stage", "3", "this", "differs", "from", "the", "deepspeed", "checkpoint", "which", "contains", "shards", "per", "worker", "exclude_frozen_parameters", "exclude", "frozen", "parameters", "when", "saving", "checkpoints"], "docstring_summary": "Provides capabilities to run training using the DeepSpeed library, with training optimizations for large", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 79, "end_line": 337, "hash": "9400285dbe5acf56a6a00dbdabe2f890", "complexity": 5, "parameters": ["accelerator", "zero_optimization", "stage", "remote_device", "offload_optimizer", "offload_parameters", "offload_params_device", "nvme_path", "params_buffer_count", "params_buffer_size", "max_in_cpu", "offload_optimizer_device", "optimizer_buffer_count", "block_size", "queue_depth", "single_submit", "overlap_events", "thread_count", "pin_memory", "sub_group_size", "contiguous_gradients", "overlap_comm", "allgather_partitions", "reduce_scatter", "allgather_bucket_size", "reduce_bucket_size", "zero_allow_untested_optimizer", "logging_batch_size_per_gpu", "int]", "config", "dict[str", "Any]]]", "logging_level", "parallel_devices", "cluster_environment", "loss_scale", "initial_scale_power", "loss_scale_window", "hysteresis", "min_loss_scale", "partition_activations", "cpu_checkpointing", "contiguous_memory_optimization", "synchronize_checkpoint_boundary", "load_full_weights", "precision_plugin", "process_group_backend", "timeout", "exclude_frozen_parameters"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "_setup_model_and_optimizers", "original_string": "def _setup_model_and_optimizers(\r\n        self, model: Module, optimizers: list[Optimizer]\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        Currently only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine` and a list with a single\r\n            deepspeed optimizer.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        # train_micro_batch_size_per_gpu is used for throughput logging purposes\r\n        # normally we set this to the batch size, but it is not available here unless the user provides it\r\n        # as part of the config\r\n        assert self.config is not None\r\n        self.config.setdefault(\"train_micro_batch_size_per_gpu\", 1)\r\n        self.model, optimizer = self._setup_model_and_optimizer(model, optimizers[0])\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self.model, [optimizer]", "language": "python", "code": "def _setup_model_and_optimizers(\r\n        self, model: Module, optimizers: list[Optimizer]\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        Currently only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine` and a list with a single\r\n            deepspeed optimizer.\r\n\r\n        \"\"\"\r\n        if len(optimizers) != 1:\r\n            raise ValueError(\r\n                f\"Currently only one optimizer is supported with DeepSpeed. Got {len(optimizers)} optimizers instead.\"\r\n            )\r\n\r\n        # train_micro_batch_size_per_gpu is used for throughput logging purposes\r\n        # normally we set this to the batch size, but it is not available here unless the user provides it\r\n        # as part of the config\r\n        assert self.config is not None\r\n        self.config.setdefault(\"train_micro_batch_size_per_gpu\", 1)\r\n        self.model, optimizer = self._setup_model_and_optimizer(model, optimizers[0])\r\n        self._set_deepspeed_activation_checkpointing()\r\n        return self.model, [optimizer]", "code_tokens": ["def", "_setup_model_and_optimizers", "(", "self", ",", "model", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ")", "-", ">", "tuple", "[", "\"", "deepspeed", ".", "DeepSpeedEngine", "\"", ",", "list", "[", "Optimizer", "]", "]", ":", "\"", "\"", "\"", "Setup", "a", "model", "and", "multiple", "optimizers", "together", ".", "Currently", "only", "a", "single", "optimizer", "is", "supported", ".", "Return", ":", "The", "model", "wrapped", "into", "a", ":", "class", ":", "`", "deepspeed", ".", "DeepSpeedEngine", "`", "and", "a", "list", "with", "a", "single", "deepspeed", "optimizer", ".", "\"", "\"", "\"", "if", "len", "(", "optimizers", ")", "!", "=", "1", ":", "raise", "ValueError", "(", "f", "\"", "Currently", "only", "one", "optimizer", "is", "supported", "with", "DeepSpeed", ".", "Got", "{", "len", "(", "optimizers", ")", "}", "optimizers", "instead", ".", "\"", ")", "assert", "self", ".", "config", "is", "not", "None", "self", ".", "config", ".", "setdefault", "(", "\"", "train_micro_batch_size_per_gpu", "\"", ",", "1", ")", "self", ".", "model", ",", "optimizer", "=", "self", ".", "_setup_model_and_optimizer", "(", "model", ",", "optimizers", "[", "0", "]", ")", "self", ".", "_set_deepspeed_activation_checkpointing", "(", ")", "return", "self", ".", "model", ",", "[", "optimizer", "]"], "docstring": "Setup a model and multiple optimizers together.\r\n\r\n        Currently only a single optimizer is supported.\r\n\r\n        Return:\r\n            The model wrapped into a :class:`deepspeed.DeepSpeedEngine` and a list with a single\r\n            deepspeed optimizer.", "docstring_tokens": ["setup", "a", "model", "and", "multiple", "optimizers", "together", "currently", "only", "a", "single", "optimizer", "is", "supported", "return", "the", "model", "wrapped", "into", "a", "class", "deepspeed", "deepspeedengine", "and", "a", "list", "with", "a", "single", "deepspeed", "optimizer"], "docstring_summary": "Setup a model and multiple optimizers together.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 406, "end_line": 430, "hash": "eef32a8564d9218d0a449f3f4eacd97a", "complexity": 2, "parameters": ["model", "optimizers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "_setup_model_and_optimizer", "original_string": "def _setup_model_and_optimizer(\r\n        self,\r\n        model: Module,\r\n        optimizer: Optional[Optimizer],\r\n        lr_scheduler: Optional[Union[LRScheduler, ReduceLROnPlateau]] = None,\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", Optimizer]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=lr_scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer", "language": "python", "code": "def _setup_model_and_optimizer(\r\n        self,\r\n        model: Module,\r\n        optimizer: Optional[Optimizer],\r\n        lr_scheduler: Optional[Union[LRScheduler, ReduceLROnPlateau]] = None,\r\n    ) -> tuple[\"deepspeed.DeepSpeedEngine\", Optimizer]:\r\n        \"\"\"Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n        deepspeed_engine, deepspeed_optimizer, _, _ = deepspeed.initialize(\r\n            args=argparse.Namespace(device_rank=self.root_device.index),\r\n            config=self.config,\r\n            model=model,\r\n            model_parameters=model_parameters,\r\n            optimizer=optimizer,\r\n            lr_scheduler=lr_scheduler,\r\n            dist_init_required=False,\r\n        )\r\n        return deepspeed_engine, deepspeed_optimizer", "code_tokens": ["def", "_setup_model_and_optimizer", "(", "self", ",", "model", ":", "Module", ",", "optimizer", ":", "Optional", "[", "Optimizer", "]", ",", "lr_scheduler", ":", "Optional", "[", "Union", "[", "LRScheduler", ",", "ReduceLROnPlateau", "]", "]", "=", "None", ",", ")", "-", ">", "tuple", "[", "\"", "deepspeed", ".", "DeepSpeedEngine", "\"", ",", "Optimizer", "]", ":", "\"", "\"", "\"", "Initialize", "one", "model", "and", "one", "optimizer", "with", "an", "optional", "learning", "rate", "scheduler", ".", "This", "calls", "`", "`", "deepspeed", ".", "initialize", "`", "`", "internally", ".", "\"", "\"", "\"", "import", "deepspeed", "model_parameters", "=", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", "deepspeed_engine", ",", "deepspeed_optimizer", ",", "_", ",", "_", "=", "deepspeed", ".", "initialize", "(", "args", "=", "argparse", ".", "Namespace", "(", "device_rank", "=", "self", ".", "root_device", ".", "index", ")", ",", "config", "=", "self", ".", "config", ",", "model", "=", "model", ",", "model_parameters", "=", "model_parameters", ",", "optimizer", "=", "optimizer", ",", "lr_scheduler", "=", "lr_scheduler", ",", "dist_init_required", "=", "False", ",", ")", "return", "deepspeed_engine", ",", "deepspeed_optimizer"], "docstring": "Initialize one model and one optimizer with an optional learning rate scheduler.\r\n\r\n        This calls ``deepspeed.initialize`` internally.", "docstring_tokens": ["initialize", "one", "model", "and", "one", "optimizer", "with", "an", "optional", "learning", "rate", "scheduler", "this", "calls", "deepspeed", "initialize", "internally"], "docstring_summary": "Initialize one model and one optimizer with an optional learning rate scheduler.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 432, "end_line": 455, "hash": "cc17499017b2e6f414c1af77ebd32948", "complexity": 1, "parameters": ["model", "optimizer", "lr_scheduler", "ReduceLROnPlateau]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "setup_optimizers", "original_string": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        # Skip initializing optimizers here as DeepSpeed handles optimizers via config.\r\n        # User may have specified config options instead in configure_optimizers, but this is handled\r\n        # via `_initialize_deepspeed_train`\r\n        # empty optimizers, schedulers\r\n        self.optimizers = []\r\n        self.lr_scheduler_configs = []", "language": "python", "code": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        # Skip initializing optimizers here as DeepSpeed handles optimizers via config.\r\n        # User may have specified config options instead in configure_optimizers, but this is handled\r\n        # via `_initialize_deepspeed_train`\r\n        # empty optimizers, schedulers\r\n        self.optimizers = []\r\n        self.lr_scheduler_configs = []", "code_tokens": ["def", "setup_optimizers", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Creates", "optimizers", "and", "schedulers", ".", "Args", ":", "trainer", ":", "the", "Trainer", ",", "these", "optimizers", "should", "be", "connected", "to", "\"", "\"", "\"", "self", ".", "optimizers", "=", "[", "]", "self", ".", "lr_scheduler_configs", "=", "[", "]"], "docstring": "Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to", "docstring_tokens": ["creates", "optimizers", "and", "schedulers", "args", "trainer", "the", "trainer", "these", "optimizers", "should", "be", "connected", "to"], "docstring_summary": "Creates optimizers and schedulers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 602, "end_line": 614, "hash": "6120e5bb6983519beef741b39bcaddf8", "complexity": 1, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "save_checkpoint", "original_string": "def save_checkpoint(self, checkpoint: dict, filepath: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: The checkpoint state dictionary\r\n            filepath: write-target file's path\r\n            storage_options: not used for ``DeepSpeedStrategy`` as ``CheckpointIO`` is not used\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        # broadcast the filepath from rank 0 to ensure all the states are saved in a common filepath\r\n        filepath = self.broadcast(filepath)\r\n\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}` as `CheckpointIO` is not used.\"\r\n            )\r\n\r\n        if self.zero_stage_3 and self._multi_device and self.is_global_zero:\r\n            warning_cache.warn(\r\n                \"When saving the DeepSpeed Stage 3 checkpoint, \"\r\n                \"each worker will save a shard of the checkpoint within a directory. \"\r\n                \"If a single file is required after training, \"\r\n                \"see https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#\"\r\n                \"deepspeed-zero-stage-3-single-file for instructions.\"\r\n            )\r\n        # Use deepspeed's internal checkpointing function to handle partitioned weights across processes\r\n        # dump states as a checkpoint dictionary object\r\n        _exclude_keys = [\"state_dict\", \"optimizer_states\"]\r\n        checkpoint = {k: v for k, v in checkpoint.items() if k not in _exclude_keys}\r\n        self.deepspeed_engine.save_checkpoint(\r\n            filepath,\r\n            client_state=checkpoint,\r\n            tag=\"checkpoint\",\r\n            exclude_frozen_parameters=self.exclude_frozen_parameters,\r\n        )", "language": "python", "code": "def save_checkpoint(self, checkpoint: dict, filepath: _PATH, storage_options: Optional[Any] = None) -> None:\r\n        \"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: The checkpoint state dictionary\r\n            filepath: write-target file's path\r\n            storage_options: not used for ``DeepSpeedStrategy`` as ``CheckpointIO`` is not used\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in\r\n\r\n        \"\"\"\r\n        # broadcast the filepath from rank 0 to ensure all the states are saved in a common filepath\r\n        filepath = self.broadcast(filepath)\r\n\r\n        if storage_options is not None:\r\n            raise TypeError(\r\n                \"`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\"\r\n                f\" is not supported for `{self.__class__.__name__}` as `CheckpointIO` is not used.\"\r\n            )\r\n\r\n        if self.zero_stage_3 and self._multi_device and self.is_global_zero:\r\n            warning_cache.warn(\r\n                \"When saving the DeepSpeed Stage 3 checkpoint, \"\r\n                \"each worker will save a shard of the checkpoint within a directory. \"\r\n                \"If a single file is required after training, \"\r\n                \"see https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#\"\r\n                \"deepspeed-zero-stage-3-single-file for instructions.\"\r\n            )\r\n        # Use deepspeed's internal checkpointing function to handle partitioned weights across processes\r\n        # dump states as a checkpoint dictionary object\r\n        _exclude_keys = [\"state_dict\", \"optimizer_states\"]\r\n        checkpoint = {k: v for k, v in checkpoint.items() if k not in _exclude_keys}\r\n        self.deepspeed_engine.save_checkpoint(\r\n            filepath,\r\n            client_state=checkpoint,\r\n            tag=\"checkpoint\",\r\n            exclude_frozen_parameters=self.exclude_frozen_parameters,\r\n        )", "code_tokens": ["def", "save_checkpoint", "(", "self", ",", "checkpoint", ":", "dict", ",", "filepath", ":", "_PATH", ",", "storage_options", ":", "Optional", "[", "Any", "]", "=", "None", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Save", "model", "/", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "-", "dump", "and", "file", "-", "write", ".", "Args", ":", "checkpoint", ":", "The", "checkpoint", "state", "dictionary", "filepath", ":", "write", "-", "target", "file", "'", "s", "path", "storage_options", ":", "not", "used", "for", "`", "`", "DeepSpeedStrategy", "`", "`", "as", "`", "`", "CheckpointIO", "`", "`", "is", "not", "used", "Raises", ":", "TypeError", ":", "If", "`", "`", "storage_options", "`", "`", "arg", "is", "passed", "in", "\"", "\"", "\"", "filepath", "=", "self", ".", "broadcast", "(", "filepath", ")", "if", "storage_options", "is", "not", "None", ":", "raise", "TypeError", "(", "\"", "`", "Trainer", ".", "save_checkpoint", "(", ".", ".", ".", ",", "storage_options", "=", ".", ".", ".", ")", "`", "with", "`", "storage_options", "`", "arg", "\"", "f", "\"", "is", "not", "supported", "for", "`", "{", "self", ".", "__class__", ".", "__name__", "}", "`", "as", "`", "CheckpointIO", "`", "is", "not", "used", ".", "\"", ")", "if", "self", ".", "zero_stage_3", "and", "self", ".", "_multi_device", "and", "self", ".", "is_global_zero", ":", "warning_cache", ".", "warn", "(", "\"", "When", "saving", "the", "DeepSpeed", "Stage", "3", "checkpoint", ",", "\"", "\"", "each", "worker", "will", "save", "a", "shard", "of", "the", "checkpoint", "within", "a", "directory", ".", "\"", "\"", "If", "a", "single", "file", "is", "required", "after", "training", ",", "\"", "\"", "see", "https", ":", "/", "/", "lightning", ".", "ai", "/", "docs", "/", "pytorch", "/", "stable", "/", "advanced", "/", "model_parallel", ".", "html", "\"", "deepspeed", "-", "zero", "-", "stage", "-", "3", "-", "single", "-", "file", "for", "instructions", ".", "\"", ")", "_exclude_keys", "=", "[", "\"", "state_dict", "\"", ",", "\"", "optimizer_states", "\"", "]", "checkpoint", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "checkpoint", ".", "items", "(", ")", "if", "k", "not", "in", "_exclude_keys", "}", "self", ".", "deepspeed_engine", ".", "save_checkpoint", "(", "filepath", ",", "client_state", "=", "checkpoint", ",", "tag", "=", "\"", "checkpoint", "\"", ",", "exclude_frozen_parameters", "=", "self", ".", "exclude_frozen_parameters", ",", ")"], "docstring": "Save model/training states as a checkpoint file through state-dump and file-write.\r\n\r\n        Args:\r\n            checkpoint: The checkpoint state dictionary\r\n            filepath: write-target file's path\r\n            storage_options: not used for ``DeepSpeedStrategy`` as ``CheckpointIO`` is not used\r\n\r\n        Raises:\r\n            TypeError:\r\n                If ``storage_options`` arg is passed in", "docstring_tokens": ["save", "model", "training", "states", "as", "a", "checkpoint", "file", "through", "state", "dump", "and", "file", "write", "args", "checkpoint", "the", "checkpoint", "state", "dictionary", "filepath", "write", "target", "file", "s", "path", "storage_options", "not", "used", "for", "deepspeedstrategy", "as", "checkpointio", "is", "not", "used", "raises", "typeerror", "if", "storage_options", "arg", "is", "passed", "in"], "docstring_summary": "Save model/training states as a checkpoint file through state-dump and file-write.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 634, "end_line": 673, "hash": "d919cac94465f62ef5d9c4a1b8d0ad6e", "complexity": 7, "parameters": ["checkpoint", "filepath", "storage_options"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\deepspeed.py", "func_name": "_restore_zero_state", "original_string": "def _restore_zero_state(self, ckpt: Mapping[str, Any], strict: bool) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        assert self.lightning_module is not None\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            # copy state_dict so _load_from_state_dict can modify it\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            # because zero3 puts placeholders in model params, this context\r\n            # manager gathers (unpartitions) the params of the current layer, then loads from\r\n            # the state dict and then re-partitions them again\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=strict,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(self.lightning_module, prefix=\"\")", "language": "python", "code": "def _restore_zero_state(self, ckpt: Mapping[str, Any], strict: bool) -> None:\r\n        \"\"\"Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.\r\n\r\n        \"\"\"\r\n        import deepspeed\r\n\r\n        assert self.lightning_module is not None\r\n\r\n        def load(module: torch.nn.Module, prefix: str = \"\") -> None:\r\n            missing_keys: list[str] = []\r\n            unexpected_keys: list[str] = []\r\n            error_msgs: list[str] = []\r\n            state_dict = ckpt[\"state_dict\"]\r\n\r\n            # copy state_dict so _load_from_state_dict can modify it\r\n            metadata = getattr(state_dict, \"_metadata\", None)\r\n            state_dict = state_dict.copy()\r\n            if metadata is not None:\r\n                state_dict._metadata = metadata\r\n\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            # because zero3 puts placeholders in model params, this context\r\n            # manager gathers (unpartitions) the params of the current layer, then loads from\r\n            # the state dict and then re-partitions them again\r\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\r\n                if self.is_global_zero:\r\n                    module._load_from_state_dict(\r\n                        state_dict=state_dict,\r\n                        prefix=prefix,\r\n                        local_metadata=local_metadata,\r\n                        strict=strict,\r\n                        missing_keys=missing_keys,\r\n                        unexpected_keys=unexpected_keys,\r\n                        error_msgs=error_msgs,\r\n                    )\r\n\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \".\")\r\n\r\n        load(self.lightning_module, prefix=\"\")", "code_tokens": ["def", "_restore_zero_state", "(", "self", ",", "ckpt", ":", "Mapping", "[", "str", ",", "Any", "]", ",", "strict", ":", "bool", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Overrides", "the", "normal", "load_state_dict", "behaviour", "in", "PyTorch", "to", "ensure", "we", "gather", "parameters", "that", "may", "be", "sharded", "across", "processes", "before", "loading", "the", "state", "dictionary", "when", "using", "ZeRO", "stage", "3", ".", "This", "is", "then", "automatically", "synced", "across", "processes", ".", "Args", ":", "ckpt", ":", "The", "ckpt", "file", ".", "\"", "\"", "\"", "import", "deepspeed", "assert", "self", ".", "lightning_module", "is", "not", "None", "def", "load", "(", "module", ":", "torch", ".", "nn", ".", "Module", ",", "prefix", ":", "str", "=", "\"", "\"", ")", "-", ">", "None", ":", "missing_keys", ":", "list", "[", "str", "]", "=", "[", "]", "unexpected_keys", ":", "list", "[", "str", "]", "=", "[", "]", "error_msgs", ":", "list", "[", "str", "]", "=", "[", "]", "state_dict", "=", "ckpt", "[", "\"", "state_dict", "\"", "]", "metadata", "=", "getattr", "(", "state_dict", ",", "\"", "_metadata", "\"", ",", "None", ")", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "if", "metadata", "is", "not", "None", ":", "state_dict", ".", "_metadata", "=", "metadata", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "with", "deepspeed", ".", "zero", ".", "GatheredParameters", "(", "list", "(", "module", ".", "parameters", "(", "recurse", "=", "False", ")", ")", ",", "modifier_rank", "=", "0", ")", ":", "if", "self", ".", "is_global_zero", ":", "module", ".", "_load_from_state_dict", "(", "state_dict", "=", "state_dict", ",", "prefix", "=", "prefix", ",", "local_metadata", "=", "local_metadata", ",", "strict", "=", "strict", ",", "missing_keys", "=", "missing_keys", ",", "unexpected_keys", "=", "unexpected_keys", ",", "error_msgs", "=", "error_msgs", ",", ")", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "if", "child", "is", "not", "None", ":", "load", "(", "child", ",", "prefix", "+", "name", "+", "\"", ".", "\"", ")", "load", "(", "self", ".", "lightning_module", ",", "prefix", "=", "\"", "\"", ")"], "docstring": "Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded\r\n        across processes before loading the state dictionary when using ZeRO stage 3. This is then automatically synced\r\n        across processes.\r\n\r\n        Args:\r\n            ckpt: The ckpt file.", "docstring_tokens": ["overrides", "the", "normal", "load_state_dict", "behaviour", "in", "pytorch", "to", "ensure", "we", "gather", "parameters", "that", "may", "be", "sharded", "across", "processes", "before", "loading", "the", "state", "dictionary", "when", "using", "zero", "stage", "3", "this", "is", "then", "automatically", "synced", "across", "processes", "args", "ckpt", "the", "ckpt", "file"], "docstring_summary": "Overrides the normal load_state_dict behaviour in PyTorch to ensure we gather parameters that may be sharded", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\deepspeed.py", "partition": "train", "function_type": "class_method", "class_name": "DeepSpeedStrategy", "start_line": 725, "end_line": 770, "hash": "2de64bb7337372295e81915fdaaed152", "complexity": 7, "parameters": ["ckpt", "Any]", "strict"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\fsdp.py", "func_name": "_setup_model", "original_string": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in model.modules()):\r\n            if _has_meta_device_parameters_or_buffers(model):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self.kwargs:\r\n                # The user has wrapped their submodules manually, don't apply the auto wrap policy.\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self.kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            log.debug(f\"setting up FSDP model with device id: {self.root_device.index}, kwargs: {self.kwargs}\")\r\n            model = FullyShardedDataParallel(\r\n                module=model,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self.kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(model, self.root_device)\r\n\r\n        # activation checkpointing needs to be set up after wrapping the model\r\n        _setup_activation_checkpointing(model, self._activation_checkpointing_kwargs)\r\n\r\n        return model", "language": "python", "code": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.\"\"\"\r\n        from torch.distributed.fsdp import FullyShardedDataParallel\r\n\r\n        if any(isinstance(mod, FullyShardedDataParallel) for mod in model.modules()):\r\n            if _has_meta_device_parameters_or_buffers(model):\r\n                rank_zero_warn(\r\n                    \"The model is already wrapped in `FSDP` but there are still parameters on the meta device.\"\r\n                )\r\n            if \"auto_wrap_policy\" in self.kwargs:\r\n                # The user has wrapped their submodules manually, don't apply the auto wrap policy.\r\n                rank_zero_warn(\r\n                    \"A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.\"\r\n                )\r\n                del self.kwargs[\"auto_wrap_policy\"]\r\n        else:\r\n            log.debug(f\"setting up FSDP model with device id: {self.root_device.index}, kwargs: {self.kwargs}\")\r\n            model = FullyShardedDataParallel(\r\n                module=model,\r\n                cpu_offload=self.cpu_offload,\r\n                mixed_precision=self.mixed_precision_config,\r\n                sharding_strategy=self.sharding_strategy,\r\n                device_id=self.root_device.index,\r\n                **self.kwargs,\r\n            )\r\n\r\n        _move_torchmetrics_to_device(model, self.root_device)\r\n\r\n        # activation checkpointing needs to be set up after wrapping the model\r\n        _setup_activation_checkpointing(model, self._activation_checkpointing_kwargs)\r\n\r\n        return model", "code_tokens": ["def", "_setup_model", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "\"", "\"", "\"", "Wraps", "the", "model", "into", "a", ":", "class", ":", "`", "~", "torch", ".", "distributed", ".", "fsdp", ".", "fully_sharded_data_parallel", ".", "FullyShardedDataParallel", "`", "module", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "if", "any", "(", "isinstance", "(", "mod", ",", "FullyShardedDataParallel", ")", "for", "mod", "in", "model", ".", "modules", "(", ")", ")", ":", "if", "_has_meta_device_parameters_or_buffers", "(", "model", ")", ":", "rank_zero_warn", "(", "\"", "The", "model", "is", "already", "wrapped", "in", "`", "FSDP", "`", "but", "there", "are", "still", "parameters", "on", "the", "meta", "device", ".", "\"", ")", "if", "\"", "auto_wrap_policy", "\"", "in", "self", ".", "kwargs", ":", "rank_zero_warn", "(", "\"", "A", "FSDP", "`", "auto_wrap_policy", "`", "is", "set", ",", "but", "the", "model", "is", "already", "wrapped", ".", "The", "policy", "will", "be", "ignored", ".", "\"", ")", "del", "self", ".", "kwargs", "[", "\"", "auto_wrap_policy", "\"", "]", "else", ":", "log", ".", "debug", "(", "f", "\"", "setting", "up", "FSDP", "model", "with", "device", "id", ":", "{", "self", ".", "root_device", ".", "index", "}", ",", "kwargs", ":", "{", "self", ".", "kwargs", "}", "\"", ")", "model", "=", "FullyShardedDataParallel", "(", "module", "=", "model", ",", "cpu_offload", "=", "self", ".", "cpu_offload", ",", "mixed_precision", "=", "self", ".", "mixed_precision_config", ",", "sharding_strategy", "=", "self", ".", "sharding_strategy", ",", "device_id", "=", "self", ".", "root_device", ".", "index", ",", "*", "*", "self", ".", "kwargs", ",", ")", "_move_torchmetrics_to_device", "(", "model", ",", "self", ".", "root_device", ")", "_setup_activation_checkpointing", "(", "model", ",", "self", ".", "_activation_checkpointing_kwargs", ")", "return", "model"], "docstring": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`\r\n        module.", "docstring_tokens": ["wraps", "the", "model", "into", "a", "class", "torch", "distributed", "fsdp", "fully_sharded_data_parallel", "fullyshardeddataparallel", "module"], "docstring_summary": "Wraps the model into a :class:`~torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel`", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "FSDPStrategy", "start_line": 291, "end_line": 323, "hash": "dc0cc3f381637c132b6903cc12ea1fe1", "complexity": 5, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\fsdp.py", "func_name": "reduce", "original_string": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "language": "python", "code": "def reduce(\r\n        self,\r\n        tensor: Union[Tensor, Any],\r\n        group: Optional[Any] = None,\r\n        reduce_op: Optional[Union[ReduceOp, str]] = \"mean\",\r\n    ) -> Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged\r\n\r\n        \"\"\"\r\n        if isinstance(tensor, Tensor):\r\n            return _sync_ddp_if_available(tensor, group, reduce_op=reduce_op)\r\n        return tensor", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Union", "[", "Tensor", ",", "Any", "]", ",", "group", ":", "Optional", "[", "Any", "]", "=", "None", ",", "reduce_op", ":", "Optional", "[", "Union", "[", "ReduceOp", ",", "str", "]", "]", "=", "\"", "mean", "\"", ",", ")", "-", ">", "Tensor", ":", "\"", "\"", "\"", "Reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "group", ":", "the", "process", "group", "to", "gather", "results", "from", ".", "Defaults", "to", "all", "processes", "(", "world", ")", "reduce_op", ":", "the", "reduction", "operation", ".", "Defaults", "to", "'", "mean", "'", "/", "'", "avg", "'", ".", "Can", "also", "be", "a", "string", "'", "sum", "'", "to", "calculate", "the", "sum", "during", "reduction", ".", "Return", ":", "reduced", "value", ",", "except", "when", "the", "input", "was", "not", "a", "tensor", "the", "output", "remains", "is", "unchanged", "\"", "\"", "\"", "if", "isinstance", "(", "tensor", ",", "Tensor", ")", ":", "return", "_sync_ddp_if_available", "(", "tensor", ",", "group", ",", "reduce_op", "=", "reduce_op", ")", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            group: the process group to gather results from. Defaults to all processes (world)\r\n            reduce_op: the reduction operation. Defaults to 'mean'/'avg'.\r\n                Can also be a string 'sum' to calculate the sum during reduction.\r\n\r\n        Return:\r\n            reduced value, except when the input was not a tensor the output remains is unchanged", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "group", "the", "process", "group", "to", "gather", "results", "from", "defaults", "to", "all", "processes", "world", "reduce_op", "the", "reduction", "operation", "defaults", "to", "mean", "avg", "can", "also", "be", "a", "string", "sum", "to", "calculate", "the", "sum", "during", "reduction", "return", "reduced", "value", "except", "when", "the", "input", "was", "not", "a", "tensor", "the", "output", "remains", "is", "unchanged"], "docstring_summary": "Reduces a tensor from several distributed processes to one aggregated tensor.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\fsdp.py", "partition": "train", "function_type": "class_method", "class_name": "FSDPStrategy", "start_line": 432, "end_line": 452, "hash": "c28cfb674366510119d5002a549b12db", "complexity": 2, "parameters": ["tensor", "Any]", "group", "reduce_op", "str]]"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\model_parallel.py", "func_name": "lightning_module_state_dict", "original_string": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Collects the state dict of the model.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        assert self.model is not None\r\n        return get_model_state_dict(self.model, options=state_dict_options)", "language": "python", "code": "def lightning_module_state_dict(self) -> dict[str, Any]:\r\n        \"\"\"Collects the state dict of the model.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_model_state_dict\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        assert self.model is not None\r\n        return get_model_state_dict(self.model, options=state_dict_options)", "code_tokens": ["def", "lightning_module_state_dict", "(", "self", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Collects", "the", "state", "dict", "of", "the", "model", ".", "Only", "returns", "a", "non", "-", "empty", "state", "dict", "on", "rank", "0", "if", "`", "`", "save_distributed_checkpoint", "=", "False", "`", "`", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict", "import", "StateDictOptions", ",", "get_model_state_dict", "state_dict_options", "=", "StateDictOptions", "(", "full_state_dict", "=", "(", "not", "self", ".", "_save_distributed_checkpoint", ")", ",", "cpu_offload", "=", "True", ")", "assert", "self", ".", "model", "is", "not", "None", "return", "get_model_state_dict", "(", "self", ".", "model", ",", "options", "=", "state_dict_options", ")"], "docstring": "Collects the state dict of the model.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.", "docstring_tokens": ["collects", "the", "state", "dict", "of", "the", "model", "only", "returns", "a", "non", "empty", "state", "dict", "on", "rank", "0", "if", "save_distributed_checkpoint", "false"], "docstring_summary": "Collects the state dict of the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\model_parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ModelParallelStrategy", "start_line": 252, "end_line": 262, "hash": "5968835545cbd5bf11b984784abc88f4", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\model_parallel.py", "func_name": "optimizer_state", "original_string": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Any]:\r\n        \"\"\"Collects the state of the given optimizer.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_optimizer_state_dict\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n        from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        assert self.model is not None\r\n\r\n        state_dict = get_optimizer_state_dict(self.model, optimizer, options=state_dict_options)\r\n        if not self._save_distributed_checkpoint and self.global_rank == 0:\r\n            # Store the optimizer state dict in standard format\r\n            state_dict = FSDP.rekey_optim_state_dict(state_dict, OptimStateKeyType.PARAM_ID, self.model)\r\n        return state_dict", "language": "python", "code": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Any]:\r\n        \"\"\"Collects the state of the given optimizer.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.\r\n\r\n        \"\"\"\r\n        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_optimizer_state_dict\r\n        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\n        from torch.distributed.fsdp import OptimStateKeyType\r\n\r\n        state_dict_options = StateDictOptions(full_state_dict=(not self._save_distributed_checkpoint), cpu_offload=True)\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        assert self.model is not None\r\n\r\n        state_dict = get_optimizer_state_dict(self.model, optimizer, options=state_dict_options)\r\n        if not self._save_distributed_checkpoint and self.global_rank == 0:\r\n            # Store the optimizer state dict in standard format\r\n            state_dict = FSDP.rekey_optim_state_dict(state_dict, OptimStateKeyType.PARAM_ID, self.model)\r\n        return state_dict", "code_tokens": ["def", "optimizer_state", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "dict", "[", "str", ",", "Any", "]", ":", "\"", "\"", "\"", "Collects", "the", "state", "of", "the", "given", "optimizer", ".", "Only", "returns", "a", "non", "-", "empty", "state", "dict", "on", "rank", "0", "if", "`", "`", "save_distributed_checkpoint", "=", "False", "`", "`", ".", "\"", "\"", "\"", "from", "torch", ".", "distributed", ".", "checkpoint", ".", "state_dict", "import", "StateDictOptions", ",", "get_optimizer_state_dict", "from", "torch", ".", "distributed", ".", "fsdp", "import", "FullyShardedDataParallel", "as", "FSDP", "from", "torch", ".", "distributed", ".", "fsdp", "import", "OptimStateKeyType", "state_dict_options", "=", "StateDictOptions", "(", "full_state_dict", "=", "(", "not", "self", ".", "_save_distributed_checkpoint", ")", ",", "cpu_offload", "=", "True", ")", "if", "isinstance", "(", "optimizer", ",", "LightningOptimizer", ")", ":", "optimizer", "=", "optimizer", ".", "_optimizer", "assert", "self", ".", "model", "is", "not", "None", "state_dict", "=", "get_optimizer_state_dict", "(", "self", ".", "model", ",", "optimizer", ",", "options", "=", "state_dict_options", ")", "if", "not", "self", ".", "_save_distributed_checkpoint", "and", "self", ".", "global_rank", "=", "=", "0", ":", "state_dict", "=", "FSDP", ".", "rekey_optim_state_dict", "(", "state_dict", ",", "OptimStateKeyType", ".", "PARAM_ID", ",", "self", ".", "model", ")", "return", "state_dict"], "docstring": "Collects the state of the given optimizer.\r\n\r\n        Only returns a non-empty state dict on rank 0 if ``save_distributed_checkpoint=False``.", "docstring_tokens": ["collects", "the", "state", "of", "the", "given", "optimizer", "only", "returns", "a", "non", "empty", "state", "dict", "on", "rank", "0", "if", "save_distributed_checkpoint", "false"], "docstring_summary": "Collects the state of the given optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\model_parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ModelParallelStrategy", "start_line": 270, "end_line": 290, "hash": "9db609fa0a57940f1943152671ca0884", "complexity": 4, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\parallel.py", "func_name": "reduce_boolean_decision", "original_string": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "language": "python", "code": "def reduce_boolean_decision(self, decision: bool, all: bool = True) -> bool:\r\n        \"\"\"Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.\r\n\r\n        \"\"\"\r\n        decision = torch.tensor(int(decision), device=self.root_device)\r\n        decision = self.reduce(\r\n            decision,\r\n            reduce_op=ReduceOp.SUM,  # type: ignore[arg-type]\r\n        )\r\n        decision = bool(decision == self.world_size) if all else bool(decision)\r\n        return decision", "code_tokens": ["def", "reduce_boolean_decision", "(", "self", ",", "decision", ":", "bool", ",", "all", ":", "bool", "=", "True", ")", "-", ">", "bool", ":", "\"", "\"", "\"", "Reduces", "a", "boolean", "decision", "over", "distributed", "processes", ".", "By", "default", "is", "analogous", "to", "`", "`", "all", "`", "`", "from", "the", "standard", "library", ",", "returning", "`", "`", "True", "`", "`", "only", "if", "all", "input", "decisions", "evaluate", "to", "`", "`", "True", "`", "`", ".", "If", "`", "`", "all", "`", "`", "is", "set", "to", "`", "`", "False", "`", "`", ",", "it", "behaves", "like", "`", "`", "any", "`", "`", "instead", ".", "Args", ":", "decision", ":", "A", "single", "input", "decision", ".", "all", ":", "Whether", "to", "logically", "emulate", "`", "`", "all", "`", "`", "or", "`", "`", "any", "`", "`", ".", "Defaults", "to", "True", ".", "Returns", ":", "bool", ":", "The", "reduced", "boolean", "decision", ".", "\"", "\"", "\"", "decision", "=", "torch", ".", "tensor", "(", "int", "(", "decision", ")", ",", "device", "=", "self", ".", "root_device", ")", "decision", "=", "self", ".", "reduce", "(", "decision", ",", "reduce_op", "=", "ReduceOp", ".", "SUM", ",", ")", "decision", "=", "bool", "(", "decision", "=", "=", "self", ".", "world_size", ")", "if", "all", "else", "bool", "(", "decision", ")", "return", "decision"], "docstring": "Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard\r\n        library, returning ``True`` only if all input decisions evaluate to ``True``. If ``all`` is set to ``False``,\r\n        it behaves like ``any`` instead.\r\n\r\n        Args:\r\n            decision: A single input decision.\r\n            all: Whether to logically emulate ``all`` or ``any``. Defaults to True.\r\n\r\n        Returns:\r\n            bool: The reduced boolean decision.", "docstring_tokens": ["reduces", "a", "boolean", "decision", "over", "distributed", "processes", "by", "default", "is", "analogous", "to", "all", "from", "the", "standard", "library", "returning", "true", "only", "if", "all", "input", "decisions", "evaluate", "to", "true", "if", "all", "is", "set", "to", "false", "it", "behaves", "like", "any", "instead", "args", "decision", "a", "single", "input", "decision", "all", "whether", "to", "logically", "emulate", "all", "or", "any", "defaults", "to", "true", "returns", "bool", "the", "reduced", "boolean", "decision"], "docstring_summary": "Reduces a boolean decision over distributed processes. By default is analogous to ``all`` from the standard", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ParallelStrategy", "start_line": 94, "end_line": 113, "hash": "90033c4ead266362945ee549432621bb", "complexity": 2, "parameters": ["decision", "all"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\parallel.py", "func_name": "block_backward_sync", "original_string": "def block_backward_sync(self) -> Generator:\r\n        \"\"\"Blocks ddp sync gradients behaviour on backwards pass.\r\n\r\n        This is useful for skipping sync when accumulating gradients, reducing communication overhead\r\n        Returns: context manager with sync behaviour off\r\n\r\n        \"\"\"\r\n        if isinstance(self.model, pl.utilities.types.DistributedDataParallel):\r\n            with self.model.no_sync():\r\n                yield None\r\n        else:\r\n            yield None", "language": "python", "code": "def block_backward_sync(self) -> Generator:\r\n        \"\"\"Blocks ddp sync gradients behaviour on backwards pass.\r\n\r\n        This is useful for skipping sync when accumulating gradients, reducing communication overhead\r\n        Returns: context manager with sync behaviour off\r\n\r\n        \"\"\"\r\n        if isinstance(self.model, pl.utilities.types.DistributedDataParallel):\r\n            with self.model.no_sync():\r\n                yield None\r\n        else:\r\n            yield None", "code_tokens": ["def", "block_backward_sync", "(", "self", ")", "-", ">", "Generator", ":", "\"", "\"", "\"", "Blocks", "ddp", "sync", "gradients", "behaviour", "on", "backwards", "pass", ".", "This", "is", "useful", "for", "skipping", "sync", "when", "accumulating", "gradients", ",", "reducing", "communication", "overhead", "Returns", ":", "context", "manager", "with", "sync", "behaviour", "off", "\"", "\"", "\"", "if", "isinstance", "(", "self", ".", "model", ",", "pl", ".", "utilities", ".", "types", ".", "DistributedDataParallel", ")", ":", "with", "self", ".", "model", ".", "no_sync", "(", ")", ":", "yield", "None", "else", ":", "yield", "None"], "docstring": "Blocks ddp sync gradients behaviour on backwards pass.\r\n\r\n        This is useful for skipping sync when accumulating gradients, reducing communication overhead\r\n        Returns: context manager with sync behaviour off", "docstring_tokens": ["blocks", "ddp", "sync", "gradients", "behaviour", "on", "backwards", "pass", "this", "is", "useful", "for", "skipping", "sync", "when", "accumulating", "gradients", "reducing", "communication", "overhead", "returns", "context", "manager", "with", "sync", "behaviour", "off"], "docstring_summary": "Blocks ddp sync gradients behaviour on backwards pass.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\parallel.py", "partition": "train", "function_type": "class_method", "class_name": "ParallelStrategy", "start_line": 116, "end_line": 127, "hash": "953dca979bc8b4f0ae1f6bfbe655113c", "complexity": 3, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\single_device.py", "func_name": "reduce", "original_string": "def reduce(self, tensor: Any | Tensor, *args: Any, **kwargs: Any) -> Any | Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only\r\n        operates with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "language": "python", "code": "def reduce(self, tensor: Any | Tensor, *args: Any, **kwargs: Any) -> Any | Tensor:\r\n        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only\r\n        operates with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation\r\n\r\n        \"\"\"\r\n        return tensor", "code_tokens": ["def", "reduce", "(", "self", ",", "tensor", ":", "Any", "|", "Tensor", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ")", "-", ">", "Any", "|", "Tensor", ":", "\"", "\"", "\"", "Reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", ".", "Since", "this", "strategy", "only", "operates", "with", "a", "single", "device", ",", "the", "reduction", "is", "simply", "the", "identity", ".", "Args", ":", "tensor", ":", "the", "tensor", "to", "sync", "and", "reduce", "*", "args", ":", "ignored", "*", "*", "kwargs", ":", "ignored", "Return", ":", "the", "unmodified", "input", "as", "reduction", "is", "not", "needed", "for", "single", "process", "operation", "\"", "\"", "\"", "return", "tensor"], "docstring": "Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only\r\n        operates with a single device, the reduction is simply the identity.\r\n\r\n        Args:\r\n            tensor: the tensor to sync and reduce\r\n            *args: ignored\r\n            **kwargs: ignored\r\n\r\n        Return:\r\n            the unmodified input as reduction is not needed for single process operation", "docstring_tokens": ["reduces", "a", "tensor", "from", "several", "distributed", "processes", "to", "one", "aggregated", "tensor", "since", "this", "strategy", "only", "operates", "with", "a", "single", "device", "the", "reduction", "is", "simply", "the", "identity", "args", "tensor", "the", "tensor", "to", "sync", "and", "reduce", "args", "ignored", "kwargs", "ignored", "return", "the", "unmodified", "input", "as", "reduction", "is", "not", "needed", "for", "single", "process", "operation"], "docstring_summary": "Reduces a tensor from several distributed processes to one aggregated tensor. Since this strategy only", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\single_device.py", "partition": "train", "function_type": "class_method", "class_name": "SingleDeviceStrategy", "start_line": 50, "end_line": 63, "hash": "8e28393baa4add55fcffba2b26f05adb", "complexity": 1, "parameters": ["tensor", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "connect", "original_string": "def connect(self, model: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called by the Trainer to connect the strategy with the model.\"\"\"\r\n        # model conversions cannot be applied at this point because `LightningModule.{setup,configure_model}` haven't\r\n        # run yet\r\n        self._lightning_module = model\r\n        self.model = model", "language": "python", "code": "def connect(self, model: \"pl.LightningModule\") -> None:\r\n        \"\"\"Called by the Trainer to connect the strategy with the model.\"\"\"\r\n        # model conversions cannot be applied at this point because `LightningModule.{setup,configure_model}` haven't\r\n        # run yet\r\n        self._lightning_module = model\r\n        self.model = model", "code_tokens": ["def", "connect", "(", "self", ",", "model", ":", "\"", "pl", ".", "LightningModule", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Called", "by", "the", "Trainer", "to", "connect", "the", "strategy", "with", "the", "model", ".", "\"", "\"", "\"", "self", ".", "_lightning_module", "=", "model", "self", ".", "model", "=", "model"], "docstring": "Called by the Trainer to connect the strategy with the model.", "docstring_tokens": ["called", "by", "the", "trainer", "to", "connect", "the", "strategy", "with", "the", "model"], "docstring_summary": "Called by the Trainer to connect the strategy with the model.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 110, "end_line": 115, "hash": "410d78601efacb5fbc4fbbad73a55e76", "complexity": 1, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "setup_environment", "original_string": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This is called before the LightningModule/DataModule setup hook which allows the user to access the accelerator\r\n        environment before setup is complete.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "language": "python", "code": "def setup_environment(self) -> None:\r\n        \"\"\"Setup any processes or distributed connections.\r\n\r\n        This is called before the LightningModule/DataModule setup hook which allows the user to access the accelerator\r\n        environment before setup is complete.\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup_device(self.root_device)", "code_tokens": ["def", "setup_environment", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Setup", "any", "processes", "or", "distributed", "connections", ".", "This", "is", "called", "before", "the", "LightningModule", "/", "DataModule", "setup", "hook", "which", "allows", "the", "user", "to", "access", "the", "accelerator", "environment", "before", "setup", "is", "complete", ".", "\"", "\"", "\"", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "setup_device", "(", "self", ".", "root_device", ")"], "docstring": "Setup any processes or distributed connections.\r\n\r\n        This is called before the LightningModule/DataModule setup hook which allows the user to access the accelerator\r\n        environment before setup is complete.", "docstring_tokens": ["setup", "any", "processes", "or", "distributed", "connections", "this", "is", "called", "before", "the", "lightningmodule", "datamodule", "setup", "hook", "which", "allows", "the", "user", "to", "access", "the", "accelerator", "environment", "before", "setup", "is", "complete"], "docstring_summary": "Setup any processes or distributed connections.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 120, "end_line": 128, "hash": "345aea142905493d84dc302199ba7566", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "setup_optimizers", "original_string": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)", "language": "python", "code": "def setup_optimizers(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to\r\n\r\n        \"\"\"\r\n        assert self.lightning_module is not None\r\n        self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)", "code_tokens": ["def", "setup_optimizers", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Creates", "optimizers", "and", "schedulers", ".", "Args", ":", "trainer", ":", "the", "Trainer", ",", "these", "optimizers", "should", "be", "connected", "to", "\"", "\"", "\"", "assert", "self", ".", "lightning_module", "is", "not", "None", "self", ".", "optimizers", ",", "self", ".", "lr_scheduler_configs", "=", "_init_optimizers_and_lr_schedulers", "(", "self", ".", "lightning_module", ")"], "docstring": "Creates optimizers and schedulers.\r\n\r\n        Args:\r\n            trainer: the Trainer, these optimizers should be connected to", "docstring_tokens": ["creates", "optimizers", "and", "schedulers", "args", "trainer", "the", "trainer", "these", "optimizers", "should", "be", "connected", "to"], "docstring_summary": "Creates optimizers and schedulers.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 130, "end_line": 138, "hash": "d143d51825f21da360d195e4daebf9d6", "complexity": 1, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "setup", "original_string": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Sets up the accelerator, plugins and initializes the optimizers (if needed).\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup(trainer)\r\n\r\n        assert self.model is not None\r\n        # let the precision plugin convert the module here so that this strategy hook can decide the order\r\n        # of operations\r\n        self.model = self.precision_plugin.convert_module(self.model)\r\n        self.model_to_device()\r\n        self.model = self._setup_model(self.model)\r\n\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            self.setup_optimizers(trainer)\r\n        self.setup_precision_plugin()\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            _optimizers_to_device(self.optimizers, self.root_device)", "language": "python", "code": "def setup(self, trainer: \"pl.Trainer\") -> None:\r\n        \"\"\"Sets up the accelerator, plugins and initializes the optimizers (if needed).\r\n\r\n        Args:\r\n            trainer: the trainer instance\r\n\r\n        \"\"\"\r\n        assert self.accelerator is not None\r\n        self.accelerator.setup(trainer)\r\n\r\n        assert self.model is not None\r\n        # let the precision plugin convert the module here so that this strategy hook can decide the order\r\n        # of operations\r\n        self.model = self.precision_plugin.convert_module(self.model)\r\n        self.model_to_device()\r\n        self.model = self._setup_model(self.model)\r\n\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            self.setup_optimizers(trainer)\r\n        self.setup_precision_plugin()\r\n        if trainer.state.fn == TrainerFn.FITTING:\r\n            _optimizers_to_device(self.optimizers, self.root_device)", "code_tokens": ["def", "setup", "(", "self", ",", "trainer", ":", "\"", "pl", ".", "Trainer", "\"", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Sets", "up", "the", "accelerator", ",", "plugins", "and", "initializes", "the", "optimizers", "(", "if", "needed", ")", ".", "Args", ":", "trainer", ":", "the", "trainer", "instance", "\"", "\"", "\"", "assert", "self", ".", "accelerator", "is", "not", "None", "self", ".", "accelerator", ".", "setup", "(", "trainer", ")", "assert", "self", ".", "model", "is", "not", "None", "self", ".", "model", "=", "self", ".", "precision_plugin", ".", "convert_module", "(", "self", ".", "model", ")", "self", ".", "model_to_device", "(", ")", "self", ".", "model", "=", "self", ".", "_setup_model", "(", "self", ".", "model", ")", "if", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "self", ".", "setup_optimizers", "(", "trainer", ")", "self", ".", "setup_precision_plugin", "(", ")", "if", "trainer", ".", "state", ".", "fn", "=", "=", "TrainerFn", ".", "FITTING", ":", "_optimizers_to_device", "(", "self", ".", "optimizers", ",", "self", ".", "root_device", ")"], "docstring": "Sets up the accelerator, plugins and initializes the optimizers (if needed).\r\n\r\n        Args:\r\n            trainer: the trainer instance", "docstring_tokens": ["sets", "up", "the", "accelerator", "plugins", "and", "initializes", "the", "optimizers", "if", "needed", "args", "trainer", "the", "trainer", "instance"], "docstring_summary": "Sets up the accelerator, plugins and initializes the optimizers (if needed).", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 140, "end_line": 161, "hash": "54d3dc4f118bef51be7c363a025f166e", "complexity": 3, "parameters": ["trainer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "setup_precision_plugin", "original_string": "def setup_precision_plugin(self) -> None:\r\n        \"\"\"Attaches the precision plugin to the strategy.\"\"\"\r\n        assert self.model is not None\r\n        model, optimizers, lr_scheduler_configs = self.precision_plugin.connect(\r\n            self.model, self.optimizers, self.lr_scheduler_configs\r\n        )\r\n        self.model = model\r\n        self.optimizers = optimizers\r\n        self.lr_scheduler_configs = lr_scheduler_configs", "language": "python", "code": "def setup_precision_plugin(self) -> None:\r\n        \"\"\"Attaches the precision plugin to the strategy.\"\"\"\r\n        assert self.model is not None\r\n        model, optimizers, lr_scheduler_configs = self.precision_plugin.connect(\r\n            self.model, self.optimizers, self.lr_scheduler_configs\r\n        )\r\n        self.model = model\r\n        self.optimizers = optimizers\r\n        self.lr_scheduler_configs = lr_scheduler_configs", "code_tokens": ["def", "setup_precision_plugin", "(", "self", ")", "-", ">", "None", ":", "\"", "\"", "\"", "Attaches", "the", "precision", "plugin", "to", "the", "strategy", ".", "\"", "\"", "\"", "assert", "self", ".", "model", "is", "not", "None", "model", ",", "optimizers", ",", "lr_scheduler_configs", "=", "self", ".", "precision_plugin", ".", "connect", "(", "self", ".", "model", ",", "self", ".", "optimizers", ",", "self", ".", "lr_scheduler_configs", ")", "self", ".", "model", "=", "model", "self", ".", "optimizers", "=", "optimizers", "self", ".", "lr_scheduler_configs", "=", "lr_scheduler_configs"], "docstring": "Attaches the precision plugin to the strategy.", "docstring_tokens": ["attaches", "the", "precision", "plugin", "to", "the", "strategy"], "docstring_summary": "Attaches the precision plugin to the strategy.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 163, "end_line": 171, "hash": "8e9a3d0d11eeae867589db26acbed354", "complexity": 1, "parameters": []}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "optimizer_state", "original_string": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom strategies.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            # there are optimizers like PyTorch's ZeroRedundancyOptimizer that shard their\r\n            # states, and to avoid OOM we consolidate the full state on rank 0 only\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        # for optimizers that are not sharded, we return the state dict on all ranks\r\n        return optimizer.state_dict()", "language": "python", "code": "def optimizer_state(self, optimizer: Optimizer) -> dict[str, Tensor]:\r\n        \"\"\"Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom strategies.\r\n\r\n        \"\"\"\r\n        if isinstance(optimizer, LightningOptimizer):\r\n            optimizer = optimizer._optimizer\r\n\r\n        if hasattr(optimizer, \"consolidate_state_dict\"):\r\n            # there are optimizers like PyTorch's ZeroRedundancyOptimizer that shard their\r\n            # states, and to avoid OOM we consolidate the full state on rank 0 only\r\n            optimizer.consolidate_state_dict()\r\n            return optimizer.state_dict() if self.is_global_zero else {}\r\n\r\n        # for optimizers that are not sharded, we return the state dict on all ranks\r\n        return optimizer.state_dict()", "code_tokens": ["def", "optimizer_state", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "dict", "[", "str", ",", "Tensor", "]", ":", "\"", "\"", "\"", "Returns", "state", "of", "an", "optimizer", ".", "Allows", "for", "syncing", "/", "collating", "optimizer", "state", "from", "processes", "in", "custom", "strategies", ".", "\"", "\"", "\"", "if", "isinstance", "(", "optimizer", ",", "LightningOptimizer", ")", ":", "optimizer", "=", "optimizer", ".", "_optimizer", "if", "hasattr", "(", "optimizer", ",", "\"", "consolidate_state_dict", "\"", ")", ":", "optimizer", ".", "consolidate_state_dict", "(", ")", "return", "optimizer", ".", "state_dict", "(", ")", "if", "self", ".", "is_global_zero", "else", "{", "}", "return", "optimizer", ".", "state_dict", "(", ")"], "docstring": "Returns state of an optimizer.\r\n\r\n        Allows for syncing/collating optimizer state from processes in custom strategies.", "docstring_tokens": ["returns", "state", "of", "an", "optimizer", "allows", "for", "syncing", "collating", "optimizer", "state", "from", "processes", "in", "custom", "strategies"], "docstring_summary": "Returns state of an optimizer.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 173, "end_line": 189, "hash": "cfa39f28268e8c1f37f7117b91af0aff", "complexity": 4, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "backward", "original_string": "def backward(\r\n        self,\r\n        closure_loss: Tensor,\r\n        optimizer: Optional[Optimizer],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> Tensor:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\r\n\r\n        Args:\r\n            closure_loss: a tensor holding the loss value to backpropagate\r\n            optimizer: An optional optimizer that gets passed down to the precision plugin's backward\r\n            \\*args: Positional arguments that get passed down to the precision plugin's backward, intended as arguments\r\n                for the actual function that performs the backward, like :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        self.pre_backward(closure_loss)\r\n        assert self.lightning_module is not None\r\n        closure_loss = self.precision_plugin.pre_backward(closure_loss, self.lightning_module)\r\n\r\n        self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\r\n\r\n        closure_loss = self.precision_plugin.post_backward(closure_loss, self.lightning_module)\r\n        self.post_backward(closure_loss)\r\n\r\n        return closure_loss", "language": "python", "code": "def backward(\r\n        self,\r\n        closure_loss: Tensor,\r\n        optimizer: Optional[Optimizer],\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> Tensor:\r\n        r\"\"\"Forwards backward-calls to the precision plugin.\r\n\r\n        Args:\r\n            closure_loss: a tensor holding the loss value to backpropagate\r\n            optimizer: An optional optimizer that gets passed down to the precision plugin's backward\r\n            \\*args: Positional arguments that get passed down to the precision plugin's backward, intended as arguments\r\n                for the actual function that performs the backward, like :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"\r\n        self.pre_backward(closure_loss)\r\n        assert self.lightning_module is not None\r\n        closure_loss = self.precision_plugin.pre_backward(closure_loss, self.lightning_module)\r\n\r\n        self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\r\n\r\n        closure_loss = self.precision_plugin.post_backward(closure_loss, self.lightning_module)\r\n        self.post_backward(closure_loss)\r\n\r\n        return closure_loss", "code_tokens": ["def", "backward", "(", "self", ",", "closure_loss", ":", "Tensor", ",", "optimizer", ":", "Optional", "[", "Optimizer", "]", ",", "*", "args", ":", "Any", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Tensor", ":", "r", "\"", "\"", "\"", "Forwards", "backward", "-", "calls", "to", "the", "precision", "plugin", ".", "Args", ":", "closure_loss", ":", "a", "tensor", "holding", "the", "loss", "value", "to", "backpropagate", "optimizer", ":", "An", "optional", "optimizer", "that", "gets", "passed", "down", "to", "the", "precision", "plugin", "'", "s", "backward", "\\", "*", "args", ":", "Positional", "arguments", "that", "get", "passed", "down", "to", "the", "precision", "plugin", "'", "s", "backward", ",", "intended", "as", "arguments", "for", "the", "actual", "function", "that", "performs", "the", "backward", ",", "like", ":", "meth", ":", "`", "~", "torch", ".", "Tensor", ".", "backward", "`", ".", "\\", "*", "*", "kwargs", ":", "Keyword", "arguments", "for", "the", "same", "purpose", "as", "`", "`", "*", "args", "`", "`", ".", "\"", "\"", "\"", "self", ".", "pre_backward", "(", "closure_loss", ")", "assert", "self", ".", "lightning_module", "is", "not", "None", "closure_loss", "=", "self", ".", "precision_plugin", ".", "pre_backward", "(", "closure_loss", ",", "self", ".", "lightning_module", ")", "self", ".", "precision_plugin", ".", "backward", "(", "closure_loss", ",", "self", ".", "lightning_module", ",", "optimizer", ",", "*", "args", ",", "*", "*", "kwargs", ")", "closure_loss", "=", "self", ".", "precision_plugin", ".", "post_backward", "(", "closure_loss", ",", "self", ".", "lightning_module", ")", "self", ".", "post_backward", "(", "closure_loss", ")", "return", "closure_loss"], "docstring": "r\"\"\"Forwards backward-calls to the precision plugin.\r\n\r\n        Args:\r\n            closure_loss: a tensor holding the loss value to backpropagate\r\n            optimizer: An optional optimizer that gets passed down to the precision plugin's backward\r\n            \\*args: Positional arguments that get passed down to the precision plugin's backward, intended as arguments\r\n                for the actual function that performs the backward, like :meth:`~torch.Tensor.backward`.\r\n            \\**kwargs: Keyword arguments for the same purpose as ``*args``.\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "forwards", "backward", "calls", "to", "the", "precision", "plugin", "args", "closure_loss", "a", "tensor", "holding", "the", "loss", "value", "to", "backpropagate", "optimizer", "an", "optional", "optimizer", "that", "gets", "passed", "down", "to", "the", "precision", "plugin", "s", "backward", "args", "positional", "arguments", "that", "get", "passed", "down", "to", "the", "precision", "plugin", "s", "backward", "intended", "as", "arguments", "for", "the", "actual", "function", "that", "performs", "the", "backward", "like", "meth", "torch", "tensor", "backward", "kwargs", "keyword", "arguments", "for", "the", "same", "purpose", "as", "args"], "docstring_summary": "r\"\"\"Forwards backward-calls to the precision plugin.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 191, "end_line": 217, "hash": "700b3c3c7c33752110c33f8b36ca155b", "complexity": 1, "parameters": ["closure_loss", "optimizer", "*args", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "optimizer_step", "original_string": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        r\"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            \\**kwargs: Keyword arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        model = model or self.lightning_module\r\n        # TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\r\n        assert isinstance(model, pl.LightningModule)\r\n        return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)", "language": "python", "code": "def optimizer_step(\r\n        self,\r\n        optimizer: Optimizer,\r\n        closure: Callable[[], Any],\r\n        model: Optional[Union[\"pl.LightningModule\", Module]] = None,\r\n        **kwargs: Any,\r\n    ) -> Any:\r\n        r\"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            \\**kwargs: Keyword arguments to ``optimizer.step``\r\n\r\n        \"\"\"\r\n        model = model or self.lightning_module\r\n        # TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\r\n        assert isinstance(model, pl.LightningModule)\r\n        return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)", "code_tokens": ["def", "optimizer_step", "(", "self", ",", "optimizer", ":", "Optimizer", ",", "closure", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "model", ":", "Optional", "[", "Union", "[", "\"", "pl", ".", "LightningModule", "\"", ",", "Module", "]", "]", "=", "None", ",", "*", "*", "kwargs", ":", "Any", ",", ")", "-", ">", "Any", ":", "r", "\"", "\"", "\"", "Performs", "the", "actual", "optimizer", "step", ".", "Args", ":", "optimizer", ":", "the", "optimizer", "performing", "the", "step", "closure", ":", "closure", "calculating", "the", "loss", "value", "model", ":", "reference", "to", "the", "model", ",", "optionally", "defining", "optimizer", "step", "related", "hooks", "\\", "*", "*", "kwargs", ":", "Keyword", "arguments", "to", "`", "`", "optimizer", ".", "step", "`", "`", "\"", "\"", "\"", "model", "=", "model", "or", "self", ".", "lightning_module", "assert", "isinstance", "(", "model", ",", "pl", ".", "LightningModule", ")", "return", "self", ".", "precision_plugin", ".", "optimizer_step", "(", "optimizer", ",", "model", "=", "model", ",", "closure", "=", "closure", ",", "*", "*", "kwargs", ")"], "docstring": "r\"\"\"Performs the actual optimizer step.\r\n\r\n        Args:\r\n            optimizer: the optimizer performing the step\r\n            closure: closure calculating the loss value\r\n            model: reference to the model, optionally defining optimizer step related hooks\r\n            \\**kwargs: Keyword arguments to ``optimizer.step``\r\n\r\n        \"\"\"", "docstring_tokens": ["r", "performs", "the", "actual", "optimizer", "step", "args", "optimizer", "the", "optimizer", "performing", "the", "step", "closure", "closure", "calculating", "the", "loss", "value", "model", "reference", "to", "the", "model", "optionally", "defining", "optimizer", "step", "related", "hooks", "kwargs", "keyword", "arguments", "to", "optimizer", "step"], "docstring_summary": "r\"\"\"Performs the actual optimizer step.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 219, "end_line": 238, "hash": "aa1ac506be60fa6081bcb705cf1eb349", "complexity": 2, "parameters": ["optimizer", "closure", "Any]", "model", "Module]]", "**kwargs"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "_setup_model_and_optimizers", "original_string": "def _setup_model_and_optimizers(self, model: Module, optimizers: list[Optimizer]) -> tuple[Module, list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`_setup_model` and :meth:`_setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        model = self._setup_model(model)\r\n        optimizers = [self._setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return model, optimizers", "language": "python", "code": "def _setup_model_and_optimizers(self, model: Module, optimizers: list[Optimizer]) -> tuple[Module, list[Optimizer]]:\r\n        \"\"\"Setup a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`_setup_model` and :meth:`_setup_optimizer` on the inputs.\r\n\r\n        \"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        model = self._setup_model(model)\r\n        optimizers = [self._setup_optimizer(optimizer) for optimizer in optimizers]\r\n        return model, optimizers", "code_tokens": ["def", "_setup_model_and_optimizers", "(", "self", ",", "model", ":", "Module", ",", "optimizers", ":", "list", "[", "Optimizer", "]", ")", "-", ">", "tuple", "[", "Module", ",", "list", "[", "Optimizer", "]", "]", ":", "\"", "\"", "\"", "Setup", "a", "model", "and", "multiple", "optimizers", "together", ".", "The", "returned", "objects", "are", "expected", "to", "be", "in", "the", "same", "order", "they", "were", "passed", "in", ".", "The", "default", "implementation", "will", "call", ":", "meth", ":", "`", "_setup_model", "`", "and", ":", "meth", ":", "`", "_setup_optimizer", "`", "on", "the", "inputs", ".", "\"", "\"", "\"", "model", "=", "self", ".", "_setup_model", "(", "model", ")", "optimizers", "=", "[", "self", ".", "_setup_optimizer", "(", "optimizer", ")", "for", "optimizer", "in", "optimizers", "]", "return", "model", ",", "optimizers"], "docstring": "Setup a model and multiple optimizers together.\r\n\r\n        The returned objects are expected to be in the same order they were passed in. The default implementation will\r\n        call :meth:`_setup_model` and :meth:`_setup_optimizer` on the inputs.", "docstring_tokens": ["setup", "a", "model", "and", "multiple", "optimizers", "together", "the", "returned", "objects", "are", "expected", "to", "be", "in", "the", "same", "order", "they", "were", "passed", "in", "the", "default", "implementation", "will", "call", "meth", "_setup_model", "and", "meth", "_setup_optimizer", "on", "the", "inputs"], "docstring_summary": "Setup a model and multiple optimizers together.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 240, "end_line": 250, "hash": "5de83dadac49b90fbd5bd2b910d2d216", "complexity": 2, "parameters": ["model", "optimizers"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "_setup_model", "original_string": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Performs setup for the model, e.g., by wrapping it by another class.\"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        return model", "language": "python", "code": "def _setup_model(self, model: Module) -> Module:\r\n        \"\"\"Performs setup for the model, e.g., by wrapping it by another class.\"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        return model", "code_tokens": ["def", "_setup_model", "(", "self", ",", "model", ":", "Module", ")", "-", ">", "Module", ":", "\"", "\"", "\"", "Performs", "setup", "for", "the", "model", ",", "e", ".", "g", ".", ",", "by", "wrapping", "it", "by", "another", "class", ".", "\"", "\"", "\"", "return", "model"], "docstring": "Performs setup for the model, e.g., by wrapping it by another class.", "docstring_tokens": ["performs", "setup", "for", "the", "model", "e", "g", "by", "wrapping", "it", "by", "another", "class"], "docstring_summary": "Performs setup for the model, e.g., by wrapping it by another class.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 252, "end_line": 255, "hash": "99a72d4e96a79c1d8b2a438ccfaa1fa5", "complexity": 1, "parameters": ["model"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "_setup_optimizer", "original_string": "def _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        return optimizer", "language": "python", "code": "def _setup_optimizer(self, optimizer: Optimizer) -> Optimizer:\r\n        \"\"\"Performs setup for the optimizer, e.g., by wrapping it by another class.\"\"\"\r\n        # TODO: standardize this across all plugins in Lightning and Fabric. Related refactor: #7324\r\n        return optimizer", "code_tokens": ["def", "_setup_optimizer", "(", "self", ",", "optimizer", ":", "Optimizer", ")", "-", ">", "Optimizer", ":", "\"", "\"", "\"", "Performs", "setup", "for", "the", "optimizer", ",", "e", ".", "g", ".", ",", "by", "wrapping", "it", "by", "another", "class", ".", "\"", "\"", "\"", "return", "optimizer"], "docstring": "Performs setup for the optimizer, e.g., by wrapping it by another class.", "docstring_tokens": ["performs", "setup", "for", "the", "optimizer", "e", "g", "by", "wrapping", "it", "by", "another", "class"], "docstring_summary": "Performs setup for the optimizer, e.g., by wrapping it by another class.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 257, "end_line": 260, "hash": "7de127835393ee82445431fff26a3b9f", "complexity": 1, "parameters": ["optimizer"]}
{"repo": "pytorch-lightning", "path": "src\\lightning\\pytorch\\strategies\\strategy.py", "func_name": "batch_to_device", "original_string": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None, dataloader_idx: int = 0) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        \"\"\"\r\n        model = self.lightning_module\r\n        device = device or self.root_device\r\n        if model is not None:\r\n            return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\r\n        return move_data_to_device(batch, device)", "language": "python", "code": "def batch_to_device(self, batch: Any, device: Optional[torch.device] = None, dataloader_idx: int = 0) -> Any:\r\n        \"\"\"Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.\r\n\r\n        \"\"\"\r\n        model = self.lightning_module\r\n        device = device or self.root_device\r\n        if model is not None:\r\n            return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\r\n        return move_data_to_device(batch, device)", "code_tokens": ["def", "batch_to_device", "(", "self", ",", "batch", ":", "Any", ",", "device", ":", "Optional", "[", "torch", ".", "device", "]", "=", "None", ",", "dataloader_idx", ":", "int", "=", "0", ")", "-", ">", "Any", ":", "\"", "\"", "\"", "Moves", "the", "batch", "to", "the", "correct", "device", ".", "The", "returned", "batch", "is", "of", "the", "same", "type", "as", "the", "input", "batch", ",", "just", "having", "all", "tensors", "on", "the", "correct", "device", ".", "Args", ":", "batch", ":", "The", "batch", "of", "samples", "to", "move", "to", "the", "correct", "device", "device", ":", "The", "target", "device", "dataloader_idx", ":", "The", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs", ".", "\"", "\"", "\"", "model", "=", "self", ".", "lightning_module", "device", "=", "device", "or", "self", ".", "root_device", "if", "model", "is", "not", "None", ":", "return", "model", ".", "_apply_batch_transfer_handler", "(", "batch", ",", "device", "=", "device", ",", "dataloader_idx", "=", "dataloader_idx", ")", "return", "move_data_to_device", "(", "batch", ",", "device", ")"], "docstring": "Moves the batch to the correct device.\r\n\r\n        The returned batch is of the same type as the input batch, just\r\n        having all tensors on the correct device.\r\n\r\n        Args:\r\n            batch: The batch of samples to move to the correct device\r\n            device: The target device\r\n            dataloader_idx: The index of the dataloader to which the batch belongs.", "docstring_tokens": ["moves", "the", "batch", "to", "the", "correct", "device", "the", "returned", "batch", "is", "of", "the", "same", "type", "as", "the", "input", "batch", "just", "having", "all", "tensors", "on", "the", "correct", "device", "args", "batch", "the", "batch", "of", "samples", "to", "move", "to", "the", "correct", "device", "device", "the", "target", "device", "dataloader_idx", "the", "index", "of", "the", "dataloader", "to", "which", "the", "batch", "belongs"], "docstring_summary": "Moves the batch to the correct device.", "url": "https://github.com/Lightning-AI/pytorch-lightning/blob/master/src\\lightning\\pytorch\\strategies\\strategy.py", "partition": "train", "function_type": "class_method", "class_name": "Strategy", "start_line": 262, "end_line": 278, "hash": "06fdf6cb9f54722ff61a70149a6916b9", "complexity": 3, "parameters": ["batch", "device", "dataloader_idx"]}
