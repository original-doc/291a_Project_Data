[
  {
    "title": "DDP training and storing rank specific info in checkpoints",
    "body": "I'm working on preserving state between start/stop of training runs in a manner that guarantees reproducible results. That is, I'd like to be able to stop my training at any given checkpoint, then restart the training from that checkpoint and finish to completion, and have these results match (exactly) the results obtained from a single continuous run. I've been able to do this on single node setups by storing the outputs of \r\n\r\n- `torch.get_rng_state()` \r\n- `torch.cuda.get_rng_state()`\r\n- `np.random.get_state()`\r\n- `random.getstate()`\r\n\r\nwithin the model checkpoint, and using the corresponding `set` method upon loading the checkpoint. I've been performing the save/load routines within a custom `pytorch_lightning.callbacks.Callback` by overriding the `on_save_checkpoint` and `on_load_checkpoint` appropriately. \r\n\r\nI'm now trying to perform the same checkpoint save/load procedure using a multi-node setup, with a DDP strategy. My attempt was to append the global-rank-specific rng states to the checkpoint dictionary, which I had thought would then be saved appropriately. However, when I executed the code, the only rng state that is preserved within the checkpoint dictionary, is the rank 0 state. Can someone please advise on how to preserve the rng states from other ranks within the checkpoint in a DDP setup? As a higher level question: if there is a better way to preserve these states between training runs rather than checkpoint storage and re-instantiation, that information would also be welcome.\r\n\r\nThe main Callback save routine I'm using is posted below. I've then been checking the contents of the saved checkpoint dictionary by using a manual `torch.load()` call.\r\n\r\npython version: 3.9.12\r\npytorch version: 2.2.0+cu121\r\npytorch_lightning version: 2.2.0\r\n\r\n```\r\nclass CustomCallback(ptl.callbacks.Callback):\r\n    def on_save_checkpoint(self, trainer, module, checkpoint):\r\n        # get random states\r\n        state = {\r\n            'torch': torch.get_rng_state().cpu(),\r\n            'cuda': torch.cuda.get_rng_state().cpu() if torch.cuda.is_available() else None,\r\n            'numpy': np.random.get_state(),\r\n            'random': random.getstate(),\r\n        }\r\n        rank = trainer.global_rank\r\n        checkpoint[f'state_{rank}'] = state.  # note: this key never appears in the saved checkpoint except for rank 0\r\n\r\n        # note: this code *does* execute, I do see the saved data for each rank, \r\n        # but I'd rather store it cleanly in the checkpoint file\r\n        torch.save(state, f'rng_state_{rank}.pt')\r\n\r\n    def on_load_checkpoint(self, trainer, module, checkpoint):\r\n        # pass for now, easy enough to update if I get the on_save_ method working appropriately\r\n        pass\r\n```\r\n      ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/21097",
    "createdAt": "2025-08-19T16:56:02Z",
    "updatedAt": "2025-08-26T16:11:20Z",
    "closedAt": "2025-08-26T16:11:13Z",
    "isAnswered": true,
    "author": {
      "login": "bardsleypt"
    },
    "answer": {
      "body": "After extensive doc-searching and even more extensive trial-and-error, I believe I have a good understanding of this issue.  Unfortunately, unless there is a flag within `pytorch`, `ddp`, `lightning`, or `CUDA` governing GPU scheduling determinism that I don't know about, my main problem of forcing _exact_ reproducibility seems impossible (or at least largely impractical) for reasons I'll summarize below. I am moving on from this problem, by simply avoiding the model stop/restart process I mentioned above via other methods. But I'm posting the information I have discovered in case it helps anyone with a similar problem.\r\n\r\nFor starters, my second comment is more-or-less correct. Each rank (i.e., each process) gets its own instantiation of the entire training run (thus all object instantiations including trainers, dataloaders, callbacks, etc.), but under the hood `ddp` + `lightning` is only letting a given process see a subset of the training data. During a single batch on a given rank, the data is put through the forward pass of the lightning module, after which the backward pass is called. The backward pass computes gradients locally (on the given rank/process) but crucially as soon as the backward pass completes, an `all-reduce` routine is called deep within the `ddp` + `lightning` source code to aggregate and synchronize all gradients across all ranks. This way, when it is time to make the optimization step, all processes have the same gradient values (i.e., the mean of the gradients over all ranks).\r\n\r\nI was able to determine the gradients are the first location I encountered a discrepancy between a continuous training run and a run that involved a stop-checkpoint-restart at a given epoch (epoch 3 in my case). At this breakpoint, the two training runs had different gradients going into the next optimization step (epoch 3 update step), where the discrepancy was O(10^-11). After a lot of additional model hacking to debug the local gradients across each rank, I determined this discrepancy came purely from the order in which the gradients are synchronized (i.e., floating point arithmetic does not obey commutativity of addition perfectly). For example, on the continuous training run with 4 ranks (GPUs), the gradients were aggregated in an order [0, 1, 2, 3], while on the restarted training run the gradients were aggregated in an order [1, 2, 3, 0] (or possibly a similar ordering, but certainly not the same order). I verified this by manually combining all of the local gradients from the restarted training run in different orders until I was able to reproduce the aggregated gradient from both the continuous training run and the restarted training run. That is, I could get different aggregated gradient values purely from the order in which I performed the addition/averaging of the local gradients, one corresponding to the restarted run and another corresponding to the continuous run. This indicated to me that the order in which the GPUs are aggregating their gradients is not the same between my two training runs, though it is still deterministic (i.e., the restarted training run always gave the same discrepancy from the continuous training run). \r\n\r\nShort of controlling the exact scheduling of GPU processes, and/or rewriting my model code to perform the aggregation/optimization steps manually, it seems _exact_ reproducibility between these two runs is not possible. If anyone does stumble across this and has more information and better ideas on this, I'd be happy to learn more here.\r\n\r\n## Debugging local gradients\r\nHere is little more information for anyone encountering similar problems. These are the steps I had to take within my `lightning` module and training code to debug and output the local (i.e., per process or per rank) gradients. I didn't find this process all that intuitive nor well-documented, so hopefully this helps someone.\r\n\r\nThe synchronization of the gradients happens almost immediately in the training process, and is handled using some form of  hook/observer/subscriber pattern. I'm not sure the specifics here, but it is definitely opaque to the high-level pytorch-lightning user. This means by the time one is at a callback such as `on_before_optimizer_step` or even `on_after_backward`, the gradients have already been synchronized and the local gradients are lost. To access the local gradients then, one needs to compute them in a manual fashion without the mentioned synchornization hook in place, store them, and then output them whenever convenient. Here is my approach:\r\n\r\n```\r\nclass MyModel(ptl.LightningModule):\r\n    def __init__(self, *args, **kwargs):\r\n        ...\r\n        self.automatic_optimization = False     # this allows calls to manual_backwards() but changes the train_step functionality\r\n        self.local_grads = [] # storage of local gradients\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ...\r\n       loss = self.forward(*args, **kwargs)\r\n        \r\n        with self.trainer.strategy.model.no_sync():  # prevents immediate aggregation of gradients \r\n            self.manual_backward(loss, retain_graph=True).   # debug backward pass to populate local gradients\r\n            self.local_grads = [p.grad.clone().detach() if p.grad is not None else None for p in self.parameters()]\r\n\r\n        opt = self.optimizers()\r\n        opt.zero_grad()  # clear any/all gradients from non-synced step above\r\n        self.manual_backward(loss)  # actual backward pass used for gradients in optimization step\r\n        opt.zero_grad()\r\n        opt.step()\r\n        \r\n        return loss\r\n```\r\nOnce the local gradients are cloned, detached, and stored inside of `local_grads`, they can be accessed from any of the various callbacks (either lightning-module callback hooks or lighting-callback hooks). Since the object is instantiated on each rank/process, each rank/process will store its own copy of the local gradients. In this manner, I was able to pull the local gradients together (after saving them to disk) for each rank, and reassemble them in different orders to achieve slightly varying aggregated gradient values.\r\n",
      "author": {
        "login": "bardsleypt"
      },
      "createdAt": "2025-08-26T16:11:13Z"
    }
  },
  {
    "title": "Avoid deepspeed plugin converting the whole model",
    "body": "I am using deepspeed plugin in lightning to train my model. I want the first part of my model to be float32, while the second part to be bfloat16 (the optimizer only trains the first part). However, I found lightning will convert the whole model to float32 if I do not specify the precision. How to keep my pre-defined model dtype untouched?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20543",
    "createdAt": "2025-01-11T01:26:45Z",
    "updatedAt": "2025-01-11T07:13:36Z",
    "closedAt": "2025-01-11T07:13:26Z",
    "isAnswered": true,
    "author": {
      "login": "Boltzmachine"
    },
    "answer": {
      "body": "I found you should implement the dtype conversion in your `LightningModule`, and avoid `DeepSpeedStrategy` from converting your module.\r\n\r\n```python\r\nfrom lightning.pytorch.plugins import DeepSpeedPrecision\r\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\r\nfrom typing_extensions import override\r\n\r\nclass DeepSpeedPrecisionWithoutModuleConversion(DeepSpeedPrecision):\r\n    @override\r\n    def convert_module(self, module):\r\n        return module\r\n```\r\n\r\nand pass to the trainer as\r\n```python\r\ntrainer = Trainer(\r\n    ...,\r\n    DeepSpeedStrategy(stage=2, precision_plugin=DeepSpeedPrecisionWithoutModuleConversion('32-true'))\r\n)\r\n```",
      "author": {
        "login": "Boltzmachine"
      },
      "createdAt": "2025-01-11T07:13:26Z"
    }
  },
  {
    "title": "Alternate \"prediction\" loops",
    "body": "I am hoping to implement an additional loop through the Trainer in order to leverage Lightnings auto*magic* handling of dataloaders and GPUs. Specifically, I want to run batches through the attribution methods from [Captum](https://captum.ai/). My first attempt was to hijack the `predition_step` of the LightningModule with a simple class attribute `self.calculate_attributes` switch:\r\n\r\n```python3\r\n    def predict_step(self, batch, batch_idx):\r\n        if self.calculate_attributes:\r\n            return self.attribution_step(batch, batch_idx)\r\n        else:\r\n            data, target = batch\r\n            return self.model(data)\r\n    \r\n    def attribution_step(self, batch, batch_idx):\r\n        data, target = batch\r\n        batch_size = data.shape[0]\r\n        baselines = torch.zeros_like(data)\r\n        attribution = self.explainer.attribute(data, baselines, target=target, internal_batch_size=batch_size)\r\n        return attribution, target\r\n```\r\n\r\nBut this has run into issues because gradients are required, and, I believe, the prediction loop disables them. I tried to get around with the `@torch.enable_grad()` decorator and explicit `.requires_grad=True` calls, but that is not working. For every attempt I get the error `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`. I have no issue running the explainers on their own on the same data. I've been looking at the depreciated [Loops interface](https://lightning.ai/docs/pytorch/LTS/extensions/loops.html) but haven't made any progress there either.\r\n\r\nIs there a proper way to implement this? Any suggestions would be appreciated.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20318",
    "createdAt": "2024-10-04T01:34:39Z",
    "updatedAt": "2024-10-04T03:58:51Z",
    "closedAt": "2024-10-04T03:58:47Z",
    "isAnswered": true,
    "author": {
      "login": "kaboroevich"
    },
    "answer": {
      "body": "Setting `Trainer(inference_mode=False)` was the answer\r\n\r\n#15925 #15765",
      "author": {
        "login": "kaboroevich"
      },
      "createdAt": "2024-10-04T03:58:47Z"
    }
  },
  {
    "title": "Saving conformer checkpoint and resuming train progress from checkpoint",
    "body": "Confused about saving checkpoint of encoder-decoder models in lightning AI. Is the saving checkpoint technique same for all models?\r\n```py\r\n  checkpoint_callback = ModelCheckpoint(\r\n      monitor='val_loss',\r\n      dirpath=\"./saved_checkpoint/\",\r\n      filename='model-{epoch:02d}-{val_wer:.2f}',\r\n      save_top_k=3,        # 3 Checkpoints\r\n      mode='min'\r\n  )\r\n```\r\n\r\nBut how to load the checkpoint again to resume the training? Is the same as normal architecture models like giving `ckpt_path` in `trainer()` \r\n```py\r\n\r\n  ckpt_path = args.checkpoint_path if args.checkpoint_path else None\r\n  trainer.fit(speech_trainer, data_module, ckpt_path=ckpt_path)   \r\n  trainer.validate(speech_trainer, data_module)\r\n```",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/20286",
    "createdAt": "2024-09-18T03:29:55Z",
    "updatedAt": "2024-09-19T15:48:02Z",
    "closedAt": "2024-09-19T15:47:58Z",
    "isAnswered": true,
    "author": {
      "login": "LuluW8071"
    },
    "answer": {
      "body": "Found the answer the code above is correct ",
      "author": {
        "login": "LuluW8071"
      },
      "createdAt": "2024-09-19T15:47:58Z"
    }
  },
  {
    "title": "confusions about load_from_checkpoint() and save_hyperparameters()",
    "body": "according to \r\n\r\n> https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html,\r\n\r\nThere is a model like this:\r\n\r\n```\r\nclass Encoder(L.LightningModule):\r\n    ...\r\n\r\nclass Decoder(L.LightningModule):\r\n    ...\r\n\r\nclass Autoencoder(L.LightningModule):\r\n    def __init__(self, encoder, decoder, *args, **kwargs):\r\n        self.save_hyperparameters(ignore=['encoder', 'decoder'])\r\n        self.encoder=encoder\r\n        self.encoder.freeze()\r\n        self.decoder=decoder\r\n        ...\r\n\r\n# training code\r\nencoder = Encoder.load_from_checkpoint(\"encoder.ckpt\")\r\ndecoder = Decoder(some hyperparameters)\r\nautoencoder = Autoencoder(encoder, decoder)\r\ntrainer.fit(autoencoder, datamodule)\r\n\r\n```\r\nWe assume that the autoencoder has been stored in the `autoencoder.ckpt` file. There are three key points I am curious about:\r\n\r\n1. Does the `autoencoder.ckpt` file include both the `encoder` and `decoder` weights?\r\n2. If `autoencoder.ckpt` contains the `encoder` weights, how can I import the weights from `encoder.ckpt` into the `autoencoder` without them being overwritten?\r\n3. If `autoencoder.ckpt` does not include the `decoder` weights, what is the procedure to save the `decoder` weights separately?\r\n",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19312",
    "createdAt": "2024-01-18T16:40:55Z",
    "updatedAt": "2024-01-28T04:19:39Z",
    "closedAt": "2024-01-28T03:50:19Z",
    "isAnswered": true,
    "author": {
      "login": "KasuganoLove"
    },
    "answer": {
      "body": "I got my answer from\r\n\r\n> https://lightning.ai/forums/t/confusions-about-load-from-checkpoint-and-save-hyperparameters/4881/2",
      "author": {
        "login": "KasuganoLove"
      },
      "createdAt": "2024-01-28T03:50:19Z"
    }
  },
  {
    "title": "No Progress bar showing in Slurm output file",
    "body": "The title explains exactly the problem I have. \r\nI would like to see the TQDM progress bar updates in the slurm output log. \r\nIt works on my computer and I get the progress bar in my terminal. \r\n\r\nHowever, when I run a sbatch script of the model I do not see the progress bar updates in the slurm output file or somewhere else in the logs directory.  I tried both the tqdm and rich progress bar but with both I do not see any output.\r\n\r\nIs this the intended behaviour for running in a slurm system? \r\nIf so it there a way to get tqdm to output to the slurm output file or maybe a way to redirect it to another file.\r\n\r\nI made sure I am using the latest Ligthning and tqdm version.  I also search everywhere on the lightning docs and github for a similar question but i could not find any. \r\n\r\nAny help or solutions are appreciated. \r\nThank you.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/19227",
    "createdAt": "2024-01-01T18:24:18Z",
    "updatedAt": "2024-07-24T04:08:37Z",
    "closedAt": "2024-01-02T11:42:18Z",
    "isAnswered": true,
    "author": {
      "login": "Cing2"
    },
    "answer": {
      "body": "Okay, I found my problem. It was a mistake from my part, I am sorry for the issue.\r\nI was using the pytorch ligthning + hydra template to construct my code and I used hydra to change the progress bar which was default set to rich to tqdm, however, I did this wrong and thus I was still using the rich progress bar on the server. This one does not provide intermediate step updates. \r\n",
      "author": {
        "login": "Cing2"
      },
      "createdAt": "2024-01-02T11:42:18Z"
    }
  },
  {
    "title": "Multiple Sequential trainings slows down speed",
    "body": "Hi.\r\nI have a task where I need to run a training script multiple time with a for-loop like this:\r\n```\r\nfor dataset in datasets:\r\n      ... Training script with datamodule, model, Trainer Init and trainer.fit()\r\n```\r\nHowever, after each loop the the training it self slows down incrementally (epoch / sec). I am thinking it is because I need to reset something but I have not been able to find that information. I am using Torch-cpu 2.0.0\r\n\r\nAny idea on what I am doing wrong? :(",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17490",
    "createdAt": "2023-04-27T07:43:47Z",
    "updatedAt": "2023-04-27T09:05:42Z",
    "closedAt": "2023-04-27T09:05:36Z",
    "isAnswered": true,
    "author": {
      "login": "HadiSDev"
    },
    "answer": {
      "body": "I tried to remove the neptune logger and model saving functionality and it seemed to have solved the issue",
      "author": {
        "login": "HadiSDev"
      },
      "createdAt": "2023-04-27T09:05:36Z"
    }
  },
  {
    "title": "Possible to change starting epoch?",
    "body": "Hey everyone!\r\n\r\nSo a project I've been helping out a bit on, https://github.com/34j/so-vits-svc-fork , is manually loading model checkpoints upon training start / initialization.\r\n\r\nCurrently, it uses a bit of a hacky way to set the current epoch to continue from:\r\nhttps://github.com/34j/so-vits-svc-fork/blob/main/src/so_vits_svc_fork/train.py#L170\r\n```py\r\ndef on_train_start(self) -> None:\r\n    if not self.tuning:\r\n        self.set_current_epoch(self._temp_epoch) # This is being loaded from the model\r\n        total_batch_idx = self.current_epoch * len(self.trainer.train_dataloader)\r\n        self.set_total_batch_idx(total_batch_idx)\r\n        global_step = total_batch_idx * self.optimizers_count\r\n        self.set_global_step(global_step)\r\n\r\ndef set_current_epoch(self, epoch: int):\r\n    LOG.info(f\"Setting current epoch to {epoch}\")\r\n    self.trainer.fit_loop.epoch_progress.current.completed = epoch\r\n    assert self.current_epoch == epoch, f\"{self.current_epoch} != {epoch}\"\r\n\r\ndef set_global_step(self, global_step: int):\r\n    LOG.info(f\"Setting global step to {global_step}\")\r\n    self.trainer.fit_loop.epoch_loop.manual_optimization.optim_step_progress.total.completed = (\r\n        global_step\r\n    )\r\n    self.trainer.fit_loop.epoch_loop.automatic_optimization.optim_progress.optimizer.step.total.completed = (\r\n        global_step\r\n    )\r\n    assert self.global_step == global_step, f\"{self.global_step} != {global_step}\"\r\n\r\ndef set_total_batch_idx(self, total_batch_idx: int):\r\n    LOG.info(f\"Setting total batch idx to {total_batch_idx}\")\r\n    self.trainer.fit_loop.epoch_loop.batch_progress.total.ready = (\r\n        total_batch_idx + 1\r\n    )\r\n    self.trainer.fit_loop.epoch_loop.batch_progress.total.completed = (\r\n        total_batch_idx\r\n    )\r\n    assert (\r\n        self.total_batch_idx == total_batch_idx + 1\r\n    ), f\"{self.total_batch_idx} != {total_batch_idx + 1}\"\r\n\r\n@property\r\ndef total_batch_idx(self) -> int:\r\n    return self.trainer.fit_loop.epoch_loop.total_batch_idx + 1\r\n```\r\n\r\nHowever, this breaks support for `max_epochs`, as I feel it's not overriding all necessary variables internally.\r\n\r\nIs there a way to properly override the current epoch it \"starts\" (or in this case, continues) training from?",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17396",
    "createdAt": "2023-04-16T17:03:22Z",
    "updatedAt": "2023-06-27T10:59:09Z",
    "closedAt": "2023-04-19T11:39:53Z",
    "isAnswered": true,
    "author": {
      "login": "Lordmau5"
    },
    "answer": {
      "body": "After looking at the Lightning code, I managed to fix it by also setting `self.trainer.fit_loop.epoch_progress.current.processed` to a specific epoch \ud83d\ude01 ",
      "author": {
        "login": "Lordmau5"
      },
      "createdAt": "2023-04-19T11:39:53Z"
    }
  },
  {
    "title": "on_training_epoch_end is never called",
    "body": "I'm training a CNN and I wanted to log some metrics at the end of each training epoch. However, I've noticed that `on_training_epoch_end` is never called while `on_validation_epoch_end` works just fine Here's an excerpt of the model containing those two:\r\n```\r\ndef training_step(self, batch):\r\n          images, labels = batch \r\n          pred = self(images)\r\n          train_loss = F.cross_entropy(pred, labels)\r\n          correct=pred.argmax(dim=1).eq(labels).sum().item()\r\n          total=len(labels)\r\n          batch_dictionary={\r\n                \"loss\": train_loss,\r\n                \"correct\": correct,\r\n                \"total\": total\r\n          }\r\n          self.training_step_outputs.append(batch_dictionary)\r\n          return batch_dictionary\r\n          \r\n\r\n      def validation_step(self, batch, batch_idx):\r\n          images, labels = batch \r\n          pred = self(images)\r\n          val_loss = F.cross_entropy(pred, labels)\r\n          correct=pred.argmax(dim=1).eq(labels).sum().item()\r\n          total=len(labels)\r\n          val_acc = correct/total\r\n          batch_dictionary={\r\n                \"loss\": val_loss,\r\n                \"acc\": val_acc,\r\n                \"correct\": correct,\r\n                \"total\": total\r\n          }\r\n          self.validation_step_outputs.append(batch_dictionary)\r\n          return batch_dictionary\r\n          \r\n      def on_training_epoch_end(self):\r\n          print('training epoch')\r\n          outputs = self.training_step_outputs;\r\n          batch_losses = [x['loss'] for x in outputs]\r\n          epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\r\n          epoch_acc = sum([x['acc'] for x in outputs])/len(outputs)\r\n          print(\"Training accuracy : \", epoch_acc)\r\n          print(\"Training loss : \", epoch_loss)\r\n          self.training_step_outputs.clear()  # free memory\r\n          \r\n      def on_validation_epoch_end(self):\r\n          outputs = self.validation_step_outputs;\r\n          batch_losses = [x['loss'] for x in outputs]\r\n          epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\r\n          epoch_acc = sum([x['acc'] for x in outputs])/len(outputs)\r\n          print(\"\\nValidation accuracy : \", epoch_acc)\r\n          print(\"Validation loss : \", epoch_loss)\r\n          val_acc.append(epoch_acc)\r\n          self.validation_step_outputs.clear()  # free memory\r\n```\r\n\r\nI've looked around and couldn't find any solutions to as to why this is happening or how to fix it.",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/17146",
    "createdAt": "2023-03-20T05:43:55Z",
    "updatedAt": "2023-03-21T14:11:46Z",
    "closedAt": "2023-03-21T14:11:42Z",
    "isAnswered": true,
    "author": {
      "login": "bavShehata"
    },
    "answer": {
      "body": "If any one finds themselves here, the correct hook is `on_train_epoch_end` and not `on_training_epoch_end` as that was a mistake in the docs as answered [here on their forums](https://lightning.ai/forums/t/on-training-epoch-end-does-not-get-called/2436)",
      "author": {
        "login": "bavShehata"
      },
      "createdAt": "2023-03-21T14:11:42Z"
    }
  },
  {
    "title": "How to gather all validation_step_outputs at validation_epoch_end and run in rank_zero properly without deadlock?",
    "body": "Hi,\r\n\r\nI am trying to gather all the output and label pairs in the validation epoch end to run a simple validation process.\r\nFirst, all validation data will be separated into different devices (controlled by DDP).\r\nThe validation step is simple as follows:\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        # use valid_metric\r\n        feat, label = batch\r\n        output = self.model(feat)\r\n        del batch, feat\r\n        torch.cuda.empty_cache()\r\n\r\n\r\n        for i, metric in enumerate(self.metrics):\r\n            metric.update(output[:,i], label[:,i])\r\n\r\n        return {\"label\": label, \"output\": output}\r\n```\r\n\r\n\r\nTo calculate an accurate metrics, I need to gather all outputs to a single device and log on rank 0 only as follows:\r\n\r\n```\r\n def validation_epoch_end(self, validation_step_outputs):\r\n        def _valid_epoch_end(validation_step_outputs):\r\n            enable_Flag = False\r\n            # AP refers to average_precision_score in torchmetrics\r\n            all_labels = list(map(itemgetter('label'), validation_step_outputs))\r\n            all_labels = torch.cat(all_labels).cpu().detach().numpy()\r\n            all_outputs = list(map(itemgetter('output'), validation_step_outputs))\r\n            all_outputs = torch.cat(all_outputs).cpu().detach().numpy()\r\n\r\n            AP = []\r\n            for i in range(1, 17+1):\r\n                AP.append(np.nan_to_num(average_precision_score(all_labels\r\n                                                                [:, i], all_outputs[:,  i])))\r\n            mAP = np.mean(AP)\r\n            tmp = EVENT_DICTIONARY_V2\r\n            tmp = tmp.copy()\r\n            for k, v in tmp.items():\r\n                #dict[\"kick-off\"] = AP[0]\r\n                tmp[k] = AP[v]\r\n            \r\n            self.log('Valid/mAP', mAP, logger=True, prog_bar=True,\r\n                     rank_zero_only=True if self.args.strategy != 'dp' and enable_Flag else False)\r\n\r\n            label_cls = (list(EVENT_DICTIONARY_V2.keys()))\r\n            zip_iterator = zip(label_cls, AP)\r\n            AP_dictionary = dict(zip_iterator)\r\n            self.log('Valid/AP', AP_dictionary, logger=True, prog_bar=False,\r\n                     rank_zero_only=True if self.args.strategy != 'dp' and enable_Flag else False)\r\n\r\n        _valid_epoch_end(validation_step_outputs)\r\n```\r\nThe program works fine. However, the metric result is always different from DP. Therefore, I need to gather all data first.\r\nHowever, the program doesn't even pass the validation step as the deadlock. Any good idea to modify this program so that I can gather ",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/13041",
    "createdAt": "2022-05-12T03:50:18Z",
    "updatedAt": "2023-04-10T10:02:11Z",
    "closedAt": "2023-03-26T11:29:17Z",
    "isAnswered": true,
    "author": {
      "login": "allanchan339"
    },
    "answer": {
      "body": "The solution is as follows:\r\nInstead of using \r\n```\r\nif self.trainer.is_global_zero:\r\n    all_val_outs = self.all_gather(...)\r\n```\r\nThe function above will hang as card 0 is trying to communicate other cards\r\n\r\nYou should write code in this way\r\n```\r\nall_val_out = self.all_gather(...)\r\n\r\nif self.trainer.is_global_zero:\r\n    # merge output and process\r\n\r\nself.trainer.strategy.barrier() #to let other cards to wait\r\n\r\n```",
      "author": {
        "login": "allanchan339"
      },
      "createdAt": "2023-03-26T11:29:17Z"
    }
  },
  {
    "title": "Disabling find_unused_parameters",
    "body": "When trying to disable `find_unused_parameters` in the trainer by doing the following, \r\n\r\n`strategy=DDPStrategy(find_unused_parameters=False)`\r\n\r\nAm being thrown an import error for `from pytorch_lightning.strategies import DDPStrategy`\r\n\r\nError: \r\n\r\n`No module named 'pytorch_lightning.strategies'`",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11664",
    "createdAt": "2022-01-30T15:25:13Z",
    "updatedAt": "2023-03-20T07:51:23Z",
    "closedAt": "2023-03-20T07:51:23Z",
    "isAnswered": true,
    "author": {
      "login": "rahulvigneswaran"
    },
    "answer": {
      "body": "`pytorch_lightning.strategies` will be available in v1.6 release and is only available in master at the moment.\r\n\r\nFor now, you can use:\r\n```\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\ntrainer = pl.Trainer(\r\n    ...,\r\n    strategy=DDPPlugin(find_unused_parameters=False),\r\n)\r\n```\r\n\r\nSee the stable version of docs (not latest) here: https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html?highlight=find_unused_parameters#when-using-ddp-plugins-set-find-unused-parameters-false",
      "author": {
        "login": "akihironitta"
      },
      "createdAt": "2022-01-30T16:08:17Z"
    }
  },
  {
    "title": "Link arguments from Datamodule into init_args of lr_scheduler",
    "body": "Hey!\r\n\r\nI'm trying to use `LightningArgumentParser.link_arguments` to link an argument from the Datamodule to the init_args of the LR scheduler with:\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\nand I get the following error:\r\n```python\r\nValueError: No action for key \"lr_scheduler.init_args.num_training_steps\".\r\n```\r\n\r\nI was wondering if such thing is possible, or is linking to `init_args` is exclusive to the model and data classes?\r\n\r\n<details>\r\n  <summary><b>Code to reproduce</b></summary>\r\n\r\n  `trainer.py`\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch.nn\r\nfrom pytorch_lightning.utilities.cli import LR_SCHEDULER_REGISTRY\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom torch.optim import Optimizer\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n\r\n\r\n@LR_SCHEDULER_REGISTRY\r\nclass WarmupLR(LambdaLR):\r\n    def __init__(\r\n        self,\r\n        optimizer: Optimizer,\r\n        warmup_proportion: float,\r\n        num_training_steps: int,\r\n        last_epoch=-1,\r\n    ) -> None:\r\n        self.num_training_steps = num_training_steps\r\n        self.num_warmup_steps = round(num_training_steps * warmup_proportion)\r\n        super().__init__(optimizer, lr_lambda=self.lr_lambda, last_epoch=last_epoch)\r\n\r\n    def lr_lambda(self, current_step: int) -> float:\r\n        if current_step < self.num_warmup_steps:\r\n            return float(current_step) / float(max(1, self.num_warmup_steps))\r\n        return max(\r\n            0.0,\r\n            float(self.num_training_steps - current_step)\r\n            / float(max(1, self.num_training_steps - self.num_warmup_steps)),\r\n        )\r\n\r\n\r\nclass DataModule(pl.LightningDataModule):\r\n    def __init__(self, name):\r\n        super().__init__()\r\n        self.length = len(name)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(Dataset())\r\n\r\n    def get_num_training_steps(self) -> int:\r\n        return self.length\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n    def __init__(self, num_labels):\r\n        super().__init__()\r\n        self.num_labels = num_labels\r\n        self.nn = torch.nn.Linear(num_labels, num_labels)\r\n\r\n    def training_step(self, *args, **kwargs):\r\n        return\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli = MyLightningCLI(\r\n        model_class=LitModel,\r\n        datamodule_class=DataModule,\r\n    )\r\n```\r\n\r\n`config.yaml`\r\n```yaml\r\ndata:\r\n  name: blablabla\r\n\r\nmodel:\r\n  num_labels: 5\r\n\r\noptimizer:\r\n  class_path: torch.optim.Adam\r\n  init_args:\r\n    lr: 0.01\r\n\r\nlr_scheduler:\r\n  warmup_proportion: 0.1\r\n\r\ntrainer:\r\n  max_epochs: 2\r\n```\r\n</details>",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/11628",
    "createdAt": "2022-01-26T12:05:23Z",
    "updatedAt": "2023-11-20T16:27:00Z",
    "closedAt": "2023-11-20T16:27:00Z",
    "isAnswered": true,
    "author": {
      "login": "LourencoVazPato"
    },
    "answer": {
      "body": "You need to add an empty `configure_optimizers` method to your model as there's a bug that disallows leaving it unimplemented. It will be fixed with #11672 \r\n\r\nThe error appears because the `lr_scheduler` arguments have not been added yet. You can see the order here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/utilities/cli.py#L603-L609\r\n\r\nSo you have two options:\r\n\r\n1. Delay the linking until we've automatically added the lr scheduler classes\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def link_optimizers_and_lr_schedulers(parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n        LightningCLI.link_optimizers_and_lr_schedulers(parser)\r\n```\r\n\r\n2. Manually add the classes yourself at this hook:\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Manually add the lr scheduler classes\r\n        parser.add_lr_scheduler_args(LR_SCHEDULER_REGISTRY.classes)\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\n\r\nAlso, the config for the scheduler should be:\r\n\r\n```yaml\r\nlr_scheduler:\r\n  class_path: __main__.WarmupLR\r\n  init_args:\r\n    warmup_proportion: 0.01\r\n    num_training_steps: 1\r\n```",
      "author": {
        "login": "carmocca"
      },
      "createdAt": "2022-01-31T16:30:14Z"
    }
  },
  {
    "title": "How to save/load only part of the weights in the model?",
    "body": "For example, part of my model's parameters are frozen, no need to train, no need to save",
    "url": "https://github.com/Lightning-AI/pytorch-lightning/discussions/9961",
    "createdAt": "2021-10-16T09:18:21Z",
    "updatedAt": "2025-08-19T11:40:19Z",
    "closedAt": "2025-08-19T11:40:19Z",
    "isAnswered": true,
    "author": {
      "login": "Achazwl"
    },
    "answer": {
      "body": "This might work:\r\n```py\r\ndef on_save_checkpoint(checkpoint):\r\n    # pop the backbone here using custom logic\r\n    del checkpoint['state_dict'][backbone_keys]\r\n\r\nLitModel.load_from_checkpoint(ckpt_path, strict=False)\r\n```",
      "author": {
        "login": "rohitgr7"
      },
      "createdAt": "2021-10-16T21:16:18Z"
    }
  }
]